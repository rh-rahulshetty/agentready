This file is a merged representation of the entire codebase, combined into a single document by Repomix.

# File Summary

## Purpose
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.

## File Format
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files (if enabled)
5. Multiple file entries, each consisting of:
  a. A header with the file path (## File: path/to/file)
  b. The full contents of the file in a code block

## Usage Guidelines
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.

## Notes
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Files are sorted by Git change count (files with more changes are at the bottom)

# Directory Structure
```
.claude/
  commands/
    speckit.analyze.md
    speckit.checklist.md
    speckit.clarify.md
    speckit.constitution.md
    speckit.implement.md
    speckit.plan.md
    speckit.specify.md
    speckit.tasks.md
    speckit.taskstoissues.md
.github/
  ISSUE_TEMPLATE/
    bug_report.md
    feature_request.md
  workflows/
    agentready-assessment.yml
    claude-code-action.yml
    docs-lint.yml
    release.yml
    security.yml
    tests.yml
    update-docs.yml
  CODEOWNERS
  dependabot.yml
  PULL_REQUEST_TEMPLATE.md
.specify/
  memory/
    constitution.md
  scripts/
    bash/
      check-prerequisites.sh
      common.sh
      create-new-feature.sh
      setup-plan.sh
      update-agent-context.sh
  templates/
    agent-file-template.md
    checklist-template.md
    plan-template.md
    spec-template.md
    tasks-template.md
coldstart-prompts/
  01-create-automated-demo.md
  02-fix-critical-security-logic-bugs-from-code-review.md
  03-bootstrap-agentready-repository-on-github.md
  04-report-header-with-repository-metadata.md
  05-improve-html-report-design-font-size-color-scheme.md
  06-report-schema-versioning.md
  07-research-report-generatorupdater-utility.md
  08-repomix-integration.md
  09-agentready-repository-agent.md
  10-customizable-html-report-themes.md
  11-fix-code-quality-issues-from-code-review.md
  12-improve-test-coverage-and-edge-case-handling.md
  13-add-security-quality-improvements-from-code-review.md
  14-align-subcommand-automated-remediation.md
  15-interactive-dashboard-with-automated-remediation.md
  16-github-app-integration-badge-status-checks.md
  17-add-bootstrap-quickstart-to-readme.md
  18-setup-release-pipeline.md
  19-github-pages-linter-integration.md
  README.md
docs/
  _layouts/
    default.html
    home.html
    page.html
  assets/
    css/
      style.css
  _config.yml
  api-reference.md
  attributes.md
  DEPLOYMENT.md
  developer-guide.md
  examples.md
  Gemfile
  index.md
  README.md
  RELEASE_PROCESS.md
  roadmaps.md
  SETUP_SUMMARY.md
  user-guide.md
examples/
  self-assessment/
    assessment-20251121-035845.json
    assessment-20251121.json
    README.md
    report-20251121-035845.html
    report-20251121-035845.md
    report-20251121.html
    report-20251121.md
scripts/
  backlog_to_issues.py
specs/
  001-agentready-scorer/
    .remediation/
      A1-proportional-scoring.md
      A2-tier-weight-distribution.md
      README.md
      U1-research-update-mechanism.md
      U2-research-validation.md
      U3-weight-validation.md
    checklists/
      requirements.md
    contracts/
      assessment-schema.json
      report-html-schema.md
      report-markdown-schema.md
      research-report-schema.md
    data-model.md
    plan.md
    quickstart.md
    research.md
    spec.md
    tasks.md
src/
  agentready/
    assessors/
      __init__.py
      base.py
      code_quality.py
      documentation.py
      structure.py
      stub_assessors.py
      testing.py
    cli/
      __init__.py
      bootstrap.py
      main.py
    data/
      agent-ready-codebase-attributes.md
      default-weights.yaml
    models/
      __init__.py
      assessment.py
      attribute.py
      citation.py
      config.py
      finding.py
      repository.py
    reporters/
      __init__.py
      base.py
      html.py
      markdown.py
    services/
      __init__.py
      bootstrap.py
      language_detector.py
      research_loader.py
      scanner.py
      scorer.py
    templates/
      bootstrap/
        issue_templates/
          bug_report.md.j2
          feature_request.md.j2
        workflows/
          agentready-assessment.yml.j2
          security.yml.j2
          tests-python.yml.j2
        CODE_OF_CONDUCT.md.j2
        CODEOWNERS.j2
        CONTRIBUTING.md.j2
        dependabot.yml.j2
        precommit-python.yaml.j2
        PULL_REQUEST_TEMPLATE.md.j2
      report.html.j2
    __init__.py
tests/
  integration/
    __init__.py
    test_scan_workflow.py
  unit/
    __init__.py
    test_assessors_structure.py
    test_models.py
    test_security.py
  __init__.py
.agentready-config.example.yaml
.gitignore
.markdown-link-check.json
.pre-commit-config.yaml
.releaserc.json
.repomixignore
agent-ready-codebase-attributes.md
BACKLOG.md
CHANGELOG.md
CLAUDE.md
CODE_OF_CONDUCT.md
CONTRIBUTING.md
GITHUB_ISSUES.md
LICENSE
pyproject.toml
README.md
repomix-output.xml
repomix.config.json
repos.txt
```

# Files

## File: .claude/commands/speckit.analyze.md
````markdown
---
description: Perform a non-destructive cross-artifact consistency and quality analysis across spec.md, plan.md, and tasks.md after task generation.
---

## User Input

```text
$ARGUMENTS
```

You **MUST** consider the user input before proceeding (if not empty).

## Goal

Identify inconsistencies, duplications, ambiguities, and underspecified items across the three core artifacts (`spec.md`, `plan.md`, `tasks.md`) before implementation. This command MUST run only after `/speckit.tasks` has successfully produced a complete `tasks.md`.

## Operating Constraints

**STRICTLY READ-ONLY**: Do **not** modify any files. Output a structured analysis report. Offer an optional remediation plan (user must explicitly approve before any follow-up editing commands would be invoked manually).

**Constitution Authority**: The project constitution (`.specify/memory/constitution.md`) is **non-negotiable** within this analysis scope. Constitution conflicts are automatically CRITICAL and require adjustment of the spec, plan, or tasks‚Äînot dilution, reinterpretation, or silent ignoring of the principle. If a principle itself needs to change, that must occur in a separate, explicit constitution update outside `/speckit.analyze`.

## Execution Steps

### 1. Initialize Analysis Context

Run `.specify/scripts/bash/check-prerequisites.sh --json --require-tasks --include-tasks` once from repo root and parse JSON for FEATURE_DIR and AVAILABLE_DOCS. Derive absolute paths:

- SPEC = FEATURE_DIR/spec.md
- PLAN = FEATURE_DIR/plan.md
- TASKS = FEATURE_DIR/tasks.md

Abort with an error message if any required file is missing (instruct the user to run missing prerequisite command).
For single quotes in args like "I'm Groot", use escape syntax: e.g 'I'\''m Groot' (or double-quote if possible: "I'm Groot").

### 2. Load Artifacts (Progressive Disclosure)

Load only the minimal necessary context from each artifact:

**From spec.md:**

- Overview/Context
- Functional Requirements
- Non-Functional Requirements
- User Stories
- Edge Cases (if present)

**From plan.md:**

- Architecture/stack choices
- Data Model references
- Phases
- Technical constraints

**From tasks.md:**

- Task IDs
- Descriptions
- Phase grouping
- Parallel markers [P]
- Referenced file paths

**From constitution:**

- Load `.specify/memory/constitution.md` for principle validation

### 3. Build Semantic Models

Create internal representations (do not include raw artifacts in output):

- **Requirements inventory**: Each functional + non-functional requirement with a stable key (derive slug based on imperative phrase; e.g., "User can upload file" ‚Üí `user-can-upload-file`)
- **User story/action inventory**: Discrete user actions with acceptance criteria
- **Task coverage mapping**: Map each task to one or more requirements or stories (inference by keyword / explicit reference patterns like IDs or key phrases)
- **Constitution rule set**: Extract principle names and MUST/SHOULD normative statements

### 4. Detection Passes (Token-Efficient Analysis)

Focus on high-signal findings. Limit to 50 findings total; aggregate remainder in overflow summary.

#### A. Duplication Detection

- Identify near-duplicate requirements
- Mark lower-quality phrasing for consolidation

#### B. Ambiguity Detection

- Flag vague adjectives (fast, scalable, secure, intuitive, robust) lacking measurable criteria
- Flag unresolved placeholders (TODO, TKTK, ???, `<placeholder>`, etc.)

#### C. Underspecification

- Requirements with verbs but missing object or measurable outcome
- User stories missing acceptance criteria alignment
- Tasks referencing files or components not defined in spec/plan

#### D. Constitution Alignment

- Any requirement or plan element conflicting with a MUST principle
- Missing mandated sections or quality gates from constitution

#### E. Coverage Gaps

- Requirements with zero associated tasks
- Tasks with no mapped requirement/story
- Non-functional requirements not reflected in tasks (e.g., performance, security)

#### F. Inconsistency

- Terminology drift (same concept named differently across files)
- Data entities referenced in plan but absent in spec (or vice versa)
- Task ordering contradictions (e.g., integration tasks before foundational setup tasks without dependency note)
- Conflicting requirements (e.g., one requires Next.js while other specifies Vue)

### 5. Severity Assignment

Use this heuristic to prioritize findings:

- **CRITICAL**: Violates constitution MUST, missing core spec artifact, or requirement with zero coverage that blocks baseline functionality
- **HIGH**: Duplicate or conflicting requirement, ambiguous security/performance attribute, untestable acceptance criterion
- **MEDIUM**: Terminology drift, missing non-functional task coverage, underspecified edge case
- **LOW**: Style/wording improvements, minor redundancy not affecting execution order

### 6. Produce Compact Analysis Report

Output a Markdown report (no file writes) with the following structure:

## Specification Analysis Report

| ID | Category | Severity | Location(s) | Summary | Recommendation |
|----|----------|----------|-------------|---------|----------------|
| A1 | Duplication | HIGH | spec.md:L120-134 | Two similar requirements ... | Merge phrasing; keep clearer version |

(Add one row per finding; generate stable IDs prefixed by category initial.)

**Coverage Summary Table:**

| Requirement Key | Has Task? | Task IDs | Notes |
|-----------------|-----------|----------|-------|

**Constitution Alignment Issues:** (if any)

**Unmapped Tasks:** (if any)

**Metrics:**

- Total Requirements
- Total Tasks
- Coverage % (requirements with >=1 task)
- Ambiguity Count
- Duplication Count
- Critical Issues Count

### 7. Provide Next Actions

At end of report, output a concise Next Actions block:

- If CRITICAL issues exist: Recommend resolving before `/speckit.implement`
- If only LOW/MEDIUM: User may proceed, but provide improvement suggestions
- Provide explicit command suggestions: e.g., "Run /speckit.specify with refinement", "Run /speckit.plan to adjust architecture", "Manually edit tasks.md to add coverage for 'performance-metrics'"

### 8. Offer Remediation

Ask the user: "Would you like me to suggest concrete remediation edits for the top N issues?" (Do NOT apply them automatically.)

## Operating Principles

### Context Efficiency

- **Minimal high-signal tokens**: Focus on actionable findings, not exhaustive documentation
- **Progressive disclosure**: Load artifacts incrementally; don't dump all content into analysis
- **Token-efficient output**: Limit findings table to 50 rows; summarize overflow
- **Deterministic results**: Rerunning without changes should produce consistent IDs and counts

### Analysis Guidelines

- **NEVER modify files** (this is read-only analysis)
- **NEVER hallucinate missing sections** (if absent, report them accurately)
- **Prioritize constitution violations** (these are always CRITICAL)
- **Use examples over exhaustive rules** (cite specific instances, not generic patterns)
- **Report zero issues gracefully** (emit success report with coverage statistics)

## Context

$ARGUMENTS
````

## File: .claude/commands/speckit.checklist.md
````markdown
---
description: Generate a custom checklist for the current feature based on user requirements.
---

## Checklist Purpose: "Unit Tests for English"

**CRITICAL CONCEPT**: Checklists are **UNIT TESTS FOR REQUIREMENTS WRITING** - they validate the quality, clarity, and completeness of requirements in a given domain.

**NOT for verification/testing**:

- ‚ùå NOT "Verify the button clicks correctly"
- ‚ùå NOT "Test error handling works"
- ‚ùå NOT "Confirm the API returns 200"
- ‚ùå NOT checking if code/implementation matches the spec

**FOR requirements quality validation**:

- ‚úÖ "Are visual hierarchy requirements defined for all card types?" (completeness)
- ‚úÖ "Is 'prominent display' quantified with specific sizing/positioning?" (clarity)
- ‚úÖ "Are hover state requirements consistent across all interactive elements?" (consistency)
- ‚úÖ "Are accessibility requirements defined for keyboard navigation?" (coverage)
- ‚úÖ "Does the spec define what happens when logo image fails to load?" (edge cases)

**Metaphor**: If your spec is code written in English, the checklist is its unit test suite. You're testing whether the requirements are well-written, complete, unambiguous, and ready for implementation - NOT whether the implementation works.

## User Input

```text
$ARGUMENTS
```

You **MUST** consider the user input before proceeding (if not empty).

## Execution Steps

1. **Setup**: Run `.specify/scripts/bash/check-prerequisites.sh --json` from repo root and parse JSON for FEATURE_DIR and AVAILABLE_DOCS list.
   - All file paths must be absolute.
   - For single quotes in args like "I'm Groot", use escape syntax: e.g 'I'\''m Groot' (or double-quote if possible: "I'm Groot").

2. **Clarify intent (dynamic)**: Derive up to THREE initial contextual clarifying questions (no pre-baked catalog). They MUST:
   - Be generated from the user's phrasing + extracted signals from spec/plan/tasks
   - Only ask about information that materially changes checklist content
   - Be skipped individually if already unambiguous in `$ARGUMENTS`
   - Prefer precision over breadth

   Generation algorithm:
   1. Extract signals: feature domain keywords (e.g., auth, latency, UX, API), risk indicators ("critical", "must", "compliance"), stakeholder hints ("QA", "review", "security team"), and explicit deliverables ("a11y", "rollback", "contracts").
   2. Cluster signals into candidate focus areas (max 4) ranked by relevance.
   3. Identify probable audience & timing (author, reviewer, QA, release) if not explicit.
   4. Detect missing dimensions: scope breadth, depth/rigor, risk emphasis, exclusion boundaries, measurable acceptance criteria.
   5. Formulate questions chosen from these archetypes:
      - Scope refinement (e.g., "Should this include integration touchpoints with X and Y or stay limited to local module correctness?")
      - Risk prioritization (e.g., "Which of these potential risk areas should receive mandatory gating checks?")
      - Depth calibration (e.g., "Is this a lightweight pre-commit sanity list or a formal release gate?")
      - Audience framing (e.g., "Will this be used by the author only or peers during PR review?")
      - Boundary exclusion (e.g., "Should we explicitly exclude performance tuning items this round?")
      - Scenario class gap (e.g., "No recovery flows detected‚Äîare rollback / partial failure paths in scope?")

   Question formatting rules:
   - If presenting options, generate a compact table with columns: Option | Candidate | Why It Matters
   - Limit to A‚ÄìE options maximum; omit table if a free-form answer is clearer
   - Never ask the user to restate what they already said
   - Avoid speculative categories (no hallucination). If uncertain, ask explicitly: "Confirm whether X belongs in scope."

   Defaults when interaction impossible:
   - Depth: Standard
   - Audience: Reviewer (PR) if code-related; Author otherwise
   - Focus: Top 2 relevance clusters

   Output the questions (label Q1/Q2/Q3). After answers: if ‚â•2 scenario classes (Alternate / Exception / Recovery / Non-Functional domain) remain unclear, you MAY ask up to TWO more targeted follow‚Äëups (Q4/Q5) with a one-line justification each (e.g., "Unresolved recovery path risk"). Do not exceed five total questions. Skip escalation if user explicitly declines more.

3. **Understand user request**: Combine `$ARGUMENTS` + clarifying answers:
   - Derive checklist theme (e.g., security, review, deploy, ux)
   - Consolidate explicit must-have items mentioned by user
   - Map focus selections to category scaffolding
   - Infer any missing context from spec/plan/tasks (do NOT hallucinate)

4. **Load feature context**: Read from FEATURE_DIR:
   - spec.md: Feature requirements and scope
   - plan.md (if exists): Technical details, dependencies
   - tasks.md (if exists): Implementation tasks

   **Context Loading Strategy**:
   - Load only necessary portions relevant to active focus areas (avoid full-file dumping)
   - Prefer summarizing long sections into concise scenario/requirement bullets
   - Use progressive disclosure: add follow-on retrieval only if gaps detected
   - If source docs are large, generate interim summary items instead of embedding raw text

5. **Generate checklist** - Create "Unit Tests for Requirements":
   - Create `FEATURE_DIR/checklists/` directory if it doesn't exist
   - Generate unique checklist filename:
     - Use short, descriptive name based on domain (e.g., `ux.md`, `api.md`, `security.md`)
     - Format: `[domain].md`
     - If file exists, append to existing file
   - Number items sequentially starting from CHK001
   - Each `/speckit.checklist` run creates a NEW file (never overwrites existing checklists)

   **CORE PRINCIPLE - Test the Requirements, Not the Implementation**:
   Every checklist item MUST evaluate the REQUIREMENTS THEMSELVES for:
   - **Completeness**: Are all necessary requirements present?
   - **Clarity**: Are requirements unambiguous and specific?
   - **Consistency**: Do requirements align with each other?
   - **Measurability**: Can requirements be objectively verified?
   - **Coverage**: Are all scenarios/edge cases addressed?

   **Category Structure** - Group items by requirement quality dimensions:
   - **Requirement Completeness** (Are all necessary requirements documented?)
   - **Requirement Clarity** (Are requirements specific and unambiguous?)
   - **Requirement Consistency** (Do requirements align without conflicts?)
   - **Acceptance Criteria Quality** (Are success criteria measurable?)
   - **Scenario Coverage** (Are all flows/cases addressed?)
   - **Edge Case Coverage** (Are boundary conditions defined?)
   - **Non-Functional Requirements** (Performance, Security, Accessibility, etc. - are they specified?)
   - **Dependencies & Assumptions** (Are they documented and validated?)
   - **Ambiguities & Conflicts** (What needs clarification?)

   **HOW TO WRITE CHECKLIST ITEMS - "Unit Tests for English"**:

   ‚ùå **WRONG** (Testing implementation):
   - "Verify landing page displays 3 episode cards"
   - "Test hover states work on desktop"
   - "Confirm logo click navigates home"

   ‚úÖ **CORRECT** (Testing requirements quality):
   - "Are the exact number and layout of featured episodes specified?" [Completeness]
   - "Is 'prominent display' quantified with specific sizing/positioning?" [Clarity]
   - "Are hover state requirements consistent across all interactive elements?" [Consistency]
   - "Are keyboard navigation requirements defined for all interactive UI?" [Coverage]
   - "Is the fallback behavior specified when logo image fails to load?" [Edge Cases]
   - "Are loading states defined for asynchronous episode data?" [Completeness]
   - "Does the spec define visual hierarchy for competing UI elements?" [Clarity]

   **ITEM STRUCTURE**:
   Each item should follow this pattern:
   - Question format asking about requirement quality
   - Focus on what's WRITTEN (or not written) in the spec/plan
   - Include quality dimension in brackets [Completeness/Clarity/Consistency/etc.]
   - Reference spec section `[Spec ¬ßX.Y]` when checking existing requirements
   - Use `[Gap]` marker when checking for missing requirements

   **EXAMPLES BY QUALITY DIMENSION**:

   Completeness:
   - "Are error handling requirements defined for all API failure modes? [Gap]"
   - "Are accessibility requirements specified for all interactive elements? [Completeness]"
   - "Are mobile breakpoint requirements defined for responsive layouts? [Gap]"

   Clarity:
   - "Is 'fast loading' quantified with specific timing thresholds? [Clarity, Spec ¬ßNFR-2]"
   - "Are 'related episodes' selection criteria explicitly defined? [Clarity, Spec ¬ßFR-5]"
   - "Is 'prominent' defined with measurable visual properties? [Ambiguity, Spec ¬ßFR-4]"

   Consistency:
   - "Do navigation requirements align across all pages? [Consistency, Spec ¬ßFR-10]"
   - "Are card component requirements consistent between landing and detail pages? [Consistency]"

   Coverage:
   - "Are requirements defined for zero-state scenarios (no episodes)? [Coverage, Edge Case]"
   - "Are concurrent user interaction scenarios addressed? [Coverage, Gap]"
   - "Are requirements specified for partial data loading failures? [Coverage, Exception Flow]"

   Measurability:
   - "Are visual hierarchy requirements measurable/testable? [Acceptance Criteria, Spec ¬ßFR-1]"
   - "Can 'balanced visual weight' be objectively verified? [Measurability, Spec ¬ßFR-2]"

   **Scenario Classification & Coverage** (Requirements Quality Focus):
   - Check if requirements exist for: Primary, Alternate, Exception/Error, Recovery, Non-Functional scenarios
   - For each scenario class, ask: "Are [scenario type] requirements complete, clear, and consistent?"
   - If scenario class missing: "Are [scenario type] requirements intentionally excluded or missing? [Gap]"
   - Include resilience/rollback when state mutation occurs: "Are rollback requirements defined for migration failures? [Gap]"

   **Traceability Requirements**:
   - MINIMUM: ‚â•80% of items MUST include at least one traceability reference
   - Each item should reference: spec section `[Spec ¬ßX.Y]`, or use markers: `[Gap]`, `[Ambiguity]`, `[Conflict]`, `[Assumption]`
   - If no ID system exists: "Is a requirement & acceptance criteria ID scheme established? [Traceability]"

   **Surface & Resolve Issues** (Requirements Quality Problems):
   Ask questions about the requirements themselves:
   - Ambiguities: "Is the term 'fast' quantified with specific metrics? [Ambiguity, Spec ¬ßNFR-1]"
   - Conflicts: "Do navigation requirements conflict between ¬ßFR-10 and ¬ßFR-10a? [Conflict]"
   - Assumptions: "Is the assumption of 'always available podcast API' validated? [Assumption]"
   - Dependencies: "Are external podcast API requirements documented? [Dependency, Gap]"
   - Missing definitions: "Is 'visual hierarchy' defined with measurable criteria? [Gap]"

   **Content Consolidation**:
   - Soft cap: If raw candidate items > 40, prioritize by risk/impact
   - Merge near-duplicates checking the same requirement aspect
   - If >5 low-impact edge cases, create one item: "Are edge cases X, Y, Z addressed in requirements? [Coverage]"

   **üö´ ABSOLUTELY PROHIBITED** - These make it an implementation test, not a requirements test:
   - ‚ùå Any item starting with "Verify", "Test", "Confirm", "Check" + implementation behavior
   - ‚ùå References to code execution, user actions, system behavior
   - ‚ùå "Displays correctly", "works properly", "functions as expected"
   - ‚ùå "Click", "navigate", "render", "load", "execute"
   - ‚ùå Test cases, test plans, QA procedures
   - ‚ùå Implementation details (frameworks, APIs, algorithms)

   **‚úÖ REQUIRED PATTERNS** - These test requirements quality:
   - ‚úÖ "Are [requirement type] defined/specified/documented for [scenario]?"
   - ‚úÖ "Is [vague term] quantified/clarified with specific criteria?"
   - ‚úÖ "Are requirements consistent between [section A] and [section B]?"
   - ‚úÖ "Can [requirement] be objectively measured/verified?"
   - ‚úÖ "Are [edge cases/scenarios] addressed in requirements?"
   - ‚úÖ "Does the spec define [missing aspect]?"

6. **Structure Reference**: Generate the checklist following the canonical template in `.specify/templates/checklist-template.md` for title, meta section, category headings, and ID formatting. If template is unavailable, use: H1 title, purpose/created meta lines, `##` category sections containing `- [ ] CHK### <requirement item>` lines with globally incrementing IDs starting at CHK001.

7. **Report**: Output full path to created checklist, item count, and remind user that each run creates a new file. Summarize:
   - Focus areas selected
   - Depth level
   - Actor/timing
   - Any explicit user-specified must-have items incorporated

**Important**: Each `/speckit.checklist` command invocation creates a checklist file using short, descriptive names unless file already exists. This allows:

- Multiple checklists of different types (e.g., `ux.md`, `test.md`, `security.md`)
- Simple, memorable filenames that indicate checklist purpose
- Easy identification and navigation in the `checklists/` folder

To avoid clutter, use descriptive types and clean up obsolete checklists when done.

## Example Checklist Types & Sample Items

**UX Requirements Quality:** `ux.md`

Sample items (testing the requirements, NOT the implementation):

- "Are visual hierarchy requirements defined with measurable criteria? [Clarity, Spec ¬ßFR-1]"
- "Is the number and positioning of UI elements explicitly specified? [Completeness, Spec ¬ßFR-1]"
- "Are interaction state requirements (hover, focus, active) consistently defined? [Consistency]"
- "Are accessibility requirements specified for all interactive elements? [Coverage, Gap]"
- "Is fallback behavior defined when images fail to load? [Edge Case, Gap]"
- "Can 'prominent display' be objectively measured? [Measurability, Spec ¬ßFR-4]"

**API Requirements Quality:** `api.md`

Sample items:

- "Are error response formats specified for all failure scenarios? [Completeness]"
- "Are rate limiting requirements quantified with specific thresholds? [Clarity]"
- "Are authentication requirements consistent across all endpoints? [Consistency]"
- "Are retry/timeout requirements defined for external dependencies? [Coverage, Gap]"
- "Is versioning strategy documented in requirements? [Gap]"

**Performance Requirements Quality:** `performance.md`

Sample items:

- "Are performance requirements quantified with specific metrics? [Clarity]"
- "Are performance targets defined for all critical user journeys? [Coverage]"
- "Are performance requirements under different load conditions specified? [Completeness]"
- "Can performance requirements be objectively measured? [Measurability]"
- "Are degradation requirements defined for high-load scenarios? [Edge Case, Gap]"

**Security Requirements Quality:** `security.md`

Sample items:

- "Are authentication requirements specified for all protected resources? [Coverage]"
- "Are data protection requirements defined for sensitive information? [Completeness]"
- "Is the threat model documented and requirements aligned to it? [Traceability]"
- "Are security requirements consistent with compliance obligations? [Consistency]"
- "Are security failure/breach response requirements defined? [Gap, Exception Flow]"

## Anti-Examples: What NOT To Do

**‚ùå WRONG - These test implementation, not requirements:**

```markdown
- [ ] CHK001 - Verify landing page displays 3 episode cards [Spec ¬ßFR-001]
- [ ] CHK002 - Test hover states work correctly on desktop [Spec ¬ßFR-003]
- [ ] CHK003 - Confirm logo click navigates to home page [Spec ¬ßFR-010]
- [ ] CHK004 - Check that related episodes section shows 3-5 items [Spec ¬ßFR-005]
```

**‚úÖ CORRECT - These test requirements quality:**

```markdown
- [ ] CHK001 - Are the number and layout of featured episodes explicitly specified? [Completeness, Spec ¬ßFR-001]
- [ ] CHK002 - Are hover state requirements consistently defined for all interactive elements? [Consistency, Spec ¬ßFR-003]
- [ ] CHK003 - Are navigation requirements clear for all clickable brand elements? [Clarity, Spec ¬ßFR-010]
- [ ] CHK004 - Is the selection criteria for related episodes documented? [Gap, Spec ¬ßFR-005]
- [ ] CHK005 - Are loading state requirements defined for asynchronous episode data? [Gap]
- [ ] CHK006 - Can "visual hierarchy" requirements be objectively measured? [Measurability, Spec ¬ßFR-001]
```

**Key Differences:**

- Wrong: Tests if the system works correctly
- Correct: Tests if the requirements are written correctly
- Wrong: Verification of behavior
- Correct: Validation of requirement quality
- Wrong: "Does it do X?"
- Correct: "Is X clearly specified?"
````

## File: .claude/commands/speckit.clarify.md
````markdown
---
description: Identify underspecified areas in the current feature spec by asking up to 5 highly targeted clarification questions and encoding answers back into the spec.
handoffs: 
  - label: Build Technical Plan
    agent: speckit.plan
    prompt: Create a plan for the spec. I am building with...
---

## User Input

```text
$ARGUMENTS
```

You **MUST** consider the user input before proceeding (if not empty).

## Outline

Goal: Detect and reduce ambiguity or missing decision points in the active feature specification and record the clarifications directly in the spec file.

Note: This clarification workflow is expected to run (and be completed) BEFORE invoking `/speckit.plan`. If the user explicitly states they are skipping clarification (e.g., exploratory spike), you may proceed, but must warn that downstream rework risk increases.

Execution steps:

1. Run `.specify/scripts/bash/check-prerequisites.sh --json --paths-only` from repo root **once** (combined `--json --paths-only` mode / `-Json -PathsOnly`). Parse minimal JSON payload fields:
   - `FEATURE_DIR`
   - `FEATURE_SPEC`
   - (Optionally capture `IMPL_PLAN`, `TASKS` for future chained flows.)
   - If JSON parsing fails, abort and instruct user to re-run `/speckit.specify` or verify feature branch environment.
   - For single quotes in args like "I'm Groot", use escape syntax: e.g 'I'\''m Groot' (or double-quote if possible: "I'm Groot").

2. Load the current spec file. Perform a structured ambiguity & coverage scan using this taxonomy. For each category, mark status: Clear / Partial / Missing. Produce an internal coverage map used for prioritization (do not output raw map unless no questions will be asked).

   Functional Scope & Behavior:
   - Core user goals & success criteria
   - Explicit out-of-scope declarations
   - User roles / personas differentiation

   Domain & Data Model:
   - Entities, attributes, relationships
   - Identity & uniqueness rules
   - Lifecycle/state transitions
   - Data volume / scale assumptions

   Interaction & UX Flow:
   - Critical user journeys / sequences
   - Error/empty/loading states
   - Accessibility or localization notes

   Non-Functional Quality Attributes:
   - Performance (latency, throughput targets)
   - Scalability (horizontal/vertical, limits)
   - Reliability & availability (uptime, recovery expectations)
   - Observability (logging, metrics, tracing signals)
   - Security & privacy (authN/Z, data protection, threat assumptions)
   - Compliance / regulatory constraints (if any)

   Integration & External Dependencies:
   - External services/APIs and failure modes
   - Data import/export formats
   - Protocol/versioning assumptions

   Edge Cases & Failure Handling:
   - Negative scenarios
   - Rate limiting / throttling
   - Conflict resolution (e.g., concurrent edits)

   Constraints & Tradeoffs:
   - Technical constraints (language, storage, hosting)
   - Explicit tradeoffs or rejected alternatives

   Terminology & Consistency:
   - Canonical glossary terms
   - Avoided synonyms / deprecated terms

   Completion Signals:
   - Acceptance criteria testability
   - Measurable Definition of Done style indicators

   Misc / Placeholders:
   - TODO markers / unresolved decisions
   - Ambiguous adjectives ("robust", "intuitive") lacking quantification

   For each category with Partial or Missing status, add a candidate question opportunity unless:
   - Clarification would not materially change implementation or validation strategy
   - Information is better deferred to planning phase (note internally)

3. Generate (internally) a prioritized queue of candidate clarification questions (maximum 5). Do NOT output them all at once. Apply these constraints:
    - Maximum of 10 total questions across the whole session.
    - Each question must be answerable with EITHER:
       - A short multiple‚Äëchoice selection (2‚Äì5 distinct, mutually exclusive options), OR
       - A one-word / short‚Äëphrase answer (explicitly constrain: "Answer in <=5 words").
    - Only include questions whose answers materially impact architecture, data modeling, task decomposition, test design, UX behavior, operational readiness, or compliance validation.
    - Ensure category coverage balance: attempt to cover the highest impact unresolved categories first; avoid asking two low-impact questions when a single high-impact area (e.g., security posture) is unresolved.
    - Exclude questions already answered, trivial stylistic preferences, or plan-level execution details (unless blocking correctness).
    - Favor clarifications that reduce downstream rework risk or prevent misaligned acceptance tests.
    - If more than 5 categories remain unresolved, select the top 5 by (Impact * Uncertainty) heuristic.

4. Sequential questioning loop (interactive):
    - Present EXACTLY ONE question at a time.
    - For multiple‚Äëchoice questions:
       - **Analyze all options** and determine the **most suitable option** based on:
          - Best practices for the project type
          - Common patterns in similar implementations
          - Risk reduction (security, performance, maintainability)
          - Alignment with any explicit project goals or constraints visible in the spec
       - Present your **recommended option prominently** at the top with clear reasoning (1-2 sentences explaining why this is the best choice).
       - Format as: `**Recommended:** Option [X] - <reasoning>`
       - Then render all options as a Markdown table:

       | Option | Description |
       |--------|-------------|
       | A | <Option A description> |
       | B | <Option B description> |
       | C | <Option C description> (add D/E as needed up to 5) |
       | Short | Provide a different short answer (<=5 words) (Include only if free-form alternative is appropriate) |

       - After the table, add: `You can reply with the option letter (e.g., "A"), accept the recommendation by saying "yes" or "recommended", or provide your own short answer.`
    - For short‚Äëanswer style (no meaningful discrete options):
       - Provide your **suggested answer** based on best practices and context.
       - Format as: `**Suggested:** <your proposed answer> - <brief reasoning>`
       - Then output: `Format: Short answer (<=5 words). You can accept the suggestion by saying "yes" or "suggested", or provide your own answer.`
    - After the user answers:
       - If the user replies with "yes", "recommended", or "suggested", use your previously stated recommendation/suggestion as the answer.
       - Otherwise, validate the answer maps to one option or fits the <=5 word constraint.
       - If ambiguous, ask for a quick disambiguation (count still belongs to same question; do not advance).
       - Once satisfactory, record it in working memory (do not yet write to disk) and move to the next queued question.
    - Stop asking further questions when:
       - All critical ambiguities resolved early (remaining queued items become unnecessary), OR
       - User signals completion ("done", "good", "no more"), OR
       - You reach 5 asked questions.
    - Never reveal future queued questions in advance.
    - If no valid questions exist at start, immediately report no critical ambiguities.

5. Integration after EACH accepted answer (incremental update approach):
    - Maintain in-memory representation of the spec (loaded once at start) plus the raw file contents.
    - For the first integrated answer in this session:
       - Ensure a `## Clarifications` section exists (create it just after the highest-level contextual/overview section per the spec template if missing).
       - Under it, create (if not present) a `### Session YYYY-MM-DD` subheading for today.
    - Append a bullet line immediately after acceptance: `- Q: <question> ‚Üí A: <final answer>`.
    - Then immediately apply the clarification to the most appropriate section(s):
       - Functional ambiguity ‚Üí Update or add a bullet in Functional Requirements.
       - User interaction / actor distinction ‚Üí Update User Stories or Actors subsection (if present) with clarified role, constraint, or scenario.
       - Data shape / entities ‚Üí Update Data Model (add fields, types, relationships) preserving ordering; note added constraints succinctly.
       - Non-functional constraint ‚Üí Add/modify measurable criteria in Non-Functional / Quality Attributes section (convert vague adjective to metric or explicit target).
       - Edge case / negative flow ‚Üí Add a new bullet under Edge Cases / Error Handling (or create such subsection if template provides placeholder for it).
       - Terminology conflict ‚Üí Normalize term across spec; retain original only if necessary by adding `(formerly referred to as "X")` once.
    - If the clarification invalidates an earlier ambiguous statement, replace that statement instead of duplicating; leave no obsolete contradictory text.
    - Save the spec file AFTER each integration to minimize risk of context loss (atomic overwrite).
    - Preserve formatting: do not reorder unrelated sections; keep heading hierarchy intact.
    - Keep each inserted clarification minimal and testable (avoid narrative drift).

6. Validation (performed after EACH write plus final pass):
   - Clarifications session contains exactly one bullet per accepted answer (no duplicates).
   - Total asked (accepted) questions ‚â§ 5.
   - Updated sections contain no lingering vague placeholders the new answer was meant to resolve.
   - No contradictory earlier statement remains (scan for now-invalid alternative choices removed).
   - Markdown structure valid; only allowed new headings: `## Clarifications`, `### Session YYYY-MM-DD`.
   - Terminology consistency: same canonical term used across all updated sections.

7. Write the updated spec back to `FEATURE_SPEC`.

8. Report completion (after questioning loop ends or early termination):
   - Number of questions asked & answered.
   - Path to updated spec.
   - Sections touched (list names).
   - Coverage summary table listing each taxonomy category with Status: Resolved (was Partial/Missing and addressed), Deferred (exceeds question quota or better suited for planning), Clear (already sufficient), Outstanding (still Partial/Missing but low impact).
   - If any Outstanding or Deferred remain, recommend whether to proceed to `/speckit.plan` or run `/speckit.clarify` again later post-plan.
   - Suggested next command.

Behavior rules:

- If no meaningful ambiguities found (or all potential questions would be low-impact), respond: "No critical ambiguities detected worth formal clarification." and suggest proceeding.
- If spec file missing, instruct user to run `/speckit.specify` first (do not create a new spec here).
- Never exceed 5 total asked questions (clarification retries for a single question do not count as new questions).
- Avoid speculative tech stack questions unless the absence blocks functional clarity.
- Respect user early termination signals ("stop", "done", "proceed").
- If no questions asked due to full coverage, output a compact coverage summary (all categories Clear) then suggest advancing.
- If quota reached with unresolved high-impact categories remaining, explicitly flag them under Deferred with rationale.

Context for prioritization: $ARGUMENTS
````

## File: .claude/commands/speckit.constitution.md
````markdown
---
description: Create or update the project constitution from interactive or provided principle inputs, ensuring all dependent templates stay in sync.
handoffs: 
  - label: Build Specification
    agent: speckit.specify
    prompt: Implement the feature specification based on the updated constitution. I want to build...
---

## User Input

```text
$ARGUMENTS
```

You **MUST** consider the user input before proceeding (if not empty).

## Outline

You are updating the project constitution at `.specify/memory/constitution.md`. This file is a TEMPLATE containing placeholder tokens in square brackets (e.g. `[PROJECT_NAME]`, `[PRINCIPLE_1_NAME]`). Your job is to (a) collect/derive concrete values, (b) fill the template precisely, and (c) propagate any amendments across dependent artifacts.

Follow this execution flow:

1. Load the existing constitution template at `.specify/memory/constitution.md`.
   - Identify every placeholder token of the form `[ALL_CAPS_IDENTIFIER]`.
   **IMPORTANT**: The user might require less or more principles than the ones used in the template. If a number is specified, respect that - follow the general template. You will update the doc accordingly.

2. Collect/derive values for placeholders:
   - If user input (conversation) supplies a value, use it.
   - Otherwise infer from existing repo context (README, docs, prior constitution versions if embedded).
   - For governance dates: `RATIFICATION_DATE` is the original adoption date (if unknown ask or mark TODO), `LAST_AMENDED_DATE` is today if changes are made, otherwise keep previous.
   - `CONSTITUTION_VERSION` must increment according to semantic versioning rules:
     - MAJOR: Backward incompatible governance/principle removals or redefinitions.
     - MINOR: New principle/section added or materially expanded guidance.
     - PATCH: Clarifications, wording, typo fixes, non-semantic refinements.
   - If version bump type ambiguous, propose reasoning before finalizing.

3. Draft the updated constitution content:
   - Replace every placeholder with concrete text (no bracketed tokens left except intentionally retained template slots that the project has chosen not to define yet‚Äîexplicitly justify any left).
   - Preserve heading hierarchy and comments can be removed once replaced unless they still add clarifying guidance.
   - Ensure each Principle section: succinct name line, paragraph (or bullet list) capturing non‚Äënegotiable rules, explicit rationale if not obvious.
   - Ensure Governance section lists amendment procedure, versioning policy, and compliance review expectations.

4. Consistency propagation checklist (convert prior checklist into active validations):
   - Read `.specify/templates/plan-template.md` and ensure any "Constitution Check" or rules align with updated principles.
   - Read `.specify/templates/spec-template.md` for scope/requirements alignment‚Äîupdate if constitution adds/removes mandatory sections or constraints.
   - Read `.specify/templates/tasks-template.md` and ensure task categorization reflects new or removed principle-driven task types (e.g., observability, versioning, testing discipline).
   - Read each command file in `.specify/templates/commands/*.md` (including this one) to verify no outdated references (agent-specific names like CLAUDE only) remain when generic guidance is required.
   - Read any runtime guidance docs (e.g., `README.md`, `docs/quickstart.md`, or agent-specific guidance files if present). Update references to principles changed.

5. Produce a Sync Impact Report (prepend as an HTML comment at top of the constitution file after update):
   - Version change: old ‚Üí new
   - List of modified principles (old title ‚Üí new title if renamed)
   - Added sections
   - Removed sections
   - Templates requiring updates (‚úÖ updated / ‚ö† pending) with file paths
   - Follow-up TODOs if any placeholders intentionally deferred.

6. Validation before final output:
   - No remaining unexplained bracket tokens.
   - Version line matches report.
   - Dates ISO format YYYY-MM-DD.
   - Principles are declarative, testable, and free of vague language ("should" ‚Üí replace with MUST/SHOULD rationale where appropriate).

7. Write the completed constitution back to `.specify/memory/constitution.md` (overwrite).

8. Output a final summary to the user with:
   - New version and bump rationale.
   - Any files flagged for manual follow-up.
   - Suggested commit message (e.g., `docs: amend constitution to vX.Y.Z (principle additions + governance update)`).

Formatting & Style Requirements:

- Use Markdown headings exactly as in the template (do not demote/promote levels).
- Wrap long rationale lines to keep readability (<100 chars ideally) but do not hard enforce with awkward breaks.
- Keep a single blank line between sections.
- Avoid trailing whitespace.

If the user supplies partial updates (e.g., only one principle revision), still perform validation and version decision steps.

If critical info missing (e.g., ratification date truly unknown), insert `TODO(<FIELD_NAME>): explanation` and include in the Sync Impact Report under deferred items.

Do not create a new template; always operate on the existing `.specify/memory/constitution.md` file.
````

## File: .claude/commands/speckit.implement.md
````markdown
---
description: Execute the implementation plan by processing and executing all tasks defined in tasks.md
---

## User Input

```text
$ARGUMENTS
```

You **MUST** consider the user input before proceeding (if not empty).

## Outline

1. Run `.specify/scripts/bash/check-prerequisites.sh --json --require-tasks --include-tasks` from repo root and parse FEATURE_DIR and AVAILABLE_DOCS list. All paths must be absolute. For single quotes in args like "I'm Groot", use escape syntax: e.g 'I'\''m Groot' (or double-quote if possible: "I'm Groot").

2. **Check checklists status** (if FEATURE_DIR/checklists/ exists):
   - Scan all checklist files in the checklists/ directory
   - For each checklist, count:
     - Total items: All lines matching `- [ ]` or `- [X]` or `- [x]`
     - Completed items: Lines matching `- [X]` or `- [x]`
     - Incomplete items: Lines matching `- [ ]`
   - Create a status table:

     ```text
     | Checklist | Total | Completed | Incomplete | Status |
     |-----------|-------|-----------|------------|--------|
     | ux.md     | 12    | 12        | 0          | ‚úì PASS |
     | test.md   | 8     | 5         | 3          | ‚úó FAIL |
     | security.md | 6   | 6         | 0          | ‚úì PASS |
     ```

   - Calculate overall status:
     - **PASS**: All checklists have 0 incomplete items
     - **FAIL**: One or more checklists have incomplete items

   - **If any checklist is incomplete**:
     - Display the table with incomplete item counts
     - **STOP** and ask: "Some checklists are incomplete. Do you want to proceed with implementation anyway? (yes/no)"
     - Wait for user response before continuing
     - If user says "no" or "wait" or "stop", halt execution
     - If user says "yes" or "proceed" or "continue", proceed to step 3

   - **If all checklists are complete**:
     - Display the table showing all checklists passed
     - Automatically proceed to step 3

3. Load and analyze the implementation context:
   - **REQUIRED**: Read tasks.md for the complete task list and execution plan
   - **REQUIRED**: Read plan.md for tech stack, architecture, and file structure
   - **IF EXISTS**: Read data-model.md for entities and relationships
   - **IF EXISTS**: Read contracts/ for API specifications and test requirements
   - **IF EXISTS**: Read research.md for technical decisions and constraints
   - **IF EXISTS**: Read quickstart.md for integration scenarios

4. **Project Setup Verification**:
   - **REQUIRED**: Create/verify ignore files based on actual project setup:

   **Detection & Creation Logic**:
   - Check if the following command succeeds to determine if the repository is a git repo (create/verify .gitignore if so):

     ```sh
     git rev-parse --git-dir 2>/dev/null
     ```

   - Check if Dockerfile* exists or Docker in plan.md ‚Üí create/verify .dockerignore
   - Check if .eslintrc* exists ‚Üí create/verify .eslintignore
   - Check if eslint.config.* exists ‚Üí ensure the config's `ignores` entries cover required patterns
   - Check if .prettierrc* exists ‚Üí create/verify .prettierignore
   - Check if .npmrc or package.json exists ‚Üí create/verify .npmignore (if publishing)
   - Check if terraform files (*.tf) exist ‚Üí create/verify .terraformignore
   - Check if .helmignore needed (helm charts present) ‚Üí create/verify .helmignore

   **If ignore file already exists**: Verify it contains essential patterns, append missing critical patterns only
   **If ignore file missing**: Create with full pattern set for detected technology

   **Common Patterns by Technology** (from plan.md tech stack):
   - **Node.js/JavaScript/TypeScript**: `node_modules/`, `dist/`, `build/`, `*.log`, `.env*`
   - **Python**: `__pycache__/`, `*.pyc`, `.venv/`, `venv/`, `dist/`, `*.egg-info/`
   - **Java**: `target/`, `*.class`, `*.jar`, `.gradle/`, `build/`
   - **C#/.NET**: `bin/`, `obj/`, `*.user`, `*.suo`, `packages/`
   - **Go**: `*.exe`, `*.test`, `vendor/`, `*.out`
   - **Ruby**: `.bundle/`, `log/`, `tmp/`, `*.gem`, `vendor/bundle/`
   - **PHP**: `vendor/`, `*.log`, `*.cache`, `*.env`
   - **Rust**: `target/`, `debug/`, `release/`, `*.rs.bk`, `*.rlib`, `*.prof*`, `.idea/`, `*.log`, `.env*`
   - **Kotlin**: `build/`, `out/`, `.gradle/`, `.idea/`, `*.class`, `*.jar`, `*.iml`, `*.log`, `.env*`
   - **C++**: `build/`, `bin/`, `obj/`, `out/`, `*.o`, `*.so`, `*.a`, `*.exe`, `*.dll`, `.idea/`, `*.log`, `.env*`
   - **C**: `build/`, `bin/`, `obj/`, `out/`, `*.o`, `*.a`, `*.so`, `*.exe`, `Makefile`, `config.log`, `.idea/`, `*.log`, `.env*`
   - **Swift**: `.build/`, `DerivedData/`, `*.swiftpm/`, `Packages/`
   - **R**: `.Rproj.user/`, `.Rhistory`, `.RData`, `.Ruserdata`, `*.Rproj`, `packrat/`, `renv/`
   - **Universal**: `.DS_Store`, `Thumbs.db`, `*.tmp`, `*.swp`, `.vscode/`, `.idea/`

   **Tool-Specific Patterns**:
   - **Docker**: `node_modules/`, `.git/`, `Dockerfile*`, `.dockerignore`, `*.log*`, `.env*`, `coverage/`
   - **ESLint**: `node_modules/`, `dist/`, `build/`, `coverage/`, `*.min.js`
   - **Prettier**: `node_modules/`, `dist/`, `build/`, `coverage/`, `package-lock.json`, `yarn.lock`, `pnpm-lock.yaml`
   - **Terraform**: `.terraform/`, `*.tfstate*`, `*.tfvars`, `.terraform.lock.hcl`
   - **Kubernetes/k8s**: `*.secret.yaml`, `secrets/`, `.kube/`, `kubeconfig*`, `*.key`, `*.crt`

5. Parse tasks.md structure and extract:
   - **Task phases**: Setup, Tests, Core, Integration, Polish
   - **Task dependencies**: Sequential vs parallel execution rules
   - **Task details**: ID, description, file paths, parallel markers [P]
   - **Execution flow**: Order and dependency requirements

6. Execute implementation following the task plan:
   - **Phase-by-phase execution**: Complete each phase before moving to the next
   - **Respect dependencies**: Run sequential tasks in order, parallel tasks [P] can run together  
   - **Follow TDD approach**: Execute test tasks before their corresponding implementation tasks
   - **File-based coordination**: Tasks affecting the same files must run sequentially
   - **Validation checkpoints**: Verify each phase completion before proceeding

7. Implementation execution rules:
   - **Setup first**: Initialize project structure, dependencies, configuration
   - **Tests before code**: If you need to write tests for contracts, entities, and integration scenarios
   - **Core development**: Implement models, services, CLI commands, endpoints
   - **Integration work**: Database connections, middleware, logging, external services
   - **Polish and validation**: Unit tests, performance optimization, documentation

8. Progress tracking and error handling:
   - Report progress after each completed task
   - Halt execution if any non-parallel task fails
   - For parallel tasks [P], continue with successful tasks, report failed ones
   - Provide clear error messages with context for debugging
   - Suggest next steps if implementation cannot proceed
   - **IMPORTANT** For completed tasks, make sure to mark the task off as [X] in the tasks file.

9. Completion validation:
   - Verify all required tasks are completed
   - Check that implemented features match the original specification
   - Validate that tests pass and coverage meets requirements
   - Confirm the implementation follows the technical plan
   - Report final status with summary of completed work

Note: This command assumes a complete task breakdown exists in tasks.md. If tasks are incomplete or missing, suggest running `/speckit.tasks` first to regenerate the task list.
````

## File: .claude/commands/speckit.plan.md
````markdown
---
description: Execute the implementation planning workflow using the plan template to generate design artifacts.
handoffs: 
  - label: Create Tasks
    agent: speckit.tasks
    prompt: Break the plan into tasks
    send: true
  - label: Create Checklist
    agent: speckit.checklist
    prompt: Create a checklist for the following domain...
---

## User Input

```text
$ARGUMENTS
```

You **MUST** consider the user input before proceeding (if not empty).

## Outline

1. **Setup**: Run `.specify/scripts/bash/setup-plan.sh --json` from repo root and parse JSON for FEATURE_SPEC, IMPL_PLAN, SPECS_DIR, BRANCH. For single quotes in args like "I'm Groot", use escape syntax: e.g 'I'\''m Groot' (or double-quote if possible: "I'm Groot").

2. **Load context**: Read FEATURE_SPEC and `.specify/memory/constitution.md`. Load IMPL_PLAN template (already copied).

3. **Execute plan workflow**: Follow the structure in IMPL_PLAN template to:
   - Fill Technical Context (mark unknowns as "NEEDS CLARIFICATION")
   - Fill Constitution Check section from constitution
   - Evaluate gates (ERROR if violations unjustified)
   - Phase 0: Generate research.md (resolve all NEEDS CLARIFICATION)
   - Phase 1: Generate data-model.md, contracts/, quickstart.md
   - Phase 1: Update agent context by running the agent script
   - Re-evaluate Constitution Check post-design

4. **Stop and report**: Command ends after Phase 2 planning. Report branch, IMPL_PLAN path, and generated artifacts.

## Phases

### Phase 0: Outline & Research

1. **Extract unknowns from Technical Context** above:
   - For each NEEDS CLARIFICATION ‚Üí research task
   - For each dependency ‚Üí best practices task
   - For each integration ‚Üí patterns task

2. **Generate and dispatch research agents**:

   ```text
   For each unknown in Technical Context:
     Task: "Research {unknown} for {feature context}"
   For each technology choice:
     Task: "Find best practices for {tech} in {domain}"
   ```

3. **Consolidate findings** in `research.md` using format:
   - Decision: [what was chosen]
   - Rationale: [why chosen]
   - Alternatives considered: [what else evaluated]

**Output**: research.md with all NEEDS CLARIFICATION resolved

### Phase 1: Design & Contracts

**Prerequisites:** `research.md` complete

1. **Extract entities from feature spec** ‚Üí `data-model.md`:
   - Entity name, fields, relationships
   - Validation rules from requirements
   - State transitions if applicable

2. **Generate API contracts** from functional requirements:
   - For each user action ‚Üí endpoint
   - Use standard REST/GraphQL patterns
   - Output OpenAPI/GraphQL schema to `/contracts/`

3. **Agent context update**:
   - Run `.specify/scripts/bash/update-agent-context.sh claude`
   - These scripts detect which AI agent is in use
   - Update the appropriate agent-specific context file
   - Add only new technology from current plan
   - Preserve manual additions between markers

**Output**: data-model.md, /contracts/*, quickstart.md, agent-specific file

## Key rules

- Use absolute paths
- ERROR on gate failures or unresolved clarifications
````

## File: .claude/commands/speckit.specify.md
````markdown
---
description: Create or update the feature specification from a natural language feature description.
handoffs: 
  - label: Build Technical Plan
    agent: speckit.plan
    prompt: Create a plan for the spec. I am building with...
  - label: Clarify Spec Requirements
    agent: speckit.clarify
    prompt: Clarify specification requirements
    send: true
---

## User Input

```text
$ARGUMENTS
```

You **MUST** consider the user input before proceeding (if not empty).

## Outline

The text the user typed after `/speckit.specify` in the triggering message **is** the feature description. Assume you always have it available in this conversation even if `$ARGUMENTS` appears literally below. Do not ask the user to repeat it unless they provided an empty command.

Given that feature description, do this:

1. **Generate a concise short name** (2-4 words) for the branch:
   - Analyze the feature description and extract the most meaningful keywords
   - Create a 2-4 word short name that captures the essence of the feature
   - Use action-noun format when possible (e.g., "add-user-auth", "fix-payment-bug")
   - Preserve technical terms and acronyms (OAuth2, API, JWT, etc.)
   - Keep it concise but descriptive enough to understand the feature at a glance
   - Examples:
     - "I want to add user authentication" ‚Üí "user-auth"
     - "Implement OAuth2 integration for the API" ‚Üí "oauth2-api-integration"
     - "Create a dashboard for analytics" ‚Üí "analytics-dashboard"
     - "Fix payment processing timeout bug" ‚Üí "fix-payment-timeout"

2. **Check for existing branches before creating new one**:
   
   a. First, fetch all remote branches to ensure we have the latest information:
      ```bash
      git fetch --all --prune
      ```
   
   b. Find the highest feature number across all sources for the short-name:
      - Remote branches: `git ls-remote --heads origin | grep -E 'refs/heads/[0-9]+-<short-name>$'`
      - Local branches: `git branch | grep -E '^[* ]*[0-9]+-<short-name>$'`
      - Specs directories: Check for directories matching `specs/[0-9]+-<short-name>`
   
   c. Determine the next available number:
      - Extract all numbers from all three sources
      - Find the highest number N
      - Use N+1 for the new branch number
   
   d. Run the script `.specify/scripts/bash/create-new-feature.sh --json "$ARGUMENTS"` with the calculated number and short-name:
      - Pass `--number N+1` and `--short-name "your-short-name"` along with the feature description
      - Bash example: `.specify/scripts/bash/create-new-feature.sh --json "$ARGUMENTS" --json --number 5 --short-name "user-auth" "Add user authentication"`
      - PowerShell example: `.specify/scripts/bash/create-new-feature.sh --json "$ARGUMENTS" -Json -Number 5 -ShortName "user-auth" "Add user authentication"`
   
   **IMPORTANT**:
   - Check all three sources (remote branches, local branches, specs directories) to find the highest number
   - Only match branches/directories with the exact short-name pattern
   - If no existing branches/directories found with this short-name, start with number 1
   - You must only ever run this script once per feature
   - The JSON is provided in the terminal as output - always refer to it to get the actual content you're looking for
   - The JSON output will contain BRANCH_NAME and SPEC_FILE paths
   - For single quotes in args like "I'm Groot", use escape syntax: e.g 'I'\''m Groot' (or double-quote if possible: "I'm Groot")

3. Load `.specify/templates/spec-template.md` to understand required sections.

4. Follow this execution flow:

    1. Parse user description from Input
       If empty: ERROR "No feature description provided"
    2. Extract key concepts from description
       Identify: actors, actions, data, constraints
    3. For unclear aspects:
       - Make informed guesses based on context and industry standards
       - Only mark with [NEEDS CLARIFICATION: specific question] if:
         - The choice significantly impacts feature scope or user experience
         - Multiple reasonable interpretations exist with different implications
         - No reasonable default exists
       - **LIMIT: Maximum 3 [NEEDS CLARIFICATION] markers total**
       - Prioritize clarifications by impact: scope > security/privacy > user experience > technical details
    4. Fill User Scenarios & Testing section
       If no clear user flow: ERROR "Cannot determine user scenarios"
    5. Generate Functional Requirements
       Each requirement must be testable
       Use reasonable defaults for unspecified details (document assumptions in Assumptions section)
    6. Define Success Criteria
       Create measurable, technology-agnostic outcomes
       Include both quantitative metrics (time, performance, volume) and qualitative measures (user satisfaction, task completion)
       Each criterion must be verifiable without implementation details
    7. Identify Key Entities (if data involved)
    8. Return: SUCCESS (spec ready for planning)

5. Write the specification to SPEC_FILE using the template structure, replacing placeholders with concrete details derived from the feature description (arguments) while preserving section order and headings.

6. **Specification Quality Validation**: After writing the initial spec, validate it against quality criteria:

   a. **Create Spec Quality Checklist**: Generate a checklist file at `FEATURE_DIR/checklists/requirements.md` using the checklist template structure with these validation items:

      ```markdown
      # Specification Quality Checklist: [FEATURE NAME]
      
      **Purpose**: Validate specification completeness and quality before proceeding to planning
      **Created**: [DATE]
      **Feature**: [Link to spec.md]
      
      ## Content Quality
      
      - [ ] No implementation details (languages, frameworks, APIs)
      - [ ] Focused on user value and business needs
      - [ ] Written for non-technical stakeholders
      - [ ] All mandatory sections completed
      
      ## Requirement Completeness
      
      - [ ] No [NEEDS CLARIFICATION] markers remain
      - [ ] Requirements are testable and unambiguous
      - [ ] Success criteria are measurable
      - [ ] Success criteria are technology-agnostic (no implementation details)
      - [ ] All acceptance scenarios are defined
      - [ ] Edge cases are identified
      - [ ] Scope is clearly bounded
      - [ ] Dependencies and assumptions identified
      
      ## Feature Readiness
      
      - [ ] All functional requirements have clear acceptance criteria
      - [ ] User scenarios cover primary flows
      - [ ] Feature meets measurable outcomes defined in Success Criteria
      - [ ] No implementation details leak into specification
      
      ## Notes
      
      - Items marked incomplete require spec updates before `/speckit.clarify` or `/speckit.plan`
      ```

   b. **Run Validation Check**: Review the spec against each checklist item:
      - For each item, determine if it passes or fails
      - Document specific issues found (quote relevant spec sections)

   c. **Handle Validation Results**:

      - **If all items pass**: Mark checklist complete and proceed to step 6

      - **If items fail (excluding [NEEDS CLARIFICATION])**:
        1. List the failing items and specific issues
        2. Update the spec to address each issue
        3. Re-run validation until all items pass (max 3 iterations)
        4. If still failing after 3 iterations, document remaining issues in checklist notes and warn user

      - **If [NEEDS CLARIFICATION] markers remain**:
        1. Extract all [NEEDS CLARIFICATION: ...] markers from the spec
        2. **LIMIT CHECK**: If more than 3 markers exist, keep only the 3 most critical (by scope/security/UX impact) and make informed guesses for the rest
        3. For each clarification needed (max 3), present options to user in this format:

           ```markdown
           ## Question [N]: [Topic]
           
           **Context**: [Quote relevant spec section]
           
           **What we need to know**: [Specific question from NEEDS CLARIFICATION marker]
           
           **Suggested Answers**:
           
           | Option | Answer | Implications |
           |--------|--------|--------------|
           | A      | [First suggested answer] | [What this means for the feature] |
           | B      | [Second suggested answer] | [What this means for the feature] |
           | C      | [Third suggested answer] | [What this means for the feature] |
           | Custom | Provide your own answer | [Explain how to provide custom input] |
           
           **Your choice**: _[Wait for user response]_
           ```

        4. **CRITICAL - Table Formatting**: Ensure markdown tables are properly formatted:
           - Use consistent spacing with pipes aligned
           - Each cell should have spaces around content: `| Content |` not `|Content|`
           - Header separator must have at least 3 dashes: `|--------|`
           - Test that the table renders correctly in markdown preview
        5. Number questions sequentially (Q1, Q2, Q3 - max 3 total)
        6. Present all questions together before waiting for responses
        7. Wait for user to respond with their choices for all questions (e.g., "Q1: A, Q2: Custom - [details], Q3: B")
        8. Update the spec by replacing each [NEEDS CLARIFICATION] marker with the user's selected or provided answer
        9. Re-run validation after all clarifications are resolved

   d. **Update Checklist**: After each validation iteration, update the checklist file with current pass/fail status

7. Report completion with branch name, spec file path, checklist results, and readiness for the next phase (`/speckit.clarify` or `/speckit.plan`).

**NOTE:** The script creates and checks out the new branch and initializes the spec file before writing.

## General Guidelines

## Quick Guidelines

- Focus on **WHAT** users need and **WHY**.
- Avoid HOW to implement (no tech stack, APIs, code structure).
- Written for business stakeholders, not developers.
- DO NOT create any checklists that are embedded in the spec. That will be a separate command.

### Section Requirements

- **Mandatory sections**: Must be completed for every feature
- **Optional sections**: Include only when relevant to the feature
- When a section doesn't apply, remove it entirely (don't leave as "N/A")

### For AI Generation

When creating this spec from a user prompt:

1. **Make informed guesses**: Use context, industry standards, and common patterns to fill gaps
2. **Document assumptions**: Record reasonable defaults in the Assumptions section
3. **Limit clarifications**: Maximum 3 [NEEDS CLARIFICATION] markers - use only for critical decisions that:
   - Significantly impact feature scope or user experience
   - Have multiple reasonable interpretations with different implications
   - Lack any reasonable default
4. **Prioritize clarifications**: scope > security/privacy > user experience > technical details
5. **Think like a tester**: Every vague requirement should fail the "testable and unambiguous" checklist item
6. **Common areas needing clarification** (only if no reasonable default exists):
   - Feature scope and boundaries (include/exclude specific use cases)
   - User types and permissions (if multiple conflicting interpretations possible)
   - Security/compliance requirements (when legally/financially significant)

**Examples of reasonable defaults** (don't ask about these):

- Data retention: Industry-standard practices for the domain
- Performance targets: Standard web/mobile app expectations unless specified
- Error handling: User-friendly messages with appropriate fallbacks
- Authentication method: Standard session-based or OAuth2 for web apps
- Integration patterns: RESTful APIs unless specified otherwise

### Success Criteria Guidelines

Success criteria must be:

1. **Measurable**: Include specific metrics (time, percentage, count, rate)
2. **Technology-agnostic**: No mention of frameworks, languages, databases, or tools
3. **User-focused**: Describe outcomes from user/business perspective, not system internals
4. **Verifiable**: Can be tested/validated without knowing implementation details

**Good examples**:

- "Users can complete checkout in under 3 minutes"
- "System supports 10,000 concurrent users"
- "95% of searches return results in under 1 second"
- "Task completion rate improves by 40%"

**Bad examples** (implementation-focused):

- "API response time is under 200ms" (too technical, use "Users see results instantly")
- "Database can handle 1000 TPS" (implementation detail, use user-facing metric)
- "React components render efficiently" (framework-specific)
- "Redis cache hit rate above 80%" (technology-specific)
````

## File: .claude/commands/speckit.tasks.md
````markdown
---
description: Generate an actionable, dependency-ordered tasks.md for the feature based on available design artifacts.
handoffs: 
  - label: Analyze For Consistency
    agent: speckit.analyze
    prompt: Run a project analysis for consistency
    send: true
  - label: Implement Project
    agent: speckit.implement
    prompt: Start the implementation in phases
    send: true
---

## User Input

```text
$ARGUMENTS
```

You **MUST** consider the user input before proceeding (if not empty).

## Outline

1. **Setup**: Run `.specify/scripts/bash/check-prerequisites.sh --json` from repo root and parse FEATURE_DIR and AVAILABLE_DOCS list. All paths must be absolute. For single quotes in args like "I'm Groot", use escape syntax: e.g 'I'\''m Groot' (or double-quote if possible: "I'm Groot").

2. **Load design documents**: Read from FEATURE_DIR:
   - **Required**: plan.md (tech stack, libraries, structure), spec.md (user stories with priorities)
   - **Optional**: data-model.md (entities), contracts/ (API endpoints), research.md (decisions), quickstart.md (test scenarios)
   - Note: Not all projects have all documents. Generate tasks based on what's available.

3. **Execute task generation workflow**:
   - Load plan.md and extract tech stack, libraries, project structure
   - Load spec.md and extract user stories with their priorities (P1, P2, P3, etc.)
   - If data-model.md exists: Extract entities and map to user stories
   - If contracts/ exists: Map endpoints to user stories
   - If research.md exists: Extract decisions for setup tasks
   - Generate tasks organized by user story (see Task Generation Rules below)
   - Generate dependency graph showing user story completion order
   - Create parallel execution examples per user story
   - Validate task completeness (each user story has all needed tasks, independently testable)

4. **Generate tasks.md**: Use `.specify.specify/templates/tasks-template.md` as structure, fill with:
   - Correct feature name from plan.md
   - Phase 1: Setup tasks (project initialization)
   - Phase 2: Foundational tasks (blocking prerequisites for all user stories)
   - Phase 3+: One phase per user story (in priority order from spec.md)
   - Each phase includes: story goal, independent test criteria, tests (if requested), implementation tasks
   - Final Phase: Polish & cross-cutting concerns
   - All tasks must follow the strict checklist format (see Task Generation Rules below)
   - Clear file paths for each task
   - Dependencies section showing story completion order
   - Parallel execution examples per story
   - Implementation strategy section (MVP first, incremental delivery)

5. **Report**: Output path to generated tasks.md and summary:
   - Total task count
   - Task count per user story
   - Parallel opportunities identified
   - Independent test criteria for each story
   - Suggested MVP scope (typically just User Story 1)
   - Format validation: Confirm ALL tasks follow the checklist format (checkbox, ID, labels, file paths)

Context for task generation: $ARGUMENTS

The tasks.md should be immediately executable - each task must be specific enough that an LLM can complete it without additional context.

## Task Generation Rules

**CRITICAL**: Tasks MUST be organized by user story to enable independent implementation and testing.

**Tests are OPTIONAL**: Only generate test tasks if explicitly requested in the feature specification or if user requests TDD approach.

### Checklist Format (REQUIRED)

Every task MUST strictly follow this format:

```text
- [ ] [TaskID] [P?] [Story?] Description with file path
```

**Format Components**:

1. **Checkbox**: ALWAYS start with `- [ ]` (markdown checkbox)
2. **Task ID**: Sequential number (T001, T002, T003...) in execution order
3. **[P] marker**: Include ONLY if task is parallelizable (different files, no dependencies on incomplete tasks)
4. **[Story] label**: REQUIRED for user story phase tasks only
   - Format: [US1], [US2], [US3], etc. (maps to user stories from spec.md)
   - Setup phase: NO story label
   - Foundational phase: NO story label  
   - User Story phases: MUST have story label
   - Polish phase: NO story label
5. **Description**: Clear action with exact file path

**Examples**:

- ‚úÖ CORRECT: `- [ ] T001 Create project structure per implementation plan`
- ‚úÖ CORRECT: `- [ ] T005 [P] Implement authentication middleware in src/middleware/auth.py`
- ‚úÖ CORRECT: `- [ ] T012 [P] [US1] Create User model in src/models/user.py`
- ‚úÖ CORRECT: `- [ ] T014 [US1] Implement UserService in src/services/user_service.py`
- ‚ùå WRONG: `- [ ] Create User model` (missing ID and Story label)
- ‚ùå WRONG: `T001 [US1] Create model` (missing checkbox)
- ‚ùå WRONG: `- [ ] [US1] Create User model` (missing Task ID)
- ‚ùå WRONG: `- [ ] T001 [US1] Create model` (missing file path)

### Task Organization

1. **From User Stories (spec.md)** - PRIMARY ORGANIZATION:
   - Each user story (P1, P2, P3...) gets its own phase
   - Map all related components to their story:
     - Models needed for that story
     - Services needed for that story
     - Endpoints/UI needed for that story
     - If tests requested: Tests specific to that story
   - Mark story dependencies (most stories should be independent)

2. **From Contracts**:
   - Map each contract/endpoint ‚Üí to the user story it serves
   - If tests requested: Each contract ‚Üí contract test task [P] before implementation in that story's phase

3. **From Data Model**:
   - Map each entity to the user story(ies) that need it
   - If entity serves multiple stories: Put in earliest story or Setup phase
   - Relationships ‚Üí service layer tasks in appropriate story phase

4. **From Setup/Infrastructure**:
   - Shared infrastructure ‚Üí Setup phase (Phase 1)
   - Foundational/blocking tasks ‚Üí Foundational phase (Phase 2)
   - Story-specific setup ‚Üí within that story's phase

### Phase Structure

- **Phase 1**: Setup (project initialization)
- **Phase 2**: Foundational (blocking prerequisites - MUST complete before user stories)
- **Phase 3+**: User Stories in priority order (P1, P2, P3...)
  - Within each story: Tests (if requested) ‚Üí Models ‚Üí Services ‚Üí Endpoints ‚Üí Integration
  - Each phase should be a complete, independently testable increment
- **Final Phase**: Polish & Cross-Cutting Concerns
````

## File: .claude/commands/speckit.taskstoissues.md
````markdown
---
description: Convert existing tasks into actionable, dependency-ordered GitHub issues for the feature based on available design artifacts.
tools: ['github/github-mcp-server/issue_write']
---

## User Input

```text
$ARGUMENTS
```

You **MUST** consider the user input before proceeding (if not empty).

## Outline

1. Run `.specify/scripts/bash/check-prerequisites.sh --json --require-tasks --include-tasks` from repo root and parse FEATURE_DIR and AVAILABLE_DOCS list. All paths must be absolute. For single quotes in args like "I'm Groot", use escape syntax: e.g 'I'\''m Groot' (or double-quote if possible: "I'm Groot").
1. From the executed script, extract the path to **tasks**.
1. Get the Git remote by running:

```bash
git config --get remote.origin.url
```

**ONLY PROCEED TO NEXT STEPS IF THE REMOTE IS A GITHUB URL**

1. For each task in the list, use the GitHub MCP server to create a new issue in the repository that is representative of the Git remote.

**UNDER NO CIRCUMSTANCES EVER CREATE ISSUES IN REPOSITORIES THAT DO NOT MATCH THE REMOTE URL**
````

## File: .github/ISSUE_TEMPLATE/bug_report.md
````markdown
---
name: Bug Report
about: Create a report to help us improve
title: '[BUG] '
labels: bug
assignees: ''
---

## Bug Description

A clear and concise description of what the bug is.

## To Reproduce

Steps to reproduce the behavior:

1. Go to '...'
2. Click on '....'
3. Scroll down to '....'
4. See error

## Expected Behavior

A clear and concise description of what you expected to happen.

## Actual Behavior

A clear and concise description of what actually happened.

## Environment

- OS: [e.g. macOS 14.0, Ubuntu 22.04]
- Version: [e.g. 1.0.0]
- Python Version: [e.g. 3.11]

## Additional Context

Add any other context about the problem here. Include screenshots if applicable.

## Possible Solution

If you have suggestions on how to fix the bug, please describe them here.
````

## File: .github/ISSUE_TEMPLATE/feature_request.md
````markdown
---
name: Feature Request
about: Suggest an idea for this project
title: '[FEATURE] '
labels: enhancement
assignees: ''
---

## Problem Statement

A clear and concise description of what the problem is. Ex. I'm always frustrated when [...]

## Proposed Solution

A clear and concise description of what you want to happen.

## Alternatives Considered

A clear and concise description of any alternative solutions or features you've considered.

## Use Cases

Describe specific use cases where this feature would be valuable.

1. Use case 1
2. Use case 2
3. Use case 3

## Additional Context

Add any other context, screenshots, or mockups about the feature request here.

## Acceptance Criteria

- [ ] Criterion 1
- [ ] Criterion 2
- [ ] Criterion 3

## Priority

How important is this feature to you?

- [ ] Critical (blocking my work)
- [ ] High (significantly improves workflow)
- [ ] Medium (nice to have)
- [ ] Low (minor enhancement)
````

## File: .github/workflows/agentready-assessment.yml
````yaml
name: AgentReady Assessment

on:
  pull_request:
    types: [opened, synchronize, reopened]
  push:
    branches: [main, master]
  workflow_dispatch:

jobs:
  assess:
    runs-on: ubuntu-latest
    permissions:
      contents: read
      pull-requests: write

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install AgentReady
        run: |
          pip install agentready

      - name: Run AgentReady Assessment
        run: |
          agentready assess . --verbose

      - name: Upload Assessment Reports
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: agentready-reports
          path: .agentready/
          retention-days: 30

      - name: Comment on PR
        if: github.event_name == 'pull_request'
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            const reportPath = '.agentready/report-latest.md';

            if (!fs.existsSync(reportPath)) {
              console.log('No report found');
              return;
            }

            const report = fs.readFileSync(reportPath, 'utf8');

            // Post comment with assessment results
            await github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: report
            });
````

## File: .github/workflows/docs-lint.yml
````yaml
name: Documentation Linting

on:
  pull_request:
    paths:
      - 'docs/**'
      - '.github/workflows/docs-lint.yml'
      - '.markdown-link-check.json'
  push:
    branches: [main]
    paths:
      - 'docs/**'
      - '.github/workflows/docs-lint.yml'
      - '.markdown-link-check.json'
  workflow_dispatch:

jobs:
  lint-docs:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '20'

      - name: Install markdown-link-check
        run: npm install -g markdown-link-check

      - name: Check links in documentation
        run: |
          find docs -name '*.md' -print0 | \
            xargs -0 -n1 markdown-link-check \
              --config .markdown-link-check.json

      - name: Comment on PR
        if: failure() && github.event_name == 'pull_request'
        uses: actions/github-script@v7
        with:
          script: |
            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: '‚ö†Ô∏è Broken links found in documentation. See workflow logs for details.'
            })
````

## File: .github/workflows/release.yml
````yaml
name: Release

on:
  push:
    branches:
      - main
  workflow_dispatch:

permissions:
  contents: write
  issues: write
  pull-requests: write

jobs:
  release:
    name: Create Release
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0
          token: ${{ secrets.GITHUB_TOKEN }}

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: 'lts/*'

      - name: Install semantic-release
        run: |
          npm install -g semantic-release @semantic-release/changelog @semantic-release/git @semantic-release/github @semantic-release/exec

      - name: Run semantic-release
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: npx semantic-release
````

## File: .github/workflows/security.yml
````yaml
name: Security

on:
  push:
    branches: [main, master]
  pull_request:
  schedule:
    - cron: '0 0 * * 0'  # Weekly on Sunday
  workflow_dispatch:

permissions:
  contents: read
  security-events: write

jobs:
  codeql:
    name: CodeQL Analysis
    runs-on: ubuntu-latest

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Initialize CodeQL
        uses: github/codeql-action/init@v3
        with:
          languages: python

      - name: Autobuild
        uses: github/codeql-action/autobuild@v3

      - name: Perform CodeQL Analysis
        uses: github/codeql-action/analyze@v3

  safety:
    name: Dependency Security Scan
    runs-on: ubuntu-latest

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: |
          pip install safety

      - name: Run safety check
        run: |
          safety check --json || true
````

## File: .github/workflows/tests.yml
````yaml
name: Tests

on:
  pull_request:
  push:
    branches: [main, master]
  workflow_dispatch:

jobs:
  test:
    runs-on: ubuntu-latest
    strategy:
      matrix:
        python-version: ['3.11', '3.12']

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python ${{ matrix.python-version }}
        uses: actions/setup-python@v5
        with:
          python-version: ${{ matrix.python-version }}

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -e ".[dev]"

      - name: Run black
        run: |
          black --check .

      - name: Run isort
        run: |
          isort --check .

      - name: Run ruff
        run: |
          ruff check .

      - name: Run pytest
        run: |
          pytest --cov=src --cov-report=xml --cov-report=term

      - name: Upload coverage to Codecov
        uses: codecov/codecov-action@v4
        if: matrix.python-version == '3.11'
        with:
          files: ./coverage.xml
          fail_ci_if_error: false
````

## File: .github/workflows/update-docs.yml
````yaml
name: Update GitHub Pages Documentation

on:
  workflow_dispatch:
    inputs:
      revision_reason:
        description: 'Reason for documentation update'
        required: true
        type: string

permissions:
  contents: write
  pull-requests: write
  issues: write

jobs:
  update-docs:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Create documentation update issue
        uses: actions/github-script@v7
        env:
          REVISION_REASON: ${{ github.event.inputs.revision_reason }}
          ACTOR: ${{ github.actor }}
        with:
          script: |
            const revisionReason = process.env.REVISION_REASON;
            const actor = process.env.ACTOR;

            const issue = await github.rest.issues.create({
              owner: context.repo.owner,
              repo: context.repo.repo,
              title: 'docs: Update GitHub Pages documentation',
              body: `## Documentation Update Request

**Reason**: ${revisionReason}

**Triggered by**: @${actor}

**Action Required**:
This workflow creates a reminder to update the GitHub Pages documentation using the \`github-pages-docs\` agent in Claude Code.

### Steps to Update Documentation:

1. **Open Claude Code** in this repository
2. **Run the command**:
   \`\`\`
   Use the @agent-github-pages-docs to revise all documentation in docs/ based on:
   - Latest CLAUDE.md updates
   - Bootstrap feature implementation status
   - Recent code changes
   - ${revisionReason}
   \`\`\`
3. **Review the changes** generated by the agent
4. **Commit and push** to this branch or create a PR
5. **Close this issue** when complete

### Why Manual Update?

The \`github-pages-docs\` agent requires Claude Code's specialized documentation capabilities. We're tracking this issue to ensure documentation stays synchronized with codebase changes.

### Future Automation

See \`BACKLOG.md\` - "Documentation Source Truth and Cascade System" (P2) for plans to automate this workflow.

---

**Related Files**:
- \`docs/\` - GitHub Pages documentation
- \`CLAUDE.md\` - Project guide (source of truth)
- \`README.md\` - User-facing documentation
- \`agent-ready-codebase-attributes.md\` - Research report (source of truth)

**Labels**: documentation, automation-needed, agent-task
              `,
              labels: ['documentation', 'automation-needed', 'agent-task']
            });

            core.setOutput('issue_number', issue.data.number);
            core.notice(`Created documentation update issue #${issue.data.number}`);
            return issue.data.number;

      - name: Check for recent source file changes
        id: check_changes
        run: |
          # Check for recent changes to source-of-truth files (last 7 days)
          CHANGES=$(git log --oneline --since="7 days ago" -- \
            CLAUDE.md \
            README.md \
            agent-ready-codebase-attributes.md \
            src/agentready/cli/bootstrap.py \
            src/agentready/services/bootstrap.py \
            || true)

          if [ -n "$CHANGES" ]; then
            echo "has_changes=true" >> $GITHUB_OUTPUT
            # Save to file to avoid injection issues
            echo "$CHANGES" > /tmp/recent_changes.txt
          else
            echo "has_changes=false" >> $GITHUB_OUTPUT
          fi

      - name: Comment with recent changes
        if: steps.check_changes.outputs.has_changes == 'true'
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            const recentChanges = fs.readFileSync('/tmp/recent_changes.txt', 'utf-8');

            await github.rest.issues.createComment({
              owner: context.repo.owner,
              repo: context.repo.repo,
              issue_number: ${{ steps.create-issue.outputs.issue_number }},
              body: `### Recent Changes to Source Files

The following commits may affect documentation:

\`\`\`
${recentChanges}
\`\`\`

Please ensure the documentation agent incorporates these changes.
              `
            });

      - name: Summary
        env:
          ISSUE_NUMBER: ${{ steps.create-issue.outputs.issue_number }}
        run: |
          echo "‚úÖ Created documentation update issue #${ISSUE_NUMBER}"
          echo ""
          echo "Next steps:"
          echo "1. Open Claude Code in this repository"
          echo "2. Use @agent-github-pages-docs to update documentation"
          echo "3. Review and commit changes"
          echo "4. Close issue #${ISSUE_NUMBER}"
````

## File: .github/CODEOWNERS
````
# CODEOWNERS file
# Define code ownership for automatic review requests

# Global owners - these users are requested for review on all PRs
# *       @owner

# Specific paths can have different owners
# /docs/  @documentation-team
# /src/   @development-team
# /.github/ @devops-team

# Default: assign to repository owner
*       @owner
````

## File: .github/dependabot.yml
````yaml
version: 2
updates:

  - package-ecosystem: "github-actions"
    directory: "/"
    schedule:
      interval: "weekly"
    labels:
      - "dependencies"
      - "github-actions"
````

## File: .github/PULL_REQUEST_TEMPLATE.md
````markdown
## Description

<!-- Provide a brief description of the changes in this PR -->

## Type of Change

- [ ] Bug fix (non-breaking change which fixes an issue)
- [ ] New feature (non-breaking change which adds functionality)
- [ ] Breaking change (fix or feature that would cause existing functionality to not work as expected)
- [ ] Documentation update
- [ ] Refactoring (no functional changes)
- [ ] Performance improvement
- [ ] Test coverage improvement

## Related Issues

<!-- Link related issues here using #issue_number -->

Fixes #
Relates to #

## Changes Made

<!-- Detailed list of changes made in this PR -->

-
-
-

## Testing

<!-- Describe the tests you ran to verify your changes -->

- [ ] Unit tests pass (`pytest`)
- [ ] Integration tests pass
- [ ] Manual testing performed
- [ ] No new warnings or errors

## Checklist

- [ ] My code follows the project's code style
- [ ] I have performed a self-review of my own code
- [ ] I have commented my code, particularly in hard-to-understand areas
- [ ] I have made corresponding changes to the documentation
- [ ] My changes generate no new warnings
- [ ] I have added tests that prove my fix is effective or that my feature works
- [ ] New and existing unit tests pass locally with my changes
- [ ] Any dependent changes have been merged and published

## Screenshots (if applicable)

<!-- Add screenshots to help explain your changes -->

## Additional Notes

<!-- Any additional information that reviewers should know -->
````

## File: .specify/scripts/bash/check-prerequisites.sh
````bash
#!/usr/bin/env bash

# Consolidated prerequisite checking script
#
# This script provides unified prerequisite checking for Spec-Driven Development workflow.
# It replaces the functionality previously spread across multiple scripts.
#
# Usage: ./check-prerequisites.sh [OPTIONS]
#
# OPTIONS:
#   --json              Output in JSON format
#   --require-tasks     Require tasks.md to exist (for implementation phase)
#   --include-tasks     Include tasks.md in AVAILABLE_DOCS list
#   --paths-only        Only output path variables (no validation)
#   --help, -h          Show help message
#
# OUTPUTS:
#   JSON mode: {"FEATURE_DIR":"...", "AVAILABLE_DOCS":["..."]}
#   Text mode: FEATURE_DIR:... \n AVAILABLE_DOCS: \n ‚úì/‚úó file.md
#   Paths only: REPO_ROOT: ... \n BRANCH: ... \n FEATURE_DIR: ... etc.

set -e

# Parse command line arguments
JSON_MODE=false
REQUIRE_TASKS=false
INCLUDE_TASKS=false
PATHS_ONLY=false

for arg in "$@"; do
    case "$arg" in
        --json)
            JSON_MODE=true
            ;;
        --require-tasks)
            REQUIRE_TASKS=true
            ;;
        --include-tasks)
            INCLUDE_TASKS=true
            ;;
        --paths-only)
            PATHS_ONLY=true
            ;;
        --help|-h)
            cat << 'EOF'
Usage: check-prerequisites.sh [OPTIONS]

Consolidated prerequisite checking for Spec-Driven Development workflow.

OPTIONS:
  --json              Output in JSON format
  --require-tasks     Require tasks.md to exist (for implementation phase)
  --include-tasks     Include tasks.md in AVAILABLE_DOCS list
  --paths-only        Only output path variables (no prerequisite validation)
  --help, -h          Show this help message

EXAMPLES:
  # Check task prerequisites (plan.md required)
  ./check-prerequisites.sh --json
  
  # Check implementation prerequisites (plan.md + tasks.md required)
  ./check-prerequisites.sh --json --require-tasks --include-tasks
  
  # Get feature paths only (no validation)
  ./check-prerequisites.sh --paths-only
  
EOF
            exit 0
            ;;
        *)
            echo "ERROR: Unknown option '$arg'. Use --help for usage information." >&2
            exit 1
            ;;
    esac
done

# Source common functions
SCRIPT_DIR="$(CDPATH="" cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
source "$SCRIPT_DIR/common.sh"

# Get feature paths and validate branch
eval $(get_feature_paths)
check_feature_branch "$CURRENT_BRANCH" "$HAS_GIT" || exit 1

# If paths-only mode, output paths and exit (support JSON + paths-only combined)
if $PATHS_ONLY; then
    if $JSON_MODE; then
        # Minimal JSON paths payload (no validation performed)
        printf '{"REPO_ROOT":"%s","BRANCH":"%s","FEATURE_DIR":"%s","FEATURE_SPEC":"%s","IMPL_PLAN":"%s","TASKS":"%s"}\n' \
            "$REPO_ROOT" "$CURRENT_BRANCH" "$FEATURE_DIR" "$FEATURE_SPEC" "$IMPL_PLAN" "$TASKS"
    else
        echo "REPO_ROOT: $REPO_ROOT"
        echo "BRANCH: $CURRENT_BRANCH"
        echo "FEATURE_DIR: $FEATURE_DIR"
        echo "FEATURE_SPEC: $FEATURE_SPEC"
        echo "IMPL_PLAN: $IMPL_PLAN"
        echo "TASKS: $TASKS"
    fi
    exit 0
fi

# Validate required directories and files
if [[ ! -d "$FEATURE_DIR" ]]; then
    echo "ERROR: Feature directory not found: $FEATURE_DIR" >&2
    echo "Run /speckit.specify first to create the feature structure." >&2
    exit 1
fi

if [[ ! -f "$IMPL_PLAN" ]]; then
    echo "ERROR: plan.md not found in $FEATURE_DIR" >&2
    echo "Run /speckit.plan first to create the implementation plan." >&2
    exit 1
fi

# Check for tasks.md if required
if $REQUIRE_TASKS && [[ ! -f "$TASKS" ]]; then
    echo "ERROR: tasks.md not found in $FEATURE_DIR" >&2
    echo "Run /speckit.tasks first to create the task list." >&2
    exit 1
fi

# Build list of available documents
docs=()

# Always check these optional docs
[[ -f "$RESEARCH" ]] && docs+=("research.md")
[[ -f "$DATA_MODEL" ]] && docs+=("data-model.md")

# Check contracts directory (only if it exists and has files)
if [[ -d "$CONTRACTS_DIR" ]] && [[ -n "$(ls -A "$CONTRACTS_DIR" 2>/dev/null)" ]]; then
    docs+=("contracts/")
fi

[[ -f "$QUICKSTART" ]] && docs+=("quickstart.md")

# Include tasks.md if requested and it exists
if $INCLUDE_TASKS && [[ -f "$TASKS" ]]; then
    docs+=("tasks.md")
fi

# Output results
if $JSON_MODE; then
    # Build JSON array of documents
    if [[ ${#docs[@]} -eq 0 ]]; then
        json_docs="[]"
    else
        json_docs=$(printf '"%s",' "${docs[@]}")
        json_docs="[${json_docs%,}]"
    fi
    
    printf '{"FEATURE_DIR":"%s","AVAILABLE_DOCS":%s}\n' "$FEATURE_DIR" "$json_docs"
else
    # Text output
    echo "FEATURE_DIR:$FEATURE_DIR"
    echo "AVAILABLE_DOCS:"
    
    # Show status of each potential document
    check_file "$RESEARCH" "research.md"
    check_file "$DATA_MODEL" "data-model.md"
    check_dir "$CONTRACTS_DIR" "contracts/"
    check_file "$QUICKSTART" "quickstart.md"
    
    if $INCLUDE_TASKS; then
        check_file "$TASKS" "tasks.md"
    fi
fi
````

## File: .specify/scripts/bash/common.sh
````bash
#!/usr/bin/env bash
# Common functions and variables for all scripts

# Get repository root, with fallback for non-git repositories
get_repo_root() {
    if git rev-parse --show-toplevel >/dev/null 2>&1; then
        git rev-parse --show-toplevel
    else
        # Fall back to script location for non-git repos
        local script_dir="$(CDPATH="" cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
        (cd "$script_dir/../../.." && pwd)
    fi
}

# Get current branch, with fallback for non-git repositories
get_current_branch() {
    # First check if SPECIFY_FEATURE environment variable is set
    if [[ -n "${SPECIFY_FEATURE:-}" ]]; then
        echo "$SPECIFY_FEATURE"
        return
    fi

    # Then check git if available
    if git rev-parse --abbrev-ref HEAD >/dev/null 2>&1; then
        git rev-parse --abbrev-ref HEAD
        return
    fi

    # For non-git repos, try to find the latest feature directory
    local repo_root=$(get_repo_root)
    local specs_dir="$repo_root/specs"

    if [[ -d "$specs_dir" ]]; then
        local latest_feature=""
        local highest=0

        for dir in "$specs_dir"/*; do
            if [[ -d "$dir" ]]; then
                local dirname=$(basename "$dir")
                if [[ "$dirname" =~ ^([0-9]{3})- ]]; then
                    local number=${BASH_REMATCH[1]}
                    number=$((10#$number))
                    if [[ "$number" -gt "$highest" ]]; then
                        highest=$number
                        latest_feature=$dirname
                    fi
                fi
            fi
        done

        if [[ -n "$latest_feature" ]]; then
            echo "$latest_feature"
            return
        fi
    fi

    echo "main"  # Final fallback
}

# Check if we have git available
has_git() {
    git rev-parse --show-toplevel >/dev/null 2>&1
}

check_feature_branch() {
    local branch="$1"
    local has_git_repo="$2"

    # For non-git repos, we can't enforce branch naming but still provide output
    if [[ "$has_git_repo" != "true" ]]; then
        echo "[specify] Warning: Git repository not detected; skipped branch validation" >&2
        return 0
    fi

    if [[ ! "$branch" =~ ^[0-9]{3}- ]]; then
        echo "ERROR: Not on a feature branch. Current branch: $branch" >&2
        echo "Feature branches should be named like: 001-feature-name" >&2
        return 1
    fi

    return 0
}

get_feature_dir() { echo "$1/specs/$2"; }

# Find feature directory by numeric prefix instead of exact branch match
# This allows multiple branches to work on the same spec (e.g., 004-fix-bug, 004-add-feature)
find_feature_dir_by_prefix() {
    local repo_root="$1"
    local branch_name="$2"
    local specs_dir="$repo_root/specs"

    # Extract numeric prefix from branch (e.g., "004" from "004-whatever")
    if [[ ! "$branch_name" =~ ^([0-9]{3})- ]]; then
        # If branch doesn't have numeric prefix, fall back to exact match
        echo "$specs_dir/$branch_name"
        return
    fi

    local prefix="${BASH_REMATCH[1]}"

    # Search for directories in specs/ that start with this prefix
    local matches=()
    if [[ -d "$specs_dir" ]]; then
        for dir in "$specs_dir"/"$prefix"-*; do
            if [[ -d "$dir" ]]; then
                matches+=("$(basename "$dir")")
            fi
        done
    fi

    # Handle results
    if [[ ${#matches[@]} -eq 0 ]]; then
        # No match found - return the branch name path (will fail later with clear error)
        echo "$specs_dir/$branch_name"
    elif [[ ${#matches[@]} -eq 1 ]]; then
        # Exactly one match - perfect!
        echo "$specs_dir/${matches[0]}"
    else
        # Multiple matches - this shouldn't happen with proper naming convention
        echo "ERROR: Multiple spec directories found with prefix '$prefix': ${matches[*]}" >&2
        echo "Please ensure only one spec directory exists per numeric prefix." >&2
        echo "$specs_dir/$branch_name"  # Return something to avoid breaking the script
    fi
}

get_feature_paths() {
    local repo_root=$(get_repo_root)
    local current_branch=$(get_current_branch)
    local has_git_repo="false"

    if has_git; then
        has_git_repo="true"
    fi

    # Use prefix-based lookup to support multiple branches per spec
    local feature_dir=$(find_feature_dir_by_prefix "$repo_root" "$current_branch")

    cat <<EOF
REPO_ROOT='$repo_root'
CURRENT_BRANCH='$current_branch'
HAS_GIT='$has_git_repo'
FEATURE_DIR='$feature_dir'
FEATURE_SPEC='$feature_dir/spec.md'
IMPL_PLAN='$feature_dir/plan.md'
TASKS='$feature_dir/tasks.md'
RESEARCH='$feature_dir/research.md'
DATA_MODEL='$feature_dir/data-model.md'
QUICKSTART='$feature_dir/quickstart.md'
CONTRACTS_DIR='$feature_dir/contracts'
EOF
}

check_file() { [[ -f "$1" ]] && echo "  ‚úì $2" || echo "  ‚úó $2"; }
check_dir() { [[ -d "$1" && -n $(ls -A "$1" 2>/dev/null) ]] && echo "  ‚úì $2" || echo "  ‚úó $2"; }
````

## File: .specify/scripts/bash/create-new-feature.sh
````bash
#!/usr/bin/env bash

set -e

JSON_MODE=false
SHORT_NAME=""
BRANCH_NUMBER=""
ARGS=()
i=1
while [ $i -le $# ]; do
    arg="${!i}"
    case "$arg" in
        --json) 
            JSON_MODE=true 
            ;;
        --short-name)
            if [ $((i + 1)) -gt $# ]; then
                echo 'Error: --short-name requires a value' >&2
                exit 1
            fi
            i=$((i + 1))
            next_arg="${!i}"
            # Check if the next argument is another option (starts with --)
            if [[ "$next_arg" == --* ]]; then
                echo 'Error: --short-name requires a value' >&2
                exit 1
            fi
            SHORT_NAME="$next_arg"
            ;;
        --number)
            if [ $((i + 1)) -gt $# ]; then
                echo 'Error: --number requires a value' >&2
                exit 1
            fi
            i=$((i + 1))
            next_arg="${!i}"
            if [[ "$next_arg" == --* ]]; then
                echo 'Error: --number requires a value' >&2
                exit 1
            fi
            BRANCH_NUMBER="$next_arg"
            ;;
        --help|-h) 
            echo "Usage: $0 [--json] [--short-name <name>] [--number N] <feature_description>"
            echo ""
            echo "Options:"
            echo "  --json              Output in JSON format"
            echo "  --short-name <name> Provide a custom short name (2-4 words) for the branch"
            echo "  --number N          Specify branch number manually (overrides auto-detection)"
            echo "  --help, -h          Show this help message"
            echo ""
            echo "Examples:"
            echo "  $0 'Add user authentication system' --short-name 'user-auth'"
            echo "  $0 'Implement OAuth2 integration for API' --number 5"
            exit 0
            ;;
        *) 
            ARGS+=("$arg") 
            ;;
    esac
    i=$((i + 1))
done

FEATURE_DESCRIPTION="${ARGS[*]}"
if [ -z "$FEATURE_DESCRIPTION" ]; then
    echo "Usage: $0 [--json] [--short-name <name>] [--number N] <feature_description>" >&2
    exit 1
fi

# Function to find the repository root by searching for existing project markers
find_repo_root() {
    local dir="$1"
    while [ "$dir" != "/" ]; do
        if [ -d "$dir/.git" ] || [ -d "$dir/.specify" ]; then
            echo "$dir"
            return 0
        fi
        dir="$(dirname "$dir")"
    done
    return 1
}

# Function to get highest number from specs directory
get_highest_from_specs() {
    local specs_dir="$1"
    local highest=0
    
    if [ -d "$specs_dir" ]; then
        for dir in "$specs_dir"/*; do
            [ -d "$dir" ] || continue
            dirname=$(basename "$dir")
            number=$(echo "$dirname" | grep -o '^[0-9]\+' || echo "0")
            number=$((10#$number))
            if [ "$number" -gt "$highest" ]; then
                highest=$number
            fi
        done
    fi
    
    echo "$highest"
}

# Function to get highest number from git branches
get_highest_from_branches() {
    local highest=0
    
    # Get all branches (local and remote)
    branches=$(git branch -a 2>/dev/null || echo "")
    
    if [ -n "$branches" ]; then
        while IFS= read -r branch; do
            # Clean branch name: remove leading markers and remote prefixes
            clean_branch=$(echo "$branch" | sed 's/^[* ]*//; s|^remotes/[^/]*/||')
            
            # Extract feature number if branch matches pattern ###-*
            if echo "$clean_branch" | grep -q '^[0-9]\{3\}-'; then
                number=$(echo "$clean_branch" | grep -o '^[0-9]\{3\}' || echo "0")
                number=$((10#$number))
                if [ "$number" -gt "$highest" ]; then
                    highest=$number
                fi
            fi
        done <<< "$branches"
    fi
    
    echo "$highest"
}

# Function to check existing branches (local and remote) and return next available number
check_existing_branches() {
    local short_name="$1"
    local specs_dir="$2"
    
    # Fetch all remotes to get latest branch info (suppress errors if no remotes)
    git fetch --all --prune 2>/dev/null || true
    
    # Find all branches matching the pattern using git ls-remote (more reliable)
    local remote_branches=$(git ls-remote --heads origin 2>/dev/null | grep -E "refs/heads/[0-9]+-${short_name}$" | sed 's/.*\/\([0-9]*\)-.*/\1/' | sort -n)
    
    # Also check local branches
    local local_branches=$(git branch 2>/dev/null | grep -E "^[* ]*[0-9]+-${short_name}$" | sed 's/^[* ]*//' | sed 's/-.*//' | sort -n)
    
    # Check specs directory as well
    local spec_dirs=""
    if [ -d "$specs_dir" ]; then
        spec_dirs=$(find "$specs_dir" -maxdepth 1 -type d -name "[0-9]*-${short_name}" 2>/dev/null | xargs -n1 basename 2>/dev/null | sed 's/-.*//' | sort -n)
    fi
    
    # Combine all sources and get the highest number
    local max_num=0
    for num in $remote_branches $local_branches $spec_dirs; do
        if [ "$num" -gt "$max_num" ]; then
            max_num=$num
        fi
    done
    
    # Return next number
    echo $((max_num + 1))
}

# Function to clean and format a branch name
clean_branch_name() {
    local name="$1"
    echo "$name" | tr '[:upper:]' '[:lower:]' | sed 's/[^a-z0-9]/-/g' | sed 's/-\+/-/g' | sed 's/^-//' | sed 's/-$//'
}

# Resolve repository root. Prefer git information when available, but fall back
# to searching for repository markers so the workflow still functions in repositories that
# were initialised with --no-git.
SCRIPT_DIR="$(CDPATH="" cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"

if git rev-parse --show-toplevel >/dev/null 2>&1; then
    REPO_ROOT=$(git rev-parse --show-toplevel)
    HAS_GIT=true
else
    REPO_ROOT="$(find_repo_root "$SCRIPT_DIR")"
    if [ -z "$REPO_ROOT" ]; then
        echo "Error: Could not determine repository root. Please run this script from within the repository." >&2
        exit 1
    fi
    HAS_GIT=false
fi

cd "$REPO_ROOT"

SPECS_DIR="$REPO_ROOT/specs"
mkdir -p "$SPECS_DIR"

# Function to generate branch name with stop word filtering and length filtering
generate_branch_name() {
    local description="$1"
    
    # Common stop words to filter out
    local stop_words="^(i|a|an|the|to|for|of|in|on|at|by|with|from|is|are|was|were|be|been|being|have|has|had|do|does|did|will|would|should|could|can|may|might|must|shall|this|that|these|those|my|your|our|their|want|need|add|get|set)$"
    
    # Convert to lowercase and split into words
    local clean_name=$(echo "$description" | tr '[:upper:]' '[:lower:]' | sed 's/[^a-z0-9]/ /g')
    
    # Filter words: remove stop words and words shorter than 3 chars (unless they're uppercase acronyms in original)
    local meaningful_words=()
    for word in $clean_name; do
        # Skip empty words
        [ -z "$word" ] && continue
        
        # Keep words that are NOT stop words AND (length >= 3 OR are potential acronyms)
        if ! echo "$word" | grep -qiE "$stop_words"; then
            if [ ${#word} -ge 3 ]; then
                meaningful_words+=("$word")
            elif echo "$description" | grep -q "\b${word^^}\b"; then
                # Keep short words if they appear as uppercase in original (likely acronyms)
                meaningful_words+=("$word")
            fi
        fi
    done
    
    # If we have meaningful words, use first 3-4 of them
    if [ ${#meaningful_words[@]} -gt 0 ]; then
        local max_words=3
        if [ ${#meaningful_words[@]} -eq 4 ]; then max_words=4; fi
        
        local result=""
        local count=0
        for word in "${meaningful_words[@]}"; do
            if [ $count -ge $max_words ]; then break; fi
            if [ -n "$result" ]; then result="$result-"; fi
            result="$result$word"
            count=$((count + 1))
        done
        echo "$result"
    else
        # Fallback to original logic if no meaningful words found
        local cleaned=$(clean_branch_name "$description")
        echo "$cleaned" | tr '-' '\n' | grep -v '^$' | head -3 | tr '\n' '-' | sed 's/-$//'
    fi
}

# Generate branch name
if [ -n "$SHORT_NAME" ]; then
    # Use provided short name, just clean it up
    BRANCH_SUFFIX=$(clean_branch_name "$SHORT_NAME")
else
    # Generate from description with smart filtering
    BRANCH_SUFFIX=$(generate_branch_name "$FEATURE_DESCRIPTION")
fi

# Determine branch number
if [ -z "$BRANCH_NUMBER" ]; then
    if [ "$HAS_GIT" = true ]; then
        # Check existing branches on remotes
        BRANCH_NUMBER=$(check_existing_branches "$BRANCH_SUFFIX" "$SPECS_DIR")
    else
        # Fall back to local directory check
        HIGHEST=$(get_highest_from_specs "$SPECS_DIR")
        BRANCH_NUMBER=$((HIGHEST + 1))
    fi
fi

FEATURE_NUM=$(printf "%03d" "$BRANCH_NUMBER")
BRANCH_NAME="${FEATURE_NUM}-${BRANCH_SUFFIX}"

# GitHub enforces a 244-byte limit on branch names
# Validate and truncate if necessary
MAX_BRANCH_LENGTH=244
if [ ${#BRANCH_NAME} -gt $MAX_BRANCH_LENGTH ]; then
    # Calculate how much we need to trim from suffix
    # Account for: feature number (3) + hyphen (1) = 4 chars
    MAX_SUFFIX_LENGTH=$((MAX_BRANCH_LENGTH - 4))
    
    # Truncate suffix at word boundary if possible
    TRUNCATED_SUFFIX=$(echo "$BRANCH_SUFFIX" | cut -c1-$MAX_SUFFIX_LENGTH)
    # Remove trailing hyphen if truncation created one
    TRUNCATED_SUFFIX=$(echo "$TRUNCATED_SUFFIX" | sed 's/-$//')
    
    ORIGINAL_BRANCH_NAME="$BRANCH_NAME"
    BRANCH_NAME="${FEATURE_NUM}-${TRUNCATED_SUFFIX}"
    
    >&2 echo "[specify] Warning: Branch name exceeded GitHub's 244-byte limit"
    >&2 echo "[specify] Original: $ORIGINAL_BRANCH_NAME (${#ORIGINAL_BRANCH_NAME} bytes)"
    >&2 echo "[specify] Truncated to: $BRANCH_NAME (${#BRANCH_NAME} bytes)"
fi

if [ "$HAS_GIT" = true ]; then
    git checkout -b "$BRANCH_NAME"
else
    >&2 echo "[specify] Warning: Git repository not detected; skipped branch creation for $BRANCH_NAME"
fi

FEATURE_DIR="$SPECS_DIR/$BRANCH_NAME"
mkdir -p "$FEATURE_DIR"

TEMPLATE="$REPO_ROOT/.specify/templates/spec-template.md"
SPEC_FILE="$FEATURE_DIR/spec.md"
if [ -f "$TEMPLATE" ]; then cp "$TEMPLATE" "$SPEC_FILE"; else touch "$SPEC_FILE"; fi

# Set the SPECIFY_FEATURE environment variable for the current session
export SPECIFY_FEATURE="$BRANCH_NAME"

if $JSON_MODE; then
    printf '{"BRANCH_NAME":"%s","SPEC_FILE":"%s","FEATURE_NUM":"%s"}\n' "$BRANCH_NAME" "$SPEC_FILE" "$FEATURE_NUM"
else
    echo "BRANCH_NAME: $BRANCH_NAME"
    echo "SPEC_FILE: $SPEC_FILE"
    echo "FEATURE_NUM: $FEATURE_NUM"
    echo "SPECIFY_FEATURE environment variable set to: $BRANCH_NAME"
fi
````

## File: .specify/scripts/bash/setup-plan.sh
````bash
#!/usr/bin/env bash

set -e

# Parse command line arguments
JSON_MODE=false
ARGS=()

for arg in "$@"; do
    case "$arg" in
        --json) 
            JSON_MODE=true 
            ;;
        --help|-h) 
            echo "Usage: $0 [--json]"
            echo "  --json    Output results in JSON format"
            echo "  --help    Show this help message"
            exit 0 
            ;;
        *) 
            ARGS+=("$arg") 
            ;;
    esac
done

# Get script directory and load common functions
SCRIPT_DIR="$(CDPATH="" cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
source "$SCRIPT_DIR/common.sh"

# Get all paths and variables from common functions
eval $(get_feature_paths)

# Check if we're on a proper feature branch (only for git repos)
check_feature_branch "$CURRENT_BRANCH" "$HAS_GIT" || exit 1

# Ensure the feature directory exists
mkdir -p "$FEATURE_DIR"

# Copy plan template if it exists
TEMPLATE="$REPO_ROOT/.specify/templates/plan-template.md"
if [[ -f "$TEMPLATE" ]]; then
    cp "$TEMPLATE" "$IMPL_PLAN"
    echo "Copied plan template to $IMPL_PLAN"
else
    echo "Warning: Plan template not found at $TEMPLATE"
    # Create a basic plan file if template doesn't exist
    touch "$IMPL_PLAN"
fi

# Output results
if $JSON_MODE; then
    printf '{"FEATURE_SPEC":"%s","IMPL_PLAN":"%s","SPECS_DIR":"%s","BRANCH":"%s","HAS_GIT":"%s"}\n' \
        "$FEATURE_SPEC" "$IMPL_PLAN" "$FEATURE_DIR" "$CURRENT_BRANCH" "$HAS_GIT"
else
    echo "FEATURE_SPEC: $FEATURE_SPEC"
    echo "IMPL_PLAN: $IMPL_PLAN" 
    echo "SPECS_DIR: $FEATURE_DIR"
    echo "BRANCH: $CURRENT_BRANCH"
    echo "HAS_GIT: $HAS_GIT"
fi
````

## File: .specify/scripts/bash/update-agent-context.sh
````bash
#!/usr/bin/env bash

# Update agent context files with information from plan.md
#
# This script maintains AI agent context files by parsing feature specifications 
# and updating agent-specific configuration files with project information.
#
# MAIN FUNCTIONS:
# 1. Environment Validation
#    - Verifies git repository structure and branch information
#    - Checks for required plan.md files and templates
#    - Validates file permissions and accessibility
#
# 2. Plan Data Extraction
#    - Parses plan.md files to extract project metadata
#    - Identifies language/version, frameworks, databases, and project types
#    - Handles missing or incomplete specification data gracefully
#
# 3. Agent File Management
#    - Creates new agent context files from templates when needed
#    - Updates existing agent files with new project information
#    - Preserves manual additions and custom configurations
#    - Supports multiple AI agent formats and directory structures
#
# 4. Content Generation
#    - Generates language-specific build/test commands
#    - Creates appropriate project directory structures
#    - Updates technology stacks and recent changes sections
#    - Maintains consistent formatting and timestamps
#
# 5. Multi-Agent Support
#    - Handles agent-specific file paths and naming conventions
#    - Supports: Claude, Gemini, Copilot, Cursor, Qwen, opencode, Codex, Windsurf, Kilo Code, Auggie CLI, Roo Code, CodeBuddy CLI, Amp, SHAI, or Amazon Q Developer CLI
#    - Can update single agents or all existing agent files
#    - Creates default Claude file if no agent files exist
#
# Usage: ./update-agent-context.sh [agent_type]
# Agent types: claude|gemini|copilot|cursor-agent|qwen|opencode|codex|windsurf|kilocode|auggie|shai|q
# Leave empty to update all existing agent files

set -e

# Enable strict error handling
set -u
set -o pipefail

#==============================================================================
# Configuration and Global Variables
#==============================================================================

# Get script directory and load common functions
SCRIPT_DIR="$(CDPATH="" cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
source "$SCRIPT_DIR/common.sh"

# Get all paths and variables from common functions
eval $(get_feature_paths)

NEW_PLAN="$IMPL_PLAN"  # Alias for compatibility with existing code
AGENT_TYPE="${1:-}"

# Agent-specific file paths  
CLAUDE_FILE="$REPO_ROOT/CLAUDE.md"
GEMINI_FILE="$REPO_ROOT/GEMINI.md"
COPILOT_FILE="$REPO_ROOT/.github/agents/copilot-instructions.md"
CURSOR_FILE="$REPO_ROOT/.cursor/rules/specify-rules.mdc"
QWEN_FILE="$REPO_ROOT/QWEN.md"
AGENTS_FILE="$REPO_ROOT/AGENTS.md"
WINDSURF_FILE="$REPO_ROOT/.windsurf/rules/specify-rules.md"
KILOCODE_FILE="$REPO_ROOT/.kilocode/rules/specify-rules.md"
AUGGIE_FILE="$REPO_ROOT/.augment/rules/specify-rules.md"
ROO_FILE="$REPO_ROOT/.roo/rules/specify-rules.md"
CODEBUDDY_FILE="$REPO_ROOT/CODEBUDDY.md"
AMP_FILE="$REPO_ROOT/AGENTS.md"
SHAI_FILE="$REPO_ROOT/SHAI.md"
Q_FILE="$REPO_ROOT/AGENTS.md"

# Template file
TEMPLATE_FILE="$REPO_ROOT/.specify/templates/agent-file-template.md"

# Global variables for parsed plan data
NEW_LANG=""
NEW_FRAMEWORK=""
NEW_DB=""
NEW_PROJECT_TYPE=""

#==============================================================================
# Utility Functions
#==============================================================================

log_info() {
    echo "INFO: $1"
}

log_success() {
    echo "‚úì $1"
}

log_error() {
    echo "ERROR: $1" >&2
}

log_warning() {
    echo "WARNING: $1" >&2
}

# Cleanup function for temporary files
cleanup() {
    local exit_code=$?
    rm -f /tmp/agent_update_*_$$
    rm -f /tmp/manual_additions_$$
    exit $exit_code
}

# Set up cleanup trap
trap cleanup EXIT INT TERM

#==============================================================================
# Validation Functions
#==============================================================================

validate_environment() {
    # Check if we have a current branch/feature (git or non-git)
    if [[ -z "$CURRENT_BRANCH" ]]; then
        log_error "Unable to determine current feature"
        if [[ "$HAS_GIT" == "true" ]]; then
            log_info "Make sure you're on a feature branch"
        else
            log_info "Set SPECIFY_FEATURE environment variable or create a feature first"
        fi
        exit 1
    fi
    
    # Check if plan.md exists
    if [[ ! -f "$NEW_PLAN" ]]; then
        log_error "No plan.md found at $NEW_PLAN"
        log_info "Make sure you're working on a feature with a corresponding spec directory"
        if [[ "$HAS_GIT" != "true" ]]; then
            log_info "Use: export SPECIFY_FEATURE=your-feature-name or create a new feature first"
        fi
        exit 1
    fi
    
    # Check if template exists (needed for new files)
    if [[ ! -f "$TEMPLATE_FILE" ]]; then
        log_warning "Template file not found at $TEMPLATE_FILE"
        log_warning "Creating new agent files will fail"
    fi
}

#==============================================================================
# Plan Parsing Functions
#==============================================================================

extract_plan_field() {
    local field_pattern="$1"
    local plan_file="$2"
    
    grep "^\*\*${field_pattern}\*\*: " "$plan_file" 2>/dev/null | \
        head -1 | \
        sed "s|^\*\*${field_pattern}\*\*: ||" | \
        sed 's/^[ \t]*//;s/[ \t]*$//' | \
        grep -v "NEEDS CLARIFICATION" | \
        grep -v "^N/A$" || echo ""
}

parse_plan_data() {
    local plan_file="$1"
    
    if [[ ! -f "$plan_file" ]]; then
        log_error "Plan file not found: $plan_file"
        return 1
    fi
    
    if [[ ! -r "$plan_file" ]]; then
        log_error "Plan file is not readable: $plan_file"
        return 1
    fi
    
    log_info "Parsing plan data from $plan_file"
    
    NEW_LANG=$(extract_plan_field "Language/Version" "$plan_file")
    NEW_FRAMEWORK=$(extract_plan_field "Primary Dependencies" "$plan_file")
    NEW_DB=$(extract_plan_field "Storage" "$plan_file")
    NEW_PROJECT_TYPE=$(extract_plan_field "Project Type" "$plan_file")
    
    # Log what we found
    if [[ -n "$NEW_LANG" ]]; then
        log_info "Found language: $NEW_LANG"
    else
        log_warning "No language information found in plan"
    fi
    
    if [[ -n "$NEW_FRAMEWORK" ]]; then
        log_info "Found framework: $NEW_FRAMEWORK"
    fi
    
    if [[ -n "$NEW_DB" ]] && [[ "$NEW_DB" != "N/A" ]]; then
        log_info "Found database: $NEW_DB"
    fi
    
    if [[ -n "$NEW_PROJECT_TYPE" ]]; then
        log_info "Found project type: $NEW_PROJECT_TYPE"
    fi
}

format_technology_stack() {
    local lang="$1"
    local framework="$2"
    local parts=()
    
    # Add non-empty parts
    [[ -n "$lang" && "$lang" != "NEEDS CLARIFICATION" ]] && parts+=("$lang")
    [[ -n "$framework" && "$framework" != "NEEDS CLARIFICATION" && "$framework" != "N/A" ]] && parts+=("$framework")
    
    # Join with proper formatting
    if [[ ${#parts[@]} -eq 0 ]]; then
        echo ""
    elif [[ ${#parts[@]} -eq 1 ]]; then
        echo "${parts[0]}"
    else
        # Join multiple parts with " + "
        local result="${parts[0]}"
        for ((i=1; i<${#parts[@]}; i++)); do
            result="$result + ${parts[i]}"
        done
        echo "$result"
    fi
}

#==============================================================================
# Template and Content Generation Functions
#==============================================================================

get_project_structure() {
    local project_type="$1"
    
    if [[ "$project_type" == *"web"* ]]; then
        echo "backend/\\nfrontend/\\ntests/"
    else
        echo "src/\\ntests/"
    fi
}

get_commands_for_language() {
    local lang="$1"
    
    case "$lang" in
        *"Python"*)
            echo "cd src && pytest && ruff check ."
            ;;
        *"Rust"*)
            echo "cargo test && cargo clippy"
            ;;
        *"JavaScript"*|*"TypeScript"*)
            echo "npm test \\&\\& npm run lint"
            ;;
        *)
            echo "# Add commands for $lang"
            ;;
    esac
}

get_language_conventions() {
    local lang="$1"
    echo "$lang: Follow standard conventions"
}

create_new_agent_file() {
    local target_file="$1"
    local temp_file="$2"
    local project_name="$3"
    local current_date="$4"
    
    if [[ ! -f "$TEMPLATE_FILE" ]]; then
        log_error "Template not found at $TEMPLATE_FILE"
        return 1
    fi
    
    if [[ ! -r "$TEMPLATE_FILE" ]]; then
        log_error "Template file is not readable: $TEMPLATE_FILE"
        return 1
    fi
    
    log_info "Creating new agent context file from template..."
    
    if ! cp "$TEMPLATE_FILE" "$temp_file"; then
        log_error "Failed to copy template file"
        return 1
    fi
    
    # Replace template placeholders
    local project_structure
    project_structure=$(get_project_structure "$NEW_PROJECT_TYPE")
    
    local commands
    commands=$(get_commands_for_language "$NEW_LANG")
    
    local language_conventions
    language_conventions=$(get_language_conventions "$NEW_LANG")
    
    # Perform substitutions with error checking using safer approach
    # Escape special characters for sed by using a different delimiter or escaping
    local escaped_lang=$(printf '%s\n' "$NEW_LANG" | sed 's/[\[\.*^$()+{}|]/\\&/g')
    local escaped_framework=$(printf '%s\n' "$NEW_FRAMEWORK" | sed 's/[\[\.*^$()+{}|]/\\&/g')
    local escaped_branch=$(printf '%s\n' "$CURRENT_BRANCH" | sed 's/[\[\.*^$()+{}|]/\\&/g')
    
    # Build technology stack and recent change strings conditionally
    local tech_stack
    if [[ -n "$escaped_lang" && -n "$escaped_framework" ]]; then
        tech_stack="- $escaped_lang + $escaped_framework ($escaped_branch)"
    elif [[ -n "$escaped_lang" ]]; then
        tech_stack="- $escaped_lang ($escaped_branch)"
    elif [[ -n "$escaped_framework" ]]; then
        tech_stack="- $escaped_framework ($escaped_branch)"
    else
        tech_stack="- ($escaped_branch)"
    fi

    local recent_change
    if [[ -n "$escaped_lang" && -n "$escaped_framework" ]]; then
        recent_change="- $escaped_branch: Added $escaped_lang + $escaped_framework"
    elif [[ -n "$escaped_lang" ]]; then
        recent_change="- $escaped_branch: Added $escaped_lang"
    elif [[ -n "$escaped_framework" ]]; then
        recent_change="- $escaped_branch: Added $escaped_framework"
    else
        recent_change="- $escaped_branch: Added"
    fi

    local substitutions=(
        "s|\[PROJECT NAME\]|$project_name|"
        "s|\[DATE\]|$current_date|"
        "s|\[EXTRACTED FROM ALL PLAN.MD FILES\]|$tech_stack|"
        "s|\[ACTUAL STRUCTURE FROM PLANS\]|$project_structure|g"
        "s|\[ONLY COMMANDS FOR ACTIVE TECHNOLOGIES\]|$commands|"
        "s|\[LANGUAGE-SPECIFIC, ONLY FOR LANGUAGES IN USE\]|$language_conventions|"
        "s|\[LAST 3 FEATURES AND WHAT THEY ADDED\]|$recent_change|"
    )
    
    for substitution in "${substitutions[@]}"; do
        if ! sed -i.bak -e "$substitution" "$temp_file"; then
            log_error "Failed to perform substitution: $substitution"
            rm -f "$temp_file" "$temp_file.bak"
            return 1
        fi
    done
    
    # Convert \n sequences to actual newlines
    newline=$(printf '\n')
    sed -i.bak2 "s/\\\\n/${newline}/g" "$temp_file"
    
    # Clean up backup files
    rm -f "$temp_file.bak" "$temp_file.bak2"
    
    return 0
}




update_existing_agent_file() {
    local target_file="$1"
    local current_date="$2"
    
    log_info "Updating existing agent context file..."
    
    # Use a single temporary file for atomic update
    local temp_file
    temp_file=$(mktemp) || {
        log_error "Failed to create temporary file"
        return 1
    }
    
    # Process the file in one pass
    local tech_stack=$(format_technology_stack "$NEW_LANG" "$NEW_FRAMEWORK")
    local new_tech_entries=()
    local new_change_entry=""
    
    # Prepare new technology entries
    if [[ -n "$tech_stack" ]] && ! grep -q "$tech_stack" "$target_file"; then
        new_tech_entries+=("- $tech_stack ($CURRENT_BRANCH)")
    fi
    
    if [[ -n "$NEW_DB" ]] && [[ "$NEW_DB" != "N/A" ]] && [[ "$NEW_DB" != "NEEDS CLARIFICATION" ]] && ! grep -q "$NEW_DB" "$target_file"; then
        new_tech_entries+=("- $NEW_DB ($CURRENT_BRANCH)")
    fi
    
    # Prepare new change entry
    if [[ -n "$tech_stack" ]]; then
        new_change_entry="- $CURRENT_BRANCH: Added $tech_stack"
    elif [[ -n "$NEW_DB" ]] && [[ "$NEW_DB" != "N/A" ]] && [[ "$NEW_DB" != "NEEDS CLARIFICATION" ]]; then
        new_change_entry="- $CURRENT_BRANCH: Added $NEW_DB"
    fi
    
    # Check if sections exist in the file
    local has_active_technologies=0
    local has_recent_changes=0
    
    if grep -q "^## Active Technologies" "$target_file" 2>/dev/null; then
        has_active_technologies=1
    fi
    
    if grep -q "^## Recent Changes" "$target_file" 2>/dev/null; then
        has_recent_changes=1
    fi
    
    # Process file line by line
    local in_tech_section=false
    local in_changes_section=false
    local tech_entries_added=false
    local changes_entries_added=false
    local existing_changes_count=0
    local file_ended=false
    
    while IFS= read -r line || [[ -n "$line" ]]; do
        # Handle Active Technologies section
        if [[ "$line" == "## Active Technologies" ]]; then
            echo "$line" >> "$temp_file"
            in_tech_section=true
            continue
        elif [[ $in_tech_section == true ]] && [[ "$line" =~ ^##[[:space:]] ]]; then
            # Add new tech entries before closing the section
            if [[ $tech_entries_added == false ]] && [[ ${#new_tech_entries[@]} -gt 0 ]]; then
                printf '%s\n' "${new_tech_entries[@]}" >> "$temp_file"
                tech_entries_added=true
            fi
            echo "$line" >> "$temp_file"
            in_tech_section=false
            continue
        elif [[ $in_tech_section == true ]] && [[ -z "$line" ]]; then
            # Add new tech entries before empty line in tech section
            if [[ $tech_entries_added == false ]] && [[ ${#new_tech_entries[@]} -gt 0 ]]; then
                printf '%s\n' "${new_tech_entries[@]}" >> "$temp_file"
                tech_entries_added=true
            fi
            echo "$line" >> "$temp_file"
            continue
        fi
        
        # Handle Recent Changes section
        if [[ "$line" == "## Recent Changes" ]]; then
            echo "$line" >> "$temp_file"
            # Add new change entry right after the heading
            if [[ -n "$new_change_entry" ]]; then
                echo "$new_change_entry" >> "$temp_file"
            fi
            in_changes_section=true
            changes_entries_added=true
            continue
        elif [[ $in_changes_section == true ]] && [[ "$line" =~ ^##[[:space:]] ]]; then
            echo "$line" >> "$temp_file"
            in_changes_section=false
            continue
        elif [[ $in_changes_section == true ]] && [[ "$line" == "- "* ]]; then
            # Keep only first 2 existing changes
            if [[ $existing_changes_count -lt 2 ]]; then
                echo "$line" >> "$temp_file"
                ((existing_changes_count++))
            fi
            continue
        fi
        
        # Update timestamp
        if [[ "$line" =~ \*\*Last\ updated\*\*:.*[0-9][0-9][0-9][0-9]-[0-9][0-9]-[0-9][0-9] ]]; then
            echo "$line" | sed "s/[0-9][0-9][0-9][0-9]-[0-9][0-9]-[0-9][0-9]/$current_date/" >> "$temp_file"
        else
            echo "$line" >> "$temp_file"
        fi
    done < "$target_file"
    
    # Post-loop check: if we're still in the Active Technologies section and haven't added new entries
    if [[ $in_tech_section == true ]] && [[ $tech_entries_added == false ]] && [[ ${#new_tech_entries[@]} -gt 0 ]]; then
        printf '%s\n' "${new_tech_entries[@]}" >> "$temp_file"
        tech_entries_added=true
    fi
    
    # If sections don't exist, add them at the end of the file
    if [[ $has_active_technologies -eq 0 ]] && [[ ${#new_tech_entries[@]} -gt 0 ]]; then
        echo "" >> "$temp_file"
        echo "## Active Technologies" >> "$temp_file"
        printf '%s\n' "${new_tech_entries[@]}" >> "$temp_file"
        tech_entries_added=true
    fi
    
    if [[ $has_recent_changes -eq 0 ]] && [[ -n "$new_change_entry" ]]; then
        echo "" >> "$temp_file"
        echo "## Recent Changes" >> "$temp_file"
        echo "$new_change_entry" >> "$temp_file"
        changes_entries_added=true
    fi
    
    # Move temp file to target atomically
    if ! mv "$temp_file" "$target_file"; then
        log_error "Failed to update target file"
        rm -f "$temp_file"
        return 1
    fi
    
    return 0
}
#==============================================================================
# Main Agent File Update Function
#==============================================================================

update_agent_file() {
    local target_file="$1"
    local agent_name="$2"
    
    if [[ -z "$target_file" ]] || [[ -z "$agent_name" ]]; then
        log_error "update_agent_file requires target_file and agent_name parameters"
        return 1
    fi
    
    log_info "Updating $agent_name context file: $target_file"
    
    local project_name
    project_name=$(basename "$REPO_ROOT")
    local current_date
    current_date=$(date +%Y-%m-%d)
    
    # Create directory if it doesn't exist
    local target_dir
    target_dir=$(dirname "$target_file")
    if [[ ! -d "$target_dir" ]]; then
        if ! mkdir -p "$target_dir"; then
            log_error "Failed to create directory: $target_dir"
            return 1
        fi
    fi
    
    if [[ ! -f "$target_file" ]]; then
        # Create new file from template
        local temp_file
        temp_file=$(mktemp) || {
            log_error "Failed to create temporary file"
            return 1
        }
        
        if create_new_agent_file "$target_file" "$temp_file" "$project_name" "$current_date"; then
            if mv "$temp_file" "$target_file"; then
                log_success "Created new $agent_name context file"
            else
                log_error "Failed to move temporary file to $target_file"
                rm -f "$temp_file"
                return 1
            fi
        else
            log_error "Failed to create new agent file"
            rm -f "$temp_file"
            return 1
        fi
    else
        # Update existing file
        if [[ ! -r "$target_file" ]]; then
            log_error "Cannot read existing file: $target_file"
            return 1
        fi
        
        if [[ ! -w "$target_file" ]]; then
            log_error "Cannot write to existing file: $target_file"
            return 1
        fi
        
        if update_existing_agent_file "$target_file" "$current_date"; then
            log_success "Updated existing $agent_name context file"
        else
            log_error "Failed to update existing agent file"
            return 1
        fi
    fi
    
    return 0
}

#==============================================================================
# Agent Selection and Processing
#==============================================================================

update_specific_agent() {
    local agent_type="$1"
    
    case "$agent_type" in
        claude)
            update_agent_file "$CLAUDE_FILE" "Claude Code"
            ;;
        gemini)
            update_agent_file "$GEMINI_FILE" "Gemini CLI"
            ;;
        copilot)
            update_agent_file "$COPILOT_FILE" "GitHub Copilot"
            ;;
        cursor-agent)
            update_agent_file "$CURSOR_FILE" "Cursor IDE"
            ;;
        qwen)
            update_agent_file "$QWEN_FILE" "Qwen Code"
            ;;
        opencode)
            update_agent_file "$AGENTS_FILE" "opencode"
            ;;
        codex)
            update_agent_file "$AGENTS_FILE" "Codex CLI"
            ;;
        windsurf)
            update_agent_file "$WINDSURF_FILE" "Windsurf"
            ;;
        kilocode)
            update_agent_file "$KILOCODE_FILE" "Kilo Code"
            ;;
        auggie)
            update_agent_file "$AUGGIE_FILE" "Auggie CLI"
            ;;
        roo)
            update_agent_file "$ROO_FILE" "Roo Code"
            ;;
        codebuddy)
            update_agent_file "$CODEBUDDY_FILE" "CodeBuddy CLI"
            ;;
        amp)
            update_agent_file "$AMP_FILE" "Amp"
            ;;
        shai)
            update_agent_file "$SHAI_FILE" "SHAI"
            ;;
        q)
            update_agent_file "$Q_FILE" "Amazon Q Developer CLI"
            ;;
        *)
            log_error "Unknown agent type '$agent_type'"
            log_error "Expected: claude|gemini|copilot|cursor-agent|qwen|opencode|codex|windsurf|kilocode|auggie|roo|amp|shai|q"
            exit 1
            ;;
    esac
}

update_all_existing_agents() {
    local found_agent=false
    
    # Check each possible agent file and update if it exists
    if [[ -f "$CLAUDE_FILE" ]]; then
        update_agent_file "$CLAUDE_FILE" "Claude Code"
        found_agent=true
    fi
    
    if [[ -f "$GEMINI_FILE" ]]; then
        update_agent_file "$GEMINI_FILE" "Gemini CLI"
        found_agent=true
    fi
    
    if [[ -f "$COPILOT_FILE" ]]; then
        update_agent_file "$COPILOT_FILE" "GitHub Copilot"
        found_agent=true
    fi
    
    if [[ -f "$CURSOR_FILE" ]]; then
        update_agent_file "$CURSOR_FILE" "Cursor IDE"
        found_agent=true
    fi
    
    if [[ -f "$QWEN_FILE" ]]; then
        update_agent_file "$QWEN_FILE" "Qwen Code"
        found_agent=true
    fi
    
    if [[ -f "$AGENTS_FILE" ]]; then
        update_agent_file "$AGENTS_FILE" "Codex/opencode"
        found_agent=true
    fi
    
    if [[ -f "$WINDSURF_FILE" ]]; then
        update_agent_file "$WINDSURF_FILE" "Windsurf"
        found_agent=true
    fi
    
    if [[ -f "$KILOCODE_FILE" ]]; then
        update_agent_file "$KILOCODE_FILE" "Kilo Code"
        found_agent=true
    fi

    if [[ -f "$AUGGIE_FILE" ]]; then
        update_agent_file "$AUGGIE_FILE" "Auggie CLI"
        found_agent=true
    fi
    
    if [[ -f "$ROO_FILE" ]]; then
        update_agent_file "$ROO_FILE" "Roo Code"
        found_agent=true
    fi

    if [[ -f "$CODEBUDDY_FILE" ]]; then
        update_agent_file "$CODEBUDDY_FILE" "CodeBuddy CLI"
        found_agent=true
    fi

    if [[ -f "$SHAI_FILE" ]]; then
        update_agent_file "$SHAI_FILE" "SHAI"
        found_agent=true
    fi

    if [[ -f "$Q_FILE" ]]; then
        update_agent_file "$Q_FILE" "Amazon Q Developer CLI"
        found_agent=true
    fi
    
    # If no agent files exist, create a default Claude file
    if [[ "$found_agent" == false ]]; then
        log_info "No existing agent files found, creating default Claude file..."
        update_agent_file "$CLAUDE_FILE" "Claude Code"
    fi
}
print_summary() {
    echo
    log_info "Summary of changes:"
    
    if [[ -n "$NEW_LANG" ]]; then
        echo "  - Added language: $NEW_LANG"
    fi
    
    if [[ -n "$NEW_FRAMEWORK" ]]; then
        echo "  - Added framework: $NEW_FRAMEWORK"
    fi
    
    if [[ -n "$NEW_DB" ]] && [[ "$NEW_DB" != "N/A" ]]; then
        echo "  - Added database: $NEW_DB"
    fi
    
    echo

    log_info "Usage: $0 [claude|gemini|copilot|cursor-agent|qwen|opencode|codex|windsurf|kilocode|auggie|codebuddy|shai|q]"
}

#==============================================================================
# Main Execution
#==============================================================================

main() {
    # Validate environment before proceeding
    validate_environment
    
    log_info "=== Updating agent context files for feature $CURRENT_BRANCH ==="
    
    # Parse the plan file to extract project information
    if ! parse_plan_data "$NEW_PLAN"; then
        log_error "Failed to parse plan data"
        exit 1
    fi
    
    # Process based on agent type argument
    local success=true
    
    if [[ -z "$AGENT_TYPE" ]]; then
        # No specific agent provided - update all existing agent files
        log_info "No agent specified, updating all existing agent files..."
        if ! update_all_existing_agents; then
            success=false
        fi
    else
        # Specific agent provided - update only that agent
        log_info "Updating specific agent: $AGENT_TYPE"
        if ! update_specific_agent "$AGENT_TYPE"; then
            success=false
        fi
    fi
    
    # Print summary
    print_summary
    
    if [[ "$success" == true ]]; then
        log_success "Agent context update completed successfully"
        exit 0
    else
        log_error "Agent context update completed with errors"
        exit 1
    fi
}

# Execute main function if script is run directly
if [[ "${BASH_SOURCE[0]}" == "${0}" ]]; then
    main "$@"
fi
````

## File: .specify/templates/agent-file-template.md
````markdown
# [PROJECT NAME] Development Guidelines

Auto-generated from all feature plans. Last updated: [DATE]

## Active Technologies

[EXTRACTED FROM ALL PLAN.MD FILES]

## Project Structure

```text
[ACTUAL STRUCTURE FROM PLANS]
```

## Commands

[ONLY COMMANDS FOR ACTIVE TECHNOLOGIES]

## Code Style

[LANGUAGE-SPECIFIC, ONLY FOR LANGUAGES IN USE]

## Recent Changes

[LAST 3 FEATURES AND WHAT THEY ADDED]

<!-- MANUAL ADDITIONS START -->
<!-- MANUAL ADDITIONS END -->
````

## File: .specify/templates/checklist-template.md
````markdown
# [CHECKLIST TYPE] Checklist: [FEATURE NAME]

**Purpose**: [Brief description of what this checklist covers]
**Created**: [DATE]
**Feature**: [Link to spec.md or relevant documentation]

**Note**: This checklist is generated by the `/speckit.checklist` command based on feature context and requirements.

<!-- 
  ============================================================================
  IMPORTANT: The checklist items below are SAMPLE ITEMS for illustration only.
  
  The /speckit.checklist command MUST replace these with actual items based on:
  - User's specific checklist request
  - Feature requirements from spec.md
  - Technical context from plan.md
  - Implementation details from tasks.md
  
  DO NOT keep these sample items in the generated checklist file.
  ============================================================================
-->

## [Category 1]

- [ ] CHK001 First checklist item with clear action
- [ ] CHK002 Second checklist item
- [ ] CHK003 Third checklist item

## [Category 2]

- [ ] CHK004 Another category item
- [ ] CHK005 Item with specific criteria
- [ ] CHK006 Final item in this category

## Notes

- Check items off as completed: `[x]`
- Add comments or findings inline
- Link to relevant resources or documentation
- Items are numbered sequentially for easy reference
````

## File: .specify/templates/plan-template.md
````markdown
# Implementation Plan: [FEATURE]

**Branch**: `[###-feature-name]` | **Date**: [DATE] | **Spec**: [link]
**Input**: Feature specification from `/specs/[###-feature-name]/spec.md`

**Note**: This template is filled in by the `/speckit.plan` command. See `.specify/templates/commands/plan.md` for the execution workflow.

## Summary

[Extract from feature spec: primary requirement + technical approach from research]

## Technical Context

<!--
  ACTION REQUIRED: Replace the content in this section with the technical details
  for the project. The structure here is presented in advisory capacity to guide
  the iteration process.
-->

**Language/Version**: [e.g., Python 3.11, Swift 5.9, Rust 1.75 or NEEDS CLARIFICATION]  
**Primary Dependencies**: [e.g., FastAPI, UIKit, LLVM or NEEDS CLARIFICATION]  
**Storage**: [if applicable, e.g., PostgreSQL, CoreData, files or N/A]  
**Testing**: [e.g., pytest, XCTest, cargo test or NEEDS CLARIFICATION]  
**Target Platform**: [e.g., Linux server, iOS 15+, WASM or NEEDS CLARIFICATION]
**Project Type**: [single/web/mobile - determines source structure]  
**Performance Goals**: [domain-specific, e.g., 1000 req/s, 10k lines/sec, 60 fps or NEEDS CLARIFICATION]  
**Constraints**: [domain-specific, e.g., <200ms p95, <100MB memory, offline-capable or NEEDS CLARIFICATION]  
**Scale/Scope**: [domain-specific, e.g., 10k users, 1M LOC, 50 screens or NEEDS CLARIFICATION]

## Constitution Check

*GATE: Must pass before Phase 0 research. Re-check after Phase 1 design.*

[Gates determined based on constitution file]

## Project Structure

### Documentation (this feature)

```text
specs/[###-feature]/
‚îú‚îÄ‚îÄ plan.md              # This file (/speckit.plan command output)
‚îú‚îÄ‚îÄ research.md          # Phase 0 output (/speckit.plan command)
‚îú‚îÄ‚îÄ data-model.md        # Phase 1 output (/speckit.plan command)
‚îú‚îÄ‚îÄ quickstart.md        # Phase 1 output (/speckit.plan command)
‚îú‚îÄ‚îÄ contracts/           # Phase 1 output (/speckit.plan command)
‚îî‚îÄ‚îÄ tasks.md             # Phase 2 output (/speckit.tasks command - NOT created by /speckit.plan)
```

### Source Code (repository root)
<!--
  ACTION REQUIRED: Replace the placeholder tree below with the concrete layout
  for this feature. Delete unused options and expand the chosen structure with
  real paths (e.g., apps/admin, packages/something). The delivered plan must
  not include Option labels.
-->

```text
# [REMOVE IF UNUSED] Option 1: Single project (DEFAULT)
src/
‚îú‚îÄ‚îÄ models/
‚îú‚îÄ‚îÄ services/
‚îú‚îÄ‚îÄ cli/
‚îî‚îÄ‚îÄ lib/

tests/
‚îú‚îÄ‚îÄ contract/
‚îú‚îÄ‚îÄ integration/
‚îî‚îÄ‚îÄ unit/

# [REMOVE IF UNUSED] Option 2: Web application (when "frontend" + "backend" detected)
backend/
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ models/
‚îÇ   ‚îú‚îÄ‚îÄ services/
‚îÇ   ‚îî‚îÄ‚îÄ api/
‚îî‚îÄ‚îÄ tests/

frontend/
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ components/
‚îÇ   ‚îú‚îÄ‚îÄ pages/
‚îÇ   ‚îî‚îÄ‚îÄ services/
‚îî‚îÄ‚îÄ tests/

# [REMOVE IF UNUSED] Option 3: Mobile + API (when "iOS/Android" detected)
api/
‚îî‚îÄ‚îÄ [same as backend above]

ios/ or android/
‚îî‚îÄ‚îÄ [platform-specific structure: feature modules, UI flows, platform tests]
```

**Structure Decision**: [Document the selected structure and reference the real
directories captured above]

## Complexity Tracking

> **Fill ONLY if Constitution Check has violations that must be justified**

| Violation | Why Needed | Simpler Alternative Rejected Because |
|-----------|------------|-------------------------------------|
| [e.g., 4th project] | [current need] | [why 3 projects insufficient] |
| [e.g., Repository pattern] | [specific problem] | [why direct DB access insufficient] |
````

## File: .specify/templates/spec-template.md
````markdown
# Feature Specification: [FEATURE NAME]

**Feature Branch**: `[###-feature-name]`  
**Created**: [DATE]  
**Status**: Draft  
**Input**: User description: "$ARGUMENTS"

## User Scenarios & Testing *(mandatory)*

<!--
  IMPORTANT: User stories should be PRIORITIZED as user journeys ordered by importance.
  Each user story/journey must be INDEPENDENTLY TESTABLE - meaning if you implement just ONE of them,
  you should still have a viable MVP (Minimum Viable Product) that delivers value.
  
  Assign priorities (P1, P2, P3, etc.) to each story, where P1 is the most critical.
  Think of each story as a standalone slice of functionality that can be:
  - Developed independently
  - Tested independently
  - Deployed independently
  - Demonstrated to users independently
-->

### User Story 1 - [Brief Title] (Priority: P1)

[Describe this user journey in plain language]

**Why this priority**: [Explain the value and why it has this priority level]

**Independent Test**: [Describe how this can be tested independently - e.g., "Can be fully tested by [specific action] and delivers [specific value]"]

**Acceptance Scenarios**:

1. **Given** [initial state], **When** [action], **Then** [expected outcome]
2. **Given** [initial state], **When** [action], **Then** [expected outcome]

---

### User Story 2 - [Brief Title] (Priority: P2)

[Describe this user journey in plain language]

**Why this priority**: [Explain the value and why it has this priority level]

**Independent Test**: [Describe how this can be tested independently]

**Acceptance Scenarios**:

1. **Given** [initial state], **When** [action], **Then** [expected outcome]

---

### User Story 3 - [Brief Title] (Priority: P3)

[Describe this user journey in plain language]

**Why this priority**: [Explain the value and why it has this priority level]

**Independent Test**: [Describe how this can be tested independently]

**Acceptance Scenarios**:

1. **Given** [initial state], **When** [action], **Then** [expected outcome]

---

[Add more user stories as needed, each with an assigned priority]

### Edge Cases

<!--
  ACTION REQUIRED: The content in this section represents placeholders.
  Fill them out with the right edge cases.
-->

- What happens when [boundary condition]?
- How does system handle [error scenario]?

## Requirements *(mandatory)*

<!--
  ACTION REQUIRED: The content in this section represents placeholders.
  Fill them out with the right functional requirements.
-->

### Functional Requirements

- **FR-001**: System MUST [specific capability, e.g., "allow users to create accounts"]
- **FR-002**: System MUST [specific capability, e.g., "validate email addresses"]  
- **FR-003**: Users MUST be able to [key interaction, e.g., "reset their password"]
- **FR-004**: System MUST [data requirement, e.g., "persist user preferences"]
- **FR-005**: System MUST [behavior, e.g., "log all security events"]

*Example of marking unclear requirements:*

- **FR-006**: System MUST authenticate users via [NEEDS CLARIFICATION: auth method not specified - email/password, SSO, OAuth?]
- **FR-007**: System MUST retain user data for [NEEDS CLARIFICATION: retention period not specified]

### Key Entities *(include if feature involves data)*

- **[Entity 1]**: [What it represents, key attributes without implementation]
- **[Entity 2]**: [What it represents, relationships to other entities]

## Success Criteria *(mandatory)*

<!--
  ACTION REQUIRED: Define measurable success criteria.
  These must be technology-agnostic and measurable.
-->

### Measurable Outcomes

- **SC-001**: [Measurable metric, e.g., "Users can complete account creation in under 2 minutes"]
- **SC-002**: [Measurable metric, e.g., "System handles 1000 concurrent users without degradation"]
- **SC-003**: [User satisfaction metric, e.g., "90% of users successfully complete primary task on first attempt"]
- **SC-004**: [Business metric, e.g., "Reduce support tickets related to [X] by 50%"]
````

## File: .specify/templates/tasks-template.md
````markdown
---

description: "Task list template for feature implementation"
---

# Tasks: [FEATURE NAME]

**Input**: Design documents from `/specs/[###-feature-name]/`
**Prerequisites**: plan.md (required), spec.md (required for user stories), research.md, data-model.md, contracts/

**Tests**: The examples below include test tasks. Tests are OPTIONAL - only include them if explicitly requested in the feature specification.

**Organization**: Tasks are grouped by user story to enable independent implementation and testing of each story.

## Format: `[ID] [P?] [Story] Description`

- **[P]**: Can run in parallel (different files, no dependencies)
- **[Story]**: Which user story this task belongs to (e.g., US1, US2, US3)
- Include exact file paths in descriptions

## Path Conventions

- **Single project**: `src/`, `tests/` at repository root
- **Web app**: `backend/src/`, `frontend/src/`
- **Mobile**: `api/src/`, `ios/src/` or `android/src/`
- Paths shown below assume single project - adjust based on plan.md structure

<!-- 
  ============================================================================
  IMPORTANT: The tasks below are SAMPLE TASKS for illustration purposes only.
  
  The /speckit.tasks command MUST replace these with actual tasks based on:
  - User stories from spec.md (with their priorities P1, P2, P3...)
  - Feature requirements from plan.md
  - Entities from data-model.md
  - Endpoints from contracts/
  
  Tasks MUST be organized by user story so each story can be:
  - Implemented independently
  - Tested independently
  - Delivered as an MVP increment
  
  DO NOT keep these sample tasks in the generated tasks.md file.
  ============================================================================
-->

## Phase 1: Setup (Shared Infrastructure)

**Purpose**: Project initialization and basic structure

- [ ] T001 Create project structure per implementation plan
- [ ] T002 Initialize [language] project with [framework] dependencies
- [ ] T003 [P] Configure linting and formatting tools

---

## Phase 2: Foundational (Blocking Prerequisites)

**Purpose**: Core infrastructure that MUST be complete before ANY user story can be implemented

**‚ö†Ô∏è CRITICAL**: No user story work can begin until this phase is complete

Examples of foundational tasks (adjust based on your project):

- [ ] T004 Setup database schema and migrations framework
- [ ] T005 [P] Implement authentication/authorization framework
- [ ] T006 [P] Setup API routing and middleware structure
- [ ] T007 Create base models/entities that all stories depend on
- [ ] T008 Configure error handling and logging infrastructure
- [ ] T009 Setup environment configuration management

**Checkpoint**: Foundation ready - user story implementation can now begin in parallel

---

## Phase 3: User Story 1 - [Title] (Priority: P1) üéØ MVP

**Goal**: [Brief description of what this story delivers]

**Independent Test**: [How to verify this story works on its own]

### Tests for User Story 1 (OPTIONAL - only if tests requested) ‚ö†Ô∏è

> **NOTE: Write these tests FIRST, ensure they FAIL before implementation**

- [ ] T010 [P] [US1] Contract test for [endpoint] in tests/contract/test_[name].py
- [ ] T011 [P] [US1] Integration test for [user journey] in tests/integration/test_[name].py

### Implementation for User Story 1

- [ ] T012 [P] [US1] Create [Entity1] model in src/models/[entity1].py
- [ ] T013 [P] [US1] Create [Entity2] model in src/models/[entity2].py
- [ ] T014 [US1] Implement [Service] in src/services/[service].py (depends on T012, T013)
- [ ] T015 [US1] Implement [endpoint/feature] in src/[location]/[file].py
- [ ] T016 [US1] Add validation and error handling
- [ ] T017 [US1] Add logging for user story 1 operations

**Checkpoint**: At this point, User Story 1 should be fully functional and testable independently

---

## Phase 4: User Story 2 - [Title] (Priority: P2)

**Goal**: [Brief description of what this story delivers]

**Independent Test**: [How to verify this story works on its own]

### Tests for User Story 2 (OPTIONAL - only if tests requested) ‚ö†Ô∏è

- [ ] T018 [P] [US2] Contract test for [endpoint] in tests/contract/test_[name].py
- [ ] T019 [P] [US2] Integration test for [user journey] in tests/integration/test_[name].py

### Implementation for User Story 2

- [ ] T020 [P] [US2] Create [Entity] model in src/models/[entity].py
- [ ] T021 [US2] Implement [Service] in src/services/[service].py
- [ ] T022 [US2] Implement [endpoint/feature] in src/[location]/[file].py
- [ ] T023 [US2] Integrate with User Story 1 components (if needed)

**Checkpoint**: At this point, User Stories 1 AND 2 should both work independently

---

## Phase 5: User Story 3 - [Title] (Priority: P3)

**Goal**: [Brief description of what this story delivers]

**Independent Test**: [How to verify this story works on its own]

### Tests for User Story 3 (OPTIONAL - only if tests requested) ‚ö†Ô∏è

- [ ] T024 [P] [US3] Contract test for [endpoint] in tests/contract/test_[name].py
- [ ] T025 [P] [US3] Integration test for [user journey] in tests/integration/test_[name].py

### Implementation for User Story 3

- [ ] T026 [P] [US3] Create [Entity] model in src/models/[entity].py
- [ ] T027 [US3] Implement [Service] in src/services/[service].py
- [ ] T028 [US3] Implement [endpoint/feature] in src/[location]/[file].py

**Checkpoint**: All user stories should now be independently functional

---

[Add more user story phases as needed, following the same pattern]

---

## Phase N: Polish & Cross-Cutting Concerns

**Purpose**: Improvements that affect multiple user stories

- [ ] TXXX [P] Documentation updates in docs/
- [ ] TXXX Code cleanup and refactoring
- [ ] TXXX Performance optimization across all stories
- [ ] TXXX [P] Additional unit tests (if requested) in tests/unit/
- [ ] TXXX Security hardening
- [ ] TXXX Run quickstart.md validation

---

## Dependencies & Execution Order

### Phase Dependencies

- **Setup (Phase 1)**: No dependencies - can start immediately
- **Foundational (Phase 2)**: Depends on Setup completion - BLOCKS all user stories
- **User Stories (Phase 3+)**: All depend on Foundational phase completion
  - User stories can then proceed in parallel (if staffed)
  - Or sequentially in priority order (P1 ‚Üí P2 ‚Üí P3)
- **Polish (Final Phase)**: Depends on all desired user stories being complete

### User Story Dependencies

- **User Story 1 (P1)**: Can start after Foundational (Phase 2) - No dependencies on other stories
- **User Story 2 (P2)**: Can start after Foundational (Phase 2) - May integrate with US1 but should be independently testable
- **User Story 3 (P3)**: Can start after Foundational (Phase 2) - May integrate with US1/US2 but should be independently testable

### Within Each User Story

- Tests (if included) MUST be written and FAIL before implementation
- Models before services
- Services before endpoints
- Core implementation before integration
- Story complete before moving to next priority

### Parallel Opportunities

- All Setup tasks marked [P] can run in parallel
- All Foundational tasks marked [P] can run in parallel (within Phase 2)
- Once Foundational phase completes, all user stories can start in parallel (if team capacity allows)
- All tests for a user story marked [P] can run in parallel
- Models within a story marked [P] can run in parallel
- Different user stories can be worked on in parallel by different team members

---

## Parallel Example: User Story 1

```bash
# Launch all tests for User Story 1 together (if tests requested):
Task: "Contract test for [endpoint] in tests/contract/test_[name].py"
Task: "Integration test for [user journey] in tests/integration/test_[name].py"

# Launch all models for User Story 1 together:
Task: "Create [Entity1] model in src/models/[entity1].py"
Task: "Create [Entity2] model in src/models/[entity2].py"
```

---

## Implementation Strategy

### MVP First (User Story 1 Only)

1. Complete Phase 1: Setup
2. Complete Phase 2: Foundational (CRITICAL - blocks all stories)
3. Complete Phase 3: User Story 1
4. **STOP and VALIDATE**: Test User Story 1 independently
5. Deploy/demo if ready

### Incremental Delivery

1. Complete Setup + Foundational ‚Üí Foundation ready
2. Add User Story 1 ‚Üí Test independently ‚Üí Deploy/Demo (MVP!)
3. Add User Story 2 ‚Üí Test independently ‚Üí Deploy/Demo
4. Add User Story 3 ‚Üí Test independently ‚Üí Deploy/Demo
5. Each story adds value without breaking previous stories

### Parallel Team Strategy

With multiple developers:

1. Team completes Setup + Foundational together
2. Once Foundational is done:
   - Developer A: User Story 1
   - Developer B: User Story 2
   - Developer C: User Story 3
3. Stories complete and integrate independently

---

## Notes

- [P] tasks = different files, no dependencies
- [Story] label maps task to specific user story for traceability
- Each user story should be independently completable and testable
- Verify tests fail before implementing
- Commit after each task or logical group
- Stop at any checkpoint to validate story independently
- Avoid: vague tasks, same file conflicts, cross-story dependencies that break independence
````

## File: coldstart-prompts/01-create-automated-demo.md
````markdown
# Coldstart Implementation Prompt: Create Automated Demo

**Priority**: P0
**Repository**: agentready (https://github.com/redhat/agentready)
**Branch Strategy**: Create feature branch from main

---

## Context

You are implementing a feature for AgentReady, a repository quality assessment tool for AI-assisted development.

### Repository Structure
```
agentready/
‚îú‚îÄ‚îÄ src/agentready/          # Source code
‚îÇ   ‚îú‚îÄ‚îÄ models/              # Data models
‚îÇ   ‚îú‚îÄ‚îÄ services/            # Scanner orchestration
‚îÇ   ‚îú‚îÄ‚îÄ assessors/           # Attribute assessments
‚îÇ   ‚îú‚îÄ‚îÄ reporters/           # Report generation (HTML, Markdown, JSON)
‚îÇ   ‚îú‚îÄ‚îÄ templates/           # Jinja2 templates
‚îÇ   ‚îî‚îÄ‚îÄ cli/                 # Click-based CLI
‚îú‚îÄ‚îÄ tests/                   # Test suite (unit + integration)
‚îú‚îÄ‚îÄ examples/                # Example reports
‚îî‚îÄ‚îÄ specs/                   # Feature specifications
```

### Key Technologies
- Python 3.11+
- Click (CLI framework)
- Jinja2 (templating)
- Pytest (testing)
- Black, isort, ruff (code quality)

### Development Workflow
1. Create feature branch: `git checkout -b NNN-feature-name`
2. Implement changes with tests
3. Run linters: `black . && isort . && ruff check .`
4. Run tests: `pytest`
5. Commit with conventional commits
6. Create PR to main

---

## Feature Requirements

Create Automated Demo

**Priority**: P0 (Critical - Showcase Value)

**Description**: Create an automated, self-contained demonstration of AgentReady that shows the tool assessing a sample repository, generating reports, and providing remediation guidance. This should be runnable with a single command and suitable for demos, presentations, and onboarding.

**Requirements**:
- Single command to run: `agentready demo`
- Self-contained sample repository (embedded in package or generated on-the-fly)
- Demonstrates full workflow:
  1. Repository analysis
  2. Attribute assessment
  3. Score calculation
  4. HTML/Markdown report generation
  5. Remediation suggestions
- Interactive terminal output showing progress
- Opens generated HTML report automatically in browser
- Reusable for presentations and stakeholder demos

**Implementation**:

```bash
# Run automated demo
agentready demo

# Output:
# ü§ñ AgentReady Demo
# ===================
#
# Creating sample repository...
# Analyzing structure...
# Running 25 assessments...
#   ‚úÖ claude_md_file (100/100)
#   ‚ùå precommit_hooks (0/100)
#   ... [progress indicators]
#
# Assessment complete!
# Score: 67.3/100 (Silver)
#
# Generating reports...
#   üìÑ demo-report.html (generated)
#   üìÑ demo-report.md (generated)
#   üìÑ demo-assessment.json (generated)
#
# Opening HTML report in browser...
```

**Sample Repository Options**:

**Option 1: Embedded Examples** (Ship with package)
```
src/agentready/demo/
‚îú‚îÄ‚îÄ sample-python-repo/      # Python project (minimal)
‚îÇ   ‚îú‚îÄ‚îÄ src/myapp/
‚îÇ   ‚îú‚îÄ‚îÄ tests/
‚îÇ   ‚îú‚îÄ‚îÄ README.md
‚îÇ   ‚îú‚îÄ‚îÄ pyproject.toml
‚îÇ   ‚îî‚îÄ‚îÄ .gitignore
‚îú‚îÄ‚îÄ sample-js-repo/          # JavaScript project
‚îÇ   ‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ package.json
‚îÇ   ‚îî‚îÄ‚îÄ README.md
‚îî‚îÄ‚îÄ sample-go-repo/          # Go project
```

**Option 2: Generate On-the-Fly** (Dynamic)
```python
def create_demo_repo(tmp_path: Path, language: str = "python") -> Path:
    """Create sample repository for demo."""
    repo = tmp_path / "demo-repo"
    repo.mkdir()

    if language == "python":
        # Create minimal Python project
        create_file(repo / "README.md", "# Demo Project\n\nSample Python project.")
        create_file(repo / "src/main.py", "def main(): pass")
        create_file(repo / ".gitignore", "*.pyc\n__pycache__/")
        # Missing: CLAUDE.md, tests/, pre-commit hooks (intentional for demo)

    return repo
```

**Demo Script Features**:
- **Progress indicators**: Show assessment running in real-time
- **Color-coded output**: Green for pass, red for fail, yellow for warnings
- **Simulated delays**: Add realistic pauses for "dramatic effect" in demos
- **Narration mode**: Optional verbose output explaining each step
- **Screenshot mode**: Generate high-quality terminal screenshots for docs
- **Record mode**: Save terminal session as GIF/video for presentations

**Configuration**:

```yaml
# Demo config embedded in package
demo:
  sample_repo: python  # or javascript, go, minimal
  show_progress: true
  auto_open_browser: true
  output_dir: ./demo-output
  narration: true  # Explain each step
  delay_ms: 500    # Pause between steps for visibility
```

**Use Cases**:

**Use Case 1: Stakeholder Demo**
```bash
# Quick 2-minute demo for leadership
agentready demo --narration
# Shows: What AgentReady does, how it scores, what reports look like
```

**Use Case 2: Onboarding New Users**
```bash
# Help new users understand the tool
agentready demo --tutorial
# Interactive walkthrough with explanations
```

**Use Case 3: Generate Demo Content for Docs**
```bash
# Create screenshots and videos for documentation
agentready demo --screenshot --record demo.gif
```

**Acceptance Criteria**:
- [ ] `agentready demo` runs without any setup
- [ ] Creates sample repository automatically
- [ ] Runs full assessment workflow
- [ ] Generates all report formats (HTML, Markdown, JSON)
- [ ] Opens HTML report in browser
- [ ] Colorful, engaging terminal output
- [ ] Completes in < 5 seconds
- [ ] No external dependencies required
- [ ] Works offline
- [ ] Includes narration mode for presentations

**Priority Justification**:
- Critical for showcasing tool value to stakeholders
- Needed for Red Hat internal demos and presentations
- Helps with user onboarding and adoption
- Low effort, high impact for visibility
- Required before pitching to other Red Hat teams

**Related**: Onboarding, documentation, marketing

**Notes**:
- Keep demo simple and fast (< 5 seconds)
- Focus on visual impact (colors, progress bars)
- Make it suitable for screenshots/videos
- Consider adding "failure scenario" demo too
- Could expand to multiple language demos
- Add to bootstrap command as part of repo setup

---


---

## Implementation Checklist

Before you begin:
- [ ] Read CLAUDE.md for project context
- [ ] Review existing similar features (if applicable)
- [ ] Understand the data model (src/agentready/models/)
- [ ] Check acceptance criteria in feature description

Implementation steps:
- [ ] Create feature branch
- [ ] Implement core functionality
- [ ] Add unit tests (target >80% coverage)
- [ ] Add integration tests (if applicable)
- [ ] Run linters and fix any issues
- [ ] Update documentation (README.md, CLAUDE.md if needed)
- [ ] Self-test the feature end-to-end
- [ ] Create PR with descriptive title and body

Code quality requirements:
- [ ] All code formatted with black (88 char lines)
- [ ] Imports sorted with isort
- [ ] No ruff violations
- [ ] All tests passing
- [ ] Type hints where appropriate
- [ ] Docstrings for public APIs

---

## Key Files to Review

Based on this feature, you should review:
- `src/agentready/models/` - Understand Assessment, Finding, Attribute models
- `src/agentready/services/scanner.py` - Scanner orchestration
- `src/agentready/assessors/base.py` - BaseAssessor pattern
- `src/agentready/reporters/` - Report generation
- `CLAUDE.md` - Project overview and guidelines
- `BACKLOG.md` - Full context of this feature

---

## Testing Strategy

For this feature, ensure:
1. **Unit tests** for core logic (80%+ coverage)
2. **Integration tests** for end-to-end workflows
3. **Edge case tests** (empty inputs, missing files, errors)
4. **Error handling tests** (graceful degradation)

Run tests:
```bash
# All tests
pytest

# With coverage
pytest --cov=src/agentready --cov-report=html

# Specific test file
pytest tests/unit/test_feature.py -v
```

---

## Success Criteria

This feature is complete when:
- ‚úÖ All acceptance criteria from feature description are met
- ‚úÖ Tests passing with >80% coverage for new code
- ‚úÖ All linters passing (black, isort, ruff)
- ‚úÖ Documentation updated
- ‚úÖ PR created with clear description
- ‚úÖ Self-tested end-to-end

---

## Questions to Clarify (if needed)

If anything is unclear during implementation:
1. Check CLAUDE.md for project patterns
2. Review similar existing features
3. Ask for clarification in PR comments
4. Reference the original backlog item

---

## Getting Started

```bash
# Clone and setup
git clone https://github.com/redhat/agentready.git
cd agentready

# Create virtual environment
uv venv && source .venv/bin/activate

# Install dependencies
uv pip install -e .
uv pip install pytest black isort ruff

# Create feature branch
git checkout -b 001-create-automated-demo

# Start implementing!
```

---

**Note**: This is a coldstart prompt. You have all context needed to implement this feature independently. Read the linked files, follow the patterns, and deliver high-quality code with tests.
````

## File: coldstart-prompts/02-fix-critical-security-logic-bugs-from-code-review.md
````markdown
# Coldstart Implementation Prompt: Fix Critical Security & Logic Bugs from Code Review

**Priority**: P0
**Repository**: agentready (https://github.com/redhat/agentready)
**Branch Strategy**: Create feature branch from main

---

## Context

You are implementing a feature for AgentReady, a repository quality assessment tool for AI-assisted development.

### Repository Structure
```
agentready/
‚îú‚îÄ‚îÄ src/agentready/          # Source code
‚îÇ   ‚îú‚îÄ‚îÄ models/              # Data models
‚îÇ   ‚îú‚îÄ‚îÄ services/            # Scanner orchestration
‚îÇ   ‚îú‚îÄ‚îÄ assessors/           # Attribute assessments
‚îÇ   ‚îú‚îÄ‚îÄ reporters/           # Report generation (HTML, Markdown, JSON)
‚îÇ   ‚îú‚îÄ‚îÄ templates/           # Jinja2 templates
‚îÇ   ‚îî‚îÄ‚îÄ cli/                 # Click-based CLI
‚îú‚îÄ‚îÄ tests/                   # Test suite (unit + integration)
‚îú‚îÄ‚îÄ examples/                # Example reports
‚îî‚îÄ‚îÄ specs/                   # Feature specifications
```

### Key Technologies
- Python 3.11+
- Click (CLI framework)
- Jinja2 (templating)
- Pytest (testing)
- Black, isort, ruff (code quality)

### Development Workflow
1. Create feature branch: `git checkout -b NNN-feature-name`
2. Implement changes with tests
3. Run linters: `black . && isort . && ruff check .`
4. Run tests: `pytest`
5. Commit with conventional commits
6. Create PR to main

---

## Feature Requirements

Fix Critical Security & Logic Bugs from Code Review

**Priority**: P0 (Critical - Security & Correctness)

**Description**: Address critical bugs discovered in code review that affect security and assessment accuracy.

**Issues to Fix**:

1. **XSS Vulnerability in HTML Reports** (CRITICAL - Security)
   - **Location**: `src/agentready/templates/report.html.j2:579`
   - **Problem**: `assessment_json|safe` disables autoescaping for JSON embedded in JavaScript
   - **Risk**: Repository names, commit messages, file paths from git could contain malicious content
   - **Fix**: Replace with `JSON.parse({{ assessment_json|tojson }})`
   - **Add**: Content Security Policy headers to HTML reports

2. **StandardLayoutAssessor Logic Bug** (CRITICAL - Incorrect Scoring)
   - **Location**: `src/agentready/assessors/structure.py:48`
   - **Problem**: `(repository.path / "tests") or (repository.path / "test")` always evaluates to first path
   - **Impact**: Projects with `test/` instead of `tests/` scored incorrectly
   - **Fix**: Check both paths properly:
     ```python
     tests_path = repository.path / "tests"
     if not tests_path.exists():
         tests_path = repository.path / "test"
     has_tests = tests_path.exists()
     ```

**Implementation**:

**File 1**: `src/agentready/templates/report.html.j2`
```jinja2
<!-- BEFORE (VULNERABLE): -->
const ASSESSMENT = {{ assessment_json|safe }};

<!-- AFTER (SECURE): -->
const ASSESSMENT = JSON.parse({{ assessment_json|tojson }});
```

**File 2**: `src/agentready/assessors/structure.py`
```python
# BEFORE (BUGGY):
standard_dirs = {
    "src": repository.path / "src",
    "tests": (repository.path / "tests") or (repository.path / "test"),  # BUG!
}

# AFTER (CORRECT):
standard_dirs = {
    "src": repository.path / "src",
}

# Check for tests directory (either tests/ or test/)
tests_path = repository.path / "tests"
if not tests_path.exists():
    tests_path = repository.path / "test"
standard_dirs["tests"] = tests_path
```

**Test Cases to Add**:
```python
def test_xss_in_repository_name():
    """Test that malicious repo names are escaped in HTML."""
    repo = Repository(
        name="<script>alert('xss')</script>",
        # ...
    )
    html = HTMLReporter().generate(assessment, output)
    assert "<script>" not in html  # Should be escaped

def test_standard_layout_with_test_dir():
    """Test that 'test/' directory is recognized (not just 'tests/')."""
    # Create repo with test/ directory only
    repo_path = tmp_path / "repo"
    (repo_path / "test").mkdir(parents=True)

    assessor = StandardLayoutAssessor()
    finding = assessor.assess(Repository(...))
    assert finding.status == "pass"  # Should recognize test/ dir
```

**Acceptance Criteria**:
- [ ] XSS vulnerability patched with `tojson` filter
- [ ] CSP headers added to HTML reports (future)
- [ ] StandardLayoutAssessor recognizes both `tests/` and `test/`
- [ ] Tests added for XSS prevention
- [ ] Tests added for both test directory naming patterns
- [ ] All existing tests still pass

**Priority Justification**:
- **Security**: XSS is a P0 vulnerability
- **Correctness**: Incorrect scoring undermines tool credibility
- **Quick fixes**: Both are 5-10 minute changes

**Related**: Issue #2 (Report improvements), Bootstrap (#1 - needs secure reports)

---


---

## Implementation Checklist

Before you begin:
- [ ] Read CLAUDE.md for project context
- [ ] Review existing similar features (if applicable)
- [ ] Understand the data model (src/agentready/models/)
- [ ] Check acceptance criteria in feature description

Implementation steps:
- [ ] Create feature branch
- [ ] Implement core functionality
- [ ] Add unit tests (target >80% coverage)
- [ ] Add integration tests (if applicable)
- [ ] Run linters and fix any issues
- [ ] Update documentation (README.md, CLAUDE.md if needed)
- [ ] Self-test the feature end-to-end
- [ ] Create PR with descriptive title and body

Code quality requirements:
- [ ] All code formatted with black (88 char lines)
- [ ] Imports sorted with isort
- [ ] No ruff violations
- [ ] All tests passing
- [ ] Type hints where appropriate
- [ ] Docstrings for public APIs

---

## Key Files to Review

Based on this feature, you should review:
- `src/agentready/models/` - Understand Assessment, Finding, Attribute models
- `src/agentready/services/scanner.py` - Scanner orchestration
- `src/agentready/assessors/base.py` - BaseAssessor pattern
- `src/agentready/reporters/` - Report generation
- `CLAUDE.md` - Project overview and guidelines
- `BACKLOG.md` - Full context of this feature

---

## Testing Strategy

For this feature, ensure:
1. **Unit tests** for core logic (80%+ coverage)
2. **Integration tests** for end-to-end workflows
3. **Edge case tests** (empty inputs, missing files, errors)
4. **Error handling tests** (graceful degradation)

Run tests:
```bash
# All tests
pytest

# With coverage
pytest --cov=src/agentready --cov-report=html

# Specific test file
pytest tests/unit/test_feature.py -v
```

---

## Success Criteria

This feature is complete when:
- ‚úÖ All acceptance criteria from feature description are met
- ‚úÖ Tests passing with >80% coverage for new code
- ‚úÖ All linters passing (black, isort, ruff)
- ‚úÖ Documentation updated
- ‚úÖ PR created with clear description
- ‚úÖ Self-tested end-to-end

---

## Questions to Clarify (if needed)

If anything is unclear during implementation:
1. Check CLAUDE.md for project patterns
2. Review similar existing features
3. Ask for clarification in PR comments
4. Reference the original backlog item

---

## Getting Started

```bash
# Clone and setup
git clone https://github.com/redhat/agentready.git
cd agentready

# Create virtual environment
uv venv && source .venv/bin/activate

# Install dependencies
uv pip install -e .
uv pip install pytest black isort ruff

# Create feature branch
git checkout -b 002-fix-critical-security-&-logic-bugs-from-code-revie

# Start implementing!
```

---

**Note**: This is a coldstart prompt. You have all context needed to implement this feature independently. Read the linked files, follow the patterns, and deliver high-quality code with tests.
````

## File: coldstart-prompts/03-bootstrap-agentready-repository-on-github.md
````markdown
# Coldstart Implementation Prompt: Bootstrap AgentReady Repository on GitHub

**Priority**: P0
**Repository**: agentready (https://github.com/redhat/agentready)
**Branch Strategy**: Create feature branch from main

---

## Context

You are implementing a feature for AgentReady, a repository quality assessment tool for AI-assisted development.

### Repository Structure
```
agentready/
‚îú‚îÄ‚îÄ src/agentready/          # Source code
‚îÇ   ‚îú‚îÄ‚îÄ models/              # Data models
‚îÇ   ‚îú‚îÄ‚îÄ services/            # Scanner orchestration
‚îÇ   ‚îú‚îÄ‚îÄ assessors/           # Attribute assessments
‚îÇ   ‚îú‚îÄ‚îÄ reporters/           # Report generation (HTML, Markdown, JSON)
‚îÇ   ‚îú‚îÄ‚îÄ templates/           # Jinja2 templates
‚îÇ   ‚îî‚îÄ‚îÄ cli/                 # Click-based CLI
‚îú‚îÄ‚îÄ tests/                   # Test suite (unit + integration)
‚îú‚îÄ‚îÄ examples/                # Example reports
‚îî‚îÄ‚îÄ specs/                   # Feature specifications
```

### Key Technologies
- Python 3.11+
- Click (CLI framework)
- Jinja2 (templating)
- Pytest (testing)
- Black, isort, ruff (code quality)

### Development Workflow
1. Create feature branch: `git checkout -b NNN-feature-name`
2. Implement changes with tests
3. Run linters: `black . && isort . && ruff check .`
4. Run tests: `pytest`
5. Commit with conventional commits
6. Create PR to main

---

## Feature Requirements

Bootstrap AgentReady Repository on GitHub

**Priority**: P0 (Critical - Dogfooding)

**Description**: Implement `agentready bootstrap` command to set up the agentready repository itself on GitHub with all best practices. This is the FIRST feature to implement - we'll dogfood our own tool!

**Vision**: Use AgentReady to bootstrap the AgentReady repository - demonstrating the tool's value while setting up our own infrastructure.

**Why P0**:
- Demonstrates tool value immediately (dogfooding)
- Sets up critical GitHub infrastructure (Actions, badges, PR templates)
- Validates bootstrap command design before external users
- Creates the foundation for GitHub App integration

**Requirements**:

1. **GitHub Repository Setup**
   - Create/update repository on GitHub via `gh` CLI
   - Set repository description and topics
   - Configure repository settings (PR requirements, branch protection)

2. **GitHub Actions Workflows**
   - `.github/workflows/agentready-assessment.yml` - Run assessment on PR/push
   - `.github/workflows/tests.yml` - Run pytest, linters
   - `.github/workflows/release.yml` - Publish to PyPI (future)

3. **GitHub Templates**
   - `.github/ISSUE_TEMPLATE/bug_report.md`
   - `.github/ISSUE_TEMPLATE/feature_request.md`
   - `.github/PULL_REQUEST_TEMPLATE.md`
   - `.github/CODEOWNERS`

4. **Pre-commit Hooks**
   - `.pre-commit-config.yaml` with black, isort, ruff
   - Conventional commit linting (commitlint)
   - Auto-run tests before commit

5. **Dependency Management**
   - Dependabot configuration (`.github/dependabot.yml`)
   - Security scanning (`.github/workflows/security.yml`)

6. **Documentation Updates**
   - Update README.md with badges
   - Add CONTRIBUTING.md
   - Add CODE_OF_CONDUCT.md (Red Hat standard)
   - Add LICENSE (Apache 2.0 or MIT)

**Command Interface**:

```bash
# Bootstrap current repository on GitHub
agentready bootstrap .

# Bootstrap with specific language
agentready bootstrap . --language python

# Bootstrap and create GitHub repo
agentready bootstrap . --create-repo redhat/agentready

# Dry run (show what would be created)
agentready bootstrap . --dry-run

# Interactive mode (confirm each file)
agentready bootstrap . --interactive
```

**What Gets Created**:

```
.github/
‚îú‚îÄ‚îÄ workflows/
‚îÇ   ‚îú‚îÄ‚îÄ agentready-assessment.yml  # Run assessment on PR
‚îÇ   ‚îú‚îÄ‚îÄ tests.yml                  # Run tests and linters
‚îÇ   ‚îî‚îÄ‚îÄ dependabot.yml            # Dependency updates
‚îú‚îÄ‚îÄ ISSUE_TEMPLATE/
‚îÇ   ‚îú‚îÄ‚îÄ bug_report.md
‚îÇ   ‚îî‚îÄ‚îÄ feature_request.md
‚îú‚îÄ‚îÄ PULL_REQUEST_TEMPLATE.md
‚îú‚îÄ‚îÄ CODEOWNERS
‚îî‚îÄ‚îÄ dependabot.yml

.pre-commit-config.yaml
CONTRIBUTING.md
CODE_OF_CONDUCT.md
LICENSE
```

**GitHub Actions Workflow Example**:

```yaml
# .github/workflows/agentready-assessment.yml
name: AgentReady Assessment

on:
  pull_request:
    types: [opened, synchronize, reopened]
  push:
    branches: [main]

jobs:
  assess:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install AgentReady
        run: |
          pip install agentready

      - name: Run Assessment
        run: |
          agentready assess . --verbose

      - name: Upload Reports
        uses: actions/upload-artifact@v4
        with:
          name: agentready-reports
          path: .agentready/

      - name: Comment on PR
        if: github.event_name == 'pull_request'
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            const report = fs.readFileSync('.agentready/report-latest.md', 'utf8');
            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: report
            });
```

**Implementation Phases**:

**Phase 1: Core Bootstrap** (This Sprint)
- Implement `agentready bootstrap` command
- Create template files for GitHub Actions, pre-commit, templates
- Test on agentready repository itself
- Commit all generated files

**Phase 2: GitHub Integration** (Next Sprint)
- Use `gh` CLI to create/update repository
- Set up branch protection rules
- Configure repository settings
- Add repository badges

**Phase 3: Language-Specific Templates** (Future)
- Python-specific templates (pytest, black, mypy)
- JavaScript-specific (eslint, prettier, jest)
- Go-specific (golangci-lint, gotestsum)

**Files to Create**:

```
src/agentready/
‚îú‚îÄ‚îÄ cli/bootstrap.py           # Bootstrap CLI command
‚îú‚îÄ‚îÄ bootstrap/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ generator.py          # File generator
‚îÇ   ‚îú‚îÄ‚îÄ github_setup.py       # GitHub integration
‚îÇ   ‚îî‚îÄ‚îÄ templates/            # Template files
‚îÇ       ‚îú‚îÄ‚îÄ github/
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ workflows/
‚îÇ       ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ agentready.yml.j2
‚îÇ       ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ tests.yml.j2
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ ISSUE_TEMPLATE/
‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ PULL_REQUEST_TEMPLATE.md.j2
‚îÇ       ‚îú‚îÄ‚îÄ precommit.yaml.j2
‚îÇ       ‚îú‚îÄ‚îÄ CONTRIBUTING.md.j2
‚îÇ       ‚îî‚îÄ‚îÄ CODE_OF_CONDUCT.md.j2
tests/unit/test_bootstrap.py
```

**Acceptance Criteria**:

- [ ] `agentready bootstrap .` creates all required files
- [ ] GitHub Actions workflows are valid and functional
- [ ] Pre-commit hooks install and run successfully
- [ ] Repository badges added to README.md
- [ ] Dry-run mode shows what would be created
- [ ] Interactive mode prompts for confirmation
- [ ] Successfully bootstrap agentready repository itself
- [ ] All generated files committed to agentready repo
- [ ] GitHub Actions runs assessment on every PR

**Success Metrics**:

- AgentReady repository on GitHub has:
  - ‚úÖ GitHub Actions running assessments
  - ‚úÖ PR template with checklist
  - ‚úÖ Issue templates for bugs/features
  - ‚úÖ Pre-commit hooks configured
  - ‚úÖ Dependabot enabled
  - ‚úÖ Repository badge showing score
  - ‚úÖ All linters passing in CI

**Priority Justification**:

This is P0 because:
1. **Dogfooding** - We need this for our own repository first
2. **Demonstrates value** - Shows AgentReady in action on itself
3. **Foundation** - Required before GitHub App integration
4. **Credibility** - Can't tell others to use it if we don't use it ourselves

**Related**: GitHub App Integration (#5), Align Command (#3)

**Notes**:
- Start with Python-specific templates (our use case)
- Keep templates simple and focused
- Use Jinja2 for template rendering
- Integrate with `gh` CLI for GitHub operations
- All templates should pass AgentReady assessment!

---


---

## Implementation Checklist

Before you begin:
- [ ] Read CLAUDE.md for project context
- [ ] Review existing similar features (if applicable)
- [ ] Understand the data model (src/agentready/models/)
- [ ] Check acceptance criteria in feature description

Implementation steps:
- [ ] Create feature branch
- [ ] Implement core functionality
- [ ] Add unit tests (target >80% coverage)
- [ ] Add integration tests (if applicable)
- [ ] Run linters and fix any issues
- [ ] Update documentation (README.md, CLAUDE.md if needed)
- [ ] Self-test the feature end-to-end
- [ ] Create PR with descriptive title and body

Code quality requirements:
- [ ] All code formatted with black (88 char lines)
- [ ] Imports sorted with isort
- [ ] No ruff violations
- [ ] All tests passing
- [ ] Type hints where appropriate
- [ ] Docstrings for public APIs

---

## Key Files to Review

Based on this feature, you should review:
- `src/agentready/models/` - Understand Assessment, Finding, Attribute models
- `src/agentready/services/scanner.py` - Scanner orchestration
- `src/agentready/assessors/base.py` - BaseAssessor pattern
- `src/agentready/reporters/` - Report generation
- `CLAUDE.md` - Project overview and guidelines
- `BACKLOG.md` - Full context of this feature

---

## Testing Strategy

For this feature, ensure:
1. **Unit tests** for core logic (80%+ coverage)
2. **Integration tests** for end-to-end workflows
3. **Edge case tests** (empty inputs, missing files, errors)
4. **Error handling tests** (graceful degradation)

Run tests:
```bash
# All tests
pytest

# With coverage
pytest --cov=src/agentready --cov-report=html

# Specific test file
pytest tests/unit/test_feature.py -v
```

---

## Success Criteria

This feature is complete when:
- ‚úÖ All acceptance criteria from feature description are met
- ‚úÖ Tests passing with >80% coverage for new code
- ‚úÖ All linters passing (black, isort, ruff)
- ‚úÖ Documentation updated
- ‚úÖ PR created with clear description
- ‚úÖ Self-tested end-to-end

---

## Questions to Clarify (if needed)

If anything is unclear during implementation:
1. Check CLAUDE.md for project patterns
2. Review similar existing features
3. Ask for clarification in PR comments
4. Reference the original backlog item

---

## Getting Started

```bash
# Clone and setup
git clone https://github.com/redhat/agentready.git
cd agentready

# Create virtual environment
uv venv && source .venv/bin/activate

# Install dependencies
uv pip install -e .
uv pip install pytest black isort ruff

# Create feature branch
git checkout -b 003-bootstrap-agentready-repository-on-github

# Start implementing!
```

---

**Note**: This is a coldstart prompt. You have all context needed to implement this feature independently. Read the linked files, follow the patterns, and deliver high-quality code with tests.
````

## File: coldstart-prompts/04-report-header-with-repository-metadata.md
````markdown
# Coldstart Implementation Prompt: Report Header with Repository Metadata

**Priority**: P0
**Repository**: agentready (https://github.com/redhat/agentready)
**Branch Strategy**: Create feature branch from main

---

## Context

You are implementing a feature for AgentReady, a repository quality assessment tool for AI-assisted development.

### Repository Structure
```
agentready/
‚îú‚îÄ‚îÄ src/agentready/          # Source code
‚îÇ   ‚îú‚îÄ‚îÄ models/              # Data models
‚îÇ   ‚îú‚îÄ‚îÄ services/            # Scanner orchestration
‚îÇ   ‚îú‚îÄ‚îÄ assessors/           # Attribute assessments
‚îÇ   ‚îú‚îÄ‚îÄ reporters/           # Report generation (HTML, Markdown, JSON)
‚îÇ   ‚îú‚îÄ‚îÄ templates/           # Jinja2 templates
‚îÇ   ‚îî‚îÄ‚îÄ cli/                 # Click-based CLI
‚îú‚îÄ‚îÄ tests/                   # Test suite (unit + integration)
‚îú‚îÄ‚îÄ examples/                # Example reports
‚îî‚îÄ‚îÄ specs/                   # Feature specifications
```

### Key Technologies
- Python 3.11+
- Click (CLI framework)
- Jinja2 (templating)
- Pytest (testing)
- Black, isort, ruff (code quality)

### Development Workflow
1. Create feature branch: `git checkout -b NNN-feature-name`
2. Implement changes with tests
3. Run linters: `black . && isort . && ruff check .`
4. Run tests: `pytest`
5. Commit with conventional commits
6. Create PR to main

---

## Feature Requirements

Report Header with Repository Metadata

**Priority**: P0 (Critical - Blocking Usability)

**Description**: Add prominent report header showing what repository/agent/code was scanned. Currently reports lack context about what was assessed.

**Problem**: Users cannot identify what the report is about without digging into the details. No repository name, path, timestamp, or assessment context visible at the top.

**Requirements**:
- **Prominent header section** at the top of all report formats (HTML, Markdown, JSON)
- Repository name (bold, large font)
- Repository path (absolute path on filesystem or GitHub URL)
- Assessment timestamp (human-readable: "November 21, 2025 at 2:11 AM")
- Branch name and commit hash
- AgentReady version used for assessment
- Who ran the assessment (username@hostname)
- Command used: `agentready assess /path/to/repo --verbose`

**HTML Report Header Design**:
```html
<header class="report-header">
  <div class="repo-info">
    <h1>AgentReady Assessment Report</h1>
    <div class="repo-name">Repository: agentready</div>
    <div class="repo-path">/Users/jeder/repos/sk/agentready</div>
    <div class="repo-git">Branch: 001-agentready-scorer | Commit: d49947c</div>
  </div>
  <div class="meta-info">
    <div>Assessed: November 21, 2025 at 2:11 AM</div>
    <div>AgentReady Version: 1.0.0</div>
    <div>Run by: jeder@macbook</div>
  </div>
</header>
```

**Markdown Report Header**:
```markdown
# ü§ñ AgentReady Assessment Report

**Repository**: agentready
**Path**: `/Users/jeder/repos/sk/agentready`
**Branch**: `001-agentready-scorer` | **Commit**: `d49947c`
**Assessed**: November 21, 2025 at 2:11 AM
**AgentReady Version**: 1.0.0
**Run by**: jeder@macbook

---
```

**JSON Report Metadata**:
```json
{
  "metadata": {
    "agentready_version": "1.0.0",
    "assessment_timestamp": "2025-11-21T02:11:05Z",
    "assessment_timestamp_human": "November 21, 2025 at 2:11 AM",
    "executed_by": "jeder@macbook",
    "command": "agentready assess . --verbose",
    "working_directory": "/Users/jeder/repos/sk/agentready"
  },
  "repository": { ... }
}
```

**Implementation**:
- Add metadata collection to Scanner
- Update all reporter templates (HTML, Markdown)
- Enhance Assessment model with metadata field
- Position header prominently (before score summary)

**Acceptance Criteria**:
- ‚úÖ User can immediately identify what repository was assessed
- ‚úÖ Timestamp shows when assessment was run
- ‚úÖ Git context (branch, commit) visible
- ‚úÖ AgentReady version tracked for reproducibility

**Related**: Report generation, usability, debugging

**Notes**:
- This is blocking adoption - users confused about report context
- Critical for multi-repository workflows
- Needed for CI/CD integration (track which build)

---


---

## Implementation Checklist

Before you begin:
- [ ] Read CLAUDE.md for project context
- [ ] Review existing similar features (if applicable)
- [ ] Understand the data model (src/agentready/models/)
- [ ] Check acceptance criteria in feature description

Implementation steps:
- [ ] Create feature branch
- [ ] Implement core functionality
- [ ] Add unit tests (target >80% coverage)
- [ ] Add integration tests (if applicable)
- [ ] Run linters and fix any issues
- [ ] Update documentation (README.md, CLAUDE.md if needed)
- [ ] Self-test the feature end-to-end
- [ ] Create PR with descriptive title and body

Code quality requirements:
- [ ] All code formatted with black (88 char lines)
- [ ] Imports sorted with isort
- [ ] No ruff violations
- [ ] All tests passing
- [ ] Type hints where appropriate
- [ ] Docstrings for public APIs

---

## Key Files to Review

Based on this feature, you should review:
- `src/agentready/models/` - Understand Assessment, Finding, Attribute models
- `src/agentready/services/scanner.py` - Scanner orchestration
- `src/agentready/assessors/base.py` - BaseAssessor pattern
- `src/agentready/reporters/` - Report generation
- `CLAUDE.md` - Project overview and guidelines
- `BACKLOG.md` - Full context of this feature

---

## Testing Strategy

For this feature, ensure:
1. **Unit tests** for core logic (80%+ coverage)
2. **Integration tests** for end-to-end workflows
3. **Edge case tests** (empty inputs, missing files, errors)
4. **Error handling tests** (graceful degradation)

Run tests:
```bash
# All tests
pytest

# With coverage
pytest --cov=src/agentready --cov-report=html

# Specific test file
pytest tests/unit/test_feature.py -v
```

---

## Success Criteria

This feature is complete when:
- ‚úÖ All acceptance criteria from feature description are met
- ‚úÖ Tests passing with >80% coverage for new code
- ‚úÖ All linters passing (black, isort, ruff)
- ‚úÖ Documentation updated
- ‚úÖ PR created with clear description
- ‚úÖ Self-tested end-to-end

---

## Questions to Clarify (if needed)

If anything is unclear during implementation:
1. Check CLAUDE.md for project patterns
2. Review similar existing features
3. Ask for clarification in PR comments
4. Reference the original backlog item

---

## Getting Started

```bash
# Clone and setup
git clone https://github.com/redhat/agentready.git
cd agentready

# Create virtual environment
uv venv && source .venv/bin/activate

# Install dependencies
uv pip install -e .
uv pip install pytest black isort ruff

# Create feature branch
git checkout -b 004-report-header-with-repository-metadata

# Start implementing!
```

---

**Note**: This is a coldstart prompt. You have all context needed to implement this feature independently. Read the linked files, follow the patterns, and deliver high-quality code with tests.
````

## File: coldstart-prompts/05-improve-html-report-design-font-size-color-scheme.md
````markdown
# Coldstart Implementation Prompt: Improve HTML Report Design (Font Size & Color Scheme)

**Priority**: P0
**Repository**: agentready (https://github.com/redhat/agentready)
**Branch Strategy**: Create feature branch from main

---

## Context

You are implementing a feature for AgentReady, a repository quality assessment tool for AI-assisted development.

### Repository Structure
```
agentready/
‚îú‚îÄ‚îÄ src/agentready/          # Source code
‚îÇ   ‚îú‚îÄ‚îÄ models/              # Data models
‚îÇ   ‚îú‚îÄ‚îÄ services/            # Scanner orchestration
‚îÇ   ‚îú‚îÄ‚îÄ assessors/           # Attribute assessments
‚îÇ   ‚îú‚îÄ‚îÄ reporters/           # Report generation (HTML, Markdown, JSON)
‚îÇ   ‚îú‚îÄ‚îÄ templates/           # Jinja2 templates
‚îÇ   ‚îî‚îÄ‚îÄ cli/                 # Click-based CLI
‚îú‚îÄ‚îÄ tests/                   # Test suite (unit + integration)
‚îú‚îÄ‚îÄ examples/                # Example reports
‚îî‚îÄ‚îÄ specs/                   # Feature specifications
```

### Key Technologies
- Python 3.11+
- Click (CLI framework)
- Jinja2 (templating)
- Pytest (testing)
- Black, isort, ruff (code quality)

### Development Workflow
1. Create feature branch: `git checkout -b NNN-feature-name`
2. Implement changes with tests
3. Run linters: `black . && isort . && ruff check .`
4. Run tests: `pytest`
5. Commit with conventional commits
6. Create PR to main

---

## Feature Requirements

Improve HTML Report Design (Font Size & Color Scheme)

**Priority**: P0 (Critical - Poor User Experience)

**Description**: Completely redesign HTML report color scheme and increase all font sizes by at least 4 points for readability.

**Problems**:
1. **Color scheme is "hideous"** (user feedback) - current purple gradient doesn't work
2. **Font sizes too small** - hard to read on modern displays
3. **Poor contrast** - some text hard to distinguish

**New Color Scheme** (Dark/Professional):
```css
:root {
  /* Base colors - mostly black, dark blue, purple, white */
  --background: #0a0e27;           /* Almost black with blue tint */
  --surface: #1a1f3a;              /* Dark blue surface */
  --surface-elevated: #252b4a;     /* Slightly lighter surface */

  /* Primary colors */
  --primary: #8b5cf6;              /* Purple (accent) */
  --primary-light: #a78bfa;        /* Light purple */
  --primary-dark: #6d28d9;         /* Dark purple */

  /* Text colors */
  --text-primary: #f8fafc;         /* Almost white */
  --text-secondary: #cbd5e1;       /* Light gray */
  --text-muted: #94a3b8;           /* Muted gray */

  /* Status colors */
  --success: #10b981;              /* Green (pass) */
  --warning: #f59e0b;              /* Amber (warning) */
  --danger: #ef4444;               /* Red (fail) */
  --neutral: #64748b;              /* Gray (skipped) */

  /* UI elements */
  --border: #334155;               /* Dark border */
  --shadow: rgba(0, 0, 0, 0.5);   /* Deep shadows */
}
```

**Font Size Increases** (+4pt minimum):
```css
/* Current ‚Üí New */
body { font-size: 14px ‚Üí 18px; }
h1 { font-size: 28px ‚Üí 36px; }
h2 { font-size: 24px ‚Üí 30px; }
h3 { font-size: 20px ‚Üí 26px; }
.score { font-size: 48px ‚Üí 56px; }
.attribute-name { font-size: 16px ‚Üí 22px; }
.evidence { font-size: 13px ‚Üí 17px; }
code { font-size: 13px ‚Üí 16px; }
```

**Design Mockup**:
```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ [Dark navy blue background #1a1f3a]                ‚îÇ
‚îÇ                                                     ‚îÇ
‚îÇ  ü§ñ AgentReady Assessment Report                   ‚îÇ
‚îÇ  Repository: agentready                            ‚îÇ
‚îÇ  /Users/jeder/repos/sk/agentready                  ‚îÇ
‚îÇ  [White text #f8fafc, 18px base font]             ‚îÇ
‚îÇ                                                     ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ [Purple accent card #8b5cf6]                       ‚îÇ
‚îÇ                                                     ‚îÇ
‚îÇ           75.4 / 100                               ‚îÇ
‚îÇ           [56px, bold, white]                      ‚îÇ
‚îÇ           ü•á Gold                                   ‚îÇ
‚îÇ                                                     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**Implementation Checklist**:
- [ ] Replace gradient backgrounds with dark blue/black
- [ ] Update all font sizes (+4pt minimum)
- [ ] Use purple (#8b5cf6) sparingly as accent color
- [ ] Ensure white text on dark backgrounds (WCAG AA)
- [ ] Update certification level colors
- [ ] Redesign score cards with new scheme
- [ ] Test with colorblind simulators
- [ ] Add light mode as alternative (with same professional palette)

**Before/After Color Comparison**:
```
Current (Problems):
- Purple gradient everywhere: #667eea ‚Üí #764ba2 ‚ùå
- Small text: 14px base ‚ùå
- Busy, overwhelming ‚ùå

New (Professional):
- Dark blue/black base: #0a0e27, #1a1f3a ‚úÖ
- Purple accents only: #8b5cf6 ‚úÖ
- Large text: 18px base ‚úÖ
- Clean, readable ‚úÖ
```

**Acceptance Criteria**:
- ‚úÖ All text easily readable (18px minimum body text)
- ‚úÖ Color scheme uses black, dark blue, purple, white palette
- ‚úÖ High contrast (WCAG 2.1 AA compliant)
- ‚úÖ Professional appearance suitable for Red Hat engineering
- ‚úÖ Purple used as accent, not dominant color

**Related**: HTML report generation, UX, accessibility

**Notes**:
- Current design blocks user adoption (visual issues)
- This is the first thing users see - must be excellent
- Consider adding screenshot to docs after redesign
- Font size critical for presentations and stakeholder reviews

---

## Schema & Configuration


---

## Implementation Checklist

Before you begin:
- [ ] Read CLAUDE.md for project context
- [ ] Review existing similar features (if applicable)
- [ ] Understand the data model (src/agentready/models/)
- [ ] Check acceptance criteria in feature description

Implementation steps:
- [ ] Create feature branch
- [ ] Implement core functionality
- [ ] Add unit tests (target >80% coverage)
- [ ] Add integration tests (if applicable)
- [ ] Run linters and fix any issues
- [ ] Update documentation (README.md, CLAUDE.md if needed)
- [ ] Self-test the feature end-to-end
- [ ] Create PR with descriptive title and body

Code quality requirements:
- [ ] All code formatted with black (88 char lines)
- [ ] Imports sorted with isort
- [ ] No ruff violations
- [ ] All tests passing
- [ ] Type hints where appropriate
- [ ] Docstrings for public APIs

---

## Key Files to Review

Based on this feature, you should review:
- `src/agentready/models/` - Understand Assessment, Finding, Attribute models
- `src/agentready/services/scanner.py` - Scanner orchestration
- `src/agentready/assessors/base.py` - BaseAssessor pattern
- `src/agentready/reporters/` - Report generation
- `CLAUDE.md` - Project overview and guidelines
- `BACKLOG.md` - Full context of this feature

---

## Testing Strategy

For this feature, ensure:
1. **Unit tests** for core logic (80%+ coverage)
2. **Integration tests** for end-to-end workflows
3. **Edge case tests** (empty inputs, missing files, errors)
4. **Error handling tests** (graceful degradation)

Run tests:
```bash
# All tests
pytest

# With coverage
pytest --cov=src/agentready --cov-report=html

# Specific test file
pytest tests/unit/test_feature.py -v
```

---

## Success Criteria

This feature is complete when:
- ‚úÖ All acceptance criteria from feature description are met
- ‚úÖ Tests passing with >80% coverage for new code
- ‚úÖ All linters passing (black, isort, ruff)
- ‚úÖ Documentation updated
- ‚úÖ PR created with clear description
- ‚úÖ Self-tested end-to-end

---

## Questions to Clarify (if needed)

If anything is unclear during implementation:
1. Check CLAUDE.md for project patterns
2. Review similar existing features
3. Ask for clarification in PR comments
4. Reference the original backlog item

---

## Getting Started

```bash
# Clone and setup
git clone https://github.com/redhat/agentready.git
cd agentready

# Create virtual environment
uv venv && source .venv/bin/activate

# Install dependencies
uv pip install -e .
uv pip install pytest black isort ruff

# Create feature branch
git checkout -b 005-improve-html-report-design-(font-size-&-color-sche

# Start implementing!
```

---

**Note**: This is a coldstart prompt. You have all context needed to implement this feature independently. Read the linked files, follow the patterns, and deliver high-quality code with tests.
````

## File: coldstart-prompts/06-report-schema-versioning.md
````markdown
# Coldstart Implementation Prompt: Report Schema Versioning

**Priority**: P3
**Repository**: agentready (https://github.com/redhat/agentready)
**Branch Strategy**: Create feature branch from main

---

## Context

You are implementing a feature for AgentReady, a repository quality assessment tool for AI-assisted development.

### Repository Structure
```
agentready/
‚îú‚îÄ‚îÄ src/agentready/          # Source code
‚îÇ   ‚îú‚îÄ‚îÄ models/              # Data models
‚îÇ   ‚îú‚îÄ‚îÄ services/            # Scanner orchestration
‚îÇ   ‚îú‚îÄ‚îÄ assessors/           # Attribute assessments
‚îÇ   ‚îú‚îÄ‚îÄ reporters/           # Report generation (HTML, Markdown, JSON)
‚îÇ   ‚îú‚îÄ‚îÄ templates/           # Jinja2 templates
‚îÇ   ‚îî‚îÄ‚îÄ cli/                 # Click-based CLI
‚îú‚îÄ‚îÄ tests/                   # Test suite (unit + integration)
‚îú‚îÄ‚îÄ examples/                # Example reports
‚îî‚îÄ‚îÄ specs/                   # Feature specifications
```

### Key Technologies
- Python 3.11+
- Click (CLI framework)
- Jinja2 (templating)
- Pytest (testing)
- Black, isort, ruff (code quality)

### Development Workflow
1. Create feature branch: `git checkout -b NNN-feature-name`
2. Implement changes with tests
3. Run linters: `black . && isort . && ruff check .`
4. Run tests: `pytest`
5. Commit with conventional commits
6. Create PR to main

---

## Feature Requirements

Report Schema Versioning

**Priority**: P3 (Important)

**Description**: Define and version the JSON/HTML/Markdown report schemas to ensure backwards compatibility and enable schema evolution.

**Requirements**:
- JSON schema for assessment reports (contracts/assessment-schema.json exists)
- HTML schema for interactive reports (contracts/report-html-schema.md exists)
- Markdown schema for version control reports (contracts/report-markdown-schema.md exists)
- Schema versioning strategy (semantic versioning)
- Backwards compatibility testing
- Schema migration tools for major version changes

**Use Case**:
```bash
# Validate report against schema
agentready validate-report assessment-2025-11-20.json

# Migrate old report to new schema version
agentready migrate-report --from 1.0.0 --to 2.0.0 old-report.json
```

**Related**: Report generation, data model evolution

**Notes**:
- Schemas exist in contracts/ directory but need formal versioning
- Consider using JSON Schema Draft 2020-12
- Tool should validate generated reports against bundled schema
- Breaking schema changes should trigger major version bump

---


---

## Implementation Checklist

Before you begin:
- [ ] Read CLAUDE.md for project context
- [ ] Review existing similar features (if applicable)
- [ ] Understand the data model (src/agentready/models/)
- [ ] Check acceptance criteria in feature description

Implementation steps:
- [ ] Create feature branch
- [ ] Implement core functionality
- [ ] Add unit tests (target >80% coverage)
- [ ] Add integration tests (if applicable)
- [ ] Run linters and fix any issues
- [ ] Update documentation (README.md, CLAUDE.md if needed)
- [ ] Self-test the feature end-to-end
- [ ] Create PR with descriptive title and body

Code quality requirements:
- [ ] All code formatted with black (88 char lines)
- [ ] Imports sorted with isort
- [ ] No ruff violations
- [ ] All tests passing
- [ ] Type hints where appropriate
- [ ] Docstrings for public APIs

---

## Key Files to Review

Based on this feature, you should review:
- `src/agentready/models/` - Understand Assessment, Finding, Attribute models
- `src/agentready/services/scanner.py` - Scanner orchestration
- `src/agentready/assessors/base.py` - BaseAssessor pattern
- `src/agentready/reporters/` - Report generation
- `CLAUDE.md` - Project overview and guidelines
- `BACKLOG.md` - Full context of this feature

---

## Testing Strategy

For this feature, ensure:
1. **Unit tests** for core logic (80%+ coverage)
2. **Integration tests** for end-to-end workflows
3. **Edge case tests** (empty inputs, missing files, errors)
4. **Error handling tests** (graceful degradation)

Run tests:
```bash
# All tests
pytest

# With coverage
pytest --cov=src/agentready --cov-report=html

# Specific test file
pytest tests/unit/test_feature.py -v
```

---

## Success Criteria

This feature is complete when:
- ‚úÖ All acceptance criteria from feature description are met
- ‚úÖ Tests passing with >80% coverage for new code
- ‚úÖ All linters passing (black, isort, ruff)
- ‚úÖ Documentation updated
- ‚úÖ PR created with clear description
- ‚úÖ Self-tested end-to-end

---

## Questions to Clarify (if needed)

If anything is unclear during implementation:
1. Check CLAUDE.md for project patterns
2. Review similar existing features
3. Ask for clarification in PR comments
4. Reference the original backlog item

---

## Getting Started

```bash
# Clone and setup
git clone https://github.com/redhat/agentready.git
cd agentready

# Create virtual environment
uv venv && source .venv/bin/activate

# Install dependencies
uv pip install -e .
uv pip install pytest black isort ruff

# Create feature branch
git checkout -b 006-report-schema-versioning

# Start implementing!
```

---

**Note**: This is a coldstart prompt. You have all context needed to implement this feature independently. Read the linked files, follow the patterns, and deliver high-quality code with tests.
````

## File: coldstart-prompts/07-research-report-generatorupdater-utility.md
````markdown
# Coldstart Implementation Prompt: Research Report Generator/Updater Utility

**Priority**: P4
**Repository**: agentready (https://github.com/redhat/agentready)
**Branch Strategy**: Create feature branch from main

---

## Context

You are implementing a feature for AgentReady, a repository quality assessment tool for AI-assisted development.

### Repository Structure
```
agentready/
‚îú‚îÄ‚îÄ src/agentready/          # Source code
‚îÇ   ‚îú‚îÄ‚îÄ models/              # Data models
‚îÇ   ‚îú‚îÄ‚îÄ services/            # Scanner orchestration
‚îÇ   ‚îú‚îÄ‚îÄ assessors/           # Attribute assessments
‚îÇ   ‚îú‚îÄ‚îÄ reporters/           # Report generation (HTML, Markdown, JSON)
‚îÇ   ‚îú‚îÄ‚îÄ templates/           # Jinja2 templates
‚îÇ   ‚îî‚îÄ‚îÄ cli/                 # Click-based CLI
‚îú‚îÄ‚îÄ tests/                   # Test suite (unit + integration)
‚îú‚îÄ‚îÄ examples/                # Example reports
‚îî‚îÄ‚îÄ specs/                   # Feature specifications
```

### Key Technologies
- Python 3.11+
- Click (CLI framework)
- Jinja2 (templating)
- Pytest (testing)
- Black, isort, ruff (code quality)

### Development Workflow
1. Create feature branch: `git checkout -b NNN-feature-name`
2. Implement changes with tests
3. Run linters: `black . && isort . && ruff check .`
4. Run tests: `pytest`
5. Commit with conventional commits
6. Create PR to main

---

## Feature Requirements

Research Report Generator/Updater Utility

**Priority**: P4 (Enhancement)

**Description**: Create a utility tool to help maintain and update the research report (agent-ready-codebase-attributes.md) following the validation schema defined in contracts/research-report-schema.md.

**Requirements**:
- Generate new research reports from templates
- Validate existing reports against schema (contracts/research-report-schema.md)
- Update/add attributes while maintaining schema compliance
- Automatically format citations and references
- Extract tier assignments and metadata
- Verify 25 attributes, 4 tiers, 20+ references
- Check for required sections (Definition, Measurable Criteria, Impact on Agent Behavior)

**Use Case**:
```bash
# Validate existing research report
agentready research validate agent-ready-codebase-attributes.md

# Generate new research report from template
agentready research init --output new-research.md

# Add new attribute to research report
agentready research add-attribute \
  --id "attribute_26" \
  --name "New Attribute" \
  --tier 2 \
  --file research.md

# Update metadata (version, date)
agentready research bump-version --type minor

# Lint and format research report
agentready research format research.md
```

**Features**:
- Schema validation (errors vs warnings per research-report-schema.md)
- Automated metadata header generation (version, date in YAML frontmatter)
- Attribute numbering consistency checks (1.1, 1.2, ..., 15.1)
- Citation deduplication and formatting
- Tier distribution balance warnings
- Category coverage analysis
- Markdown formatting enforcement (consistent structure)
- Reference URL reachability checks

**Related**: Research report maintenance, schema compliance, documentation quality

**Notes**:
- Must follow contracts/research-report-schema.md validation rules
- Should prevent invalid reports from being committed
- Could integrate with pre-commit hooks for research report changes
- Consider CLI commands under `agentready research` subcommand
- Tool should be self-documenting (help users fix validation errors)
- Future: Could use LLMs to help generate attribute descriptions from academic papers

---


---

## Implementation Checklist

Before you begin:
- [ ] Read CLAUDE.md for project context
- [ ] Review existing similar features (if applicable)
- [ ] Understand the data model (src/agentready/models/)
- [ ] Check acceptance criteria in feature description

Implementation steps:
- [ ] Create feature branch
- [ ] Implement core functionality
- [ ] Add unit tests (target >80% coverage)
- [ ] Add integration tests (if applicable)
- [ ] Run linters and fix any issues
- [ ] Update documentation (README.md, CLAUDE.md if needed)
- [ ] Self-test the feature end-to-end
- [ ] Create PR with descriptive title and body

Code quality requirements:
- [ ] All code formatted with black (88 char lines)
- [ ] Imports sorted with isort
- [ ] No ruff violations
- [ ] All tests passing
- [ ] Type hints where appropriate
- [ ] Docstrings for public APIs

---

## Key Files to Review

Based on this feature, you should review:
- `src/agentready/models/` - Understand Assessment, Finding, Attribute models
- `src/agentready/services/scanner.py` - Scanner orchestration
- `src/agentready/assessors/base.py` - BaseAssessor pattern
- `src/agentready/reporters/` - Report generation
- `CLAUDE.md` - Project overview and guidelines
- `BACKLOG.md` - Full context of this feature

---

## Testing Strategy

For this feature, ensure:
1. **Unit tests** for core logic (80%+ coverage)
2. **Integration tests** for end-to-end workflows
3. **Edge case tests** (empty inputs, missing files, errors)
4. **Error handling tests** (graceful degradation)

Run tests:
```bash
# All tests
pytest

# With coverage
pytest --cov=src/agentready --cov-report=html

# Specific test file
pytest tests/unit/test_feature.py -v
```

---

## Success Criteria

This feature is complete when:
- ‚úÖ All acceptance criteria from feature description are met
- ‚úÖ Tests passing with >80% coverage for new code
- ‚úÖ All linters passing (black, isort, ruff)
- ‚úÖ Documentation updated
- ‚úÖ PR created with clear description
- ‚úÖ Self-tested end-to-end

---

## Questions to Clarify (if needed)

If anything is unclear during implementation:
1. Check CLAUDE.md for project patterns
2. Review similar existing features
3. Ask for clarification in PR comments
4. Reference the original backlog item

---

## Getting Started

```bash
# Clone and setup
git clone https://github.com/redhat/agentready.git
cd agentready

# Create virtual environment
uv venv && source .venv/bin/activate

# Install dependencies
uv pip install -e .
uv pip install pytest black isort ruff

# Create feature branch
git checkout -b 007-research-report-generator/updater-utility

# Start implementing!
```

---

**Note**: This is a coldstart prompt. You have all context needed to implement this feature independently. Read the linked files, follow the patterns, and deliver high-quality code with tests.
````

## File: coldstart-prompts/08-repomix-integration.md
````markdown
# Coldstart Implementation Prompt: Repomix Integration

**Priority**: P4
**Repository**: agentready (https://github.com/redhat/agentready)
**Branch Strategy**: Create feature branch from main

---

## Context

You are implementing a feature for AgentReady, a repository quality assessment tool for AI-assisted development.

### Repository Structure
```
agentready/
‚îú‚îÄ‚îÄ src/agentready/          # Source code
‚îÇ   ‚îú‚îÄ‚îÄ models/              # Data models
‚îÇ   ‚îú‚îÄ‚îÄ services/            # Scanner orchestration
‚îÇ   ‚îú‚îÄ‚îÄ assessors/           # Attribute assessments
‚îÇ   ‚îú‚îÄ‚îÄ reporters/           # Report generation (HTML, Markdown, JSON)
‚îÇ   ‚îú‚îÄ‚îÄ templates/           # Jinja2 templates
‚îÇ   ‚îî‚îÄ‚îÄ cli/                 # Click-based CLI
‚îú‚îÄ‚îÄ tests/                   # Test suite (unit + integration)
‚îú‚îÄ‚îÄ examples/                # Example reports
‚îî‚îÄ‚îÄ specs/                   # Feature specifications
```

### Key Technologies
- Python 3.11+
- Click (CLI framework)
- Jinja2 (templating)
- Pytest (testing)
- Black, isort, ruff (code quality)

### Development Workflow
1. Create feature branch: `git checkout -b NNN-feature-name`
2. Implement changes with tests
3. Run linters: `black . && isort . && ruff check .`
4. Run tests: `pytest`
5. Commit with conventional commits
6. Create PR to main

---

## Feature Requirements

Repomix Integration

**Priority**: P4 (Enhancement)

**Description**: Integrate with Repomix (https://github.com/yamadashy/repomix) to generate AI-optimized repository context files for both new and existing repositories.

**Requirements**:
- Generate Repomix output for existing repositories
- Include Repomix configuration in bootstrapped new repositories
- Optional GitHub Actions integration for automatic regeneration
- Support Repomix configuration customization
- Integrate with agentready assessment workflow

**Use Case**:
```bash
# Generate Repomix output for current repository
agentready repomix-generate

# Bootstrap new repo with Repomix integration
agentready init --repo my-project --language python --repomix

# This would:
# 1. Set up Repomix configuration
# 2. Add GitHub Action for automatic regeneration
# 3. Generate initial repository context file
# 4. Include Repomix output in .gitignore appropriately
```

**Features**:
- Automatic Repomix configuration based on repository language
- GitHub Actions workflow for CI/CD integration
- Custom ignore patterns aligned with agentready assessment
- Support for both markdown and XML output formats
- Integration with agentready bootstrap command

**Related**: Repository initialization, AI-assisted development workflows

**Notes**:
- Repomix generates optimized repository context for LLMs
- Could enhance CLAUDE.md with reference to Repomix output
- Should align with existing .gitignore patterns
- Consider adding Repomix freshness check to assessment attributes
- May improve agentready's own repository understanding

---


---

## Implementation Checklist

Before you begin:
- [ ] Read CLAUDE.md for project context
- [ ] Review existing similar features (if applicable)
- [ ] Understand the data model (src/agentready/models/)
- [ ] Check acceptance criteria in feature description

Implementation steps:
- [ ] Create feature branch
- [ ] Implement core functionality
- [ ] Add unit tests (target >80% coverage)
- [ ] Add integration tests (if applicable)
- [ ] Run linters and fix any issues
- [ ] Update documentation (README.md, CLAUDE.md if needed)
- [ ] Self-test the feature end-to-end
- [ ] Create PR with descriptive title and body

Code quality requirements:
- [ ] All code formatted with black (88 char lines)
- [ ] Imports sorted with isort
- [ ] No ruff violations
- [ ] All tests passing
- [ ] Type hints where appropriate
- [ ] Docstrings for public APIs

---

## Key Files to Review

Based on this feature, you should review:
- `src/agentready/models/` - Understand Assessment, Finding, Attribute models
- `src/agentready/services/scanner.py` - Scanner orchestration
- `src/agentready/assessors/base.py` - BaseAssessor pattern
- `src/agentready/reporters/` - Report generation
- `CLAUDE.md` - Project overview and guidelines
- `BACKLOG.md` - Full context of this feature

---

## Testing Strategy

For this feature, ensure:
1. **Unit tests** for core logic (80%+ coverage)
2. **Integration tests** for end-to-end workflows
3. **Edge case tests** (empty inputs, missing files, errors)
4. **Error handling tests** (graceful degradation)

Run tests:
```bash
# All tests
pytest

# With coverage
pytest --cov=src/agentready --cov-report=html

# Specific test file
pytest tests/unit/test_feature.py -v
```

---

## Success Criteria

This feature is complete when:
- ‚úÖ All acceptance criteria from feature description are met
- ‚úÖ Tests passing with >80% coverage for new code
- ‚úÖ All linters passing (black, isort, ruff)
- ‚úÖ Documentation updated
- ‚úÖ PR created with clear description
- ‚úÖ Self-tested end-to-end

---

## Questions to Clarify (if needed)

If anything is unclear during implementation:
1. Check CLAUDE.md for project patterns
2. Review similar existing features
3. Ask for clarification in PR comments
4. Reference the original backlog item

---

## Getting Started

```bash
# Clone and setup
git clone https://github.com/redhat/agentready.git
cd agentready

# Create virtual environment
uv venv && source .venv/bin/activate

# Install dependencies
uv pip install -e .
uv pip install pytest black isort ruff

# Create feature branch
git checkout -b 008-repomix-integration

# Start implementing!
```

---

**Note**: This is a coldstart prompt. You have all context needed to implement this feature independently. Read the linked files, follow the patterns, and deliver high-quality code with tests.
````

## File: coldstart-prompts/09-agentready-repository-agent.md
````markdown
# Coldstart Implementation Prompt: AgentReady Repository Agent

**Priority**: P3
**Repository**: agentready (https://github.com/redhat/agentready)
**Branch Strategy**: Create feature branch from main

---

## Context

You are implementing a feature for AgentReady, a repository quality assessment tool for AI-assisted development.

### Repository Structure
```
agentready/
‚îú‚îÄ‚îÄ src/agentready/          # Source code
‚îÇ   ‚îú‚îÄ‚îÄ models/              # Data models
‚îÇ   ‚îú‚îÄ‚îÄ services/            # Scanner orchestration
‚îÇ   ‚îú‚îÄ‚îÄ assessors/           # Attribute assessments
‚îÇ   ‚îú‚îÄ‚îÄ reporters/           # Report generation (HTML, Markdown, JSON)
‚îÇ   ‚îú‚îÄ‚îÄ templates/           # Jinja2 templates
‚îÇ   ‚îî‚îÄ‚îÄ cli/                 # Click-based CLI
‚îú‚îÄ‚îÄ tests/                   # Test suite (unit + integration)
‚îú‚îÄ‚îÄ examples/                # Example reports
‚îî‚îÄ‚îÄ specs/                   # Feature specifications
```

### Key Technologies
- Python 3.11+
- Click (CLI framework)
- Jinja2 (templating)
- Pytest (testing)
- Black, isort, ruff (code quality)

### Development Workflow
1. Create feature branch: `git checkout -b NNN-feature-name`
2. Implement changes with tests
3. Run linters: `black . && isort . && ruff check .`
4. Run tests: `pytest`
5. Commit with conventional commits
6. Create PR to main

---

## Feature Requirements

AgentReady Repository Agent

**Priority**: P3 (Important)

**Description**: Create a specialized Claude Code agent for the AgentReady repository to assist with development, testing, and maintenance tasks.

**Requirements**:
- Agent with deep knowledge of AgentReady architecture
- Understands assessment workflow, scoring logic, and report generation
- Can help with:
  - Implementing new assessors
  - Enhancing existing assessors
  - Writing tests for new features
  - Debugging assessment issues
  - Improving report templates
  - Optimizing performance

**Use Case**:
```bash
# In Claude Code, use the agentready-dev agent
/agentready-dev implement new assessor for dependency security scanning
/agentready-dev debug why Python type annotation detection is failing
/agentready-dev optimize assessment performance for large repositories
```

**Features**:
- Pre-loaded context about AgentReady codebase structure
- Knowledge of assessment attributes and scoring algorithm
- Understanding of tier-based weighting system
- Familiar with reporter implementations (HTML, Markdown)
- Can generate new assessors following established patterns

**Implementation**:
- Create `.claude/agents/agentready-dev.md` with agent specification
- Include links to key design documents (data-model.md, plan.md, research.md)
- Provide common development patterns and examples
- Reference test structure and coverage requirements

**Related**: Development workflow, code generation, testing

**Notes**:
- Agent should follow constitution principles (library-first, TDD when requested)
- Should know about stub assessors and how to expand them
- Can help with performance benchmarking and optimization
- Should understand the research report structure and attribute definitions

---


---

## Implementation Checklist

Before you begin:
- [ ] Read CLAUDE.md for project context
- [ ] Review existing similar features (if applicable)
- [ ] Understand the data model (src/agentready/models/)
- [ ] Check acceptance criteria in feature description

Implementation steps:
- [ ] Create feature branch
- [ ] Implement core functionality
- [ ] Add unit tests (target >80% coverage)
- [ ] Add integration tests (if applicable)
- [ ] Run linters and fix any issues
- [ ] Update documentation (README.md, CLAUDE.md if needed)
- [ ] Self-test the feature end-to-end
- [ ] Create PR with descriptive title and body

Code quality requirements:
- [ ] All code formatted with black (88 char lines)
- [ ] Imports sorted with isort
- [ ] No ruff violations
- [ ] All tests passing
- [ ] Type hints where appropriate
- [ ] Docstrings for public APIs

---

## Key Files to Review

Based on this feature, you should review:
- `src/agentready/models/` - Understand Assessment, Finding, Attribute models
- `src/agentready/services/scanner.py` - Scanner orchestration
- `src/agentready/assessors/base.py` - BaseAssessor pattern
- `src/agentready/reporters/` - Report generation
- `CLAUDE.md` - Project overview and guidelines
- `BACKLOG.md` - Full context of this feature

---

## Testing Strategy

For this feature, ensure:
1. **Unit tests** for core logic (80%+ coverage)
2. **Integration tests** for end-to-end workflows
3. **Edge case tests** (empty inputs, missing files, errors)
4. **Error handling tests** (graceful degradation)

Run tests:
```bash
# All tests
pytest

# With coverage
pytest --cov=src/agentready --cov-report=html

# Specific test file
pytest tests/unit/test_feature.py -v
```

---

## Success Criteria

This feature is complete when:
- ‚úÖ All acceptance criteria from feature description are met
- ‚úÖ Tests passing with >80% coverage for new code
- ‚úÖ All linters passing (black, isort, ruff)
- ‚úÖ Documentation updated
- ‚úÖ PR created with clear description
- ‚úÖ Self-tested end-to-end

---

## Questions to Clarify (if needed)

If anything is unclear during implementation:
1. Check CLAUDE.md for project patterns
2. Review similar existing features
3. Ask for clarification in PR comments
4. Reference the original backlog item

---

## Getting Started

```bash
# Clone and setup
git clone https://github.com/redhat/agentready.git
cd agentready

# Create virtual environment
uv venv && source .venv/bin/activate

# Install dependencies
uv pip install -e .
uv pip install pytest black isort ruff

# Create feature branch
git checkout -b 009-agentready-repository-agent

# Start implementing!
```

---

**Note**: This is a coldstart prompt. You have all context needed to implement this feature independently. Read the linked files, follow the patterns, and deliver high-quality code with tests.
````

## File: coldstart-prompts/10-customizable-html-report-themes.md
````markdown
# Coldstart Implementation Prompt: Customizable HTML Report Themes

**Priority**: P4
**Repository**: agentready (https://github.com/redhat/agentready)
**Branch Strategy**: Create feature branch from main

---

## Context

You are implementing a feature for AgentReady, a repository quality assessment tool for AI-assisted development.

### Repository Structure
```
agentready/
‚îú‚îÄ‚îÄ src/agentready/          # Source code
‚îÇ   ‚îú‚îÄ‚îÄ models/              # Data models
‚îÇ   ‚îú‚îÄ‚îÄ services/            # Scanner orchestration
‚îÇ   ‚îú‚îÄ‚îÄ assessors/           # Attribute assessments
‚îÇ   ‚îú‚îÄ‚îÄ reporters/           # Report generation (HTML, Markdown, JSON)
‚îÇ   ‚îú‚îÄ‚îÄ templates/           # Jinja2 templates
‚îÇ   ‚îî‚îÄ‚îÄ cli/                 # Click-based CLI
‚îú‚îÄ‚îÄ tests/                   # Test suite (unit + integration)
‚îú‚îÄ‚îÄ examples/                # Example reports
‚îî‚îÄ‚îÄ specs/                   # Feature specifications
```

### Key Technologies
- Python 3.11+
- Click (CLI framework)
- Jinja2 (templating)
- Pytest (testing)
- Black, isort, ruff (code quality)

### Development Workflow
1. Create feature branch: `git checkout -b NNN-feature-name`
2. Implement changes with tests
3. Run linters: `black . && isort . && ruff check .`
4. Run tests: `pytest`
5. Commit with conventional commits
6. Create PR to main

---

## Feature Requirements

Customizable HTML Report Themes

**Priority**: P4 (Enhancement)

**Description**: Allow users to customize the appearance of HTML reports with themes, color schemes, and layout options.

**Requirements**:
- Theme system for HTML reports
- Pre-built themes (default, dark mode, high contrast, colorblind-friendly)
- Custom theme support via configuration
- Maintain accessibility standards (WCAG 2.1 AA)
- Preview themes before applying

**Use Case**:
```yaml
# .agentready-config.yaml
report_theme: dark  # or 'light', 'high-contrast', 'custom'

custom_theme:
  primary_color: "#2563eb"
  success_color: "#10b981"
  warning_color: "#f59e0b"
  danger_color: "#ef4444"
  background: "#1e293b"
  text: "#e2e8f0"
  font_family: "Inter, sans-serif"
```

**Features**:
- **Theme dropdown in top-right corner of HTML report** (runtime switching)
- **Quick dark/light mode toggle button** (one-click switching between dark and light)
- Multiple built-in themes (light, dark, high-contrast, solarized, dracula)
- Dark mode support with proper color inversion
- Custom color palettes
- Font selection (system fonts + web-safe fonts)
- Layout density options (compact, comfortable, spacious)
- Logo/branding customization
- Export theme as reusable configuration
- Save theme preference to localStorage (persists across reports)

**Implementation**:
- CSS custom properties (variables) for theming
- JavaScript theme switcher in HTML report (no page reload)
- Theme loader in HTMLReporter
- Validate theme configurations
- Preserve accessibility in all themes (WCAG 2.1 AA)
- Add theme preview command: `agentready theme-preview dark`
- Embed all theme CSS in single HTML file (offline-capable)

**Related**: HTML report generation, user experience

**Notes**:
- All themes must maintain WCAG 2.1 AA contrast ratios
- Dark mode should invert appropriately, not just be dark
- Consider colorblind-friendly palettes (Viridis, ColorBrewer)
- Custom themes should be shareable (export/import)
- Could add theme gallery in documentation

---


---

## Implementation Checklist

Before you begin:
- [ ] Read CLAUDE.md for project context
- [ ] Review existing similar features (if applicable)
- [ ] Understand the data model (src/agentready/models/)
- [ ] Check acceptance criteria in feature description

Implementation steps:
- [ ] Create feature branch
- [ ] Implement core functionality
- [ ] Add unit tests (target >80% coverage)
- [ ] Add integration tests (if applicable)
- [ ] Run linters and fix any issues
- [ ] Update documentation (README.md, CLAUDE.md if needed)
- [ ] Self-test the feature end-to-end
- [ ] Create PR with descriptive title and body

Code quality requirements:
- [ ] All code formatted with black (88 char lines)
- [ ] Imports sorted with isort
- [ ] No ruff violations
- [ ] All tests passing
- [ ] Type hints where appropriate
- [ ] Docstrings for public APIs

---

## Key Files to Review

Based on this feature, you should review:
- `src/agentready/models/` - Understand Assessment, Finding, Attribute models
- `src/agentready/services/scanner.py` - Scanner orchestration
- `src/agentready/assessors/base.py` - BaseAssessor pattern
- `src/agentready/reporters/` - Report generation
- `CLAUDE.md` - Project overview and guidelines
- `BACKLOG.md` - Full context of this feature

---

## Testing Strategy

For this feature, ensure:
1. **Unit tests** for core logic (80%+ coverage)
2. **Integration tests** for end-to-end workflows
3. **Edge case tests** (empty inputs, missing files, errors)
4. **Error handling tests** (graceful degradation)

Run tests:
```bash
# All tests
pytest

# With coverage
pytest --cov=src/agentready --cov-report=html

# Specific test file
pytest tests/unit/test_feature.py -v
```

---

## Success Criteria

This feature is complete when:
- ‚úÖ All acceptance criteria from feature description are met
- ‚úÖ Tests passing with >80% coverage for new code
- ‚úÖ All linters passing (black, isort, ruff)
- ‚úÖ Documentation updated
- ‚úÖ PR created with clear description
- ‚úÖ Self-tested end-to-end

---

## Questions to Clarify (if needed)

If anything is unclear during implementation:
1. Check CLAUDE.md for project patterns
2. Review similar existing features
3. Ask for clarification in PR comments
4. Reference the original backlog item

---

## Getting Started

```bash
# Clone and setup
git clone https://github.com/redhat/agentready.git
cd agentready

# Create virtual environment
uv venv && source .venv/bin/activate

# Install dependencies
uv pip install -e .
uv pip install pytest black isort ruff

# Create feature branch
git checkout -b 010-customizable-html-report-themes

# Start implementing!
```

---

**Note**: This is a coldstart prompt. You have all context needed to implement this feature independently. Read the linked files, follow the patterns, and deliver high-quality code with tests.
````

## File: coldstart-prompts/11-fix-code-quality-issues-from-code-review.md
````markdown
# Coldstart Implementation Prompt: Fix Code Quality Issues from Code Review

**Priority**: P1
**Repository**: agentready (https://github.com/redhat/agentready)
**Branch Strategy**: Create feature branch from main

---

## Context

You are implementing a feature for AgentReady, a repository quality assessment tool for AI-assisted development.

### Repository Structure
```
agentready/
‚îú‚îÄ‚îÄ src/agentready/          # Source code
‚îÇ   ‚îú‚îÄ‚îÄ models/              # Data models
‚îÇ   ‚îú‚îÄ‚îÄ services/            # Scanner orchestration
‚îÇ   ‚îú‚îÄ‚îÄ assessors/           # Attribute assessments
‚îÇ   ‚îú‚îÄ‚îÄ reporters/           # Report generation (HTML, Markdown, JSON)
‚îÇ   ‚îú‚îÄ‚îÄ templates/           # Jinja2 templates
‚îÇ   ‚îî‚îÄ‚îÄ cli/                 # Click-based CLI
‚îú‚îÄ‚îÄ tests/                   # Test suite (unit + integration)
‚îú‚îÄ‚îÄ examples/                # Example reports
‚îî‚îÄ‚îÄ specs/                   # Feature specifications
```

### Key Technologies
- Python 3.11+
- Click (CLI framework)
- Jinja2 (templating)
- Pytest (testing)
- Black, isort, ruff (code quality)

### Development Workflow
1. Create feature branch: `git checkout -b NNN-feature-name`
2. Implement changes with tests
3. Run linters: `black . && isort . && ruff check .`
4. Run tests: `pytest`
5. Commit with conventional commits
6. Create PR to main

---

## Feature Requirements

Fix Code Quality Issues from Code Review

**Priority**: P1 (High - Quality & Reliability)

**Description**: Address P1 issues discovered in code review that affect reliability, accuracy, and code quality.

**Issues to Fix**:

1. **TOCTOU (Time-of-Check-Time-of-Use) in File Operations**
   - **Location**: Multiple assessors (`documentation.py:46-50`, `documentation.py:174-191`)
   - **Problem**: Check if file exists, then read in separate operation - file could be deleted in between
   - **Impact**: Crashes instead of graceful degradation
   - **Fix**: Use try-except around file reads instead of existence checks
   ```python
   # BEFORE:
   if claude_md_path.exists():
       size = claude_md_path.stat().st_size

   # AFTER:
   try:
       with open(claude_md_path, "r") as f:
           size = len(f.read())
   except FileNotFoundError:
       return Finding(...status="fail"...)
   except OSError as e:
       return Finding.error(self.attribute, f"Could not read: {e}")
   ```

2. **Inaccurate Type Annotation Detection**
   - **Location**: `src/agentready/assessors/code_quality.py:98-102`
   - **Problem**: Regex-based detection has false positives (string literals, dict literals)
   - **Impact**: Inflated type annotation coverage scores
   - **Fix**: Use AST parsing instead of regex:
   ```python
   import ast
   tree = ast.parse(content)
   for node in ast.walk(tree):
       if isinstance(node, ast.FunctionDef):
           total_functions += 1
           has_annotations = (node.returns is not None or
                            any(arg.annotation for arg in node.args.args))
           if has_annotations:
               typed_functions += 1
   ```

3. **Assessment Validation Semantic Confusion**
   - **Location**: `src/agentready/models/assessment.py:54-59`
   - **Problem**: Field named `attributes_skipped` but includes `error` and `not_applicable` statuses
   - **Impact**: Confusing API, unclear semantics
   - **Fix**: Rename to `attributes_not_assessed` OR add separate counters

**Acceptance Criteria**:
- [ ] All file operations use try-except pattern
- [ ] Type annotation detection uses AST parsing
- [ ] Assessment model fields clearly named
- [ ] Tests added for TOCTOU edge cases
- [ ] Tests added for type annotation false positives
- [ ] Documentation updated

**Priority Justification**: These affect reliability and measurement accuracy - critical for a quality assessment tool.

**Related**: Testing improvements, code quality

---


---

## Implementation Checklist

Before you begin:
- [ ] Read CLAUDE.md for project context
- [ ] Review existing similar features (if applicable)
- [ ] Understand the data model (src/agentready/models/)
- [ ] Check acceptance criteria in feature description

Implementation steps:
- [ ] Create feature branch
- [ ] Implement core functionality
- [ ] Add unit tests (target >80% coverage)
- [ ] Add integration tests (if applicable)
- [ ] Run linters and fix any issues
- [ ] Update documentation (README.md, CLAUDE.md if needed)
- [ ] Self-test the feature end-to-end
- [ ] Create PR with descriptive title and body

Code quality requirements:
- [ ] All code formatted with black (88 char lines)
- [ ] Imports sorted with isort
- [ ] No ruff violations
- [ ] All tests passing
- [ ] Type hints where appropriate
- [ ] Docstrings for public APIs

---

## Key Files to Review

Based on this feature, you should review:
- `src/agentready/models/` - Understand Assessment, Finding, Attribute models
- `src/agentready/services/scanner.py` - Scanner orchestration
- `src/agentready/assessors/base.py` - BaseAssessor pattern
- `src/agentready/reporters/` - Report generation
- `CLAUDE.md` - Project overview and guidelines
- `BACKLOG.md` - Full context of this feature

---

## Testing Strategy

For this feature, ensure:
1. **Unit tests** for core logic (80%+ coverage)
2. **Integration tests** for end-to-end workflows
3. **Edge case tests** (empty inputs, missing files, errors)
4. **Error handling tests** (graceful degradation)

Run tests:
```bash
# All tests
pytest

# With coverage
pytest --cov=src/agentready --cov-report=html

# Specific test file
pytest tests/unit/test_feature.py -v
```

---

## Success Criteria

This feature is complete when:
- ‚úÖ All acceptance criteria from feature description are met
- ‚úÖ Tests passing with >80% coverage for new code
- ‚úÖ All linters passing (black, isort, ruff)
- ‚úÖ Documentation updated
- ‚úÖ PR created with clear description
- ‚úÖ Self-tested end-to-end

---

## Questions to Clarify (if needed)

If anything is unclear during implementation:
1. Check CLAUDE.md for project patterns
2. Review similar existing features
3. Ask for clarification in PR comments
4. Reference the original backlog item

---

## Getting Started

```bash
# Clone and setup
git clone https://github.com/redhat/agentready.git
cd agentready

# Create virtual environment
uv venv && source .venv/bin/activate

# Install dependencies
uv pip install -e .
uv pip install pytest black isort ruff

# Create feature branch
git checkout -b 011-fix-code-quality-issues-from-code-review

# Start implementing!
```

---

**Note**: This is a coldstart prompt. You have all context needed to implement this feature independently. Read the linked files, follow the patterns, and deliver high-quality code with tests.
````

## File: coldstart-prompts/12-improve-test-coverage-and-edge-case-handling.md
````markdown
# Coldstart Implementation Prompt: Improve Test Coverage and Edge Case Handling

**Priority**: P1
**Repository**: agentready (https://github.com/redhat/agentready)
**Branch Strategy**: Create feature branch from main

---

## Context

You are implementing a feature for AgentReady, a repository quality assessment tool for AI-assisted development.

### Repository Structure
```
agentready/
‚îú‚îÄ‚îÄ src/agentready/          # Source code
‚îÇ   ‚îú‚îÄ‚îÄ models/              # Data models
‚îÇ   ‚îú‚îÄ‚îÄ services/            # Scanner orchestration
‚îÇ   ‚îú‚îÄ‚îÄ assessors/           # Attribute assessments
‚îÇ   ‚îú‚îÄ‚îÄ reporters/           # Report generation (HTML, Markdown, JSON)
‚îÇ   ‚îú‚îÄ‚îÄ templates/           # Jinja2 templates
‚îÇ   ‚îî‚îÄ‚îÄ cli/                 # Click-based CLI
‚îú‚îÄ‚îÄ tests/                   # Test suite (unit + integration)
‚îú‚îÄ‚îÄ examples/                # Example reports
‚îî‚îÄ‚îÄ specs/                   # Feature specifications
```

### Key Technologies
- Python 3.11+
- Click (CLI framework)
- Jinja2 (templating)
- Pytest (testing)
- Black, isort, ruff (code quality)

### Development Workflow
1. Create feature branch: `git checkout -b NNN-feature-name`
2. Implement changes with tests
3. Run linters: `black . && isort . && ruff check .`
4. Run tests: `pytest`
5. Commit with conventional commits
6. Create PR to main

---

## Feature Requirements

Improve Test Coverage and Edge Case Handling

**Priority**: P1 (High - Quality Assurance)

**Description**: Increase test coverage from 37% to >80% and add tests for critical edge cases discovered in code review.

**Critical Test Gaps**:

1. **Error Handling Paths** (Currently 0% coverage)
   - OSError, PermissionError in file operations
   - MissingToolError in assessors
   - Invalid repository paths
   - Malformed git repositories

2. **Edge Cases** (No tests)
   - Empty repositories
   - Binary files instead of text
   - Symlinks in repository
   - Very large repositories (>10k files)
   - Repositories with `test/` vs `tests/` directories

3. **Security Test Cases**
   - XSS in repository names, commit messages
   - Path traversal attempts
   - Malicious file names

4. **Scorer Edge Cases**
   - All attributes skipped (score should be 0.0)
   - Config weights don't sum to 1.0
   - Division by zero scenarios

**Implementation**:
```python
# tests/unit/test_edge_cases.py
def test_empty_repository(tmp_path):
    """Test assessment of completely empty repository."""
    # Create empty git repo
    repo = Repository(path=tmp_path, ...)
    scanner = Scanner(repo, config=None)
    assessment = scanner.scan(assessors)
    # Should not crash, should have valid score
    assert 0.0 <= assessment.overall_score <= 100.0

def test_permission_denied_file(tmp_path):
    """Test graceful handling of permission errors."""
    # Create unreadable file
    restricted = tmp_path / "CLAUDE.md"
    restricted.write_text("test")
    restricted.chmod(0o000)

    assessor = CLAUDEmdAssessor()
    finding = assessor.assess(Repository(...))
    assert finding.status == "error"
    assert "permission" in finding.error_message.lower()

def test_binary_file_as_readme(tmp_path):
    """Test handling of binary files."""
    readme = tmp_path / "README.md"
    readme.write_bytes(b"\x00\x01\x02\x03")

    assessor = READMEAssessor()
    finding = assessor.assess(Repository(...))
    # Should not crash
```

**Acceptance Criteria**:
- [ ] Test coverage increased to >80%
- [ ] All error handling paths tested
- [ ] Edge cases for empty/malformed repos covered
- [ ] Security test cases added
- [ ] Integration tests for complete workflows
- [ ] CI runs coverage report and fails <75%

**Priority Justification**: Quality assessment tool must be thoroughly tested. Current 37% coverage is unacceptable.

**Related**: CI/CD improvements, reliability

---


---

## Implementation Checklist

Before you begin:
- [ ] Read CLAUDE.md for project context
- [ ] Review existing similar features (if applicable)
- [ ] Understand the data model (src/agentready/models/)
- [ ] Check acceptance criteria in feature description

Implementation steps:
- [ ] Create feature branch
- [ ] Implement core functionality
- [ ] Add unit tests (target >80% coverage)
- [ ] Add integration tests (if applicable)
- [ ] Run linters and fix any issues
- [ ] Update documentation (README.md, CLAUDE.md if needed)
- [ ] Self-test the feature end-to-end
- [ ] Create PR with descriptive title and body

Code quality requirements:
- [ ] All code formatted with black (88 char lines)
- [ ] Imports sorted with isort
- [ ] No ruff violations
- [ ] All tests passing
- [ ] Type hints where appropriate
- [ ] Docstrings for public APIs

---

## Key Files to Review

Based on this feature, you should review:
- `src/agentready/models/` - Understand Assessment, Finding, Attribute models
- `src/agentready/services/scanner.py` - Scanner orchestration
- `src/agentready/assessors/base.py` - BaseAssessor pattern
- `src/agentready/reporters/` - Report generation
- `CLAUDE.md` - Project overview and guidelines
- `BACKLOG.md` - Full context of this feature

---

## Testing Strategy

For this feature, ensure:
1. **Unit tests** for core logic (80%+ coverage)
2. **Integration tests** for end-to-end workflows
3. **Edge case tests** (empty inputs, missing files, errors)
4. **Error handling tests** (graceful degradation)

Run tests:
```bash
# All tests
pytest

# With coverage
pytest --cov=src/agentready --cov-report=html

# Specific test file
pytest tests/unit/test_feature.py -v
```

---

## Success Criteria

This feature is complete when:
- ‚úÖ All acceptance criteria from feature description are met
- ‚úÖ Tests passing with >80% coverage for new code
- ‚úÖ All linters passing (black, isort, ruff)
- ‚úÖ Documentation updated
- ‚úÖ PR created with clear description
- ‚úÖ Self-tested end-to-end

---

## Questions to Clarify (if needed)

If anything is unclear during implementation:
1. Check CLAUDE.md for project patterns
2. Review similar existing features
3. Ask for clarification in PR comments
4. Reference the original backlog item

---

## Getting Started

```bash
# Clone and setup
git clone https://github.com/redhat/agentready.git
cd agentready

# Create virtual environment
uv venv && source .venv/bin/activate

# Install dependencies
uv pip install -e .
uv pip install pytest black isort ruff

# Create feature branch
git checkout -b 012-improve-test-coverage-and-edge-case-handling

# Start implementing!
```

---

**Note**: This is a coldstart prompt. You have all context needed to implement this feature independently. Read the linked files, follow the patterns, and deliver high-quality code with tests.
````

## File: coldstart-prompts/13-add-security-quality-improvements-from-code-review.md
````markdown
# Coldstart Implementation Prompt: Add Security & Quality Improvements from Code Review

**Priority**: P2
**Repository**: agentready (https://github.com/redhat/agentready)
**Branch Strategy**: Create feature branch from main

---

## Context

You are implementing a feature for AgentReady, a repository quality assessment tool for AI-assisted development.

### Repository Structure
```
agentready/
‚îú‚îÄ‚îÄ src/agentready/          # Source code
‚îÇ   ‚îú‚îÄ‚îÄ models/              # Data models
‚îÇ   ‚îú‚îÄ‚îÄ services/            # Scanner orchestration
‚îÇ   ‚îú‚îÄ‚îÄ assessors/           # Attribute assessments
‚îÇ   ‚îú‚îÄ‚îÄ reporters/           # Report generation (HTML, Markdown, JSON)
‚îÇ   ‚îú‚îÄ‚îÄ templates/           # Jinja2 templates
‚îÇ   ‚îî‚îÄ‚îÄ cli/                 # Click-based CLI
‚îú‚îÄ‚îÄ tests/                   # Test suite (unit + integration)
‚îú‚îÄ‚îÄ examples/                # Example reports
‚îî‚îÄ‚îÄ specs/                   # Feature specifications
```

### Key Technologies
- Python 3.11+
- Click (CLI framework)
- Jinja2 (templating)
- Pytest (testing)
- Black, isort, ruff (code quality)

### Development Workflow
1. Create feature branch: `git checkout -b NNN-feature-name`
2. Implement changes with tests
3. Run linters: `black . && isort . && ruff check .`
4. Run tests: `pytest`
5. Commit with conventional commits
6. Create PR to main

---

## Feature Requirements

Add Security & Quality Improvements from Code Review

**Priority**: P2 (Medium - Polish)

**Description**: Address P2 improvements from code review for better UX and robustness.

**Improvements**:

1. **Input Validation Warnings**
   - Warn when scanning sensitive directories (`/etc`, `/.ssh`, `/var`)
   - Confirm before scanning large repositories (>10k files)

2. **Scorer Semantic Clarity**
   - Document behavior when all attributes skipped (returns 0.0)
   - Consider returning `None` or special value for "not assessable"
   - Add explicit documentation of edge cases

3. **Content Security Policy Headers**
   - Add CSP to HTML reports for defense-in-depth
   - Prevent inline script execution
   - Whitelist only necessary sources

**Implementation**:
```python
# In CLI
sensitive_dirs = ['/etc', '/sys', '/proc', '/.ssh', '/var']
if any(str(repo_path).startswith(p) for p in sensitive_dirs):
    click.confirm(
        f"Warning: Scanning sensitive directory {repo_path}. Continue?",
        abort=True
    )

# In HTMLReporter
csp_header = (
    "<meta http-equiv='Content-Security-Policy' "
    "content=\"default-src 'self'; script-src 'unsafe-inline'; "
    "style-src 'unsafe-inline'\">"
)
```

**Acceptance Criteria**:
- [ ] Warnings for sensitive directories
- [ ] CSP headers in HTML reports
- [ ] Scorer edge cases documented
- [ ] User guide updated with best practices

**Priority Justification**: UX polish and defense-in-depth, not critical bugs.

**Related**: User experience, security hardening

---


---

## Implementation Checklist

Before you begin:
- [ ] Read CLAUDE.md for project context
- [ ] Review existing similar features (if applicable)
- [ ] Understand the data model (src/agentready/models/)
- [ ] Check acceptance criteria in feature description

Implementation steps:
- [ ] Create feature branch
- [ ] Implement core functionality
- [ ] Add unit tests (target >80% coverage)
- [ ] Add integration tests (if applicable)
- [ ] Run linters and fix any issues
- [ ] Update documentation (README.md, CLAUDE.md if needed)
- [ ] Self-test the feature end-to-end
- [ ] Create PR with descriptive title and body

Code quality requirements:
- [ ] All code formatted with black (88 char lines)
- [ ] Imports sorted with isort
- [ ] No ruff violations
- [ ] All tests passing
- [ ] Type hints where appropriate
- [ ] Docstrings for public APIs

---

## Key Files to Review

Based on this feature, you should review:
- `src/agentready/models/` - Understand Assessment, Finding, Attribute models
- `src/agentready/services/scanner.py` - Scanner orchestration
- `src/agentready/assessors/base.py` - BaseAssessor pattern
- `src/agentready/reporters/` - Report generation
- `CLAUDE.md` - Project overview and guidelines
- `BACKLOG.md` - Full context of this feature

---

## Testing Strategy

For this feature, ensure:
1. **Unit tests** for core logic (80%+ coverage)
2. **Integration tests** for end-to-end workflows
3. **Edge case tests** (empty inputs, missing files, errors)
4. **Error handling tests** (graceful degradation)

Run tests:
```bash
# All tests
pytest

# With coverage
pytest --cov=src/agentready --cov-report=html

# Specific test file
pytest tests/unit/test_feature.py -v
```

---

## Success Criteria

This feature is complete when:
- ‚úÖ All acceptance criteria from feature description are met
- ‚úÖ Tests passing with >80% coverage for new code
- ‚úÖ All linters passing (black, isort, ruff)
- ‚úÖ Documentation updated
- ‚úÖ PR created with clear description
- ‚úÖ Self-tested end-to-end

---

## Questions to Clarify (if needed)

If anything is unclear during implementation:
1. Check CLAUDE.md for project patterns
2. Review similar existing features
3. Ask for clarification in PR comments
4. Reference the original backlog item

---

## Getting Started

```bash
# Clone and setup
git clone https://github.com/redhat/agentready.git
cd agentready

# Create virtual environment
uv venv && source .venv/bin/activate

# Install dependencies
uv pip install -e .
uv pip install pytest black isort ruff

# Create feature branch
git checkout -b 013-add-security-&-quality-improvements-from-code-revi

# Start implementing!
```

---

**Note**: This is a coldstart prompt. You have all context needed to implement this feature independently. Read the linked files, follow the patterns, and deliver high-quality code with tests.
````

## File: coldstart-prompts/14-align-subcommand-automated-remediation.md
````markdown
# Coldstart Implementation Prompt: Align Subcommand (Automated Remediation)

**Priority**: P1
**Repository**: agentready (https://github.com/redhat/agentready)
**Branch Strategy**: Create feature branch from main

---

## Context

You are implementing a feature for AgentReady, a repository quality assessment tool for AI-assisted development.

### Repository Structure
```
agentready/
‚îú‚îÄ‚îÄ src/agentready/          # Source code
‚îÇ   ‚îú‚îÄ‚îÄ models/              # Data models
‚îÇ   ‚îú‚îÄ‚îÄ services/            # Scanner orchestration
‚îÇ   ‚îú‚îÄ‚îÄ assessors/           # Attribute assessments
‚îÇ   ‚îú‚îÄ‚îÄ reporters/           # Report generation (HTML, Markdown, JSON)
‚îÇ   ‚îú‚îÄ‚îÄ templates/           # Jinja2 templates
‚îÇ   ‚îî‚îÄ‚îÄ cli/                 # Click-based CLI
‚îú‚îÄ‚îÄ tests/                   # Test suite (unit + integration)
‚îú‚îÄ‚îÄ examples/                # Example reports
‚îî‚îÄ‚îÄ specs/                   # Feature specifications
```

### Key Technologies
- Python 3.11+
- Click (CLI framework)
- Jinja2 (templating)
- Pytest (testing)
- Black, isort, ruff (code quality)

### Development Workflow
1. Create feature branch: `git checkout -b NNN-feature-name`
2. Implement changes with tests
3. Run linters: `black . && isort . && ruff check .`
4. Run tests: `pytest`
5. Commit with conventional commits
6. Create PR to main

---

## Feature Requirements

Align Subcommand (Automated Remediation)

**Priority**: P1 (Critical)

**Description**: Implement `agentready align` subcommand that automatically fixes failing attributes by generating and applying changes to the repository.

**Vision**: One command to align your repository with best practices - automatically create missing files, configure tooling, and improve code quality.

**Core Command**:

```bash
# Align repository to best practices
agentready align .

# Preview changes without applying
agentready align . --dry-run

# Apply specific attributes only
agentready align . --attributes claude_md_file,precommit_hooks

# Create GitHub PR instead of direct changes
agentready align . --create-pr

# Interactive mode (confirm each change)
agentready align . --interactive
```

**Supported Fixes**:

1. **Template-Based Fixes** (Auto-applicable):
   - `claude_md_file`: Generate CLAUDE.md from repository analysis
   - `gitignore_completeness`: Add missing patterns to .gitignore
   - `precommit_hooks`: Create .pre-commit-config.yaml with language-specific hooks
   - `readme_structure`: Scaffold missing README sections
   - `lock_files`: Generate lock files (package-lock.json, requirements.txt, etc.)
   - `issue_pr_templates`: Create .github/ISSUE_TEMPLATE and PULL_REQUEST_TEMPLATE
   - `conventional_commits`: Add commitlint configuration

2. **Command-Based Fixes** (Execute commands):
   - `lock_files`: Run `npm install`, `poetry lock`, `go mod tidy`
   - `precommit_hooks`: Run `pre-commit install`
   - `dependency_freshness`: Run `npm update`, `pip install --upgrade`

3. **AI-Powered Fixes** (Require LLM, optional):
   - `type_annotations`: Add type hints to Python functions
   - `inline_documentation`: Generate docstrings from function signatures
   - `cyclomatic_complexity`: Refactor high-complexity functions
   - `file_size_limits`: Split large files into smaller modules

**Workflow**:

```bash
# User runs alignment
$ agentready align . --dry-run

AgentReady Alignment Preview
============================

Repository: /Users/jeder/my-project
Current Score: 62.4/100 (Silver)
Projected Score: 84.7/100 (Gold) üéØ

Changes to be applied:

  ‚úÖ claude_md_file (10 points)
     CREATE CLAUDE.md (1.2 KB)

  ‚úÖ precommit_hooks (3 points)
     CREATE .pre-commit-config.yaml (845 bytes)
     RUN pre-commit install

  ‚úÖ gitignore_completeness (3 points)
     MODIFY .gitignore (+15 patterns)

  ‚ö†Ô∏è  type_annotations (10 points) - requires AI
     MODIFY 23 Python files (add type hints)
     Use --ai to enable AI-powered fixes

Total: 3 automatic fixes, 1 AI fix available
Apply changes? [y/N]
```

**Implementation**:

```python
# src/agentready/fixers/base.py
class BaseFixer(ABC):
    """Base class for attribute fixers."""

    @abstractmethod
    def can_fix(self, finding: Finding) -> bool:
        """Check if this fixer can fix the finding."""
        pass

    @abstractmethod
    def generate_fix(self, repository: Repository, finding: Finding) -> Fix:
        """Generate fix for the finding."""
        pass

# src/agentready/fixers/template_fixer.py
class TemplateFixer(BaseFixer):
    """Fixer that generates files from templates."""

    def generate_fix(self, repository: Repository, finding: Finding) -> Fix:
        template = self.load_template(finding.attribute.id)
        content = self.render_template(template, repository)
        return FileCreationFix(path="CLAUDE.md", content=content)

# src/agentready/cli/align.py
@cli.command()
@click.argument("repository", type=click.Path(exists=True), default=".")
@click.option("--dry-run", is_flag=True, help="Preview changes without applying")
@click.option("--create-pr", is_flag=True, help="Create GitHub PR instead of direct changes")
@click.option("--interactive", is_flag=True, help="Confirm each change")
@click.option("--attributes", help="Comma-separated attribute IDs to fix")
@click.option("--ai", is_flag=True, help="Enable AI-powered fixes (requires API key)")
def align(repository, dry_run, create_pr, interactive, attributes, ai):
    """Align repository with best practices by applying automatic fixes."""

    # Run assessment first
    assessment = run_assessment(repository)

    # Identify fixable failures
    failures = [f for f in assessment.findings if f.status == "fail"]
    fixable = identify_fixable_failures(failures, enable_ai=ai)

    # Generate fixes
    fixes = [fixer.generate_fix(repo, finding) for finding in fixable]

    # Preview changes
    show_fix_preview(fixes, assessment.overall_score, projected_score)

    if dry_run:
        return

    if interactive and not confirm_each_fix(fixes):
        return

    # Apply fixes
    if create_pr:
        create_github_pr_with_fixes(fixes)
    else:
        apply_fixes(fixes)

    # Re-run assessment to show improvement
    new_assessment = run_assessment(repository)
    show_improvement(assessment.overall_score, new_assessment.overall_score)
```

**Fix Types**:

```python
class Fix(ABC):
    """Base class for fixes."""
    attribute_id: str
    description: str

class FileCreationFix(Fix):
    """Create a new file."""
    path: Path
    content: str

class FileModificationFix(Fix):
    """Modify existing file."""
    path: Path
    changes: List[TextChange]

class CommandFix(Fix):
    """Execute command."""
    command: str
    working_dir: Path

class MultiStepFix(Fix):
    """Combination of multiple fixes."""
    steps: List[Fix]
```

**GitHub PR Integration**:

```bash
# Create PR with fixes
$ agentready align . --create-pr

Creating fix branch: agentready-align-20251121
Applying 3 fixes...
  ‚úÖ Created CLAUDE.md
  ‚úÖ Created .pre-commit-config.yaml
  ‚úÖ Modified .gitignore

Committing changes...
Pushing to origin...

Created PR: https://github.com/redhat/my-project/pull/42
  Title: "Improve AgentReady score from 62.4 to 84.7 (Silver ‚Üí Gold)"
  Score improvement: +22.3 points
  Attributes fixed: 3
```

**Configuration**:

```yaml
# .agentready-config.yaml
align:
  enabled: true

  auto_fix:
    # Attributes to automatically fix without confirmation
    - claude_md_file
    - gitignore_completeness
    - precommit_hooks

  confirm_before_fix:
    # Attributes requiring confirmation
    - type_annotations
    - cyclomatic_complexity

  never_fix:
    # Attributes to skip (user will fix manually)
    - container_setup
    - openapi_specs

  ai_fixes:
    enabled: false  # Require --ai flag
    provider: "anthropic"  # or "openai"
    model: "claude-3-5-sonnet-20241022"
    max_tokens: 4096
```

**Use Cases**:

**Use Case 1: New Repository Setup**
```bash
# Clone new project
git clone github.com/redhat/new-project
cd new-project

# Align to best practices
agentready align . --interactive

# Review and commit changes
git add .
git commit -m "chore: Align repository with AgentReady best practices"
```

**Use Case 2: Continuous Improvement**
```bash
# Weekly CI job to check and create alignment PRs
agentready align . --create-pr --dry-run
# If score < threshold, create PR automatically
```

**Use Case 3: Pre-commit Hook**
```bash
# .pre-commit-config.yaml
- repo: local
  hooks:
    - id: agentready-align
      name: AgentReady Alignment Check
      entry: agentready align --dry-run
      language: system
      pass_filenames: false
```

**Safety Features**:

- **Dry-run by default** for destructive operations
- **Git worktree** for isolated changes (optional)
- **Backup creation** before modifying files
- **Rollback support** if fixes fail
- **Validation** of generated files before writing
- **Interactive confirmation** for AI-powered fixes

**Related**: Automated remediation, repository improvement, onboarding

**Notes**:
- Start with template-based fixes (highest ROI, lowest risk)
- AI-powered fixes require API key and user consent
- Some attributes cannot be automatically fixed (requires human judgment)
- Consider integration with `git stash` for safety
- Could generate shell script of changes for manual review
- Align with Red Hat's AI-assisted development workflow

---


---

## Implementation Checklist

Before you begin:
- [ ] Read CLAUDE.md for project context
- [ ] Review existing similar features (if applicable)
- [ ] Understand the data model (src/agentready/models/)
- [ ] Check acceptance criteria in feature description

Implementation steps:
- [ ] Create feature branch
- [ ] Implement core functionality
- [ ] Add unit tests (target >80% coverage)
- [ ] Add integration tests (if applicable)
- [ ] Run linters and fix any issues
- [ ] Update documentation (README.md, CLAUDE.md if needed)
- [ ] Self-test the feature end-to-end
- [ ] Create PR with descriptive title and body

Code quality requirements:
- [ ] All code formatted with black (88 char lines)
- [ ] Imports sorted with isort
- [ ] No ruff violations
- [ ] All tests passing
- [ ] Type hints where appropriate
- [ ] Docstrings for public APIs

---

## Key Files to Review

Based on this feature, you should review:
- `src/agentready/models/` - Understand Assessment, Finding, Attribute models
- `src/agentready/services/scanner.py` - Scanner orchestration
- `src/agentready/assessors/base.py` - BaseAssessor pattern
- `src/agentready/reporters/` - Report generation
- `CLAUDE.md` - Project overview and guidelines
- `BACKLOG.md` - Full context of this feature

---

## Testing Strategy

For this feature, ensure:
1. **Unit tests** for core logic (80%+ coverage)
2. **Integration tests** for end-to-end workflows
3. **Edge case tests** (empty inputs, missing files, errors)
4. **Error handling tests** (graceful degradation)

Run tests:
```bash
# All tests
pytest

# With coverage
pytest --cov=src/agentready --cov-report=html

# Specific test file
pytest tests/unit/test_feature.py -v
```

---

## Success Criteria

This feature is complete when:
- ‚úÖ All acceptance criteria from feature description are met
- ‚úÖ Tests passing with >80% coverage for new code
- ‚úÖ All linters passing (black, isort, ruff)
- ‚úÖ Documentation updated
- ‚úÖ PR created with clear description
- ‚úÖ Self-tested end-to-end

---

## Questions to Clarify (if needed)

If anything is unclear during implementation:
1. Check CLAUDE.md for project patterns
2. Review similar existing features
3. Ask for clarification in PR comments
4. Reference the original backlog item

---

## Getting Started

```bash
# Clone and setup
git clone https://github.com/redhat/agentready.git
cd agentready

# Create virtual environment
uv venv && source .venv/bin/activate

# Install dependencies
uv pip install -e .
uv pip install pytest black isort ruff

# Create feature branch
git checkout -b 014-align-subcommand-(automated-remediation)

# Start implementing!
```

---

**Note**: This is a coldstart prompt. You have all context needed to implement this feature independently. Read the linked files, follow the patterns, and deliver high-quality code with tests.
````

## File: coldstart-prompts/15-interactive-dashboard-with-automated-remediation.md
````markdown
# Coldstart Implementation Prompt: Interactive Dashboard with Automated Remediation

**Priority**: P2
**Repository**: agentready (https://github.com/redhat/agentready)
**Branch Strategy**: Create feature branch from main

---

## Context

You are implementing a feature for AgentReady, a repository quality assessment tool for AI-assisted development.

### Repository Structure
```
agentready/
‚îú‚îÄ‚îÄ src/agentready/          # Source code
‚îÇ   ‚îú‚îÄ‚îÄ models/              # Data models
‚îÇ   ‚îú‚îÄ‚îÄ services/            # Scanner orchestration
‚îÇ   ‚îú‚îÄ‚îÄ assessors/           # Attribute assessments
‚îÇ   ‚îú‚îÄ‚îÄ reporters/           # Report generation (HTML, Markdown, JSON)
‚îÇ   ‚îú‚îÄ‚îÄ templates/           # Jinja2 templates
‚îÇ   ‚îî‚îÄ‚îÄ cli/                 # Click-based CLI
‚îú‚îÄ‚îÄ tests/                   # Test suite (unit + integration)
‚îú‚îÄ‚îÄ examples/                # Example reports
‚îî‚îÄ‚îÄ specs/                   # Feature specifications
```

### Key Technologies
- Python 3.11+
- Click (CLI framework)
- Jinja2 (templating)
- Pytest (testing)
- Black, isort, ruff (code quality)

### Development Workflow
1. Create feature branch: `git checkout -b NNN-feature-name`
2. Implement changes with tests
3. Run linters: `black . && isort . && ruff check .`
4. Run tests: `pytest`
5. Commit with conventional commits
6. Create PR to main

---

## Feature Requirements

Interactive Dashboard with Automated Remediation

**Priority**: P2 (High Value)

**Description**: Transform the static HTML report into an interactive dashboard that enables one-click remediation via automated GitHub issue creation and draft PR generation.

**Vision**: "How to Align Your Repo with Best Practices" - Click a button ‚Üí Open GitHub issue with draft PR containing fixes.

**Core Features**:

1. **Interactive Dashboard Mode**
   - Real-time assessment (WebSocket updates)
   - Live filtering and sorting (already have this)
   - Action buttons on each failing attribute
   - Progress tracking across multiple runs
   - Historical trend visualization

2. **One-Click Remediation Actions**
   - "Fix This" button on each failing attribute
   - Generates GitHub issue automatically
   - Creates draft PR with proposed changes
   - Links issue to PR
   - Assigns to repository owner/team

3. **Automated Fix Generation**
   - Template-based fixes for common issues:
     - Missing .gitignore ‚Üí Generate language-specific .gitignore
     - No CLAUDE.md ‚Üí Generate template with repo analysis
     - Missing pre-commit hooks ‚Üí Add .pre-commit-config.yaml
     - No lock files ‚Üí Generate appropriate lock file
     - Missing README sections ‚Üí Scaffold missing sections
   - AI-powered fixes for complex issues:
     - Refactor high-complexity functions
     - Add type annotations to functions
     - Generate docstrings from function signatures
     - Split large files

4. **GitHub Integration**
   - OAuth authentication with GitHub
   - gh CLI integration for seamless workflow
   - Create issues via GitHub API
   - Create draft PRs via GitHub API
   - Auto-label issues (e.g., `agentready`, `automated-fix`, `tier-1-essential`)
   - Link to AgentReady assessment report

**Technical Architecture**:

```yaml
# New components needed:

Backend (Optional - could be fully client-side with gh CLI):
  - GitHub OAuth app for authentication
  - Issue/PR template generator
  - Fix generator service (template-based + AI-powered)
  - Assessment history tracker

Frontend (Enhanced HTML report):
  - Action buttons with loading states
  - GitHub auth flow UI
  - Progress indicators
  - Toast notifications for actions
  - Modal dialogs for fix preview

CLI Extensions:
  - agentready dashboard .  # Launch local web server
  - agentready fix <attribute-id>  # Generate fix for specific attribute
  - agentready create-issue <attribute-id>  # Create GitHub issue
  - agentready create-pr <attribute-id>  # Create draft PR
```

**Use Cases**:

**Use Case 1: Quick Fixes**
```bash
# User runs assessment
agentready assess . --dashboard

# Opens dashboard in browser at http://localhost:8000
# User clicks "Fix This" on "Missing CLAUDE.md"
# ‚Üí Creates issue: "Add CLAUDE.md configuration file"
# ‚Üí Creates draft PR with generated CLAUDE.md template
# ‚Üí PR includes: project analysis, detected languages, suggested structure
```

**Use Case 2: Batch Remediation**
```bash
# Dashboard shows all failures
# User selects multiple attributes
# Clicks "Fix All Selected"
# ‚Üí Creates single issue: "Improve AgentReady Score from Silver to Gold"
# ‚Üí Creates draft PR with all fixes
# ‚Üí PR description includes before/after score projection
```

**Use Case 3: CI/CD Integration**
```bash
# GitHub Action runs assessment
# Posts comment on PR with assessment results
# Includes links to create remediation issues
# Can auto-create draft PR for improvements
```

**Implementation Approach**:

**Phase 1: Client-Side with gh CLI** (Simplest, no backend needed)
- Use JavaScript in HTML report to call gh CLI via local proxy
- Generate fix files locally
- Use `gh issue create` and `gh pr create`
- Works for users with gh CLI installed

**Phase 2: Dashboard Server** (Enhanced UX)
- Flask/FastAPI server serving dashboard
- WebSocket for live updates
- GitHub OAuth for authentication
- Background workers for fix generation

**Phase 3: Cloud Service** (SaaS offering)
- Hosted dashboard at agentready.dev
- GitHub App installation
- Webhook integration for continuous monitoring
- Team collaboration features

**Fix Templates by Attribute**:

```yaml
claude_md_file:
  type: template
  generates:
    - file: CLAUDE.md
      content: |
        # {repository_name}

        ## Overview
        {auto_generated_description}

        ## Architecture
        {detected_patterns}

        ## Development
        {build_commands}

lock_files:
  type: command
  commands:
    - condition: has_package_json
      run: npm install
    - condition: has_pyproject_toml
      run: poetry lock || pip freeze > requirements.txt

precommit_hooks:
  type: template
  generates:
    - file: .pre-commit-config.yaml
      content: {language_specific_hooks}

readme_structure:
  type: enhancement
  modifies: README.md
  adds_sections:
    - Installation
    - Usage
    - Development
    - Contributing

type_annotations:
  type: ai_powered
  uses: ast_analysis + llm
  modifies: "*.py"
  adds: type_hints
```

**Dashboard vs Report Decision**:

**Keep Both**:
- Static reports for CI/CD, documentation, archiving
- Dashboard for interactive development workflow
- Reports can link to dashboard for remediation
- Dashboard can export static reports

**Benefits of Dashboard**:
- ‚úÖ Interactive remediation workflow
- ‚úÖ Live assessment updates
- ‚úÖ Progress tracking over time
- ‚úÖ Team collaboration (comments, assignments)
- ‚úÖ Automated fix preview before applying
- ‚úÖ Integration with existing tools (GitHub, IDEs)

**Challenges**:
- Authentication complexity (GitHub OAuth)
- Fix generation quality (need good templates + AI)
- PR review overhead (lots of automated PRs)
- Maintaining fix templates as best practices evolve

**Recommended Approach**:

1. **Start with enhanced static report**:
   - Add "Create Issue" buttons that generate gh CLI commands
   - Users copy/paste commands to create issues
   - Include fix templates in issue descriptions

2. **Add local dashboard** (Phase 2):
   - Flask server with WebSocket updates
   - GitHub integration via gh CLI
   - Generate fixes, preview diffs, create PRs

3. **Consider hosted service** (Phase 3+):
   - If adoption is high
   - SaaS model for teams
   - Continuous monitoring and recommendations

**Related**: GitHub integration, automation, remediation, UX

**Notes**:
- This is a MAJOR feature that could become a standalone product
- Consider MVP: "Copy this gh command" buttons in HTML report
- AI-powered fix generation requires careful validation
- Some fixes (like refactoring) need human review
- Could integrate with existing tools (Dependabot, Renovate)
- May want to partner with GitHub for official integration

---


---

## Implementation Checklist

Before you begin:
- [ ] Read CLAUDE.md for project context
- [ ] Review existing similar features (if applicable)
- [ ] Understand the data model (src/agentready/models/)
- [ ] Check acceptance criteria in feature description

Implementation steps:
- [ ] Create feature branch
- [ ] Implement core functionality
- [ ] Add unit tests (target >80% coverage)
- [ ] Add integration tests (if applicable)
- [ ] Run linters and fix any issues
- [ ] Update documentation (README.md, CLAUDE.md if needed)
- [ ] Self-test the feature end-to-end
- [ ] Create PR with descriptive title and body

Code quality requirements:
- [ ] All code formatted with black (88 char lines)
- [ ] Imports sorted with isort
- [ ] No ruff violations
- [ ] All tests passing
- [ ] Type hints where appropriate
- [ ] Docstrings for public APIs

---

## Key Files to Review

Based on this feature, you should review:
- `src/agentready/models/` - Understand Assessment, Finding, Attribute models
- `src/agentready/services/scanner.py` - Scanner orchestration
- `src/agentready/assessors/base.py` - BaseAssessor pattern
- `src/agentready/reporters/` - Report generation
- `CLAUDE.md` - Project overview and guidelines
- `BACKLOG.md` - Full context of this feature

---

## Testing Strategy

For this feature, ensure:
1. **Unit tests** for core logic (80%+ coverage)
2. **Integration tests** for end-to-end workflows
3. **Edge case tests** (empty inputs, missing files, errors)
4. **Error handling tests** (graceful degradation)

Run tests:
```bash
# All tests
pytest

# With coverage
pytest --cov=src/agentready --cov-report=html

# Specific test file
pytest tests/unit/test_feature.py -v
```

---

## Success Criteria

This feature is complete when:
- ‚úÖ All acceptance criteria from feature description are met
- ‚úÖ Tests passing with >80% coverage for new code
- ‚úÖ All linters passing (black, isort, ruff)
- ‚úÖ Documentation updated
- ‚úÖ PR created with clear description
- ‚úÖ Self-tested end-to-end

---

## Questions to Clarify (if needed)

If anything is unclear during implementation:
1. Check CLAUDE.md for project patterns
2. Review similar existing features
3. Ask for clarification in PR comments
4. Reference the original backlog item

---

## Getting Started

```bash
# Clone and setup
git clone https://github.com/redhat/agentready.git
cd agentready

# Create virtual environment
uv venv && source .venv/bin/activate

# Install dependencies
uv pip install -e .
uv pip install pytest black isort ruff

# Create feature branch
git checkout -b 015-interactive-dashboard-with-automated-remediation

# Start implementing!
```

---

**Note**: This is a coldstart prompt. You have all context needed to implement this feature independently. Read the linked files, follow the patterns, and deliver high-quality code with tests.
````

## File: coldstart-prompts/16-github-app-integration-badge-status-checks.md
````markdown
# Coldstart Implementation Prompt: GitHub App Integration (Badge & Status Checks)

**Priority**: P2
**Repository**: agentready (https://github.com/redhat/agentready)
**Branch Strategy**: Create feature branch from main

---

## Context

You are implementing a feature for AgentReady, a repository quality assessment tool for AI-assisted development.

### Repository Structure
```
agentready/
‚îú‚îÄ‚îÄ src/agentready/          # Source code
‚îÇ   ‚îú‚îÄ‚îÄ models/              # Data models
‚îÇ   ‚îú‚îÄ‚îÄ services/            # Scanner orchestration
‚îÇ   ‚îú‚îÄ‚îÄ assessors/           # Attribute assessments
‚îÇ   ‚îú‚îÄ‚îÄ reporters/           # Report generation (HTML, Markdown, JSON)
‚îÇ   ‚îú‚îÄ‚îÄ templates/           # Jinja2 templates
‚îÇ   ‚îî‚îÄ‚îÄ cli/                 # Click-based CLI
‚îú‚îÄ‚îÄ tests/                   # Test suite (unit + integration)
‚îú‚îÄ‚îÄ examples/                # Example reports
‚îî‚îÄ‚îÄ specs/                   # Feature specifications
```

### Key Technologies
- Python 3.11+
- Click (CLI framework)
- Jinja2 (templating)
- Pytest (testing)
- Black, isort, ruff (code quality)

### Development Workflow
1. Create feature branch: `git checkout -b NNN-feature-name`
2. Implement changes with tests
3. Run linters: `black . && isort . && ruff check .`
4. Run tests: `pytest`
5. Commit with conventional commits
6. Create PR to main

---

## Feature Requirements

GitHub App Integration (Badge & Status Checks)

**Priority**: P2 (High Value)

**Description**: Create a GitHub App that provides badge integration, PR status checks, and automated assessment comments to help Red Hat engineering teams track and improve repository quality.

**Core Features**:

1. **Repository Badge**
   - Shields.io-compatible SVG badge showing certification level
   - Endpoint: `https://agentready.redhat.com/badge/{owner}/{repo}.svg`
   - Dynamic color based on certification (Platinum=purple, Gold=yellow, Silver=silver, Bronze=brown)
   - Include score: "AgentReady: 85.2 (Gold)"
   - Click badge to view latest assessment report

2. **GitHub Actions Integration**
   - Create official `agentready/assess-action` GitHub Action
   - Run assessment on PR events (opened, synchronized, reopened)
   - Run assessment on push to main/master
   - Support custom triggers via workflow_dispatch

3. **PR Status Checks**
   - Use GitHub Commit Status API to report assessment results
   - Set check status: success (>90), warning (75-89), failure (<75)
   - Configurable thresholds via `.agentready-config.yaml`
   - Block PR merge if score below threshold (optional)
   - Link to detailed HTML report in check details

4. **PR Comments**
   - Automated bot comments on PRs with assessment summary
   - Show score delta: "Score changed: 72.4 ‚Üí 78.3 (+5.9)"
   - List new failures and fixes
   - Collapsible sections for full findings
   - Trend chart showing last 10 assessments (ASCII or embedded image)
   - Include remediation suggestions for new failures

**Technical Implementation**:

**Phase 1: GitHub Actions Integration**
```yaml
# .github/workflows/agentready.yml
name: AgentReady Assessment
on:
  pull_request:
    types: [opened, synchronize, reopened]
  push:
    branches: [main, master]

jobs:
  assess:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - uses: agentready/assess-action@v1
        with:
          threshold: 75
          post-comment: true
          update-status: true
```

**Phase 2: Badge Service**
```python
# FastAPI endpoint for badge generation
@app.get("/badge/{owner}/{repo}.svg")
async def get_badge(owner: str, repo: str):
    # Fetch latest assessment from GitHub Actions artifacts
    # Or run quick assessment on-demand
    score, level = get_latest_assessment(owner, repo)
    color = LEVEL_COLORS[level]
    return SVGResponse(generate_badge(score, level, color))
```

**Phase 3: GitHub App**
- App permissions: Contents (read), Checks (write), Pull requests (write)
- Webhook events: push, pull_request
- Installed via GitHub Marketplace (Red Hat internal)
- Dashboard at agentready.redhat.com showing:
  - Repository list with scores
  - Historical trends
  - Organization-wide statistics
  - Top repositories by improvement

**Integration Points**:

1. **GitHub Actions Artifacts**
   - Store assessment reports as workflow artifacts
   - Keep last 30 days of reports for trend analysis
   - Generate downloadable HTML/JSON/Markdown reports

2. **GitHub Status API**
   ```python
   POST /repos/{owner}/{repo}/statuses/{commit_sha}
   {
     "state": "success",  # or "pending", "failure", "error"
     "target_url": "https://agentready.redhat.com/reports/{run_id}",
     "description": "AgentReady: 85.2 (Gold)",
     "context": "agentready/assessment"
   }
   ```

3. **GitHub Checks API** (preferred over Status API)
   ```python
   POST /repos/{owner}/{repo}/check-runs
   {
     "name": "AgentReady Assessment",
     "status": "completed",
     "conclusion": "success",
     "output": {
       "title": "Score: 85.2/100 (Gold)",
       "summary": "Passed 20/25 attributes",
       "text": "Detailed findings..."
     }
   }
   ```

**Use Cases**:

**Use Case 1: Add Badge to README**
```markdown
# My Project

[![AgentReady](https://agentready.redhat.com/badge/redhat/my-project.svg)](https://agentready.redhat.com/reports/redhat/my-project)
```

**Use Case 2: Enforce Quality Gates**
```yaml
# .agentready-config.yaml
github:
  status_checks:
    enabled: true
    min_score: 75  # Block merge if score < 75
    require_improvement: true  # Block if score decreased
```

**Use Case 3: Track Organization Progress**
- Dashboard shows all repos in Red Hat org
- Filter by team, language, certification level
- Identify repos needing attention
- Celebrate improvements (score increases)

**Configuration**:

```yaml
# .agentready-config.yaml
github:
  badge:
    enabled: true
    style: flat-square  # or flat, plastic, for-the-badge
    label: "AgentReady"

  actions:
    enabled: true
    trigger_on: [pull_request, push]
    post_comment: true
    update_status: true
    upload_artifacts: true

  status_checks:
    enabled: true
    min_score: 75
    require_improvement: false

  comments:
    enabled: true
    show_delta: true
    show_trend: true
    collapse_details: true
```

**Implementation Checklist**:

- [ ] Create `agentready/assess-action` GitHub Action
- [ ] Implement badge generation service
- [ ] Add GitHub Status API integration
- [ ] Add GitHub Checks API integration
- [ ] Implement PR comment generation
- [ ] Add score delta calculation
- [ ] Create assessment artifact storage
- [ ] Build organization dashboard
- [ ] Add Red Hat SSO authentication
- [ ] Deploy to Red Hat infrastructure
- [ ] Create documentation for Red Hat teams
- [ ] Add to Red Hat developer onboarding

**Related**: CI/CD integration, automation, visibility, quality gates

**Notes**:
- Focus on internal Red Hat adoption first
- Badge service could be hosted on Red Hat infrastructure
- Dashboard should integrate with Red Hat IdM for authentication
- Consider integration with Red Hat's existing code quality tools
- GitHub App should be installable via Red Hat GitHub Enterprise
- All data stays within Red Hat infrastructure (no external services)
- Align with Red Hat's OpenShift AI strategy for agentic development
- Could become part of Red Hat's AI-assisted development workflow

---

## Backlog Metadata

**Created**: 2025-11-21
**Last Updated**: 2025-11-21
**Total Items**: 14 (11 original + 3 from code review)

## Priority Summary

- **P0 (Critical)**: 4 items - Security/Logic Bugs (FIX FIRST!), Bootstrap Command, Report Header Metadata, HTML Design Improvements
- **P1 (Critical)**: 4 items - Code Quality Fixes, Test Coverage Improvements, Align Subcommand
- **P2 (High Value)**: 3 items - Security Polish, Interactive Dashboard, GitHub App Integration
- **P3 (Important)**: 2 items - Report Schema Versioning, AgentReady Repository Agent
- **P4 (Enhancement)**: 3 items - Research Report Utility, Repomix Integration, Customizable Themes


---

## Implementation Checklist

Before you begin:
- [ ] Read CLAUDE.md for project context
- [ ] Review existing similar features (if applicable)
- [ ] Understand the data model (src/agentready/models/)
- [ ] Check acceptance criteria in feature description

Implementation steps:
- [ ] Create feature branch
- [ ] Implement core functionality
- [ ] Add unit tests (target >80% coverage)
- [ ] Add integration tests (if applicable)
- [ ] Run linters and fix any issues
- [ ] Update documentation (README.md, CLAUDE.md if needed)
- [ ] Self-test the feature end-to-end
- [ ] Create PR with descriptive title and body

Code quality requirements:
- [ ] All code formatted with black (88 char lines)
- [ ] Imports sorted with isort
- [ ] No ruff violations
- [ ] All tests passing
- [ ] Type hints where appropriate
- [ ] Docstrings for public APIs

---

## Key Files to Review

Based on this feature, you should review:
- `src/agentready/models/` - Understand Assessment, Finding, Attribute models
- `src/agentready/services/scanner.py` - Scanner orchestration
- `src/agentready/assessors/base.py` - BaseAssessor pattern
- `src/agentready/reporters/` - Report generation
- `CLAUDE.md` - Project overview and guidelines
- `BACKLOG.md` - Full context of this feature

---

## Testing Strategy

For this feature, ensure:
1. **Unit tests** for core logic (80%+ coverage)
2. **Integration tests** for end-to-end workflows
3. **Edge case tests** (empty inputs, missing files, errors)
4. **Error handling tests** (graceful degradation)

Run tests:
```bash
# All tests
pytest

# With coverage
pytest --cov=src/agentready --cov-report=html

# Specific test file
pytest tests/unit/test_feature.py -v
```

---

## Success Criteria

This feature is complete when:
- ‚úÖ All acceptance criteria from feature description are met
- ‚úÖ Tests passing with >80% coverage for new code
- ‚úÖ All linters passing (black, isort, ruff)
- ‚úÖ Documentation updated
- ‚úÖ PR created with clear description
- ‚úÖ Self-tested end-to-end

---

## Questions to Clarify (if needed)

If anything is unclear during implementation:
1. Check CLAUDE.md for project patterns
2. Review similar existing features
3. Ask for clarification in PR comments
4. Reference the original backlog item

---

## Getting Started

```bash
# Clone and setup
git clone https://github.com/redhat/agentready.git
cd agentready

# Create virtual environment
uv venv && source .venv/bin/activate

# Install dependencies
uv pip install -e .
uv pip install pytest black isort ruff

# Create feature branch
git checkout -b 016-github-app-integration-(badge-&-status-checks)

# Start implementing!
```

---

**Note**: This is a coldstart prompt. You have all context needed to implement this feature independently. Read the linked files, follow the patterns, and deliver high-quality code with tests.
````

## File: coldstart-prompts/17-add-bootstrap-quickstart-to-readme.md
````markdown
# Coldstart Prompt: Add Bootstrap Quickstart to README.md

**Priority**: P0 (Critical - Discoverability)
**Estimated Effort**: 30 minutes
**Generated**: 2025-11-21

---

## Context

AgentReady is a repository quality assessment tool that evaluates repositories against 25 agent-ready attributes. The `bootstrap` command (already implemented) sets up complete GitHub infrastructure in one command.

**Problem**: README.md currently has zero mentions of the bootstrap command. New users landing on GitHub only see `agentready assess` examples, missing the recommended entry point.

**Solution**: Add a dead-simple, copy-paste Bootstrap quickstart to README.md that positions bootstrap as the primary getting-started flow.

---

## Requirements from BACKLOG.md

### Description
Add a dead-simple, copy-paste Bootstrap quickstart to README.md. Currently, bootstrap documentation only exists in docs/user-guide.md, but README.md (the first thing users see on GitHub) has zero mentions of the bootstrap command.

### Problem
- New users landing on GitHub see only `agentready assess` commands
- Bootstrap command (the recommended entry point) is buried in documentation
- Missing the "most dead simple copy paste thing" for bootstrap

### Requirements
- Add prominent Bootstrap quickstart section to README.md
- 4-5 line copy-paste command sequence
- Show what gets created (bullets with checkmarks)
- Link to detailed tutorial in docs/user-guide.md
- Position ABOVE or alongside the assess quickstart

### Implementation

Add to README.md after Installation section:

```markdown
## Quick Start: Bootstrap (Recommended)

Transform your repository with one command:

```bash
cd /path/to/your/repo
agentready bootstrap .
git add . && git commit -m "build: Bootstrap agent-ready infrastructure"
git push
```

**What you get:**
- ‚úÖ GitHub Actions workflows (tests, security, AgentReady assessment)
- ‚úÖ Pre-commit hooks (formatters, linters)
- ‚úÖ Issue/PR templates
- ‚úÖ Dependabot configuration
- ‚úÖ Automated assessment on every PR

**Duration**: <60 seconds

[See detailed Bootstrap tutorial ‚Üí](docs/user-guide.md#bootstrap-your-repository)

---

## Quick Start: Assessment Only

For one-time analysis without infrastructure changes:

```bash
cd /path/to/your/repo
agentready assess .
open .agentready/report-latest.html
```
```

### Acceptance Criteria
- [ ] Bootstrap quickstart added to README.md
- [ ] Positioned prominently (before or alongside assess quickstart)
- [ ] Maximum 10 lines of code
- [ ] Shows clear value prop (what you get)
- [ ] Links to detailed docs
- [ ] Uses conventional commit message format

### Related
- Bootstrap command (#3 - already implemented)
- Documentation workflow
- User onboarding

### Notes
- Bootstrap is already implemented - this is just documentation
- README.md is the entry point for 90% of users
- This is blocking adoption - users don't know bootstrap exists
- Should match the style in docs/user-guide.md lines 87-115

---

## Implementation Checklist

### Step 1: Read Current README.md
- [ ] Read `/Users/jeder/repos/sk/agentready/README.md`
- [ ] Note the current Quick Start section (around line 14)
- [ ] Identify insertion point (after Installation, before current Quick Start)

### Step 2: Read Bootstrap Documentation
- [ ] Read `/Users/jeder/repos/sk/agentready/docs/user-guide.md` lines 87-115
- [ ] Note the style, tone, and structure of the existing bootstrap quickstart
- [ ] Ensure consistency with docs

### Step 3: Update README.md
- [ ] Add new section "## Quick Start: Bootstrap (Recommended)"
- [ ] Include 4-line command sequence
- [ ] Add bullet list showing what gets created
- [ ] Add duration indicator (<60 seconds)
- [ ] Link to detailed tutorial in docs/
- [ ] Rename existing Quick Start to "## Quick Start: Assessment Only"
- [ ] Add horizontal rule between sections

### Step 4: Verify Formatting
- [ ] Run `markdownlint README.md` to check formatting
- [ ] Fix any linting errors
- [ ] Ensure links work (test relative link to docs/user-guide.md)

### Step 5: Update Table of Contents (if exists)
- [ ] Check if README.md has a table of contents
- [ ] Add Bootstrap quickstart to TOC if present
- [ ] Update section numbering if needed

---

## Key Files to Review

Before starting implementation, review these files:

1. **README.md** (current state)
   - Location: `/Users/jeder/repos/sk/agentready/README.md`
   - Current Quick Start section around line 14
   - Installation section position

2. **docs/user-guide.md** (source of truth for bootstrap)
   - Location: `/Users/jeder/repos/sk/agentready/docs/user-guide.md`
   - Lines 87-115: Bootstrap quickstart section
   - Style guide for documentation

3. **BACKLOG.md** (original requirements)
   - Location: `/Users/jeder/repos/sk/agentready/BACKLOG.md`
   - Lines 165-237: Full backlog item

---

## Testing Strategy

### Manual Testing
1. **Markdown Rendering**
   - View README.md on GitHub (or use GitHub markdown preview)
   - Ensure checkmarks (‚úÖ) render correctly
   - Verify code blocks have proper syntax highlighting
   - Check that link to docs/user-guide.md works

2. **Link Verification**
   - Click link to docs/user-guide.md#bootstrap-your-repository
   - Ensure anchor link works correctly
   - Verify relative path is correct

3. **Copy-Paste Test**
   - Copy the command sequence
   - Paste into terminal (don't execute - just verify formatting)
   - Ensure no hidden characters or formatting issues

### Linting
```bash
# Run markdownlint on README.md
markdownlint README.md

# Fix any issues
markdownlint --fix README.md
```

---

## Success Criteria

**Functional:**
- [ ] Bootstrap quickstart is the first Quick Start section in README.md
- [ ] Command sequence is 4-5 lines maximum (copy-paste friendly)
- [ ] Bullet list shows clear value proposition
- [ ] Link to detailed docs works correctly
- [ ] Assessment quickstart is still present but secondary

**Quality:**
- [ ] Passes markdownlint validation
- [ ] Matches style and tone of docs/user-guide.md
- [ ] No broken links
- [ ] Checkmarks render correctly on GitHub
- [ ] Code blocks have proper bash syntax highlighting

**Adoption:**
- [ ] New users can immediately see bootstrap command
- [ ] Value prop is clear (what you get in <60 seconds)
- [ ] Path to detailed docs is obvious

---

## Getting Started

### Prerequisites
- AgentReady repository cloned locally
- Text editor or IDE
- markdownlint installed (`npm install -g markdownlint-cli`)

### Quick Start Commands

```bash
# Navigate to repository
cd /Users/jeder/repos/sk/agentready

# Read current README.md
cat README.md | head -50

# Read bootstrap docs for reference
cat docs/user-guide.md | sed -n '87,115p'

# Edit README.md
code README.md  # or vim, nano, etc.

# Run linter
markdownlint README.md

# Fix issues if any
markdownlint --fix README.md

# Preview changes
git diff README.md

# Commit changes
git add README.md
git commit -m "docs: Add bootstrap quickstart to README.md

- Add 'Quick Start: Bootstrap' section as primary entry point
- Move existing quick start to 'Quick Start: Assessment Only'
- Include 4-line command sequence for copy-paste
- Show value prop (what you get in <60 seconds)
- Link to detailed tutorial in docs/user-guide.md

Fixes #[issue-number]"
```

---

## Expected Diff Preview

```diff
--- a/README.md
+++ b/README.md
@@ -30,7 +30,31 @@
 pip install agentready
 ```

-## Quick Start
+## Quick Start: Bootstrap (Recommended)
+
+Transform your repository with one command:
+
+```bash
+cd /path/to/your/repo
+agentready bootstrap .
+git add . && git commit -m "build: Bootstrap agent-ready infrastructure"
+git push
+```
+
+**What you get:**
+- ‚úÖ GitHub Actions workflows (tests, security, AgentReady assessment)
+- ‚úÖ Pre-commit hooks (formatters, linters)
+- ‚úÖ Issue/PR templates
+- ‚úÖ Dependabot configuration
+- ‚úÖ Automated assessment on every PR
+
+**Duration**: <60 seconds
+
+[See detailed Bootstrap tutorial ‚Üí](docs/user-guide.md#bootstrap-your-repository)
+
+---
+
+## Quick Start: Assessment Only

 For one-time analysis without infrastructure changes:
```

---

## Notes

- Bootstrap command is already fully implemented - this is documentation only
- README.md is the #1 user entry point (90% of users see this first)
- This is blocking adoption - users don't discover bootstrap exists
- The docs/user-guide.md has excellent bootstrap documentation (lines 87-115)
- Match that style and tone for consistency

---

## Related Documentation

- **User Guide Bootstrap Section**: `docs/user-guide.md#bootstrap-your-repository`
- **Bootstrap Implementation**: `src/agentready/cli/bootstrap.py`
- **BACKLOG Item**: `BACKLOG.md` lines 165-237

---

**Last Updated**: 2025-11-21
**Status**: üî¥ Not Started
**Assignee**: Unassigned
````

## File: coldstart-prompts/18-setup-release-pipeline.md
````markdown
# Coldstart Prompt: Setup GitHub Release Pipeline

## Objective
Set up a production-grade release pipeline following GitHub best practices for automated releases, semantic versioning, changelog generation, and asset management.

## Context
This prompt guides you through establishing a comprehensive release automation workflow that:
- Uses semantic versioning (SemVer)
- Automates changelog generation from conventional commits
- Creates GitHub releases with proper assets
- Publishes to package registries (PyPI, npm, etc.)
- Maintains release notes and documentation
- Follows security and signing best practices

## Prerequisites
- Repository uses conventional commits (enforced via pre-commit hooks)
- Repository has a clear versioning strategy
- CI/CD pipelines are already configured
- Branch protection rules are in place for main/master

## Tasks

### 1. Configure Semantic Release Automation

**Create `.github/workflows/release.yml`:**

```yaml
name: Release

on:
  push:
    branches:
      - main
  workflow_dispatch:

permissions:
  contents: write
  issues: write
  pull-requests: write
  packages: write

jobs:
  release:
    name: Create Release
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0
          token: ${{ secrets.GITHUB_TOKEN }}

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: 'lts/*'

      - name: Install semantic-release
        run: |
          npm install -g semantic-release @semantic-release/changelog @semantic-release/git @semantic-release/github

      - name: Run semantic-release
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: npx semantic-release
```

### 2. Create Semantic Release Configuration

**Create `.releaserc.json`:**

```json
{
  "branches": ["main", "master"],
  "plugins": [
    "@semantic-release/commit-analyzer",
    "@semantic-release/release-notes-generator",
    [
      "@semantic-release/changelog",
      {
        "changelogFile": "CHANGELOG.md"
      }
    ],
    [
      "@semantic-release/github",
      {
        "assets": [
          {
            "path": "dist/**",
            "label": "Distribution files"
          }
        ]
      }
    ],
    [
      "@semantic-release/git",
      {
        "assets": ["CHANGELOG.md", "package.json"],
        "message": "chore(release): ${nextRelease.version} [skip ci]\n\n${nextRelease.notes}"
      }
    ]
  ]
}
```

### 3. Setup Package Publishing (Language-Specific)

#### For Python Projects:

**Create `.github/workflows/publish-pypi.yml`:**

```yaml
name: Publish to PyPI

on:
  release:
    types: [published]

permissions:
  contents: read
  id-token: write

jobs:
  publish:
    name: Publish to PyPI
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.x'

      - name: Install build dependencies
        run: |
          python -m pip install --upgrade pip
          pip install build twine

      - name: Build package
        run: python -m build

      - name: Publish to PyPI
        uses: pypa/gh-action-pypi-publish@release/v1
        with:
          repository-url: https://upload.pypi.org/legacy/
```

#### For Node.js Projects:

**Create `.github/workflows/publish-npm.yml`:**

```yaml
name: Publish to npm

on:
  release:
    types: [published]

permissions:
  contents: read
  packages: write

jobs:
  publish:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - uses: actions/setup-node@v4
        with:
          node-version: 'lts/*'
          registry-url: 'https://registry.npmjs.org'

      - run: npm ci
      - run: npm publish
        env:
          NODE_AUTH_TOKEN: ${{ secrets.NPM_TOKEN }}
```

### 4. Configure Release Drafter (Alternative/Complementary Approach)

**Create `.github/release-drafter.yml`:**

```yaml
name-template: 'v$RESOLVED_VERSION'
tag-template: 'v$RESOLVED_VERSION'
categories:
  - title: 'üöÄ Features'
    labels:
      - 'feature'
      - 'enhancement'
  - title: 'üêõ Bug Fixes'
    labels:
      - 'fix'
      - 'bugfix'
      - 'bug'
  - title: 'üß∞ Maintenance'
    labels:
      - 'chore'
      - 'dependencies'
  - title: 'üìö Documentation'
    labels:
      - 'documentation'
      - 'docs'
  - title: 'üîí Security'
    labels:
      - 'security'
change-template: '- $TITLE @$AUTHOR (#$NUMBER)'
version-resolver:
  major:
    labels:
      - 'major'
      - 'breaking'
  minor:
    labels:
      - 'minor'
      - 'feature'
  patch:
    labels:
      - 'patch'
      - 'fix'
      - 'bugfix'
  default: patch
template: |
  ## Changes

  $CHANGES

  ## Contributors

  $CONTRIBUTORS
```

**Create `.github/workflows/release-drafter.yml`:**

```yaml
name: Release Drafter

on:
  push:
    branches:
      - main
  pull_request:
    types: [opened, reopened, synchronize]

permissions:
  contents: read
  pull-requests: write

jobs:
  update_release_draft:
    runs-on: ubuntu-latest
    steps:
      - uses: release-drafter/release-drafter@v6
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
```

### 5. Add Release Signing and Provenance

**Create `.github/workflows/release-provenance.yml`:**

```yaml
name: Generate SLSA Provenance

on:
  release:
    types: [published]

permissions:
  actions: read
  id-token: write
  contents: write

jobs:
  provenance:
    uses: slsa-framework/slsa-github-generator/.github/workflows/generator_generic_slsa3.yml@v1.9.0
    with:
      base64-subjects: "${{ needs.build.outputs.digests }}"
      upload-assets: true
```

### 6. Create Release Documentation

**Create `docs/RELEASE_PROCESS.md`:**

```markdown
# Release Process

## Overview
This project uses automated semantic releases based on conventional commits.

## Release Types

- **Major (X.0.0)**: Breaking changes (commit prefix: `feat!:` or `fix!:`)
- **Minor (x.Y.0)**: New features (commit prefix: `feat:`)
- **Patch (x.y.Z)**: Bug fixes (commit prefix: `fix:`)

## Automated Release Workflow

1. Commits are merged to `main` branch
2. Semantic-release analyzes commit messages
3. Version is automatically determined
4. CHANGELOG.md is updated
5. Git tag is created
6. GitHub Release is published
7. Package is published to registry

## Manual Release Trigger

To manually trigger a release:
```bash
gh workflow run release.yml
```

## Pre-release Process

For beta/alpha releases:
```bash
git checkout -b beta
git push origin beta
```

Then update `.releaserc.json` to include beta branch.

## Hotfix Process

1. Create hotfix branch from latest release tag
2. Apply fix with conventional commit
3. Merge to main with expedited review
4. Release automation handles versioning

## Rollback Procedure

To rollback a release:
```bash
# Delete the tag locally and remotely
git tag -d v1.2.3
git push origin :refs/tags/v1.2.3

# Delete the GitHub release
gh release delete v1.2.3 --yes

# Revert the release commit
git revert <release-commit-sha>
git push origin main
```
```

### 7. Setup Release Checklist Template

**Create `.github/RELEASE_CHECKLIST.md`:**

```markdown
# Release Checklist

## Pre-Release
- [ ] All tests passing on main branch
- [ ] Documentation is up to date
- [ ] CHANGELOG.md reflects all changes
- [ ] Security vulnerabilities addressed
- [ ] Dependencies are up to date
- [ ] Version number follows SemVer
- [ ] Migration guide written (if breaking changes)

## Release
- [ ] Release workflow triggered
- [ ] GitHub Release created successfully
- [ ] Package published to registry
- [ ] Release assets uploaded
- [ ] Provenance attestation generated

## Post-Release
- [ ] Release announcement published
- [ ] Documentation site updated
- [ ] Social media announcement (if applicable)
- [ ] Dependent projects notified
- [ ] Issue/PR milestones closed
- [ ] Monitor for regression reports
```

### 8. Configure Branch Protection for Releases

**Required settings via GitHub UI or API:**

```bash
# Enable branch protection for main
gh api repos/:owner/:repo/branches/main/protection \
  --method PUT \
  --field required_status_checks='{"strict":true,"contexts":["test","lint","build"]}' \
  --field enforce_admins=true \
  --field required_pull_request_reviews='{"dismissal_restrictions":{},"dismiss_stale_reviews":true,"require_code_owner_reviews":true,"required_approving_review_count":1}' \
  --field restrictions=null
```

## Success Criteria

- ‚úÖ Releases are created automatically on merge to main
- ‚úÖ Version numbers follow semantic versioning
- ‚úÖ CHANGELOG.md is automatically updated
- ‚úÖ GitHub Releases include proper release notes
- ‚úÖ Packages are published to registry automatically
- ‚úÖ Release assets are signed and verifiable
- ‚úÖ Documentation is clear for contributors
- ‚úÖ Rollback procedure is documented and tested

## Testing the Pipeline

1. Create a test commit with conventional commit message:
   ```bash
   git commit -m "feat: add test feature for release pipeline"
   ```

2. Push to main (or create PR and merge)

3. Verify:
   - GitHub Actions workflow runs successfully
   - New release appears in GitHub Releases
   - CHANGELOG.md is updated
   - Package version is bumped
   - Registry contains new version

## Maintenance

- Review release workflow quarterly
- Update semantic-release plugins regularly
- Monitor GitHub Actions usage and costs
- Audit release permissions and tokens
- Test rollback procedure annually

## Resources

- [Semantic Versioning](https://semver.org/)
- [Conventional Commits](https://www.conventionalcommits.org/)
- [GitHub Actions: Publishing packages](https://docs.github.com/en/actions/publishing-packages)
- [SLSA Framework](https://slsa.dev/)
- [Sigstore for signing](https://www.sigstore.dev/)

## Troubleshooting

### Release workflow fails
- Check conventional commit format
- Verify GITHUB_TOKEN permissions
- Review semantic-release configuration

### Package publish fails
- Verify registry credentials
- Check package name availability
- Review version conflict errors

### Version not incrementing
- Ensure commits use conventional format
- Check branch configuration in .releaserc.json
- Verify no `[skip ci]` in commit messages
````

## File: coldstart-prompts/19-github-pages-linter-integration.md
````markdown
# Coldstart Prompt: GitHub Pages Linter Integration

**Priority**: P2 (Medium - Quality)
**Estimated Effort**: 2-3 hours
**Generated**: 2025-11-21

---

## Context

AgentReady has a GitHub Pages documentation site in the `docs/` directory using Jekyll. The site includes:
- User guide, developer guide, API reference
- Strategic roadmaps documentation
- Attributes documentation
- Examples

**Current State:**
- GitHub Pages site configured in `docs/_config.yml`
- Repository: `ambient-code/agentready`
- GitHub Pages URL: https://ambient-code.github.io/agentready/
- Existing CI/CD: `.github/workflows/tests.yml`, `security.yml`, `agentready-assessment.yml`, `update-docs.yml`

**Problem:** No automated linting for Jekyll/GitHub Pages content before deployment. Need to catch issues like:
- Broken internal links
- Invalid Jekyll frontmatter
- Missing required fields
- Liquid syntax errors
- Markdown syntax issues

---

## Objective

Find an appropriate linter for GitHub Pages/Jekyll sites and integrate it into the repository's existing CI/CD automation.

---

## Requirements

### 1. Research GitHub Pages Linters

Evaluate and select the best linter for this use case. Consider:

**Popular Options:**
- **htmlproofer** (Ruby) - Validates HTML, checks links, images
- **markdown-link-check** (Node.js) - Checks Markdown links
- **jekyll build --strict** - Built-in Jekyll validation
- **actionlint** - GitHub Actions workflow linter
- **markdownlint-cli** (already used for markdown files)

**Selection Criteria:**
- Compatible with Jekyll/GitHub Pages
- Checks internal and external links
- Validates frontmatter/metadata
- Fast execution (<2 min for our docs)
- Good GitHub Actions support
- Active maintenance

### 2. Integration with Existing Automation

**Existing Workflows to Consider:**
- `.github/workflows/tests.yml` - Runs on PR and push to main
- `.github/workflows/update-docs.yml` - Documentation updates
- `.github/workflows/security.yml` - Security scanning
- `.github/workflows/agentready-assessment.yml` - Repository assessment

**Integration Options:**

**Option A: Add to existing `tests.yml`**
- Pro: Single workflow, faster feedback
- Con: Slower overall test run

**Option B: Create new `docs-lint.yml`**
- Pro: Separate concerns, parallel execution
- Con: One more workflow to maintain

**Option C: Add to `update-docs.yml`**
- Pro: Runs when docs change
- Con: Only triggered by workflow_dispatch

**Recommendation:** Create new `docs-lint.yml` that runs on:
- Pull requests (any file in `docs/`)
- Pushes to main (any file in `docs/`)
- Manual dispatch

### 3. Linting Scope

**What to Lint:**
- All Markdown files in `docs/`
- Jekyll configuration (`docs/_config.yml`)
- Liquid templates in `docs/_layouts/` (if any)
- Internal links between documentation pages
- External links to major sources (Anthropic, Microsoft, etc.)

**What NOT to Lint:**
- Source code (already linted by black, isort, ruff)
- Generated files (`.agentready/`)
- Repomix outputs

### 4. Error Handling

**Failure Behavior:**
- Broken internal links ‚Üí FAIL (block PR)
- Invalid frontmatter ‚Üí FAIL (block PR)
- Broken external links ‚Üí WARN (don't block, just notify)
- Missing alt text on images ‚Üí WARN

### 5. Configuration

Create `.github/workflows/docs-lint.yml` with:
- Triggers on PR/push to `docs/**` paths
- Install linter dependencies
- Run linter with appropriate flags
- Upload results as artifact
- Comment on PR with findings (if any)

---

## Implementation Checklist

### Step 1: Research Linters
- [ ] Review htmlproofer documentation
- [ ] Review markdown-link-check documentation
- [ ] Test `jekyll build --strict` locally
- [ ] Compare performance and features
- [ ] Select best option for our use case

### Step 2: Test Locally
- [ ] Install selected linter locally
- [ ] Run on `docs/` directory
- [ ] Identify any existing issues
- [ ] Fix critical issues (broken links, invalid frontmatter)
- [ ] Document configuration options

### Step 3: Create GitHub Actions Workflow
- [ ] Create `.github/workflows/docs-lint.yml`
- [ ] Configure triggers (PR, push to main, workflow_dispatch)
- [ ] Add path filters (only run when `docs/**` changes)
- [ ] Install linter in workflow
- [ ] Run linter with appropriate flags
- [ ] Upload results as artifact

### Step 4: Configure Linter
- [ ] Create linter config file (if needed)
- [ ] Ignore external link checks or make them warnings
- [ ] Configure frontmatter validation
- [ ] Set up link checking
- [ ] Define success/failure criteria

### Step 5: Add PR Comments
- [ ] Use GitHub Actions to comment on PR with results
- [ ] Show summary of issues found
- [ ] Link to full linter output artifact
- [ ] Only comment if issues found

### Step 6: Documentation
- [ ] Update `CLAUDE.md` with linting workflow
- [ ] Add section to developer guide about docs linting
- [ ] Document how to run linter locally
- [ ] Add troubleshooting for common issues

### Step 7: Test & Validate
- [ ] Create test PR with intentional docs issues
- [ ] Verify workflow triggers correctly
- [ ] Verify issues are caught and reported
- [ ] Verify PR comments appear
- [ ] Fix any workflow bugs

---

## Key Files to Review

### Existing Workflows
- `.github/workflows/tests.yml` - Current test automation
- `.github/workflows/update-docs.yml` - Docs update workflow
- `.github/workflows/security.yml` - Security scanning pattern
- `.github/workflows/agentready-assessment.yml` - Assessment pattern

### Documentation Structure
- `docs/_config.yml` - Jekyll configuration
- `docs/index.md` - Homepage
- `docs/user-guide.md` - User documentation
- `docs/developer-guide.md` - Developer documentation
- `docs/roadmaps.md` - Strategic roadmaps
- `docs/attributes.md` - Attribute documentation
- `docs/api-reference.md` - API reference
- `docs/examples.md` - Examples

### Project Configuration
- `CLAUDE.md` - Development workflow documentation (update with linting info)
- `.gitignore` - Ensure linter outputs are ignored

---

## Recommended Implementation: htmlproofer

**Why htmlproofer:**
- Industry standard for Jekyll sites
- Checks HTML output (catches more issues than markdown-only linters)
- Built-in link checking (internal and external)
- Image validation
- GitHub Actions support
- Active maintenance

**Workflow Example:**

```yaml
name: Documentation Linting

on:
  pull_request:
    paths:
      - 'docs/**'
      - '.github/workflows/docs-lint.yml'
  push:
    branches: [main]
    paths:
      - 'docs/**'
  workflow_dispatch:

jobs:
  lint-docs:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Ruby
        uses: ruby/setup-ruby@v1
        with:
          ruby-version: '3.2'
          bundler-cache: true

      - name: Install Jekyll and dependencies
        run: |
          cd docs
          bundle install

      - name: Build Jekyll site
        run: |
          cd docs
          bundle exec jekyll build --strict

      - name: Install htmlproofer
        run: gem install html-proofer

      - name: Run htmlproofer
        run: |
          htmlproofer docs/_site \
            --disable-external \
            --allow-hash-href \
            --check-html \
            --check-img-http \
            --enforce-https \
            --report-invalid-tags \
            --report-missing-names \
            --log-level :info

      - name: Upload results
        if: failure()
        uses: actions/upload-artifact@v4
        with:
          name: htmlproofer-results
          path: tmp/.htmlproofer/

      - name: Comment on PR
        if: failure() && github.event_name == 'pull_request'
        uses: actions/github-script@v7
        with:
          script: |
            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: '‚ö†Ô∏è Documentation linting failed. See workflow logs for details.'
            })
```

**Configuration File (`docs/Gemfile`):**

```ruby
source 'https://rubygems.org'

gem 'jekyll', '~> 4.3'
gem 'jekyll-feed'
gem 'jekyll-seo-tag'
gem 'jekyll-sitemap'
gem 'html-proofer'

group :jekyll_plugins do
  gem 'jekyll-feed'
  gem 'jekyll-seo-tag'
  gem 'jekyll-sitemap'
end
```

---

## Alternative: markdown-link-check

**Why markdown-link-check:**
- Lighter weight (Node.js, not Ruby)
- Faster execution
- Simpler setup
- Good for link-only validation

**Workflow Example:**

```yaml
name: Documentation Link Check

on:
  pull_request:
    paths:
      - 'docs/**'
  push:
    branches: [main]
    paths:
      - 'docs/**'
  workflow_dispatch:

jobs:
  link-check:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '20'

      - name: Install markdown-link-check
        run: npm install -g markdown-link-check

      - name: Check links in documentation
        run: |
          find docs -name '*.md' -print0 | \
            xargs -0 -n1 markdown-link-check \
              --config .markdown-link-check.json

      - name: Comment on PR
        if: failure() && github.event_name == 'pull_request'
        uses: actions/github-script@v7
        with:
          script: |
            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: '‚ö†Ô∏è Broken links found in documentation. See workflow logs for details.'
            })
```

**Configuration (`.markdown-link-check.json`):**

```json
{
  "ignorePatterns": [
    {
      "pattern": "^http://localhost"
    }
  ],
  "timeout": "20s",
  "retryOn429": true,
  "retryCount": 3,
  "fallbackRetryDelay": "30s",
  "aliveStatusCodes": [200, 206, 301, 302, 308]
}
```

---

## Testing Strategy

### Unit Testing (Local)
```bash
# Install linter
gem install html-proofer  # or npm install -g markdown-link-check

# Test on docs directory
cd docs
bundle exec jekyll build --strict
htmlproofer _site --disable-external

# Fix any issues found
# Re-run until clean
```

### Integration Testing (CI)
1. Create test PR with intentional issues:
   - Broken internal link
   - Invalid frontmatter
   - Missing image
2. Verify workflow catches issues
3. Fix issues
4. Verify workflow passes

### Manual Verification
- [ ] Workflow runs on PR creation
- [ ] Workflow runs on push to main
- [ ] Workflow can be manually triggered
- [ ] Results are uploaded as artifacts
- [ ] PR comments appear when issues found
- [ ] Workflow passes when docs are clean

---

## Success Criteria

**Functional:**
- [ ] Linter integrated into GitHub Actions
- [ ] Runs automatically on PRs touching docs
- [ ] Catches broken internal links
- [ ] Validates Jekyll frontmatter
- [ ] Reports issues clearly in workflow logs
- [ ] Comments on PR when issues found

**Quality:**
- [ ] Fast execution (<2 min for our docs)
- [ ] No false positives on valid links
- [ ] Clear error messages
- [ ] Easy to run locally

**Documentation:**
- [ ] Developer guide updated with linting info
- [ ] Local testing instructions provided
- [ ] Troubleshooting guide for common issues
- [ ] CLAUDE.md updated with workflow info

---

## Getting Started

### Quick Start Commands

```bash
# Navigate to repository
cd /Users/jeder/repos/sk/agentready

# Research linters (choose one)
# Option 1: htmlproofer
gem install html-proofer
cd docs && bundle install && bundle exec jekyll build --strict
htmlproofer _site --disable-external

# Option 2: markdown-link-check
npm install -g markdown-link-check
find docs -name '*.md' -exec markdown-link-check {} \;

# Create workflow file
code .github/workflows/docs-lint.yml

# Test locally
# Fix any issues found

# Create test PR
git checkout -b add-docs-linter
git add .github/workflows/docs-lint.yml
git commit -m "ci: Add GitHub Pages linter to CI/CD"
git push -u origin add-docs-linter

# Create PR and verify workflow runs
gh pr create --title "Add documentation linting to CI/CD" --body "Adds automated linting for GitHub Pages documentation"
```

---

## Notes

- GitHub Pages builds Jekyll automatically, but we want to catch issues earlier (in CI)
- Consider rate limiting for external link checks (GitHub Actions has limited outbound bandwidth)
- htmlproofer can be slow on large sites - may want to cache build artifacts
- markdown-link-check is faster but less comprehensive
- Can run both: markdown-link-check for speed, htmlproofer for comprehensive checks
- Add badge to README.md showing docs linting status

---

## Related Documentation

- **Jekyll Documentation**: https://jekyllrb.com/docs/
- **htmlproofer**: https://github.com/gjtorikian/html-proofer
- **markdown-link-check**: https://github.com/tcort/markdown-link-check
- **GitHub Pages**: https://docs.github.com/en/pages
- **GitHub Actions**: https://docs.github.com/en/actions

---

## Related Issues

- Bootstrap Command (#3) - Created GitHub Actions workflows
- Report improvements (#4, #5) - Documentation that needs linting

---

**Last Updated**: 2025-11-21
**Status**: üî¥ Not Started
**Assignee**: Unassigned
**Labels**: ci/cd, documentation, quality
````

## File: docs/_layouts/default.html
````html
<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta name="description" content="{{ site.description }}">

  <title>{% if page.title %}{{ page.title }} | {% endif %}{{ site.title }}</title>

  <!-- SEO Tags -->
  {% seo %}

  <!-- Styles -->
  <link rel="stylesheet" href="{{ '/assets/css/style.css' | relative_url }}">

  <!-- Favicon (add your own) -->
  <link rel="icon" type="image/svg+xml" href="{{ '/assets/favicon.svg' | relative_url }}">
</head>
<body>
  <!-- Skip to main content for accessibility -->
  <a href="#main-content" class="skip-to-main">Skip to main content</a>

  <!-- Header -->
  <header>
    <nav>
      <div class="container">
        <ul>
          {% for item in site.navigation %}
            <li><a href="{{ item.url | relative_url }}">{{ item.title }}</a></li>
          {% endfor %}
          <li><a href="{{ site.repository }}" target="_blank" rel="noopener">GitHub</a></li>
        </ul>
      </div>
    </nav>
  </header>

  <!-- Main Content -->
  <main id="main-content" class="container">
    <div class="content">
      {{ content }}
    </div>
  </main>

  <!-- Footer -->
  <footer style="background-color: var(--color-gray-100); padding: var(--space-8) 0; margin-top: var(--space-16); text-align: center;">
    <div class="container">
      <p style="color: var(--color-gray-600); margin-bottom: var(--space-4);">
        <strong>AgentReady</strong> v{{ site.version }} ‚Äî Open source under MIT License
      </p>
      <p style="color: var(--color-gray-500); font-size: var(--text-sm);">
        Built with ‚ù§Ô∏è for AI-assisted development
      </p>
      <p style="margin-top: var(--space-4);">
        <a href="{{ site.repository }}" target="_blank" rel="noopener">GitHub</a> ‚Ä¢
        <a href="{{ site.repository }}/issues" target="_blank" rel="noopener">Issues</a> ‚Ä¢
        <a href="{{ site.repository }}/discussions" target="_blank" rel="noopener">Discussions</a>
      </p>
    </div>
  </footer>
</body>
</html>
````

## File: docs/_layouts/home.html
````html
---
layout: default
---

{{ content }}
````

## File: docs/_layouts/page.html
````html
---
layout: default
---

<h1>{{ page.title }}</h1>

{{ content }}
````

## File: docs/assets/css/style.css
````css
/* AgentReady Documentation - Custom Styles */

/* ============================================
   CSS Variables (Design Tokens)
   ============================================ */

:root {
  /* Colors - Primary Palette */
  --color-primary: #2563eb;
  --color-primary-hover: #1d4ed8;
  --color-primary-light: #dbeafe;

  /* Colors - Certification Levels */
  --color-platinum: #e5e4e2;
  --color-gold: #ffd700;
  --color-silver: #c0c0c0;
  --color-bronze: #cd7f32;
  --color-needs-improvement: #8b4513;

  /* Colors - Status */
  --color-success: #16a34a;
  --color-success-bg: #dcfce7;
  --color-error: #dc2626;
  --color-error-bg: #fee2e2;
  --color-warning: #ea580c;
  --color-warning-bg: #ffedd5;
  --color-info: #0891b2;
  --color-info-bg: #cffafe;

  /* Colors - Neutral */
  --color-gray-50: #f9fafb;
  --color-gray-100: #f3f4f6;
  --color-gray-200: #e5e7eb;
  --color-gray-300: #d1d5db;
  --color-gray-400: #9ca3af;
  --color-gray-500: #6b7280;
  --color-gray-600: #4b5563;
  --color-gray-700: #374151;
  --color-gray-800: #1f2937;
  --color-gray-900: #111827;

  /* Typography */
  --font-sans: -apple-system, BlinkMacSystemFont, 'Segoe UI', 'Roboto', 'Helvetica Neue', Arial, sans-serif;
  --font-mono: 'SF Mono', 'Monaco', 'Inconsolata', 'Fira Code', 'Consolas', monospace;

  /* Font Sizes */
  --text-xs: 0.75rem;      /* 12px */
  --text-sm: 0.875rem;     /* 14px */
  --text-base: 1rem;       /* 16px */
  --text-lg: 1.125rem;     /* 18px */
  --text-xl: 1.25rem;      /* 20px */
  --text-2xl: 1.5rem;      /* 24px */
  --text-3xl: 1.875rem;    /* 30px */
  --text-4xl: 2.25rem;     /* 36px */
  --text-5xl: 3rem;        /* 48px */

  /* Spacing */
  --space-1: 0.25rem;      /* 4px */
  --space-2: 0.5rem;       /* 8px */
  --space-3: 0.75rem;      /* 12px */
  --space-4: 1rem;         /* 16px */
  --space-6: 1.5rem;       /* 24px */
  --space-8: 2rem;         /* 32px */
  --space-12: 3rem;        /* 48px */
  --space-16: 4rem;        /* 64px */

  /* Border Radius */
  --radius-sm: 0.25rem;
  --radius-md: 0.5rem;
  --radius-lg: 0.75rem;
  --radius-xl: 1rem;

  /* Shadows */
  --shadow-sm: 0 1px 2px 0 rgba(0, 0, 0, 0.05);
  --shadow-md: 0 4px 6px -1px rgba(0, 0, 0, 0.1);
  --shadow-lg: 0 10px 15px -3px rgba(0, 0, 0, 0.1);
  --shadow-xl: 0 20px 25px -5px rgba(0, 0, 0, 0.1);
}

/* ============================================
   Base Styles
   ============================================ */

* {
  box-sizing: border-box;
}

body {
  font-family: var(--font-sans);
  font-size: var(--text-base);
  line-height: 1.7;
  color: var(--color-gray-800);
  background-color: #ffffff;
  margin: 0;
  padding: 0;
}

/* ============================================
   Typography
   ============================================ */

h1, h2, h3, h4, h5, h6 {
  font-weight: 700;
  line-height: 1.3;
  margin-top: var(--space-8);
  margin-bottom: var(--space-4);
  color: var(--color-gray-900);
}

h1 {
  font-size: var(--text-4xl);
  margin-top: 0;
}

h2 {
  font-size: var(--text-3xl);
  border-bottom: 2px solid var(--color-gray-200);
  padding-bottom: var(--space-2);
}

h3 {
  font-size: var(--text-2xl);
}

h4 {
  font-size: var(--text-xl);
}

h5, h6 {
  font-size: var(--text-lg);
}

p {
  margin-top: 0;
  margin-bottom: var(--space-4);
}

a {
  color: var(--color-primary);
  text-decoration: none;
  transition: color 0.2s ease;
}

a:hover {
  color: var(--color-primary-hover);
  text-decoration: underline;
}

strong, b {
  font-weight: 600;
  color: var(--color-gray-900);
}

em, i {
  font-style: italic;
}

/* ============================================
   Code Blocks
   ============================================ */

code {
  font-family: var(--font-mono);
  font-size: 0.9em;
  background-color: var(--color-gray-100);
  padding: 0.2em 0.4em;
  border-radius: var(--radius-sm);
  color: var(--color-gray-800);
}

pre {
  font-family: var(--font-mono);
  font-size: var(--text-sm);
  background-color: var(--color-gray-900);
  color: #f8f8f2;
  padding: var(--space-4);
  border-radius: var(--radius-md);
  overflow-x: auto;
  margin: var(--space-4) 0;
  line-height: 1.5;
}

pre code {
  background-color: transparent;
  padding: 0;
  color: inherit;
  font-size: inherit;
}

/* ============================================
   Buttons
   ============================================ */

.button {
  display: inline-block;
  padding: var(--space-3) var(--space-6);
  font-size: var(--text-base);
  font-weight: 600;
  text-align: center;
  text-decoration: none;
  border-radius: var(--radius-md);
  transition: all 0.2s ease;
  cursor: pointer;
  border: none;
}

.button-primary {
  background-color: var(--color-primary);
  color: white;
}

.button-primary:hover {
  background-color: var(--color-primary-hover);
  text-decoration: none;
  transform: translateY(-1px);
  box-shadow: var(--shadow-md);
}

.button-secondary {
  background-color: white;
  color: var(--color-primary);
  border: 2px solid var(--color-primary);
}

.button-secondary:hover {
  background-color: var(--color-primary-light);
  text-decoration: none;
}

.button-large {
  padding: var(--space-4) var(--space-8);
  font-size: var(--text-lg);
}

/* ============================================
   Hero Section
   ============================================ */

.hero {
  text-align: center;
  padding: var(--space-12) 0;
  background: linear-gradient(135deg, var(--color-primary-light) 0%, white 100%);
  border-radius: var(--radius-lg);
  margin: var(--space-8) 0;
}

.hero-tagline {
  font-size: var(--text-xl);
  color: var(--color-gray-600);
  max-width: 700px;
  margin: var(--space-6) auto;
}

.hero-buttons {
  display: flex;
  gap: var(--space-4);
  justify-content: center;
  margin-top: var(--space-8);
  flex-wrap: wrap;
}

/* ============================================
   Feature Grid
   ============================================ */

.feature-grid {
  display: grid;
  grid-template-columns: repeat(auto-fit, minmax(300px, 1fr));
  gap: var(--space-6);
  margin: var(--space-8) 0;
}

.feature {
  background-color: white;
  padding: var(--space-6);
  border-radius: var(--radius-lg);
  border: 1px solid var(--color-gray-200);
  transition: all 0.3s ease;
}

.feature:hover {
  border-color: var(--color-primary);
  box-shadow: var(--shadow-lg);
  transform: translateY(-2px);
}

.feature h3 {
  margin-top: 0;
  font-size: var(--text-xl);
  color: var(--color-gray-900);
}

.feature p {
  margin-bottom: 0;
  color: var(--color-gray-600);
}

/* ============================================
   Certification Ladder
   ============================================ */

.certification-ladder {
  display: flex;
  flex-direction: column;
  gap: var(--space-3);
  margin: var(--space-8) 0;
  max-width: 600px;
}

.cert-level {
  display: grid;
  grid-template-columns: 150px 100px 1fr;
  align-items: center;
  gap: var(--space-4);
  padding: var(--space-4);
  border-radius: var(--radius-md);
  border: 2px solid var(--color-gray-200);
  transition: all 0.2s ease;
}

.cert-level:hover {
  box-shadow: var(--shadow-md);
  transform: translateX(4px);
}

.cert-level.platinum {
  border-color: var(--color-platinum);
  background: linear-gradient(135deg, #f5f5f5 0%, white 100%);
}

.cert-level.gold {
  border-color: var(--color-gold);
  background: linear-gradient(135deg, #fffdf0 0%, white 100%);
}

.cert-level.silver {
  border-color: var(--color-silver);
  background: linear-gradient(135deg, #f8f8f8 0%, white 100%);
}

.cert-level.bronze {
  border-color: var(--color-bronze);
  background: linear-gradient(135deg, #fff5f0 0%, white 100%);
}

.cert-level.needs-improvement {
  border-color: var(--color-needs-improvement);
  background: linear-gradient(135deg, #fef5f0 0%, white 100%);
}

.cert-badge {
  font-size: var(--text-lg);
  font-weight: 700;
}

.cert-range {
  font-family: var(--font-mono);
  font-weight: 600;
  color: var(--color-gray-700);
}

.cert-desc {
  color: var(--color-gray-600);
  font-size: var(--text-sm);
}

/* ============================================
   Tables
   ============================================ */

table {
  width: 100%;
  border-collapse: collapse;
  margin: var(--space-6) 0;
  font-size: var(--text-sm);
}

thead {
  background-color: var(--color-gray-100);
}

th {
  padding: var(--space-3);
  text-align: left;
  font-weight: 600;
  color: var(--color-gray-900);
  border-bottom: 2px solid var(--color-gray-300);
}

td {
  padding: var(--space-3);
  border-bottom: 1px solid var(--color-gray-200);
}

tr:hover {
  background-color: var(--color-gray-50);
}

/* ============================================
   Use Case Grid
   ============================================ */

.use-case-grid {
  display: grid;
  grid-template-columns: repeat(auto-fit, minmax(280px, 1fr));
  gap: var(--space-6);
  margin: var(--space-8) 0;
}

.use-case {
  background-color: var(--color-gray-50);
  padding: var(--space-6);
  border-radius: var(--radius-md);
  border-left: 4px solid var(--color-primary);
}

.use-case h4 {
  margin-top: 0;
  font-size: var(--text-lg);
  color: var(--color-gray-900);
}

.use-case p {
  margin-bottom: 0;
  color: var(--color-gray-600);
  font-size: var(--text-sm);
}

/* ============================================
   CTA Section
   ============================================ */

.cta-section {
  text-align: center;
  background: linear-gradient(135deg, var(--color-primary) 0%, #1e40af 100%);
  color: white;
  padding: var(--space-12);
  border-radius: var(--radius-lg);
  margin: var(--space-12) 0;
}

.cta-section h3 {
  margin-top: 0;
  color: white;
  font-size: var(--text-3xl);
}

.cta-section pre {
  background-color: rgba(0, 0, 0, 0.3);
  margin: var(--space-6) auto;
  max-width: 500px;
}

.cta-section .button-primary {
  background-color: white;
  color: var(--color-primary);
  margin-top: var(--space-6);
}

.cta-section .button-primary:hover {
  background-color: var(--color-gray-100);
}

/* ============================================
   Lists
   ============================================ */

ul, ol {
  margin: var(--space-4) 0;
  padding-left: var(--space-6);
}

li {
  margin-bottom: var(--space-2);
  line-height: 1.7;
}

ul ul, ol ul, ul ol, ol ol {
  margin-top: var(--space-2);
  margin-bottom: var(--space-2);
}

/* ============================================
   Blockquotes
   ============================================ */

blockquote {
  margin: var(--space-6) 0;
  padding: var(--space-4) var(--space-6);
  border-left: 4px solid var(--color-primary);
  background-color: var(--color-gray-50);
  border-radius: var(--radius-sm);
  font-style: italic;
  color: var(--color-gray-700);
}

blockquote p:last-child {
  margin-bottom: 0;
}

/* ============================================
   Horizontal Rule
   ============================================ */

hr {
  border: none;
  border-top: 2px solid var(--color-gray-200);
  margin: var(--space-12) 0;
}

/* ============================================
   Navigation (if using custom navigation)
   ============================================ */

nav {
  background-color: var(--color-gray-900);
  color: white;
  padding: var(--space-4) 0;
}

nav ul {
  list-style: none;
  margin: 0;
  padding: 0;
  display: flex;
  gap: var(--space-6);
  justify-content: center;
  flex-wrap: wrap;
}

nav a {
  color: white;
  text-decoration: none;
  font-weight: 500;
  padding: var(--space-2) var(--space-4);
  border-radius: var(--radius-sm);
  transition: background-color 0.2s ease;
}

nav a:hover {
  background-color: rgba(255, 255, 255, 0.1);
  text-decoration: none;
}

/* ============================================
   Content Container
   ============================================ */

.container {
  max-width: 1200px;
  margin: 0 auto;
  padding: var(--space-6) var(--space-4);
}

.content {
  max-width: 800px;
  margin: 0 auto;
}

/* ============================================
   Status Badges
   ============================================ */

.badge {
  display: inline-block;
  padding: var(--space-1) var(--space-3);
  border-radius: var(--radius-md);
  font-size: var(--text-xs);
  font-weight: 600;
  text-transform: uppercase;
  letter-spacing: 0.05em;
}

.badge-success {
  background-color: var(--color-success-bg);
  color: var(--color-success);
}

.badge-error {
  background-color: var(--color-error-bg);
  color: var(--color-error);
}

.badge-warning {
  background-color: var(--color-warning-bg);
  color: var(--color-warning);
}

.badge-info {
  background-color: var(--color-info-bg);
  color: var(--color-info);
}

/* ============================================
   Responsive Design
   ============================================ */

@media (max-width: 768px) {
  :root {
    --text-4xl: 2rem;
    --text-3xl: 1.5rem;
    --text-2xl: 1.25rem;
  }

  .hero-buttons {
    flex-direction: column;
    align-items: center;
  }

  .feature-grid {
    grid-template-columns: 1fr;
  }

  .cert-level {
    grid-template-columns: 1fr;
    text-align: center;
  }

  .use-case-grid {
    grid-template-columns: 1fr;
  }

  table {
    font-size: var(--text-xs);
  }

  th, td {
    padding: var(--space-2);
  }
}

/* ============================================
   Print Styles
   ============================================ */

@media print {
  body {
    font-size: 12pt;
    line-height: 1.5;
  }

  .hero-buttons,
  .cta-section,
  nav {
    display: none;
  }

  a {
    color: black;
    text-decoration: underline;
  }

  pre {
    border: 1px solid var(--color-gray-300);
    page-break-inside: avoid;
  }

  h2, h3, h4, h5, h6 {
    page-break-after: avoid;
  }
}

/* ============================================
   Syntax Highlighting (Rouge/Pygments)
   ============================================ */

.highlight {
  background-color: var(--color-gray-900);
  color: #f8f8f2;
  border-radius: var(--radius-md);
  padding: var(--space-4);
  overflow-x: auto;
  margin: var(--space-4) 0;
}

.highlight .k { color: #66d9ef; } /* Keyword */
.highlight .s { color: #e6db74; } /* String */
.highlight .c { color: #75715e; } /* Comment */
.highlight .n { color: #f8f8f2; } /* Name */
.highlight .o { color: #f92672; } /* Operator */
.highlight .m { color: #ae81ff; } /* Number */
.highlight .nf { color: #a6e22e; } /* Function */

/* ============================================
   Accessibility
   ============================================ */

/* Focus styles for keyboard navigation */
a:focus,
button:focus,
.button:focus {
  outline: 2px solid var(--color-primary);
  outline-offset: 2px;
}

/* Skip to main content link */
.skip-to-main {
  position: absolute;
  top: -40px;
  left: 0;
  background: var(--color-primary);
  color: white;
  padding: var(--space-2) var(--space-4);
  text-decoration: none;
  z-index: 100;
}

.skip-to-main:focus {
  top: 0;
}

/* ============================================
   Dark Mode Support (Optional)
   ============================================ */

@media (prefers-color-scheme: dark) {
  /* Uncomment to enable dark mode
  body {
    background-color: var(--color-gray-900);
    color: var(--color-gray-100);
  }

  h1, h2, h3, h4, h5, h6 {
    color: white;
  }

  code {
    background-color: var(--color-gray-800);
    color: var(--color-gray-100);
  }

  .feature {
    background-color: var(--color-gray-800);
    border-color: var(--color-gray-700);
  }
  */
}
````

## File: docs/DEPLOYMENT.md
````markdown
# GitHub Pages Deployment Guide

Complete instructions for deploying AgentReady documentation to GitHub Pages.

## Quick Start

1. **Enable GitHub Pages** in repository settings
2. **Configure source**: `docs/` directory from `main` branch
3. **Wait for deployment** (~2 minutes)
4. **Visit**: `https://yourusername.github.io/agentready`

---

## Detailed Setup Instructions

### Step 1: Repository Configuration

1. **Navigate to repository settings**:
   - Go to your AgentReady repository on GitHub
   - Click "Settings" tab
   - Click "Pages" in left sidebar

2. **Configure source**:
   - **Source**: Deploy from a branch
   - **Branch**: `main` (or your default branch)
   - **Folder**: `/docs`
   - Click "Save"

3. **Optional: Custom domain**:
   - Add custom domain if desired
   - Configure DNS records (see GitHub documentation)

### Step 2: Update _config.yml

Edit `docs/_config.yml` to match your repository:

```yaml
# Site settings
title: AgentReady
description: Repository Quality Assessment for AI-Assisted Development

# Repository information
repository: YOUR_USERNAME/agentready
github_username: YOUR_USERNAME

# URLs
baseurl: ""  # Subpath (e.g., "/agentready" if not on root domain)
url: "https://YOUR_USERNAME.github.io"  # Base hostname
```

**Important**:
- Replace `YOUR_USERNAME` with your GitHub username
- If using custom domain, update `url` accordingly
- Set `baseurl` to `/agentready` if repository is not organization homepage

### Step 3: Verify Configuration

```bash
# Test locally with Jekyll (optional)
cd docs/

# Install Jekyll (if not already installed)
gem install bundler jekyll

# Create Gemfile
cat > Gemfile << 'EOF'
source "https://rubygems.org"
gem "github-pages", group: :jekyll_plugins
gem "jekyll-seo-tag"
gem "jekyll-sitemap"
EOF

# Install dependencies
bundle install

# Serve locally
bundle exec jekyll serve

# Visit http://localhost:4000 in browser
```

### Step 4: Commit and Push

```bash
# From repository root
git add docs/
git commit -m "docs: Add GitHub Pages documentation site"
git push origin main
```

### Step 5: Monitor Deployment

1. **Go to Actions tab** in GitHub repository
2. **Wait for "pages-build-deployment" workflow** to complete
3. **Deployment time**: Usually 1-3 minutes
4. **Check status**:
   - Green check: Deployment successful
   - Red X: Deployment failed (check logs)

### Step 6: Visit Your Site

Once deployment completes:

- **URL**: `https://yourusername.github.io/agentready`
- **Custom domain** (if configured): `https://your-domain.com`

---

## Troubleshooting

### Issue: 404 Not Found

**Possible causes**:
1. GitHub Pages not enabled in settings
2. Wrong branch/folder selected
3. Deployment still in progress
4. `baseurl` misconfigured in `_config.yml`

**Solutions**:
```bash
# Check _config.yml
grep -E "baseurl|url" docs/_config.yml

# Verify branch is correct
git branch --show-current

# Check GitHub Actions logs for errors
# (visit /actions tab in repository)
```

### Issue: Broken Links

**Cause**: Incorrect `baseurl` in `_config.yml`

**Solution**:
```yaml
# For repository named "agentready" at github.io/agentready
baseurl: "/agentready"
url: "https://yourusername.github.io"

# For custom domain or organization site
baseurl: ""
url: "https://your-domain.com"
```

**Test links**:
```bash
# Search for absolute URLs (should use relative_url filter)
grep -r "href=\"/" docs/*.md

# Correct format:
# href="{{ '/user-guide' | relative_url }}"
```

### Issue: CSS Not Loading

**Cause**: CSS file not found due to path issues

**Solution**:

1. **Verify file exists**:
   ```bash
   ls docs/assets/css/style.css
   ```

2. **Check layout references**:
   ```html
   <!-- Should be: -->
   <link rel="stylesheet" href="{{ '/assets/css/style.css' | relative_url }}">

   <!-- Not: -->
   <link rel="stylesheet" href="/assets/css/style.css">
   ```

3. **Clear browser cache** and refresh

### Issue: Jekyll Build Failure

**Check GitHub Actions logs**:

1. Go to repository ‚Üí Actions tab
2. Click latest workflow run
3. Expand failed step
4. Look for error messages

**Common errors**:

**YAML syntax error**:
```
Error: Invalid YAML in _config.yml
```
**Solution**: Validate YAML syntax with online validator

**Missing plugin**:
```
Error: jekyll-seo-tag not found
```
**Solution**: Add to `_config.yml`:
```yaml
plugins:
  - jekyll-seo-tag
  - jekyll-sitemap
```

**Liquid syntax error**:
```
Error: Liquid Exception in layouts/default.html
```
**Solution**: Check for missing `{% endfor %}`, `{% endif %}`, etc.

---

## Local Development

### Setup Jekyll Locally

```bash
# Install Ruby (required for Jekyll)
# macOS: Already installed
# Linux: apt-get install ruby-full
# Windows: Use RubyInstaller

# Install Bundler and Jekyll
gem install bundler jekyll

# Navigate to docs directory
cd docs/

# Create Gemfile (if not exists)
cat > Gemfile << 'EOF'
source "https://rubygems.org"
gem "github-pages", group: :jekyll_plugins
gem "jekyll-seo-tag"
gem "jekyll-sitemap"
gem "webrick"  # Required for Ruby 3.0+
EOF

# Install dependencies
bundle install

# Serve site locally
bundle exec jekyll serve

# Or with live reload
bundle exec jekyll serve --livereload

# Visit http://localhost:4000
```

### Development Workflow

```bash
# 1. Make changes to Markdown files
vim docs/user-guide.md

# 2. Jekyll auto-regenerates (if using --livereload)
# Or manually restart server

# 3. View changes in browser (refresh if needed)

# 4. Commit when satisfied
git add docs/user-guide.md
git commit -m "docs: Update user guide installation section"
git push
```

### Testing Links Locally

```bash
# Install html-proofer
gem install html-proofer

# Build site
bundle exec jekyll build

# Test all links
htmlproofer ./_site --disable-external

# Test including external links (slower)
htmlproofer ./_site
```

---

## Custom Domain Setup

### 1. Add CNAME File

```bash
# Create CNAME file in docs/
echo "docs.agentready.dev" > docs/CNAME

# Commit
git add docs/CNAME
git commit -m "docs: Add custom domain CNAME"
git push
```

### 2. Configure DNS

Add DNS records at your domain registrar:

**For apex domain (example.com)**:
```
A    @    185.199.108.153
A    @    185.199.109.153
A    @    185.199.110.153
A    @    185.199.111.153
```

**For subdomain (docs.example.com)**:
```
CNAME    docs    yourusername.github.io.
```

### 3. Enable HTTPS

1. Go to repository Settings ‚Üí Pages
2. Check "Enforce HTTPS"
3. Wait for certificate provisioning (~24 hours)

### 4. Update _config.yml

```yaml
url: "https://docs.agentready.dev"
baseurl: ""
```

---

## Automation & CI/CD

### Auto-Deploy on Push

GitHub Pages automatically deploys when you push to `main` branch.

**No additional configuration needed!**

### Custom GitHub Actions Workflow (Optional)

Create `.github/workflows/docs.yml` for advanced control:

```yaml
name: Deploy Documentation

on:
  push:
    branches: [main]
    paths:
      - 'docs/**'
  workflow_dispatch:

jobs:
  deploy:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - uses: ruby/setup-ruby@v1
        with:
          ruby-version: '3.1'
          bundler-cache: true

      - name: Build Jekyll site
        run: |
          cd docs
          bundle install
          bundle exec jekyll build

      - name: Deploy to GitHub Pages
        uses: peaceiris/actions-gh-pages@v3
        with:
          github_token: ${{ secrets.GITHUB_TOKEN }}
          publish_dir: ./docs/_site
```

---

## Content Updates

### Adding New Pages

1. **Create Markdown file** in `docs/`:
   ```bash
   cat > docs/new-page.md << 'EOF'
   ---
   layout: page
   title: New Page
   ---

   # New Page

   Content goes here.
   EOF
   ```

2. **Add to navigation** in `_config.yml`:
   ```yaml
   navigation:
     # ... existing items ...
     - title: New Page
       url: /new-page
   ```

3. **Commit and push**:
   ```bash
   git add docs/new-page.md docs/_config.yml
   git commit -m "docs: Add new page"
   git push
   ```

### Updating Existing Content

```bash
# Edit file
vim docs/user-guide.md

# Preview locally (optional)
bundle exec jekyll serve

# Commit and push
git add docs/user-guide.md
git commit -m "docs: Update installation instructions"
git push

# GitHub Pages auto-deploys in ~2 minutes
```

---

## Performance Optimization

### Image Optimization

```bash
# Optimize images before adding
brew install imageoptim-cli  # macOS
imageoptim docs/assets/images/*.png

# Or use online tools:
# - TinyPNG (https://tinypng.com)
# - Squoosh (https://squoosh.app)
```

### Caching

GitHub Pages automatically sets cache headers for static assets.

**No additional configuration needed.**

### Minimize File Sizes

```bash
# Minify CSS (if not using Jekyll plugins)
npm install -g csso-cli
csso docs/assets/css/style.css -o docs/assets/css/style.min.css
```

---

## Monitoring & Analytics

### Google Analytics (Optional)

1. **Get tracking ID** from Google Analytics
2. **Add to _config.yml**:
   ```yaml
   google_analytics: UA-XXXXXXXXX-X
   ```
3. **Tracking script auto-added** by jekyll-seo-tag plugin

### GitHub Traffic

View built-in analytics:
1. Go to repository Insights tab
2. Click "Traffic"
3. See page views and visitor stats

---

## Backup & Versioning

### Site Backups

Your documentation is version-controlled in git. **No separate backup needed.**

To download static site:

```bash
# Build site locally
bundle exec jekyll build

# Copy _site directory
cp -r docs/_site ~/backups/agentready-docs-$(date +%Y%m%d)
```

### Rollback

```bash
# Revert to previous commit
git log --oneline docs/  # Find commit hash
git revert <commit-hash>
git push

# Or reset to specific commit (destructive)
git reset --hard <commit-hash>
git push --force  # Use with caution
```

---

## Support

- **Jekyll Documentation**: https://jekyllrb.com/docs/
- **GitHub Pages Docs**: https://docs.github.com/en/pages
- **Jekyll Themes**: https://github.com/topics/jekyll-theme
- **Troubleshooting**: https://docs.github.com/en/pages/setting-up-a-github-pages-site-with-jekyll/troubleshooting-jekyll-build-errors-for-github-pages-sites

---

## Next Steps

After deploying:

1. **Test all pages** and links
2. **Share URL** with team/community
3. **Set up custom domain** (optional)
4. **Enable analytics** (optional)
5. **Add to README** as documentation link

**Documentation URL**: Add to main README.md:

```markdown
## Documentation

Complete documentation available at: https://yourusername.github.io/agentready

- [User Guide](https://yourusername.github.io/agentready/user-guide)
- [Developer Guide](https://yourusername.github.io/agentready/developer-guide)
- [API Reference](https://yourusername.github.io/agentready/api-reference)
```

---

**Ready to deploy?** Follow steps 1-6 above. Deployment typically completes in under 3 minutes.
````

## File: docs/Gemfile
````
source "https://rubygems.org"

# GitHub Pages gem includes Jekyll and all GitHub-supported plugins
gem "github-pages", group: :jekyll_plugins

# Additional plugins
gem "jekyll-seo-tag"
gem "jekyll-sitemap"
gem "jekyll-feed"

# Required for Ruby 3.0+
gem "webrick"

# Development dependencies (optional)
group :development do
  gem "html-proofer"
end
````

## File: docs/README.md
````markdown
# AgentReady Documentation

This directory contains the complete GitHub Pages documentation site for AgentReady.

## Structure

```
docs/
‚îú‚îÄ‚îÄ _config.yml              # Jekyll configuration
‚îú‚îÄ‚îÄ _layouts/                # Page templates
‚îÇ   ‚îú‚îÄ‚îÄ default.html         # Base layout
‚îÇ   ‚îú‚îÄ‚îÄ home.html            # Homepage layout
‚îÇ   ‚îî‚îÄ‚îÄ page.html            # Standard page layout
‚îú‚îÄ‚îÄ assets/
‚îÇ   ‚îî‚îÄ‚îÄ css/
‚îÇ       ‚îî‚îÄ‚îÄ style.css        # Custom styles
‚îú‚îÄ‚îÄ index.md                 # Homepage
‚îú‚îÄ‚îÄ user-guide.md            # User documentation
‚îú‚îÄ‚îÄ developer-guide.md       # Contributor guide
‚îú‚îÄ‚îÄ attributes.md            # Attribute reference
‚îú‚îÄ‚îÄ api-reference.md         # API documentation
‚îú‚îÄ‚îÄ examples.md              # Examples and showcase
‚îú‚îÄ‚îÄ DEPLOYMENT.md            # Deployment instructions
‚îî‚îÄ‚îÄ README.md                # This file
```

## Local Development

### Prerequisites

- Ruby 2.7+ (3.0+ recommended)
- Bundler
- Jekyll

### Setup

```bash
# Navigate to docs directory
cd docs/

# Install dependencies
bundle install

# Serve locally
bundle exec jekyll serve

# Or with live reload
bundle exec jekyll serve --livereload

# Visit http://localhost:4000
```

### Making Changes

1. **Edit Markdown files** (*.md)
2. **Jekyll auto-regenerates** site (if using --livereload)
3. **Refresh browser** to see changes
4. **Commit when satisfied**

## Deployment

See [DEPLOYMENT.md](DEPLOYMENT.md) for complete deployment instructions.

**Quick deploy**:

1. Enable GitHub Pages in repository settings
2. Select `docs/` directory as source
3. Push changes to main branch
4. Visit `https://yourusername.github.io/agentready`

## Content Guidelines

### Adding New Pages

1. Create new `.md` file in `docs/`:
   ```markdown
   ---
   layout: page
   title: Page Title
   ---

   # Page Title

   Content here...
   ```

2. Add to navigation in `_config.yml`:
   ```yaml
   navigation:
     - title: Page Title
       url: /page-url
   ```

### Markdown Formatting

- Use `#` for h1, `##` for h2, etc.
- Code blocks with triple backticks: ` ```python `
- Links: `[text](url)` or `[text]({{ '/page' | relative_url }})`
- Images: `![alt]({{ '/assets/images/file.png' | relative_url }})`

### Front Matter

All pages should have front matter:

```yaml
---
layout: page  # or 'home' for homepage
title: Page Title
---
```

## Styling

Custom styles in `assets/css/style.css` using CSS variables for:

- Colors (certification levels, status indicators)
- Typography (fonts, sizes, weights)
- Spacing (consistent padding/margins)
- Components (buttons, grids, tables)

**Design system**:
- Primary color: `#2563eb` (blue)
- Font: System fonts for speed
- Responsive: Mobile-first design
- Accessible: WCAG AA compliant

## Testing

### Link Checking

```bash
# Install html-proofer
gem install html-proofer

# Build site
bundle exec jekyll build

# Test links
htmlproofer ./_site --disable-external
```

### Markdown Linting

```bash
# Install markdownlint
npm install -g markdownlint-cli

# Lint all Markdown files
markdownlint docs/*.md
```

## Maintenance

### Updating Dependencies

```bash
# Update Gemfile.lock
bundle update

# Test locally
bundle exec jekyll serve

# Commit if working
git add Gemfile.lock
git commit -m "chore: Update Jekyll dependencies"
```

### Checking for Broken Links

Periodically check for broken external links:

```bash
# Build and test
bundle exec jekyll build
htmlproofer ./_site --external_only --http-status-ignore "999"
```

## Support

- **Jekyll Docs**: https://jekyllrb.com/docs/
- **GitHub Pages**: https://docs.github.com/en/pages
- **Markdown Guide**: https://www.markdownguide.org/

## Contributing

Contributions to documentation are welcome!

1. Fork repository
2. Create feature branch
3. Make changes in `docs/`
4. Test locally
5. Submit pull request

**See main [CLAUDE.md](../CLAUDE.md) for development workflow.**

---

**Questions?** Open an issue or discussion on GitHub.
````

## File: docs/RELEASE_PROCESS.md
````markdown
# Release Process

## Overview

AgentReady uses automated semantic releases based on conventional commits. Releases are created automatically when commits are merged to the `main` branch.

## Release Types

Releases follow [Semantic Versioning](https://semver.org/):

- **Major (X.0.0)**: Breaking changes (commit prefix: `feat!:` or `fix!:` or `BREAKING CHANGE:`)
- **Minor (x.Y.0)**: New features (commit prefix: `feat:`)
- **Patch (x.y.Z)**: Bug fixes (commit prefix: `fix:`)

## Automated Release Workflow

When commits are merged to `main`:

1. **Semantic-release analyzes** commit messages since the last release
2. **Version is determined** based on conventional commit types
3. **CHANGELOG.md is updated** with release notes
4. **pyproject.toml version** is bumped automatically
5. **Git tag is created** (e.g., `v1.0.0`)
6. **GitHub Release is published** with release notes
7. **Changes are committed** back to main with `[skip ci]`

## Conventional Commit Format

All commits must follow the [Conventional Commits](https://www.conventionalcommits.org/) specification:

```
<type>(<scope>): <description>

[optional body]

[optional footer(s)]
```

### Common Types

- `feat:` - New feature (triggers minor release)
- `fix:` - Bug fix (triggers patch release)
- `docs:` - Documentation changes (no release)
- `chore:` - Maintenance tasks (no release)
- `refactor:` - Code refactoring (no release)
- `test:` - Test changes (no release)
- `ci:` - CI/CD changes (no release)

### Breaking Changes

To trigger a major version bump, use one of these:

```bash
# With ! after type
feat!: redesign assessment API

# With BREAKING CHANGE footer
feat: update scoring algorithm

BREAKING CHANGE: Assessment.score is now a float instead of int
```

## Manual Release Trigger

To manually trigger a release without a commit:

```bash
# Trigger release workflow via GitHub CLI
gh workflow run release.yml

# Or via GitHub UI
# Actions ‚Üí Release ‚Üí Run workflow ‚Üí Run workflow
```

## Pre-release Process

For alpha/beta releases (not yet configured):

```bash
# Future: Create pre-release from beta branch
git checkout -b beta
git push origin beta
```

Then update `.releaserc.json` to include beta branch configuration.

## Hotfix Process

For urgent production fixes:

1. **Create hotfix branch** from the latest release tag:
   ```bash
   git checkout -b hotfix/critical-bug v1.2.3
   ```

2. **Apply fix** with conventional commit:
   ```bash
   git commit -m "fix: resolve critical security issue"
   ```

3. **Push and create PR** to main:
   ```bash
   git push origin hotfix/critical-bug
   gh pr create --base main --title "fix: critical security hotfix"
   ```

4. **Merge to main** - Release automation handles versioning

## Rollback Procedure

To rollback a release:

### 1. Delete the tag and release

```bash
# Delete tag locally
git tag -d v1.2.3

# Delete tag remotely
git push origin :refs/tags/v1.2.3

# Delete GitHub release
gh release delete v1.2.3 --yes
```

### 2. Revert the release commit

```bash
# Find the release commit
git log --oneline | grep "chore(release)"

# Revert it
git revert <release-commit-sha>
git push origin main
```

### 3. Restore previous version

Edit `pyproject.toml` to restore the previous version number and commit.

## Release Checklist

Before a major release, ensure:

- [ ] All tests passing on main branch
- [ ] Documentation is up to date
- [ ] Security vulnerabilities addressed
- [ ] Dependencies are up to date (run `uv pip list --outdated`)
- [ ] Self-assessment score is current
- [ ] Migration guide written (if breaking changes)
- [ ] Examples updated for new features

## Monitoring Releases

After a release is published:

1. **Verify GitHub Release** - Check release notes are accurate
2. **Monitor issues** - Watch for regression reports
3. **Check workflows** - Ensure no failures in release workflow
4. **Update milestones** - Close completed milestone, create next one

## Troubleshooting

### Release workflow fails

- Check commit message format matches conventional commits
- Verify `GITHUB_TOKEN` has sufficient permissions
- Review semantic-release logs in Actions tab
- Ensure no merge conflicts in CHANGELOG.md

### Version not incrementing

- Ensure commits use conventional commit format (`feat:`, `fix:`, etc.)
- Check that commits aren't marked `[skip ci]`
- Verify `.releaserc.json` branch configuration matches current branch
- Review semantic-release dry-run output

### CHANGELOG conflicts

If CHANGELOG.md has merge conflicts:

1. Resolve conflicts manually
2. Commit the resolution
3. Semantic-release will include the fix in next release

## Resources

- [Semantic Versioning](https://semver.org/)
- [Conventional Commits](https://www.conventionalcommits.org/)
- [Semantic Release Documentation](https://semantic-release.gitbook.io/)
- [GitHub Actions: Publishing packages](https://docs.github.com/en/actions/publishing-packages)

## Version History

| Version | Date | Highlights |
|---------|------|------------|
| 1.0.0 | 2025-11-21 | Initial release with core assessment engine |

---

**Last Updated**: 2025-11-21
**Maintained By**: AgentReady Team
````

## File: docs/SETUP_SUMMARY.md
````markdown
# GitHub Pages Documentation - Setup Summary

Complete GitHub Pages documentation site for AgentReady has been created.

## What Was Created

### Core Documentation Pages

1. **Homepage** (`index.md`)
   - Hero section with value proposition
   - Feature highlights (6 key features)
   - Quick start guide
   - Certification level ladder
   - Attribute overview by tier
   - Report format comparison
   - Use cases and testimonials

2. **User Guide** (`user-guide.md`)
   - Installation instructions (pip, source, development)
   - Quick start tutorial
   - Running assessments (basic and advanced)
   - Understanding reports (HTML, Markdown, JSON)
   - Configuration customization
   - Complete CLI reference
   - Troubleshooting section

3. **Developer Guide** (`developer-guide.md`)
   - Development environment setup
   - Architecture overview (data flow, components, patterns)
   - Implementing new assessors (step-by-step)
   - Testing guidelines (unit, integration)
   - Code quality standards (black, isort, ruff, mypy)
   - Contributing workflow (branching, commits, PRs)
   - Release process

4. **Attributes Reference** (`attributes.md`)
   - Complete 25-attribute reference
   - Tier system explanation
   - Detailed breakdown of Tier 1 attributes (5 essential)
   - Tier 2-4 summaries
   - Implementation status table
   - Links to full research document

5. **API Reference** (`api-reference.md`)
   - Core models (Repository, Attribute, Finding, Assessment, Remediation)
   - Services (Scanner, Scorer, LanguageDetector)
   - Assessors (BaseAssessor interface)
   - Reporters (HTML, Markdown, JSON)
   - Complete usage examples
   - CI/CD integration patterns
   - Error handling guidelines

6. **Examples & Showcase** (`examples.md`)
   - AgentReady self-assessment (75.4/100 Gold)
   - Report interpretation guide
   - Common remediation patterns
   - Integration examples (GitHub Actions, badges, tracking)

### Site Infrastructure

7. **Jekyll Configuration** (`_config.yml`)
   - Site metadata and settings
   - Navigation structure (6 main pages)
   - Plugin configuration (seo-tag, sitemap, feed)
   - Certification level definitions
   - Theme settings

8. **Layouts** (`_layouts/`)
   - `default.html`: Base template with header, nav, footer
   - `home.html`: Homepage wrapper
   - `page.html`: Standard page wrapper

9. **Styling** (`assets/css/style.css`)
   - CSS custom properties (design tokens)
   - Modern, developer-friendly theme
   - Responsive design (mobile-first)
   - Component styles (buttons, grids, tables, badges)
   - Certification ladder styling
   - Code syntax highlighting
   - Accessibility features
   - Print styles
   - Dark mode support (commented out, ready to enable)

10. **Deployment Resources**
    - `DEPLOYMENT.md`: Complete deployment guide
    - `README.md`: Documentation overview
    - `Gemfile`: Ruby dependencies

## File Structure

```
docs/
‚îú‚îÄ‚îÄ _config.yml              # Jekyll configuration
‚îú‚îÄ‚îÄ _layouts/                # Page templates
‚îÇ   ‚îú‚îÄ‚îÄ default.html         # Base layout (header, nav, footer)
‚îÇ   ‚îú‚îÄ‚îÄ home.html            # Homepage layout
‚îÇ   ‚îî‚îÄ‚îÄ page.html            # Standard page layout
‚îú‚îÄ‚îÄ assets/
‚îÇ   ‚îî‚îÄ‚îÄ css/
‚îÇ       ‚îî‚îÄ‚îÄ style.css        # Custom styles (500+ lines)
‚îú‚îÄ‚îÄ index.md                 # Homepage (2,850 lines)
‚îú‚îÄ‚îÄ user-guide.md            # User documentation (1,400 lines)
‚îú‚îÄ‚îÄ developer-guide.md       # Contributor guide (2,300 lines)
‚îú‚îÄ‚îÄ attributes.md            # Attribute reference (1,800 lines)
‚îú‚îÄ‚îÄ api-reference.md         # API documentation (1,900 lines)
‚îú‚îÄ‚îÄ examples.md              # Examples and showcase (1,600 lines)
‚îú‚îÄ‚îÄ DEPLOYMENT.md            # Deployment guide (900 lines)
‚îú‚îÄ‚îÄ README.md                # Documentation overview (200 lines)
‚îú‚îÄ‚îÄ Gemfile                  # Ruby dependencies
‚îî‚îÄ‚îÄ SETUP_SUMMARY.md         # This file
```

**Total**: 13 files, ~12,000 lines of comprehensive documentation

## Key Features

### Design & UX
- Clean, modern interface with professional styling
- Responsive design (works on mobile, tablet, desktop)
- Fast loading (no external CDN dependencies)
- Accessible (WCAG AA compliant, keyboard navigation)
- Print-friendly styles
- Dark mode ready (commented out, easy to enable)

### Content Quality
- Research-backed information with citations
- Code examples with copy buttons
- Step-by-step tutorials
- Real-world use cases
- Troubleshooting guides
- Actionable remediation patterns

### Developer Experience
- Complete API reference with type signatures
- Integration examples (CI/CD, tracking)
- Clear architecture documentation
- Testing guidelines
- Contributing workflow

### SEO & Discoverability
- Jekyll SEO plugin configured
- Sitemap generation
- RSS feed support
- Social media meta tags
- Descriptive page titles

## Next Steps: Deployment

### 1. Update Configuration

Edit `docs/_config.yml`:

```yaml
# Replace with your actual repository
repository: YOUR_USERNAME/agentready
github_username: YOUR_USERNAME

# Update base URL if needed
url: "https://YOUR_USERNAME.github.io"
baseurl: ""  # Set to "/agentready" if not org homepage
```

### 2. Enable GitHub Pages

1. Go to repository Settings ‚Üí Pages
2. Source: Deploy from a branch
3. Branch: `main`
4. Folder: `/docs`
5. Click "Save"

### 3. Wait for Deployment

- Deployment time: 1-3 minutes
- Monitor: Actions tab ‚Üí "pages-build-deployment"
- Visit: `https://YOUR_USERNAME.github.io/agentready`

### 4. Test Locally (Optional)

```bash
cd docs/
bundle install
bundle exec jekyll serve
# Visit http://localhost:4000
```

### 5. Verify

Check all pages load correctly:
- [ ] Homepage (index.md)
- [ ] User Guide (user-guide.md)
- [ ] Developer Guide (developer-guide.md)
- [ ] Attributes (attributes.md)
- [ ] API Reference (api-reference.md)
- [ ] Examples (examples.md)
- [ ] Navigation links work
- [ ] Internal links work
- [ ] Code highlighting works
- [ ] Styles applied correctly

## Customization Options

### Adding Pages

1. Create new `.md` file in `docs/`
2. Add front matter:
   ```yaml
   ---
   layout: page
   title: Page Title
   ---
   ```
3. Add to `_config.yml` navigation

### Styling Changes

Edit `docs/assets/css/style.css`:
- CSS variables at top (colors, fonts, spacing)
- Component styles below
- Mobile styles in media queries

### Theme Switching

To enable dark mode:
1. Uncomment dark mode section in `style.css`
2. Adjust colors as needed
3. Test in browser with dark mode enabled

### Custom Domain

1. Create `docs/CNAME` with your domain
2. Configure DNS records (see DEPLOYMENT.md)
3. Update `_config.yml` URL

## Maintenance

### Updating Content

```bash
# Edit Markdown files
vim docs/user-guide.md

# Test locally (optional)
cd docs && bundle exec jekyll serve

# Commit and push
git add docs/user-guide.md
git commit -m "docs: Update user guide"
git push

# Auto-deploys in ~2 minutes
```

### Dependency Updates

```bash
cd docs/
bundle update
git add Gemfile.lock
git commit -m "chore: Update Jekyll dependencies"
```

## Documentation Highlights

### Comprehensive Coverage
- **Installation**: Multiple methods (pip, source, dev)
- **Usage**: Basic to advanced workflows
- **API**: Complete reference with examples
- **Attributes**: All 25 attributes documented
- **Examples**: Real-world patterns and integrations
- **Troubleshooting**: Common issues and solutions

### Developer-Friendly
- Code examples with syntax highlighting
- Copy-paste ready commands
- Step-by-step tutorials
- Clear architecture explanations
- Testing and quality guidelines

### User-Focused
- Quick start in <5 minutes
- Visual examples and screenshots
- Certification level explanations
- Report interpretation guide
- Remediation patterns

### Production-Ready
- Self-contained (no external dependencies)
- Fast loading (<100KB total)
- Accessible (keyboard navigation, screen readers)
- SEO optimized
- Mobile responsive

## Support Resources

- **Jekyll Docs**: https://jekyllrb.com/docs/
- **GitHub Pages**: https://docs.github.com/en/pages
- **Markdown Guide**: https://www.markdownguide.org/
- **DEPLOYMENT.md**: Complete deployment guide
- **README.md**: Documentation overview

## Quality Metrics

- **Pages**: 6 main documentation pages
- **Content**: ~12,000 lines of documentation
- **Code Examples**: 50+ code blocks with syntax highlighting
- **Sections**: 100+ organized sections with clear headings
- **Links**: Internal navigation fully cross-linked
- **Accessibility**: WCAG AA compliant
- **Performance**: Fast loading, no CDN dependencies
- **SEO**: Optimized with jekyll-seo-tag

## Success Criteria

‚úÖ **Complete**: All 6 main documentation pages created
‚úÖ **Comprehensive**: User guide, dev guide, API reference, attributes
‚úÖ **Professional**: Modern design, clean styling, responsive
‚úÖ **Accessible**: WCAG compliant, keyboard navigation
‚úÖ **Deployable**: Ready for GitHub Pages deployment
‚úÖ **Maintainable**: Clear structure, easy to update
‚úÖ **User-Friendly**: Quick starts, examples, troubleshooting

---

**Ready to deploy!** Follow steps 1-5 above to publish your documentation.

**Estimated time to live**: <10 minutes from enabling GitHub Pages.
````

## File: examples/self-assessment/assessment-20251121-035845.json
````json
{
  "repository": {
    "path": "/Users/jeder/repos/sk/agentready",
    "name": "agentready",
    "url": "git@github.com:ambient-code/agentready.git",
    "branch": "main",
    "commit_hash": "f968aaeb4391bdd31cc815b69d0ce7d5c3d99bce",
    "languages": {
      "YAML": 9,
      "Markdown": 72,
      "Shell": 5,
      "JSON": 3,
      "Python": 36
    },
    "total_files": 150,
    "total_lines": 65059
  },
  "timestamp": "2025-11-21T03:58:45.449559",
  "overall_score": 80.0,
  "certification_level": "Gold",
  "attributes_assessed": 10,
  "attributes_skipped": 15,
  "attributes_total": 25,
  "findings": [
    {
      "attribute": {
        "id": "claude_md_file",
        "name": "CLAUDE.md Configuration Files",
        "category": "Context Window Optimization",
        "tier": 1,
        "description": "Project-specific configuration for Claude Code",
        "criteria": "CLAUDE.md file exists in repository root",
        "default_weight": 0.1
      },
      "status": "pass",
      "score": 100.0,
      "measured_value": "present",
      "threshold": "present",
      "evidence": [
        "CLAUDE.md found at /Users/jeder/repos/sk/agentready/CLAUDE.md"
      ],
      "remediation": null,
      "error_message": null
    },
    {
      "attribute": {
        "id": "readme_structure",
        "name": "README Structure",
        "category": "Documentation Standards",
        "tier": 1,
        "description": "Well-structured README with key sections",
        "criteria": "README.md with installation, usage, and development sections",
        "default_weight": 0.1
      },
      "status": "pass",
      "score": 100.0,
      "measured_value": "3/3 sections",
      "threshold": "3/3 sections",
      "evidence": [
        "Found 3/3 essential sections",
        "Installation: \u2713",
        "Usage: \u2713",
        "Development: \u2713"
      ],
      "remediation": null,
      "error_message": null
    },
    {
      "attribute": {
        "id": "type_annotations",
        "name": "Type Annotations",
        "category": "Code Quality",
        "tier": 1,
        "description": "Type hints in function signatures",
        "criteria": ">80% of functions have type annotations",
        "default_weight": 0.1
      },
      "status": "pass",
      "score": 100.0,
      "measured_value": "96.5%",
      "threshold": "\u226580%",
      "evidence": [
        "Typed functions: 167/173",
        "Coverage: 96.5%"
      ],
      "remediation": null,
      "error_message": null
    },
    {
      "attribute": {
        "id": "standard_layout",
        "name": "Standard Project Layouts",
        "category": "Repository Structure",
        "tier": 1,
        "description": "Follows standard project structure for language",
        "criteria": "Standard directories (src/, tests/, docs/) present",
        "default_weight": 0.1
      },
      "status": "pass",
      "score": 100.0,
      "measured_value": "2/2 directories",
      "threshold": "2/2 directories",
      "evidence": [
        "Found 2/2 standard directories",
        "src/: \u2713",
        "tests/: \u2713"
      ],
      "remediation": null,
      "error_message": null
    },
    {
      "attribute": {
        "id": "lock_files",
        "name": "Lock Files for Reproducibility",
        "category": "Dependency Management",
        "tier": 1,
        "description": "Lock files present for dependency pinning",
        "criteria": "package-lock.json, yarn.lock, poetry.lock, or requirements.txt with versions",
        "default_weight": 0.1
      },
      "status": "fail",
      "score": 0.0,
      "measured_value": "none",
      "threshold": "at least one lock file",
      "evidence": [
        "No lock files found"
      ],
      "remediation": {
        "summary": "Add lock file for dependency reproducibility",
        "steps": [
          "Use npm install, poetry lock, or equivalent to generate lock file"
        ],
        "tools": [],
        "commands": [
          "npm install  # generates package-lock.json"
        ],
        "examples": [],
        "citations": []
      },
      "error_message": null
    },
    {
      "attribute": {
        "id": "test_coverage",
        "name": "Test Coverage Requirements",
        "category": "Testing & CI/CD",
        "tier": 2,
        "description": "Test coverage thresholds configured and enforced",
        "criteria": ">80% code coverage",
        "default_weight": 0.03
      },
      "status": "pass",
      "score": 100.0,
      "measured_value": "configured",
      "threshold": "configured with >80% threshold",
      "evidence": [
        "Coverage configuration found",
        "pytest-cov dependency present"
      ],
      "remediation": null,
      "error_message": null
    },
    {
      "attribute": {
        "id": "precommit_hooks",
        "name": "Pre-commit Hooks & CI/CD Linting",
        "category": "Testing & CI/CD",
        "tier": 2,
        "description": "Pre-commit hooks configured for linting and formatting",
        "criteria": ".pre-commit-config.yaml exists",
        "default_weight": 0.03
      },
      "status": "pass",
      "score": 100.0,
      "measured_value": "configured",
      "threshold": "configured",
      "evidence": [
        ".pre-commit-config.yaml found"
      ],
      "remediation": null,
      "error_message": null
    },
    {
      "attribute": {
        "id": "conventional_commits",
        "name": "Conventional Commit Messages",
        "category": "Git & Version Control",
        "tier": 2,
        "description": "Follows conventional commit format",
        "criteria": "\u226580% of recent commits follow convention",
        "default_weight": 0.03
      },
      "status": "fail",
      "score": 0.0,
      "measured_value": "not configured",
      "threshold": "configured",
      "evidence": [
        "No commitlint or husky configuration"
      ],
      "remediation": {
        "summary": "Configure conventional commits with commitlint",
        "steps": [
          "Install commitlint",
          "Configure husky for commit-msg hook"
        ],
        "tools": [
          "commitlint",
          "husky"
        ],
        "commands": [
          "npm install --save-dev @commitlint/cli @commitlint/config-conventional husky"
        ],
        "examples": [],
        "citations": []
      },
      "error_message": null
    },
    {
      "attribute": {
        "id": "gitignore_completeness",
        "name": ".gitignore Completeness",
        "category": "Git & Version Control",
        "tier": 2,
        "description": "Comprehensive .gitignore file",
        "criteria": ".gitignore exists and covers common patterns",
        "default_weight": 0.03
      },
      "status": "pass",
      "score": 100.0,
      "measured_value": "580 bytes",
      "threshold": ">50 bytes",
      "evidence": [
        ".gitignore found (580 bytes)"
      ],
      "remediation": null,
      "error_message": null
    },
    {
      "attribute": {
        "id": "cyclomatic_complexity",
        "name": "Cyclomatic Complexity Thresholds",
        "category": "Code Quality",
        "tier": 3,
        "description": "Cyclomatic complexity thresholds enforced",
        "criteria": "Average complexity <10, no functions >15",
        "default_weight": 0.03
      },
      "status": "pass",
      "score": 100.0,
      "measured_value": "3.1",
      "threshold": "<10.0",
      "evidence": [
        "Average cyclomatic complexity: 3.1"
      ],
      "remediation": null,
      "error_message": null
    },
    {
      "attribute": {
        "id": "one_command_setup",
        "name": "One-Command Build/Setup",
        "category": "Build & Development",
        "tier": 2,
        "description": "Assessment for One-Command Build/Setup",
        "criteria": "To be implemented",
        "default_weight": 0.03
      },
      "status": "not_applicable",
      "score": null,
      "measured_value": null,
      "threshold": null,
      "evidence": [
        "One-Command Build/Setup assessment not yet implemented"
      ],
      "remediation": null,
      "error_message": null
    },
    {
      "attribute": {
        "id": "concise_documentation",
        "name": "Concise Structured Documentation",
        "category": "Context Window Optimization",
        "tier": 2,
        "description": "Assessment for Concise Structured Documentation",
        "criteria": "To be implemented",
        "default_weight": 0.03
      },
      "status": "not_applicable",
      "score": null,
      "measured_value": null,
      "threshold": null,
      "evidence": [
        "Concise Structured Documentation assessment not yet implemented"
      ],
      "remediation": null,
      "error_message": null
    },
    {
      "attribute": {
        "id": "inline_documentation",
        "name": "Inline Documentation",
        "category": "Documentation Standards",
        "tier": 2,
        "description": "Assessment for Inline Documentation",
        "criteria": "To be implemented",
        "default_weight": 0.03
      },
      "status": "not_applicable",
      "score": null,
      "measured_value": null,
      "threshold": null,
      "evidence": [
        "Inline Documentation assessment not yet implemented"
      ],
      "remediation": null,
      "error_message": null
    },
    {
      "attribute": {
        "id": "file_size_limits",
        "name": "File Size Limits",
        "category": "Context Window Optimization",
        "tier": 2,
        "description": "Assessment for File Size Limits",
        "criteria": "To be implemented",
        "default_weight": 0.03
      },
      "status": "not_applicable",
      "score": null,
      "measured_value": null,
      "threshold": null,
      "evidence": [
        "File Size Limits assessment not yet implemented"
      ],
      "remediation": null,
      "error_message": null
    },
    {
      "attribute": {
        "id": "dependency_freshness",
        "name": "Dependency Freshness & Security",
        "category": "Dependency Management",
        "tier": 2,
        "description": "Assessment for Dependency Freshness & Security",
        "criteria": "To be implemented",
        "default_weight": 0.03
      },
      "status": "not_applicable",
      "score": null,
      "measured_value": null,
      "threshold": null,
      "evidence": [
        "Dependency Freshness & Security assessment not yet implemented"
      ],
      "remediation": null,
      "error_message": null
    },
    {
      "attribute": {
        "id": "separation_concerns",
        "name": "Separation of Concerns",
        "category": "Repository Structure",
        "tier": 2,
        "description": "Assessment for Separation of Concerns",
        "criteria": "To be implemented",
        "default_weight": 0.03
      },
      "status": "not_applicable",
      "score": null,
      "measured_value": null,
      "threshold": null,
      "evidence": [
        "Separation of Concerns assessment not yet implemented"
      ],
      "remediation": null,
      "error_message": null
    },
    {
      "attribute": {
        "id": "structured_logging",
        "name": "Structured Logging",
        "category": "Error Handling",
        "tier": 3,
        "description": "Assessment for Structured Logging",
        "criteria": "To be implemented",
        "default_weight": 0.03
      },
      "status": "not_applicable",
      "score": null,
      "measured_value": null,
      "threshold": null,
      "evidence": [
        "Structured Logging assessment not yet implemented"
      ],
      "remediation": null,
      "error_message": null
    },
    {
      "attribute": {
        "id": "openapi_specs",
        "name": "OpenAPI/Swagger Specifications",
        "category": "API Documentation",
        "tier": 3,
        "description": "Assessment for OpenAPI/Swagger Specifications",
        "criteria": "To be implemented",
        "default_weight": 0.03
      },
      "status": "not_applicable",
      "score": null,
      "measured_value": null,
      "threshold": null,
      "evidence": [
        "OpenAPI/Swagger Specifications assessment not yet implemented"
      ],
      "remediation": null,
      "error_message": null
    },
    {
      "attribute": {
        "id": "architecture_decisions",
        "name": "Architecture Decision Records",
        "category": "Documentation Standards",
        "tier": 3,
        "description": "Assessment for Architecture Decision Records",
        "criteria": "To be implemented",
        "default_weight": 0.03
      },
      "status": "not_applicable",
      "score": null,
      "measured_value": null,
      "threshold": null,
      "evidence": [
        "Architecture Decision Records assessment not yet implemented"
      ],
      "remediation": null,
      "error_message": null
    },
    {
      "attribute": {
        "id": "semantic_naming",
        "name": "Semantic File & Directory Naming",
        "category": "Modularity",
        "tier": 3,
        "description": "Assessment for Semantic File & Directory Naming",
        "criteria": "To be implemented",
        "default_weight": 0.03
      },
      "status": "not_applicable",
      "score": null,
      "measured_value": null,
      "threshold": null,
      "evidence": [
        "Semantic File & Directory Naming assessment not yet implemented"
      ],
      "remediation": null,
      "error_message": null
    },
    {
      "attribute": {
        "id": "security_scanning",
        "name": "Security Scanning Automation",
        "category": "Security",
        "tier": 4,
        "description": "Assessment for Security Scanning Automation",
        "criteria": "To be implemented",
        "default_weight": 0.01
      },
      "status": "not_applicable",
      "score": null,
      "measured_value": null,
      "threshold": null,
      "evidence": [
        "Security Scanning Automation assessment not yet implemented"
      ],
      "remediation": null,
      "error_message": null
    },
    {
      "attribute": {
        "id": "performance_benchmarks",
        "name": "Performance Benchmarks",
        "category": "Performance",
        "tier": 4,
        "description": "Assessment for Performance Benchmarks",
        "criteria": "To be implemented",
        "default_weight": 0.01
      },
      "status": "not_applicable",
      "score": null,
      "measured_value": null,
      "threshold": null,
      "evidence": [
        "Performance Benchmarks assessment not yet implemented"
      ],
      "remediation": null,
      "error_message": null
    },
    {
      "attribute": {
        "id": "code_smells",
        "name": "Code Smell Elimination",
        "category": "Code Quality",
        "tier": 4,
        "description": "Assessment for Code Smell Elimination",
        "criteria": "To be implemented",
        "default_weight": 0.01
      },
      "status": "not_applicable",
      "score": null,
      "measured_value": null,
      "threshold": null,
      "evidence": [
        "Code Smell Elimination assessment not yet implemented"
      ],
      "remediation": null,
      "error_message": null
    },
    {
      "attribute": {
        "id": "issue_pr_templates",
        "name": "Issue & Pull Request Templates",
        "category": "Git & Version Control",
        "tier": 4,
        "description": "Assessment for Issue & Pull Request Templates",
        "criteria": "To be implemented",
        "default_weight": 0.01
      },
      "status": "not_applicable",
      "score": null,
      "measured_value": null,
      "threshold": null,
      "evidence": [
        "Issue & Pull Request Templates assessment not yet implemented"
      ],
      "remediation": null,
      "error_message": null
    },
    {
      "attribute": {
        "id": "container_setup",
        "name": "Container/Virtualization Setup",
        "category": "Build & Development",
        "tier": 4,
        "description": "Assessment for Container/Virtualization Setup",
        "criteria": "To be implemented",
        "default_weight": 0.01
      },
      "status": "not_applicable",
      "score": null,
      "measured_value": null,
      "threshold": null,
      "evidence": [
        "Container/Virtualization Setup assessment not yet implemented"
      ],
      "remediation": null,
      "error_message": null
    }
  ],
  "config": null,
  "duration_seconds": 0.2
}
````

## File: examples/self-assessment/assessment-20251121.json
````json
{
  "repository": {
    "path": "/Users/jeder/repos/sk/agentready",
    "name": "agentready",
    "url": null,
    "branch": "001-agentready-scorer",
    "commit_hash": "adfc4c823a7b25d5b1404c9b994461e290257dce",
    "languages": {
      "Markdown": 36,
      "Shell": 5,
      "Python": 31
    },
    "total_files": 79,
    "total_lines": 13852
  },
  "timestamp": "2025-11-21T02:11:05.766923",
  "overall_score": 75.4,
  "certification_level": "Gold",
  "attributes_assessed": 10,
  "attributes_skipped": 15,
  "attributes_total": 25,
  "findings": [
    {
      "attribute": {
        "id": "claude_md_file",
        "name": "CLAUDE.md Configuration Files",
        "category": "Context Window Optimization",
        "tier": 1,
        "description": "Project-specific configuration for Claude Code",
        "criteria": "CLAUDE.md file exists in repository root",
        "default_weight": 0.1
      },
      "status": "pass",
      "score": 100.0,
      "measured_value": "present",
      "threshold": "present",
      "evidence": [
        "CLAUDE.md found at /Users/jeder/repos/sk/agentready/CLAUDE.md"
      ],
      "remediation": null,
      "error_message": null
    },
    {
      "attribute": {
        "id": "readme_structure",
        "name": "README Structure",
        "category": "Documentation Standards",
        "tier": 1,
        "description": "Well-structured README with key sections",
        "criteria": "README.md with installation, usage, and development sections",
        "default_weight": 0.1
      },
      "status": "pass",
      "score": 100.0,
      "measured_value": "3/3 sections",
      "threshold": "3/3 sections",
      "evidence": [
        "Found 3/3 essential sections",
        "Installation: \u2713",
        "Usage: \u2713",
        "Development: \u2713"
      ],
      "remediation": null,
      "error_message": null
    },
    {
      "attribute": {
        "id": "type_annotations",
        "name": "Type Annotations",
        "category": "Code Quality",
        "tier": 1,
        "description": "Type hints in function signatures",
        "criteria": ">80% of functions have type annotations",
        "default_weight": 0.1
      },
      "status": "pass",
      "score": 100.0,
      "measured_value": "95.9%",
      "threshold": "\u226580%",
      "evidence": [
        "Typed functions: 142/148",
        "Coverage: 95.9%"
      ],
      "remediation": null,
      "error_message": null
    },
    {
      "attribute": {
        "id": "standard_layout",
        "name": "Standard Project Layouts",
        "category": "Repository Structure",
        "tier": 1,
        "description": "Follows standard project structure for language",
        "criteria": "Standard directories (src/, tests/, docs/) present",
        "default_weight": 0.1
      },
      "status": "pass",
      "score": 100.0,
      "measured_value": "2/2 directories",
      "threshold": "2/2 directories",
      "evidence": [
        "Found 2/2 standard directories",
        "src/: \u2713",
        "tests/: \u2713"
      ],
      "remediation": null,
      "error_message": null
    },
    {
      "attribute": {
        "id": "lock_files",
        "name": "Lock Files for Reproducibility",
        "category": "Dependency Management",
        "tier": 1,
        "description": "Lock files present for dependency pinning",
        "criteria": "package-lock.json, yarn.lock, poetry.lock, or requirements.txt with versions",
        "default_weight": 0.1
      },
      "status": "fail",
      "score": 0.0,
      "measured_value": "none",
      "threshold": "at least one lock file",
      "evidence": [
        "No lock files found"
      ],
      "remediation": {
        "summary": "Add lock file for dependency reproducibility",
        "steps": [
          "Use npm install, poetry lock, or equivalent to generate lock file"
        ],
        "tools": [],
        "commands": [
          "npm install  # generates package-lock.json"
        ],
        "examples": [],
        "citations": []
      },
      "error_message": null
    },
    {
      "attribute": {
        "id": "test_coverage",
        "name": "Test Coverage Requirements",
        "category": "Testing & CI/CD",
        "tier": 2,
        "description": "Test coverage thresholds configured and enforced",
        "criteria": ">80% code coverage",
        "default_weight": 0.03
      },
      "status": "pass",
      "score": 100.0,
      "measured_value": "configured",
      "threshold": "configured with >80% threshold",
      "evidence": [
        "Coverage configuration found",
        "pytest-cov dependency present"
      ],
      "remediation": null,
      "error_message": null
    },
    {
      "attribute": {
        "id": "precommit_hooks",
        "name": "Pre-commit Hooks & CI/CD Linting",
        "category": "Testing & CI/CD",
        "tier": 2,
        "description": "Pre-commit hooks configured for linting and formatting",
        "criteria": ".pre-commit-config.yaml exists",
        "default_weight": 0.03
      },
      "status": "fail",
      "score": 0.0,
      "measured_value": "not configured",
      "threshold": "configured",
      "evidence": [
        ".pre-commit-config.yaml not found"
      ],
      "remediation": {
        "summary": "Configure pre-commit hooks for automated code quality checks",
        "steps": [
          "Install pre-commit framework",
          "Create .pre-commit-config.yaml",
          "Add hooks for linting and formatting",
          "Install hooks: pre-commit install",
          "Run on all files: pre-commit run --all-files"
        ],
        "tools": [
          "pre-commit"
        ],
        "commands": [
          "pip install pre-commit",
          "pre-commit install",
          "pre-commit run --all-files"
        ],
        "examples": [
          "# .pre-commit-config.yaml\nrepos:\n  - repo: https://github.com/pre-commit/pre-commit-hooks\n    rev: v4.4.0\n    hooks:\n      - id: trailing-whitespace\n      - id: end-of-file-fixer\n      - id: check-yaml\n      - id: check-added-large-files\n\n  - repo: https://github.com/psf/black\n    rev: 23.3.0\n    hooks:\n      - id: black\n\n  - repo: https://github.com/pycqa/isort\n    rev: 5.12.0\n    hooks:\n      - id: isort\n"
        ],
        "citations": [
          {
            "source": "pre-commit.com",
            "title": "Pre-commit Framework",
            "url": "https://pre-commit.com/",
            "relevance": "Official pre-commit documentation"
          }
        ]
      },
      "error_message": null
    },
    {
      "attribute": {
        "id": "conventional_commits",
        "name": "Conventional Commit Messages",
        "category": "Git & Version Control",
        "tier": 2,
        "description": "Follows conventional commit format",
        "criteria": "\u226580% of recent commits follow convention",
        "default_weight": 0.03
      },
      "status": "fail",
      "score": 0.0,
      "measured_value": "not configured",
      "threshold": "configured",
      "evidence": [
        "No commitlint or husky configuration"
      ],
      "remediation": {
        "summary": "Configure conventional commits with commitlint",
        "steps": [
          "Install commitlint",
          "Configure husky for commit-msg hook"
        ],
        "tools": [
          "commitlint",
          "husky"
        ],
        "commands": [
          "npm install --save-dev @commitlint/cli @commitlint/config-conventional husky"
        ],
        "examples": [],
        "citations": []
      },
      "error_message": null
    },
    {
      "attribute": {
        "id": "gitignore_completeness",
        "name": ".gitignore Completeness",
        "category": "Git & Version Control",
        "tier": 2,
        "description": "Comprehensive .gitignore file",
        "criteria": ".gitignore exists and covers common patterns",
        "default_weight": 0.03
      },
      "status": "pass",
      "score": 100.0,
      "measured_value": "580 bytes",
      "threshold": ">50 bytes",
      "evidence": [
        ".gitignore found (580 bytes)"
      ],
      "remediation": null,
      "error_message": null
    },
    {
      "attribute": {
        "id": "cyclomatic_complexity",
        "name": "Cyclomatic Complexity Thresholds",
        "category": "Code Quality",
        "tier": 3,
        "description": "Cyclomatic complexity thresholds enforced",
        "criteria": "Average complexity <10, no functions >15",
        "default_weight": 0.03
      },
      "status": "pass",
      "score": 100.0,
      "measured_value": "3.1",
      "threshold": "<10.0",
      "evidence": [
        "Average cyclomatic complexity: 3.1"
      ],
      "remediation": null,
      "error_message": null
    },
    {
      "attribute": {
        "id": "one_command_setup",
        "name": "One-Command Build/Setup",
        "category": "Build & Development",
        "tier": 2,
        "description": "Assessment for One-Command Build/Setup",
        "criteria": "To be implemented",
        "default_weight": 0.03
      },
      "status": "not_applicable",
      "score": null,
      "measured_value": null,
      "threshold": null,
      "evidence": [
        "One-Command Build/Setup assessment not yet implemented"
      ],
      "remediation": null,
      "error_message": null
    },
    {
      "attribute": {
        "id": "concise_documentation",
        "name": "Concise Structured Documentation",
        "category": "Context Window Optimization",
        "tier": 2,
        "description": "Assessment for Concise Structured Documentation",
        "criteria": "To be implemented",
        "default_weight": 0.03
      },
      "status": "not_applicable",
      "score": null,
      "measured_value": null,
      "threshold": null,
      "evidence": [
        "Concise Structured Documentation assessment not yet implemented"
      ],
      "remediation": null,
      "error_message": null
    },
    {
      "attribute": {
        "id": "inline_documentation",
        "name": "Inline Documentation",
        "category": "Documentation Standards",
        "tier": 2,
        "description": "Assessment for Inline Documentation",
        "criteria": "To be implemented",
        "default_weight": 0.03
      },
      "status": "not_applicable",
      "score": null,
      "measured_value": null,
      "threshold": null,
      "evidence": [
        "Inline Documentation assessment not yet implemented"
      ],
      "remediation": null,
      "error_message": null
    },
    {
      "attribute": {
        "id": "file_size_limits",
        "name": "File Size Limits",
        "category": "Context Window Optimization",
        "tier": 2,
        "description": "Assessment for File Size Limits",
        "criteria": "To be implemented",
        "default_weight": 0.03
      },
      "status": "not_applicable",
      "score": null,
      "measured_value": null,
      "threshold": null,
      "evidence": [
        "File Size Limits assessment not yet implemented"
      ],
      "remediation": null,
      "error_message": null
    },
    {
      "attribute": {
        "id": "dependency_freshness",
        "name": "Dependency Freshness & Security",
        "category": "Dependency Management",
        "tier": 2,
        "description": "Assessment for Dependency Freshness & Security",
        "criteria": "To be implemented",
        "default_weight": 0.03
      },
      "status": "not_applicable",
      "score": null,
      "measured_value": null,
      "threshold": null,
      "evidence": [
        "Dependency Freshness & Security assessment not yet implemented"
      ],
      "remediation": null,
      "error_message": null
    },
    {
      "attribute": {
        "id": "separation_concerns",
        "name": "Separation of Concerns",
        "category": "Repository Structure",
        "tier": 2,
        "description": "Assessment for Separation of Concerns",
        "criteria": "To be implemented",
        "default_weight": 0.03
      },
      "status": "not_applicable",
      "score": null,
      "measured_value": null,
      "threshold": null,
      "evidence": [
        "Separation of Concerns assessment not yet implemented"
      ],
      "remediation": null,
      "error_message": null
    },
    {
      "attribute": {
        "id": "structured_logging",
        "name": "Structured Logging",
        "category": "Error Handling",
        "tier": 3,
        "description": "Assessment for Structured Logging",
        "criteria": "To be implemented",
        "default_weight": 0.03
      },
      "status": "not_applicable",
      "score": null,
      "measured_value": null,
      "threshold": null,
      "evidence": [
        "Structured Logging assessment not yet implemented"
      ],
      "remediation": null,
      "error_message": null
    },
    {
      "attribute": {
        "id": "openapi_specs",
        "name": "OpenAPI/Swagger Specifications",
        "category": "API Documentation",
        "tier": 3,
        "description": "Assessment for OpenAPI/Swagger Specifications",
        "criteria": "To be implemented",
        "default_weight": 0.03
      },
      "status": "not_applicable",
      "score": null,
      "measured_value": null,
      "threshold": null,
      "evidence": [
        "OpenAPI/Swagger Specifications assessment not yet implemented"
      ],
      "remediation": null,
      "error_message": null
    },
    {
      "attribute": {
        "id": "architecture_decisions",
        "name": "Architecture Decision Records",
        "category": "Documentation Standards",
        "tier": 3,
        "description": "Assessment for Architecture Decision Records",
        "criteria": "To be implemented",
        "default_weight": 0.03
      },
      "status": "not_applicable",
      "score": null,
      "measured_value": null,
      "threshold": null,
      "evidence": [
        "Architecture Decision Records assessment not yet implemented"
      ],
      "remediation": null,
      "error_message": null
    },
    {
      "attribute": {
        "id": "semantic_naming",
        "name": "Semantic File & Directory Naming",
        "category": "Modularity",
        "tier": 3,
        "description": "Assessment for Semantic File & Directory Naming",
        "criteria": "To be implemented",
        "default_weight": 0.03
      },
      "status": "not_applicable",
      "score": null,
      "measured_value": null,
      "threshold": null,
      "evidence": [
        "Semantic File & Directory Naming assessment not yet implemented"
      ],
      "remediation": null,
      "error_message": null
    },
    {
      "attribute": {
        "id": "security_scanning",
        "name": "Security Scanning Automation",
        "category": "Security",
        "tier": 4,
        "description": "Assessment for Security Scanning Automation",
        "criteria": "To be implemented",
        "default_weight": 0.01
      },
      "status": "not_applicable",
      "score": null,
      "measured_value": null,
      "threshold": null,
      "evidence": [
        "Security Scanning Automation assessment not yet implemented"
      ],
      "remediation": null,
      "error_message": null
    },
    {
      "attribute": {
        "id": "performance_benchmarks",
        "name": "Performance Benchmarks",
        "category": "Performance",
        "tier": 4,
        "description": "Assessment for Performance Benchmarks",
        "criteria": "To be implemented",
        "default_weight": 0.01
      },
      "status": "not_applicable",
      "score": null,
      "measured_value": null,
      "threshold": null,
      "evidence": [
        "Performance Benchmarks assessment not yet implemented"
      ],
      "remediation": null,
      "error_message": null
    },
    {
      "attribute": {
        "id": "code_smells",
        "name": "Code Smell Elimination",
        "category": "Code Quality",
        "tier": 4,
        "description": "Assessment for Code Smell Elimination",
        "criteria": "To be implemented",
        "default_weight": 0.01
      },
      "status": "not_applicable",
      "score": null,
      "measured_value": null,
      "threshold": null,
      "evidence": [
        "Code Smell Elimination assessment not yet implemented"
      ],
      "remediation": null,
      "error_message": null
    },
    {
      "attribute": {
        "id": "issue_pr_templates",
        "name": "Issue & Pull Request Templates",
        "category": "Git & Version Control",
        "tier": 4,
        "description": "Assessment for Issue & Pull Request Templates",
        "criteria": "To be implemented",
        "default_weight": 0.01
      },
      "status": "not_applicable",
      "score": null,
      "measured_value": null,
      "threshold": null,
      "evidence": [
        "Issue & Pull Request Templates assessment not yet implemented"
      ],
      "remediation": null,
      "error_message": null
    },
    {
      "attribute": {
        "id": "container_setup",
        "name": "Container/Virtualization Setup",
        "category": "Build & Development",
        "tier": 4,
        "description": "Assessment for Container/Virtualization Setup",
        "criteria": "To be implemented",
        "default_weight": 0.01
      },
      "status": "not_applicable",
      "score": null,
      "measured_value": null,
      "threshold": null,
      "evidence": [
        "Container/Virtualization Setup assessment not yet implemented"
      ],
      "remediation": null,
      "error_message": null
    }
  ],
  "config": null,
  "duration_seconds": 0.2
}
````

## File: examples/self-assessment/report-20251121-035845.html
````html
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>AgentReady Assessment - agentready</title>
    <style>
        /* Reset and Base Styles */
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif;
            line-height: 1.6;
            color: #333;
            background: #f5f5f5;
            padding: 20px;
        }

        .container {
            max-width: 1200px;
            margin: 0 auto;
            background: white;
            padding: 40px;
            border-radius: 8px;
            box-shadow: 0 2px 8px rgba(0,0,0,0.1);
        }

        /* Header */
        header {
            border-bottom: 3px solid #2563eb;
            padding-bottom: 20px;
            margin-bottom: 30px;
        }

        h1 {
            font-size: 2rem;
            color: #1e293b;
            margin-bottom: 10px;
        }

        .subtitle {
            color: #64748b;
            font-size: 1rem;
        }

        /* Summary Cards */
        .summary-grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(200px, 1fr));
            gap: 20px;
            margin: 30px 0;
        }

        .summary-card {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 20px;
            border-radius: 8px;
            text-align: center;
        }

        .summary-card.score {
            background: linear-gradient(135deg, #f093fb 0%, #f5576c 100%);
        }

        .summary-card.certification {
            background: linear-gradient(135deg, #4facfe 0%, #00f2fe 100%);
        }

        .summary-card.assessed {
            background: linear-gradient(135deg, #43e97b 0%, #38f9d7 100%);
        }

        .summary-card.duration {
            background: linear-gradient(135deg, #fa709a 0%, #fee140 100%);
        }

        .summary-card h3 {
            font-size: 0.9rem;
            font-weight: 500;
            opacity: 0.9;
            margin-bottom: 10px;
        }

        .summary-card .value {
            font-size: 2rem;
            font-weight: 700;
        }

        /* Controls */
        .controls {
            display: flex;
            flex-wrap: wrap;
            gap: 15px;
            margin: 30px 0;
            padding: 20px;
            background: #f8fafc;
            border-radius: 8px;
        }

        .control-group {
            display: flex;
            align-items: center;
            gap: 10px;
        }

        .control-group label {
            font-weight: 600;
            color: #475569;
            font-size: 0.9rem;
        }

        select, input[type="search"] {
            padding: 8px 12px;
            border: 1px solid #cbd5e1;
            border-radius: 6px;
            font-size: 0.9rem;
            background: white;
        }

        input[type="search"] {
            min-width: 250px;
        }

        .filter-btn {
            padding: 8px 16px;
            border: 2px solid #e2e8f0;
            background: white;
            border-radius: 6px;
            cursor: pointer;
            font-size: 0.9rem;
            transition: all 0.2s;
        }

        .filter-btn:hover {
            border-color: #2563eb;
            background: #eff6ff;
        }

        .filter-btn.active {
            background: #2563eb;
            color: white;
            border-color: #2563eb;
        }

        .badge {
            display: inline-block;
            padding: 2px 8px;
            border-radius: 12px;
            font-size: 0.75rem;
            font-weight: 600;
            margin-left: 5px;
        }

        /* Certification Ladder */
        .cert-ladder {
            display: flex;
            justify-content: space-between;
            margin: 30px 0;
            padding: 20px;
            background: #f8fafc;
            border-radius: 8px;
        }

        .cert-level {
            flex: 1;
            text-align: center;
            padding: 15px 10px;
            border-radius: 6px;
            margin: 0 5px;
            transition: all 0.3s;
        }

        .cert-level.platinum { background: linear-gradient(135deg, #e0e7ff, #c7d2fe); }
        .cert-level.gold { background: linear-gradient(135deg, #fef3c7, #fde68a); }
        .cert-level.silver { background: linear-gradient(135deg, #e5e7eb, #d1d5db); }
        .cert-level.bronze { background: linear-gradient(135deg, #fed7aa, #fdba74); }
        .cert-level.needs-improvement { background: linear-gradient(135deg, #fecaca, #fca5a5); }

        .cert-level.active {
            transform: scale(1.1);
            box-shadow: 0 4px 12px rgba(0,0,0,0.15);
            border: 3px solid #2563eb;
        }

        .cert-level h4 {
            font-size: 0.9rem;
            margin-bottom: 5px;
        }

        .cert-level .range {
            font-size: 0.8rem;
            opacity: 0.7;
        }

        /* Findings */
        .findings {
            margin: 30px 0;
        }

        .finding {
            border: 1px solid #e2e8f0;
            border-radius: 8px;
            margin-bottom: 15px;
            overflow: hidden;
            transition: all 0.2s;
        }

        .finding:hover {
            box-shadow: 0 4px 12px rgba(0,0,0,0.08);
        }

        .finding-header {
            padding: 15px 20px;
            background: #f8fafc;
            cursor: pointer;
            display: flex;
            justify-content: space-between;
            align-items: center;
        }

        .finding-header:hover {
            background: #f1f5f9;
        }

        .finding-title {
            display: flex;
            align-items: center;
            gap: 15px;
            flex: 1;
        }

        .status-icon {
            font-size: 1.5rem;
        }

        .finding-info h3 {
            font-size: 1rem;
            color: #1e293b;
            margin-bottom: 3px;
        }

        .finding-meta {
            font-size: 0.85rem;
            color: #64748b;
        }

        .tier-badge {
            padding: 4px 12px;
            border-radius: 12px;
            font-size: 0.75rem;
            font-weight: 600;
        }

        .tier-1 { background: #fee2e2; color: #991b1b; }
        .tier-2 { background: #fed7aa; color: #9a3412; }
        .tier-3 { background: #fef3c7; color: #92400e; }
        .tier-4 { background: #d1fae5; color: #065f46; }

        .score-display {
            font-size: 1.5rem;
            font-weight: 700;
            min-width: 80px;
            text-align: right;
        }

        .score-pass { color: #059669; }
        .score-fail { color: #dc2626; }
        .score-skip { color: #6b7280; }

        .finding-body {
            padding: 20px;
            display: none;
        }

        .finding.expanded .finding-body {
            display: block;
        }

        .finding-section {
            margin-bottom: 20px;
        }

        .finding-section h4 {
            font-size: 0.9rem;
            color: #475569;
            margin-bottom: 10px;
            text-transform: uppercase;
            letter-spacing: 0.05em;
        }

        .evidence-list, .remediation-steps {
            list-style: none;
            padding-left: 0;
        }

        .evidence-list li {
            padding: 8px 0;
            border-bottom: 1px solid #f1f5f9;
        }

        .evidence-list li:before {
            content: "‚ñ∏ ";
            color: #2563eb;
            font-weight: 700;
            margin-right: 8px;
        }

        .remediation-steps li {
            padding: 10px 0 10px 30px;
            position: relative;
        }

        .remediation-steps li:before {
            content: counter(step);
            counter-increment: step;
            position: absolute;
            left: 0;
            top: 10px;
            background: #2563eb;
            color: white;
            width: 20px;
            height: 20px;
            border-radius: 50%;
            display: flex;
            align-items: center;
            justify-content: center;
            font-size: 0.75rem;
            font-weight: 700;
        }

        .remediation-steps {
            counter-reset: step;
        }

        code {
            background: #f1f5f9;
            padding: 2px 6px;
            border-radius: 3px;
            font-family: 'Courier New', monospace;
            font-size: 0.9em;
        }

        pre {
            background: #1e293b;
            color: #e2e8f0;
            padding: 15px;
            border-radius: 6px;
            overflow-x: auto;
            margin: 10px 0;
        }

        pre code {
            background: none;
            color: inherit;
            padding: 0;
        }

        /* Footer */
        footer {
            margin-top: 50px;
            padding-top: 20px;
            border-top: 1px solid #e2e8f0;
            text-align: center;
            color: #64748b;
            font-size: 0.9rem;
        }

        /* Responsive */
        @media (max-width: 768px) {
            .container {
                padding: 20px;
            }

            .summary-grid {
                grid-template-columns: 1fr;
            }

            .controls {
                flex-direction: column;
            }

            input[type="search"] {
                min-width: 100%;
            }

            .cert-ladder {
                flex-direction: column;
            }

            .cert-level {
                margin: 5px 0;
            }
        }

        /* Hidden class */
        .hidden {
            display: none !important;
        }
    </style>
</head>
<body>
    <div class="container">
        <header>
            <h1>ü§ñ AgentReady Assessment Report</h1>
            <p class="subtitle">agentready ‚Ä¢ November 21, 2025 at 03:58</p>
        </header>

        <!-- Summary Cards -->
        <div class="summary-grid">
            <div class="summary-card score">
                <h3>Overall Score</h3>
                <div class="value">80.0</div>
            </div>
            <div class="summary-card certification">
                <h3>Certification</h3>
                <div class="value">Gold</div>
            </div>
            <div class="summary-card assessed">
                <h3>Assessed</h3>
                <div class="value">10/25</div>
            </div>
            <div class="summary-card duration">
                <h3>Duration</h3>
                <div class="value">0.2s</div>
            </div>
        </div>

        <!-- Certification Ladder -->
        <div class="cert-ladder">
            <div class="cert-level platinum ">
                <h4>üíé Platinum</h4>
                <p class="range">90-100</p>
            </div>
            <div class="cert-level gold active">
                <h4>ü•á Gold</h4>
                <p class="range">75-89</p>
            </div>
            <div class="cert-level silver ">
                <h4>ü•à Silver</h4>
                <p class="range">60-74</p>
            </div>
            <div class="cert-level bronze ">
                <h4>ü•â Bronze</h4>
                <p class="range">40-59</p>
            </div>
            <div class="cert-level needs-improvement ">
                <h4>‚ö†Ô∏è Needs Work</h4>
                <p class="range">0-39</p>
            </div>
        </div>

        <!-- Controls -->
        <div class="controls">
            <div class="control-group">
                <label>Filter:</label>
                <button class="filter-btn active" data-filter="all">All <span class="badge">25</span></button>
                <button class="filter-btn" data-filter="pass">Pass <span class="badge">8</span></button>
                <button class="filter-btn" data-filter="fail">Fail <span class="badge">2</span></button>
                <button class="filter-btn" data-filter="skipped">Skipped <span class="badge">15</span></button>
            </div>

            <div class="control-group">
                <label>Sort:</label>
                <select id="sort-select">
                    <option value="category">Category</option>
                    <option value="score-desc">Score (High to Low)</option>
                    <option value="score-asc">Score (Low to High)</option>
                    <option value="tier">Tier (Essential First)</option>
                </select>
            </div>

            <div class="control-group">
                <input type="search" id="search-box" placeholder="Search attributes...">
            </div>
        </div>

        <!-- Findings -->
        <div class="findings" id="findings-container">
            
            <div class="finding"
                 data-status="pass"
                 data-tier="1"
                 data-category="Context Window Optimization"
                 data-score="100.0"
                 data-name="claude.md configuration files">

                <div class="finding-header" onclick="toggleFinding(this)">
                    <div class="finding-title">
                        <span class="status-icon">
                            ‚úÖ
                            
                        </span>
                        <div class="finding-info">
                            <h3>CLAUDE.md Configuration Files</h3>
                            <div class="finding-meta">
                                Context Window Optimization ‚Ä¢
                                <span class="tier-badge tier-1">Tier 1</span>
                                 ‚Ä¢ present
                            </div>
                        </div>
                    </div>
                    
                    <div class="score-display score-pass">
                        100
                    </div>
                    
                </div>

                <div class="finding-body">
                    
                    <div class="finding-section">
                        <h4>Evidence</h4>
                        <ul class="evidence-list">
                            
                            <li>CLAUDE.md found at /Users/jeder/repos/sk/agentready/CLAUDE.md</li>
                            
                        </ul>
                    </div>
                    

                    

                    
                </div>
            </div>
            
            <div class="finding"
                 data-status="pass"
                 data-tier="1"
                 data-category="Documentation Standards"
                 data-score="100.0"
                 data-name="readme structure">

                <div class="finding-header" onclick="toggleFinding(this)">
                    <div class="finding-title">
                        <span class="status-icon">
                            ‚úÖ
                            
                        </span>
                        <div class="finding-info">
                            <h3>README Structure</h3>
                            <div class="finding-meta">
                                Documentation Standards ‚Ä¢
                                <span class="tier-badge tier-1">Tier 1</span>
                                 ‚Ä¢ 3/3 sections
                            </div>
                        </div>
                    </div>
                    
                    <div class="score-display score-pass">
                        100
                    </div>
                    
                </div>

                <div class="finding-body">
                    
                    <div class="finding-section">
                        <h4>Evidence</h4>
                        <ul class="evidence-list">
                            
                            <li>Found 3/3 essential sections</li>
                            
                            <li>Installation: ‚úì</li>
                            
                            <li>Usage: ‚úì</li>
                            
                            <li>Development: ‚úì</li>
                            
                        </ul>
                    </div>
                    

                    

                    
                </div>
            </div>
            
            <div class="finding"
                 data-status="pass"
                 data-tier="1"
                 data-category="Code Quality"
                 data-score="100.0"
                 data-name="type annotations">

                <div class="finding-header" onclick="toggleFinding(this)">
                    <div class="finding-title">
                        <span class="status-icon">
                            ‚úÖ
                            
                        </span>
                        <div class="finding-info">
                            <h3>Type Annotations</h3>
                            <div class="finding-meta">
                                Code Quality ‚Ä¢
                                <span class="tier-badge tier-1">Tier 1</span>
                                 ‚Ä¢ 96.5%
                            </div>
                        </div>
                    </div>
                    
                    <div class="score-display score-pass">
                        100
                    </div>
                    
                </div>

                <div class="finding-body">
                    
                    <div class="finding-section">
                        <h4>Evidence</h4>
                        <ul class="evidence-list">
                            
                            <li>Typed functions: 167/173</li>
                            
                            <li>Coverage: 96.5%</li>
                            
                        </ul>
                    </div>
                    

                    

                    
                </div>
            </div>
            
            <div class="finding"
                 data-status="pass"
                 data-tier="1"
                 data-category="Repository Structure"
                 data-score="100.0"
                 data-name="standard project layouts">

                <div class="finding-header" onclick="toggleFinding(this)">
                    <div class="finding-title">
                        <span class="status-icon">
                            ‚úÖ
                            
                        </span>
                        <div class="finding-info">
                            <h3>Standard Project Layouts</h3>
                            <div class="finding-meta">
                                Repository Structure ‚Ä¢
                                <span class="tier-badge tier-1">Tier 1</span>
                                 ‚Ä¢ 2/2 directories
                            </div>
                        </div>
                    </div>
                    
                    <div class="score-display score-pass">
                        100
                    </div>
                    
                </div>

                <div class="finding-body">
                    
                    <div class="finding-section">
                        <h4>Evidence</h4>
                        <ul class="evidence-list">
                            
                            <li>Found 2/2 standard directories</li>
                            
                            <li>src/: ‚úì</li>
                            
                            <li>tests/: ‚úì</li>
                            
                        </ul>
                    </div>
                    

                    

                    
                </div>
            </div>
            
            <div class="finding"
                 data-status="fail"
                 data-tier="1"
                 data-category="Dependency Management"
                 data-score="0"
                 data-name="lock files for reproducibility">

                <div class="finding-header" onclick="toggleFinding(this)">
                    <div class="finding-title">
                        <span class="status-icon">
                            ‚ùå
                            
                        </span>
                        <div class="finding-info">
                            <h3>Lock Files for Reproducibility</h3>
                            <div class="finding-meta">
                                Dependency Management ‚Ä¢
                                <span class="tier-badge tier-1">Tier 1</span>
                                 ‚Ä¢ none
                            </div>
                        </div>
                    </div>
                    
                    <div class="score-display score-fail">
                        0
                    </div>
                    
                </div>

                <div class="finding-body">
                    
                    <div class="finding-section">
                        <h4>Evidence</h4>
                        <ul class="evidence-list">
                            
                            <li>No lock files found</li>
                            
                        </ul>
                    </div>
                    

                    
                    <div class="finding-section">
                        <h4>Remediation</h4>
                        <p><strong>Add lock file for dependency reproducibility</strong></p>

                        
                        <ol class="remediation-steps">
                            
                            <li>Use npm install, poetry lock, or equivalent to generate lock file</li>
                            
                        </ol>
                        

                        
                        <h4 style="margin-top: 15px;">Commands</h4>
                        <pre><code>npm install  # generates package-lock.json</code></pre>
                        

                        
                    </div>
                    

                    
                </div>
            </div>
            
            <div class="finding"
                 data-status="pass"
                 data-tier="2"
                 data-category="Testing &amp; CI/CD"
                 data-score="100.0"
                 data-name="test coverage requirements">

                <div class="finding-header" onclick="toggleFinding(this)">
                    <div class="finding-title">
                        <span class="status-icon">
                            ‚úÖ
                            
                        </span>
                        <div class="finding-info">
                            <h3>Test Coverage Requirements</h3>
                            <div class="finding-meta">
                                Testing &amp; CI/CD ‚Ä¢
                                <span class="tier-badge tier-2">Tier 2</span>
                                 ‚Ä¢ configured
                            </div>
                        </div>
                    </div>
                    
                    <div class="score-display score-pass">
                        100
                    </div>
                    
                </div>

                <div class="finding-body">
                    
                    <div class="finding-section">
                        <h4>Evidence</h4>
                        <ul class="evidence-list">
                            
                            <li>Coverage configuration found</li>
                            
                            <li>pytest-cov dependency present</li>
                            
                        </ul>
                    </div>
                    

                    

                    
                </div>
            </div>
            
            <div class="finding"
                 data-status="pass"
                 data-tier="2"
                 data-category="Testing &amp; CI/CD"
                 data-score="100.0"
                 data-name="pre-commit hooks &amp; ci/cd linting">

                <div class="finding-header" onclick="toggleFinding(this)">
                    <div class="finding-title">
                        <span class="status-icon">
                            ‚úÖ
                            
                        </span>
                        <div class="finding-info">
                            <h3>Pre-commit Hooks &amp; CI/CD Linting</h3>
                            <div class="finding-meta">
                                Testing &amp; CI/CD ‚Ä¢
                                <span class="tier-badge tier-2">Tier 2</span>
                                 ‚Ä¢ configured
                            </div>
                        </div>
                    </div>
                    
                    <div class="score-display score-pass">
                        100
                    </div>
                    
                </div>

                <div class="finding-body">
                    
                    <div class="finding-section">
                        <h4>Evidence</h4>
                        <ul class="evidence-list">
                            
                            <li>.pre-commit-config.yaml found</li>
                            
                        </ul>
                    </div>
                    

                    

                    
                </div>
            </div>
            
            <div class="finding"
                 data-status="fail"
                 data-tier="2"
                 data-category="Git &amp; Version Control"
                 data-score="0"
                 data-name="conventional commit messages">

                <div class="finding-header" onclick="toggleFinding(this)">
                    <div class="finding-title">
                        <span class="status-icon">
                            ‚ùå
                            
                        </span>
                        <div class="finding-info">
                            <h3>Conventional Commit Messages</h3>
                            <div class="finding-meta">
                                Git &amp; Version Control ‚Ä¢
                                <span class="tier-badge tier-2">Tier 2</span>
                                 ‚Ä¢ not configured
                            </div>
                        </div>
                    </div>
                    
                    <div class="score-display score-fail">
                        0
                    </div>
                    
                </div>

                <div class="finding-body">
                    
                    <div class="finding-section">
                        <h4>Evidence</h4>
                        <ul class="evidence-list">
                            
                            <li>No commitlint or husky configuration</li>
                            
                        </ul>
                    </div>
                    

                    
                    <div class="finding-section">
                        <h4>Remediation</h4>
                        <p><strong>Configure conventional commits with commitlint</strong></p>

                        
                        <ol class="remediation-steps">
                            
                            <li>Install commitlint</li>
                            
                            <li>Configure husky for commit-msg hook</li>
                            
                        </ol>
                        

                        
                        <h4 style="margin-top: 15px;">Commands</h4>
                        <pre><code>npm install --save-dev @commitlint/cli @commitlint/config-conventional husky</code></pre>
                        

                        
                    </div>
                    

                    
                </div>
            </div>
            
            <div class="finding"
                 data-status="pass"
                 data-tier="2"
                 data-category="Git &amp; Version Control"
                 data-score="100.0"
                 data-name=".gitignore completeness">

                <div class="finding-header" onclick="toggleFinding(this)">
                    <div class="finding-title">
                        <span class="status-icon">
                            ‚úÖ
                            
                        </span>
                        <div class="finding-info">
                            <h3>.gitignore Completeness</h3>
                            <div class="finding-meta">
                                Git &amp; Version Control ‚Ä¢
                                <span class="tier-badge tier-2">Tier 2</span>
                                 ‚Ä¢ 580 bytes
                            </div>
                        </div>
                    </div>
                    
                    <div class="score-display score-pass">
                        100
                    </div>
                    
                </div>

                <div class="finding-body">
                    
                    <div class="finding-section">
                        <h4>Evidence</h4>
                        <ul class="evidence-list">
                            
                            <li>.gitignore found (580 bytes)</li>
                            
                        </ul>
                    </div>
                    

                    

                    
                </div>
            </div>
            
            <div class="finding"
                 data-status="pass"
                 data-tier="3"
                 data-category="Code Quality"
                 data-score="100.0"
                 data-name="cyclomatic complexity thresholds">

                <div class="finding-header" onclick="toggleFinding(this)">
                    <div class="finding-title">
                        <span class="status-icon">
                            ‚úÖ
                            
                        </span>
                        <div class="finding-info">
                            <h3>Cyclomatic Complexity Thresholds</h3>
                            <div class="finding-meta">
                                Code Quality ‚Ä¢
                                <span class="tier-badge tier-3">Tier 3</span>
                                 ‚Ä¢ 3.1
                            </div>
                        </div>
                    </div>
                    
                    <div class="score-display score-pass">
                        100
                    </div>
                    
                </div>

                <div class="finding-body">
                    
                    <div class="finding-section">
                        <h4>Evidence</h4>
                        <ul class="evidence-list">
                            
                            <li>Average cyclomatic complexity: 3.1</li>
                            
                        </ul>
                    </div>
                    

                    

                    
                </div>
            </div>
            
            <div class="finding"
                 data-status="not_applicable"
                 data-tier="2"
                 data-category="Build &amp; Development"
                 data-score="0"
                 data-name="one-command build/setup">

                <div class="finding-header" onclick="toggleFinding(this)">
                    <div class="finding-title">
                        <span class="status-icon">
                            ‚äò
                            
                        </span>
                        <div class="finding-info">
                            <h3>One-Command Build/Setup</h3>
                            <div class="finding-meta">
                                Build &amp; Development ‚Ä¢
                                <span class="tier-badge tier-2">Tier 2</span>
                                
                            </div>
                        </div>
                    </div>
                    
                    <div class="score-display score-skip">‚Äî</div>
                    
                </div>

                <div class="finding-body">
                    
                    <div class="finding-section">
                        <h4>Evidence</h4>
                        <ul class="evidence-list">
                            
                            <li>One-Command Build/Setup assessment not yet implemented</li>
                            
                        </ul>
                    </div>
                    

                    

                    
                </div>
            </div>
            
            <div class="finding"
                 data-status="not_applicable"
                 data-tier="2"
                 data-category="Context Window Optimization"
                 data-score="0"
                 data-name="concise structured documentation">

                <div class="finding-header" onclick="toggleFinding(this)">
                    <div class="finding-title">
                        <span class="status-icon">
                            ‚äò
                            
                        </span>
                        <div class="finding-info">
                            <h3>Concise Structured Documentation</h3>
                            <div class="finding-meta">
                                Context Window Optimization ‚Ä¢
                                <span class="tier-badge tier-2">Tier 2</span>
                                
                            </div>
                        </div>
                    </div>
                    
                    <div class="score-display score-skip">‚Äî</div>
                    
                </div>

                <div class="finding-body">
                    
                    <div class="finding-section">
                        <h4>Evidence</h4>
                        <ul class="evidence-list">
                            
                            <li>Concise Structured Documentation assessment not yet implemented</li>
                            
                        </ul>
                    </div>
                    

                    

                    
                </div>
            </div>
            
            <div class="finding"
                 data-status="not_applicable"
                 data-tier="2"
                 data-category="Documentation Standards"
                 data-score="0"
                 data-name="inline documentation">

                <div class="finding-header" onclick="toggleFinding(this)">
                    <div class="finding-title">
                        <span class="status-icon">
                            ‚äò
                            
                        </span>
                        <div class="finding-info">
                            <h3>Inline Documentation</h3>
                            <div class="finding-meta">
                                Documentation Standards ‚Ä¢
                                <span class="tier-badge tier-2">Tier 2</span>
                                
                            </div>
                        </div>
                    </div>
                    
                    <div class="score-display score-skip">‚Äî</div>
                    
                </div>

                <div class="finding-body">
                    
                    <div class="finding-section">
                        <h4>Evidence</h4>
                        <ul class="evidence-list">
                            
                            <li>Inline Documentation assessment not yet implemented</li>
                            
                        </ul>
                    </div>
                    

                    

                    
                </div>
            </div>
            
            <div class="finding"
                 data-status="not_applicable"
                 data-tier="2"
                 data-category="Context Window Optimization"
                 data-score="0"
                 data-name="file size limits">

                <div class="finding-header" onclick="toggleFinding(this)">
                    <div class="finding-title">
                        <span class="status-icon">
                            ‚äò
                            
                        </span>
                        <div class="finding-info">
                            <h3>File Size Limits</h3>
                            <div class="finding-meta">
                                Context Window Optimization ‚Ä¢
                                <span class="tier-badge tier-2">Tier 2</span>
                                
                            </div>
                        </div>
                    </div>
                    
                    <div class="score-display score-skip">‚Äî</div>
                    
                </div>

                <div class="finding-body">
                    
                    <div class="finding-section">
                        <h4>Evidence</h4>
                        <ul class="evidence-list">
                            
                            <li>File Size Limits assessment not yet implemented</li>
                            
                        </ul>
                    </div>
                    

                    

                    
                </div>
            </div>
            
            <div class="finding"
                 data-status="not_applicable"
                 data-tier="2"
                 data-category="Dependency Management"
                 data-score="0"
                 data-name="dependency freshness &amp; security">

                <div class="finding-header" onclick="toggleFinding(this)">
                    <div class="finding-title">
                        <span class="status-icon">
                            ‚äò
                            
                        </span>
                        <div class="finding-info">
                            <h3>Dependency Freshness &amp; Security</h3>
                            <div class="finding-meta">
                                Dependency Management ‚Ä¢
                                <span class="tier-badge tier-2">Tier 2</span>
                                
                            </div>
                        </div>
                    </div>
                    
                    <div class="score-display score-skip">‚Äî</div>
                    
                </div>

                <div class="finding-body">
                    
                    <div class="finding-section">
                        <h4>Evidence</h4>
                        <ul class="evidence-list">
                            
                            <li>Dependency Freshness &amp; Security assessment not yet implemented</li>
                            
                        </ul>
                    </div>
                    

                    

                    
                </div>
            </div>
            
            <div class="finding"
                 data-status="not_applicable"
                 data-tier="2"
                 data-category="Repository Structure"
                 data-score="0"
                 data-name="separation of concerns">

                <div class="finding-header" onclick="toggleFinding(this)">
                    <div class="finding-title">
                        <span class="status-icon">
                            ‚äò
                            
                        </span>
                        <div class="finding-info">
                            <h3>Separation of Concerns</h3>
                            <div class="finding-meta">
                                Repository Structure ‚Ä¢
                                <span class="tier-badge tier-2">Tier 2</span>
                                
                            </div>
                        </div>
                    </div>
                    
                    <div class="score-display score-skip">‚Äî</div>
                    
                </div>

                <div class="finding-body">
                    
                    <div class="finding-section">
                        <h4>Evidence</h4>
                        <ul class="evidence-list">
                            
                            <li>Separation of Concerns assessment not yet implemented</li>
                            
                        </ul>
                    </div>
                    

                    

                    
                </div>
            </div>
            
            <div class="finding"
                 data-status="not_applicable"
                 data-tier="3"
                 data-category="Error Handling"
                 data-score="0"
                 data-name="structured logging">

                <div class="finding-header" onclick="toggleFinding(this)">
                    <div class="finding-title">
                        <span class="status-icon">
                            ‚äò
                            
                        </span>
                        <div class="finding-info">
                            <h3>Structured Logging</h3>
                            <div class="finding-meta">
                                Error Handling ‚Ä¢
                                <span class="tier-badge tier-3">Tier 3</span>
                                
                            </div>
                        </div>
                    </div>
                    
                    <div class="score-display score-skip">‚Äî</div>
                    
                </div>

                <div class="finding-body">
                    
                    <div class="finding-section">
                        <h4>Evidence</h4>
                        <ul class="evidence-list">
                            
                            <li>Structured Logging assessment not yet implemented</li>
                            
                        </ul>
                    </div>
                    

                    

                    
                </div>
            </div>
            
            <div class="finding"
                 data-status="not_applicable"
                 data-tier="3"
                 data-category="API Documentation"
                 data-score="0"
                 data-name="openapi/swagger specifications">

                <div class="finding-header" onclick="toggleFinding(this)">
                    <div class="finding-title">
                        <span class="status-icon">
                            ‚äò
                            
                        </span>
                        <div class="finding-info">
                            <h3>OpenAPI/Swagger Specifications</h3>
                            <div class="finding-meta">
                                API Documentation ‚Ä¢
                                <span class="tier-badge tier-3">Tier 3</span>
                                
                            </div>
                        </div>
                    </div>
                    
                    <div class="score-display score-skip">‚Äî</div>
                    
                </div>

                <div class="finding-body">
                    
                    <div class="finding-section">
                        <h4>Evidence</h4>
                        <ul class="evidence-list">
                            
                            <li>OpenAPI/Swagger Specifications assessment not yet implemented</li>
                            
                        </ul>
                    </div>
                    

                    

                    
                </div>
            </div>
            
            <div class="finding"
                 data-status="not_applicable"
                 data-tier="3"
                 data-category="Documentation Standards"
                 data-score="0"
                 data-name="architecture decision records">

                <div class="finding-header" onclick="toggleFinding(this)">
                    <div class="finding-title">
                        <span class="status-icon">
                            ‚äò
                            
                        </span>
                        <div class="finding-info">
                            <h3>Architecture Decision Records</h3>
                            <div class="finding-meta">
                                Documentation Standards ‚Ä¢
                                <span class="tier-badge tier-3">Tier 3</span>
                                
                            </div>
                        </div>
                    </div>
                    
                    <div class="score-display score-skip">‚Äî</div>
                    
                </div>

                <div class="finding-body">
                    
                    <div class="finding-section">
                        <h4>Evidence</h4>
                        <ul class="evidence-list">
                            
                            <li>Architecture Decision Records assessment not yet implemented</li>
                            
                        </ul>
                    </div>
                    

                    

                    
                </div>
            </div>
            
            <div class="finding"
                 data-status="not_applicable"
                 data-tier="3"
                 data-category="Modularity"
                 data-score="0"
                 data-name="semantic file &amp; directory naming">

                <div class="finding-header" onclick="toggleFinding(this)">
                    <div class="finding-title">
                        <span class="status-icon">
                            ‚äò
                            
                        </span>
                        <div class="finding-info">
                            <h3>Semantic File &amp; Directory Naming</h3>
                            <div class="finding-meta">
                                Modularity ‚Ä¢
                                <span class="tier-badge tier-3">Tier 3</span>
                                
                            </div>
                        </div>
                    </div>
                    
                    <div class="score-display score-skip">‚Äî</div>
                    
                </div>

                <div class="finding-body">
                    
                    <div class="finding-section">
                        <h4>Evidence</h4>
                        <ul class="evidence-list">
                            
                            <li>Semantic File &amp; Directory Naming assessment not yet implemented</li>
                            
                        </ul>
                    </div>
                    

                    

                    
                </div>
            </div>
            
            <div class="finding"
                 data-status="not_applicable"
                 data-tier="4"
                 data-category="Security"
                 data-score="0"
                 data-name="security scanning automation">

                <div class="finding-header" onclick="toggleFinding(this)">
                    <div class="finding-title">
                        <span class="status-icon">
                            ‚äò
                            
                        </span>
                        <div class="finding-info">
                            <h3>Security Scanning Automation</h3>
                            <div class="finding-meta">
                                Security ‚Ä¢
                                <span class="tier-badge tier-4">Tier 4</span>
                                
                            </div>
                        </div>
                    </div>
                    
                    <div class="score-display score-skip">‚Äî</div>
                    
                </div>

                <div class="finding-body">
                    
                    <div class="finding-section">
                        <h4>Evidence</h4>
                        <ul class="evidence-list">
                            
                            <li>Security Scanning Automation assessment not yet implemented</li>
                            
                        </ul>
                    </div>
                    

                    

                    
                </div>
            </div>
            
            <div class="finding"
                 data-status="not_applicable"
                 data-tier="4"
                 data-category="Performance"
                 data-score="0"
                 data-name="performance benchmarks">

                <div class="finding-header" onclick="toggleFinding(this)">
                    <div class="finding-title">
                        <span class="status-icon">
                            ‚äò
                            
                        </span>
                        <div class="finding-info">
                            <h3>Performance Benchmarks</h3>
                            <div class="finding-meta">
                                Performance ‚Ä¢
                                <span class="tier-badge tier-4">Tier 4</span>
                                
                            </div>
                        </div>
                    </div>
                    
                    <div class="score-display score-skip">‚Äî</div>
                    
                </div>

                <div class="finding-body">
                    
                    <div class="finding-section">
                        <h4>Evidence</h4>
                        <ul class="evidence-list">
                            
                            <li>Performance Benchmarks assessment not yet implemented</li>
                            
                        </ul>
                    </div>
                    

                    

                    
                </div>
            </div>
            
            <div class="finding"
                 data-status="not_applicable"
                 data-tier="4"
                 data-category="Code Quality"
                 data-score="0"
                 data-name="code smell elimination">

                <div class="finding-header" onclick="toggleFinding(this)">
                    <div class="finding-title">
                        <span class="status-icon">
                            ‚äò
                            
                        </span>
                        <div class="finding-info">
                            <h3>Code Smell Elimination</h3>
                            <div class="finding-meta">
                                Code Quality ‚Ä¢
                                <span class="tier-badge tier-4">Tier 4</span>
                                
                            </div>
                        </div>
                    </div>
                    
                    <div class="score-display score-skip">‚Äî</div>
                    
                </div>

                <div class="finding-body">
                    
                    <div class="finding-section">
                        <h4>Evidence</h4>
                        <ul class="evidence-list">
                            
                            <li>Code Smell Elimination assessment not yet implemented</li>
                            
                        </ul>
                    </div>
                    

                    

                    
                </div>
            </div>
            
            <div class="finding"
                 data-status="not_applicable"
                 data-tier="4"
                 data-category="Git &amp; Version Control"
                 data-score="0"
                 data-name="issue &amp; pull request templates">

                <div class="finding-header" onclick="toggleFinding(this)">
                    <div class="finding-title">
                        <span class="status-icon">
                            ‚äò
                            
                        </span>
                        <div class="finding-info">
                            <h3>Issue &amp; Pull Request Templates</h3>
                            <div class="finding-meta">
                                Git &amp; Version Control ‚Ä¢
                                <span class="tier-badge tier-4">Tier 4</span>
                                
                            </div>
                        </div>
                    </div>
                    
                    <div class="score-display score-skip">‚Äî</div>
                    
                </div>

                <div class="finding-body">
                    
                    <div class="finding-section">
                        <h4>Evidence</h4>
                        <ul class="evidence-list">
                            
                            <li>Issue &amp; Pull Request Templates assessment not yet implemented</li>
                            
                        </ul>
                    </div>
                    

                    

                    
                </div>
            </div>
            
            <div class="finding"
                 data-status="not_applicable"
                 data-tier="4"
                 data-category="Build &amp; Development"
                 data-score="0"
                 data-name="container/virtualization setup">

                <div class="finding-header" onclick="toggleFinding(this)">
                    <div class="finding-title">
                        <span class="status-icon">
                            ‚äò
                            
                        </span>
                        <div class="finding-info">
                            <h3>Container/Virtualization Setup</h3>
                            <div class="finding-meta">
                                Build &amp; Development ‚Ä¢
                                <span class="tier-badge tier-4">Tier 4</span>
                                
                            </div>
                        </div>
                    </div>
                    
                    <div class="score-display score-skip">‚Äî</div>
                    
                </div>

                <div class="finding-body">
                    
                    <div class="finding-section">
                        <h4>Evidence</h4>
                        <ul class="evidence-list">
                            
                            <li>Container/Virtualization Setup assessment not yet implemented</li>
                            
                        </ul>
                    </div>
                    

                    

                    
                </div>
            </div>
            
        </div>

        <footer>
            <p>Generated by <strong>AgentReady v1.0.0</strong></p>
            <p>Repository: /Users/jeder/repos/sk/agentready ‚Ä¢ Branch: main ‚Ä¢ Commit: f968aaeb</p>
            <p style="margin-top: 10px; font-size: 0.85rem;">
                ü§ñ Generated with <a href="https://claude.com/claude-code" style="color: #2563eb;">Claude Code</a>
            </p>
        </footer>
    </div>

    <script>
        // Embedded assessment data (properly escaped to prevent XSS)
        const ASSESSMENT = JSON.parse("{\"repository\": {\"path\": \"/Users/jeder/repos/sk/agentready\", \"name\": \"agentready\", \"url\": \"git@github.com:ambient-code/agentready.git\", \"branch\": \"main\", \"commit_hash\": \"f968aaeb4391bdd31cc815b69d0ce7d5c3d99bce\", \"languages\": {\"YAML\": 9, \"Markdown\": 72, \"Shell\": 5, \"JSON\": 3, \"Python\": 36}, \"total_files\": 150, \"total_lines\": 65059}, \"timestamp\": \"2025-11-21T03:58:45.449559\", \"overall_score\": 80.0, \"certification_level\": \"Gold\", \"attributes_assessed\": 10, \"attributes_skipped\": 15, \"attributes_total\": 25, \"findings\": [{\"attribute\": {\"id\": \"claude_md_file\", \"name\": \"CLAUDE.md Configuration Files\", \"category\": \"Context Window Optimization\", \"tier\": 1, \"description\": \"Project-specific configuration for Claude Code\", \"criteria\": \"CLAUDE.md file exists in repository root\", \"default_weight\": 0.1}, \"status\": \"pass\", \"score\": 100.0, \"measured_value\": \"present\", \"threshold\": \"present\", \"evidence\": [\"CLAUDE.md found at /Users/jeder/repos/sk/agentready/CLAUDE.md\"], \"remediation\": null, \"error_message\": null}, {\"attribute\": {\"id\": \"readme_structure\", \"name\": \"README Structure\", \"category\": \"Documentation Standards\", \"tier\": 1, \"description\": \"Well-structured README with key sections\", \"criteria\": \"README.md with installation, usage, and development sections\", \"default_weight\": 0.1}, \"status\": \"pass\", \"score\": 100.0, \"measured_value\": \"3/3 sections\", \"threshold\": \"3/3 sections\", \"evidence\": [\"Found 3/3 essential sections\", \"Installation: \\u2713\", \"Usage: \\u2713\", \"Development: \\u2713\"], \"remediation\": null, \"error_message\": null}, {\"attribute\": {\"id\": \"type_annotations\", \"name\": \"Type Annotations\", \"category\": \"Code Quality\", \"tier\": 1, \"description\": \"Type hints in function signatures\", \"criteria\": \"\u003e80% of functions have type annotations\", \"default_weight\": 0.1}, \"status\": \"pass\", \"score\": 100.0, \"measured_value\": \"96.5%\", \"threshold\": \"\\u226580%\", \"evidence\": [\"Typed functions: 167/173\", \"Coverage: 96.5%\"], \"remediation\": null, \"error_message\": null}, {\"attribute\": {\"id\": \"standard_layout\", \"name\": \"Standard Project Layouts\", \"category\": \"Repository Structure\", \"tier\": 1, \"description\": \"Follows standard project structure for language\", \"criteria\": \"Standard directories (src/, tests/, docs/) present\", \"default_weight\": 0.1}, \"status\": \"pass\", \"score\": 100.0, \"measured_value\": \"2/2 directories\", \"threshold\": \"2/2 directories\", \"evidence\": [\"Found 2/2 standard directories\", \"src/: \\u2713\", \"tests/: \\u2713\"], \"remediation\": null, \"error_message\": null}, {\"attribute\": {\"id\": \"lock_files\", \"name\": \"Lock Files for Reproducibility\", \"category\": \"Dependency Management\", \"tier\": 1, \"description\": \"Lock files present for dependency pinning\", \"criteria\": \"package-lock.json, yarn.lock, poetry.lock, or requirements.txt with versions\", \"default_weight\": 0.1}, \"status\": \"fail\", \"score\": 0.0, \"measured_value\": \"none\", \"threshold\": \"at least one lock file\", \"evidence\": [\"No lock files found\"], \"remediation\": {\"summary\": \"Add lock file for dependency reproducibility\", \"steps\": [\"Use npm install, poetry lock, or equivalent to generate lock file\"], \"tools\": [], \"commands\": [\"npm install  # generates package-lock.json\"], \"examples\": [], \"citations\": []}, \"error_message\": null}, {\"attribute\": {\"id\": \"test_coverage\", \"name\": \"Test Coverage Requirements\", \"category\": \"Testing \u0026 CI/CD\", \"tier\": 2, \"description\": \"Test coverage thresholds configured and enforced\", \"criteria\": \"\u003e80% code coverage\", \"default_weight\": 0.03}, \"status\": \"pass\", \"score\": 100.0, \"measured_value\": \"configured\", \"threshold\": \"configured with \u003e80% threshold\", \"evidence\": [\"Coverage configuration found\", \"pytest-cov dependency present\"], \"remediation\": null, \"error_message\": null}, {\"attribute\": {\"id\": \"precommit_hooks\", \"name\": \"Pre-commit Hooks \u0026 CI/CD Linting\", \"category\": \"Testing \u0026 CI/CD\", \"tier\": 2, \"description\": \"Pre-commit hooks configured for linting and formatting\", \"criteria\": \".pre-commit-config.yaml exists\", \"default_weight\": 0.03}, \"status\": \"pass\", \"score\": 100.0, \"measured_value\": \"configured\", \"threshold\": \"configured\", \"evidence\": [\".pre-commit-config.yaml found\"], \"remediation\": null, \"error_message\": null}, {\"attribute\": {\"id\": \"conventional_commits\", \"name\": \"Conventional Commit Messages\", \"category\": \"Git \u0026 Version Control\", \"tier\": 2, \"description\": \"Follows conventional commit format\", \"criteria\": \"\\u226580% of recent commits follow convention\", \"default_weight\": 0.03}, \"status\": \"fail\", \"score\": 0.0, \"measured_value\": \"not configured\", \"threshold\": \"configured\", \"evidence\": [\"No commitlint or husky configuration\"], \"remediation\": {\"summary\": \"Configure conventional commits with commitlint\", \"steps\": [\"Install commitlint\", \"Configure husky for commit-msg hook\"], \"tools\": [\"commitlint\", \"husky\"], \"commands\": [\"npm install --save-dev @commitlint/cli @commitlint/config-conventional husky\"], \"examples\": [], \"citations\": []}, \"error_message\": null}, {\"attribute\": {\"id\": \"gitignore_completeness\", \"name\": \".gitignore Completeness\", \"category\": \"Git \u0026 Version Control\", \"tier\": 2, \"description\": \"Comprehensive .gitignore file\", \"criteria\": \".gitignore exists and covers common patterns\", \"default_weight\": 0.03}, \"status\": \"pass\", \"score\": 100.0, \"measured_value\": \"580 bytes\", \"threshold\": \"\u003e50 bytes\", \"evidence\": [\".gitignore found (580 bytes)\"], \"remediation\": null, \"error_message\": null}, {\"attribute\": {\"id\": \"cyclomatic_complexity\", \"name\": \"Cyclomatic Complexity Thresholds\", \"category\": \"Code Quality\", \"tier\": 3, \"description\": \"Cyclomatic complexity thresholds enforced\", \"criteria\": \"Average complexity \u003c10, no functions \u003e15\", \"default_weight\": 0.03}, \"status\": \"pass\", \"score\": 100.0, \"measured_value\": \"3.1\", \"threshold\": \"\u003c10.0\", \"evidence\": [\"Average cyclomatic complexity: 3.1\"], \"remediation\": null, \"error_message\": null}, {\"attribute\": {\"id\": \"one_command_setup\", \"name\": \"One-Command Build/Setup\", \"category\": \"Build \u0026 Development\", \"tier\": 2, \"description\": \"Assessment for One-Command Build/Setup\", \"criteria\": \"To be implemented\", \"default_weight\": 0.03}, \"status\": \"not_applicable\", \"score\": null, \"measured_value\": null, \"threshold\": null, \"evidence\": [\"One-Command Build/Setup assessment not yet implemented\"], \"remediation\": null, \"error_message\": null}, {\"attribute\": {\"id\": \"concise_documentation\", \"name\": \"Concise Structured Documentation\", \"category\": \"Context Window Optimization\", \"tier\": 2, \"description\": \"Assessment for Concise Structured Documentation\", \"criteria\": \"To be implemented\", \"default_weight\": 0.03}, \"status\": \"not_applicable\", \"score\": null, \"measured_value\": null, \"threshold\": null, \"evidence\": [\"Concise Structured Documentation assessment not yet implemented\"], \"remediation\": null, \"error_message\": null}, {\"attribute\": {\"id\": \"inline_documentation\", \"name\": \"Inline Documentation\", \"category\": \"Documentation Standards\", \"tier\": 2, \"description\": \"Assessment for Inline Documentation\", \"criteria\": \"To be implemented\", \"default_weight\": 0.03}, \"status\": \"not_applicable\", \"score\": null, \"measured_value\": null, \"threshold\": null, \"evidence\": [\"Inline Documentation assessment not yet implemented\"], \"remediation\": null, \"error_message\": null}, {\"attribute\": {\"id\": \"file_size_limits\", \"name\": \"File Size Limits\", \"category\": \"Context Window Optimization\", \"tier\": 2, \"description\": \"Assessment for File Size Limits\", \"criteria\": \"To be implemented\", \"default_weight\": 0.03}, \"status\": \"not_applicable\", \"score\": null, \"measured_value\": null, \"threshold\": null, \"evidence\": [\"File Size Limits assessment not yet implemented\"], \"remediation\": null, \"error_message\": null}, {\"attribute\": {\"id\": \"dependency_freshness\", \"name\": \"Dependency Freshness \u0026 Security\", \"category\": \"Dependency Management\", \"tier\": 2, \"description\": \"Assessment for Dependency Freshness \u0026 Security\", \"criteria\": \"To be implemented\", \"default_weight\": 0.03}, \"status\": \"not_applicable\", \"score\": null, \"measured_value\": null, \"threshold\": null, \"evidence\": [\"Dependency Freshness \u0026 Security assessment not yet implemented\"], \"remediation\": null, \"error_message\": null}, {\"attribute\": {\"id\": \"separation_concerns\", \"name\": \"Separation of Concerns\", \"category\": \"Repository Structure\", \"tier\": 2, \"description\": \"Assessment for Separation of Concerns\", \"criteria\": \"To be implemented\", \"default_weight\": 0.03}, \"status\": \"not_applicable\", \"score\": null, \"measured_value\": null, \"threshold\": null, \"evidence\": [\"Separation of Concerns assessment not yet implemented\"], \"remediation\": null, \"error_message\": null}, {\"attribute\": {\"id\": \"structured_logging\", \"name\": \"Structured Logging\", \"category\": \"Error Handling\", \"tier\": 3, \"description\": \"Assessment for Structured Logging\", \"criteria\": \"To be implemented\", \"default_weight\": 0.03}, \"status\": \"not_applicable\", \"score\": null, \"measured_value\": null, \"threshold\": null, \"evidence\": [\"Structured Logging assessment not yet implemented\"], \"remediation\": null, \"error_message\": null}, {\"attribute\": {\"id\": \"openapi_specs\", \"name\": \"OpenAPI/Swagger Specifications\", \"category\": \"API Documentation\", \"tier\": 3, \"description\": \"Assessment for OpenAPI/Swagger Specifications\", \"criteria\": \"To be implemented\", \"default_weight\": 0.03}, \"status\": \"not_applicable\", \"score\": null, \"measured_value\": null, \"threshold\": null, \"evidence\": [\"OpenAPI/Swagger Specifications assessment not yet implemented\"], \"remediation\": null, \"error_message\": null}, {\"attribute\": {\"id\": \"architecture_decisions\", \"name\": \"Architecture Decision Records\", \"category\": \"Documentation Standards\", \"tier\": 3, \"description\": \"Assessment for Architecture Decision Records\", \"criteria\": \"To be implemented\", \"default_weight\": 0.03}, \"status\": \"not_applicable\", \"score\": null, \"measured_value\": null, \"threshold\": null, \"evidence\": [\"Architecture Decision Records assessment not yet implemented\"], \"remediation\": null, \"error_message\": null}, {\"attribute\": {\"id\": \"semantic_naming\", \"name\": \"Semantic File \u0026 Directory Naming\", \"category\": \"Modularity\", \"tier\": 3, \"description\": \"Assessment for Semantic File \u0026 Directory Naming\", \"criteria\": \"To be implemented\", \"default_weight\": 0.03}, \"status\": \"not_applicable\", \"score\": null, \"measured_value\": null, \"threshold\": null, \"evidence\": [\"Semantic File \u0026 Directory Naming assessment not yet implemented\"], \"remediation\": null, \"error_message\": null}, {\"attribute\": {\"id\": \"security_scanning\", \"name\": \"Security Scanning Automation\", \"category\": \"Security\", \"tier\": 4, \"description\": \"Assessment for Security Scanning Automation\", \"criteria\": \"To be implemented\", \"default_weight\": 0.01}, \"status\": \"not_applicable\", \"score\": null, \"measured_value\": null, \"threshold\": null, \"evidence\": [\"Security Scanning Automation assessment not yet implemented\"], \"remediation\": null, \"error_message\": null}, {\"attribute\": {\"id\": \"performance_benchmarks\", \"name\": \"Performance Benchmarks\", \"category\": \"Performance\", \"tier\": 4, \"description\": \"Assessment for Performance Benchmarks\", \"criteria\": \"To be implemented\", \"default_weight\": 0.01}, \"status\": \"not_applicable\", \"score\": null, \"measured_value\": null, \"threshold\": null, \"evidence\": [\"Performance Benchmarks assessment not yet implemented\"], \"remediation\": null, \"error_message\": null}, {\"attribute\": {\"id\": \"code_smells\", \"name\": \"Code Smell Elimination\", \"category\": \"Code Quality\", \"tier\": 4, \"description\": \"Assessment for Code Smell Elimination\", \"criteria\": \"To be implemented\", \"default_weight\": 0.01}, \"status\": \"not_applicable\", \"score\": null, \"measured_value\": null, \"threshold\": null, \"evidence\": [\"Code Smell Elimination assessment not yet implemented\"], \"remediation\": null, \"error_message\": null}, {\"attribute\": {\"id\": \"issue_pr_templates\", \"name\": \"Issue \u0026 Pull Request Templates\", \"category\": \"Git \u0026 Version Control\", \"tier\": 4, \"description\": \"Assessment for Issue \u0026 Pull Request Templates\", \"criteria\": \"To be implemented\", \"default_weight\": 0.01}, \"status\": \"not_applicable\", \"score\": null, \"measured_value\": null, \"threshold\": null, \"evidence\": [\"Issue \u0026 Pull Request Templates assessment not yet implemented\"], \"remediation\": null, \"error_message\": null}, {\"attribute\": {\"id\": \"container_setup\", \"name\": \"Container/Virtualization Setup\", \"category\": \"Build \u0026 Development\", \"tier\": 4, \"description\": \"Assessment for Container/Virtualization Setup\", \"criteria\": \"To be implemented\", \"default_weight\": 0.01}, \"status\": \"not_applicable\", \"score\": null, \"measured_value\": null, \"threshold\": null, \"evidence\": [\"Container/Virtualization Setup assessment not yet implemented\"], \"remediation\": null, \"error_message\": null}], \"config\": null, \"duration_seconds\": 0.2}");

        // Toggle finding expansion
        function toggleFinding(header) {
            const finding = header.parentElement;
            finding.classList.toggle('expanded');
        }

        // Filter functionality
        document.querySelectorAll('.filter-btn').forEach(btn => {
            btn.addEventListener('click', () => {
                // Update active button
                document.querySelectorAll('.filter-btn').forEach(b => b.classList.remove('active'));
                btn.classList.add('active');

                const filter = btn.dataset.filter;
                const findings = document.querySelectorAll('.finding');

                findings.forEach(finding => {
                    const status = finding.dataset.status;

                    if (filter === 'all') {
                        finding.classList.remove('hidden');
                    } else if (filter === 'skipped') {
                        if (status === 'skipped' || status === 'not_applicable' || status === 'error') {
                            finding.classList.remove('hidden');
                        } else {
                            finding.classList.add('hidden');
                        }
                    } else {
                        if (status === filter) {
                            finding.classList.remove('hidden');
                        } else {
                            finding.classList.add('hidden');
                        }
                    }
                });
            });
        });

        // Sort functionality
        document.getElementById('sort-select').addEventListener('change', (e) => {
            const sortBy = e.target.value;
            const container = document.getElementById('findings-container');
            const findings = Array.from(container.querySelectorAll('.finding'));

            findings.sort((a, b) => {
                if (sortBy === 'category') {
                    return a.dataset.category.localeCompare(b.dataset.category);
                } else if (sortBy === 'score-desc') {
                    return parseFloat(b.dataset.score) - parseFloat(a.dataset.score);
                } else if (sortBy === 'score-asc') {
                    return parseFloat(a.dataset.score) - parseFloat(b.dataset.score);
                } else if (sortBy === 'tier') {
                    return parseInt(a.dataset.tier) - parseInt(b.dataset.tier);
                }
            });

            findings.forEach(finding => container.appendChild(finding));
        });

        // Search functionality
        document.getElementById('search-box').addEventListener('input', (e) => {
            const query = e.target.value.toLowerCase();
            const findings = document.querySelectorAll('.finding');

            findings.forEach(finding => {
                const name = finding.dataset.name;
                const category = finding.dataset.category.toLowerCase();

                if (name.includes(query) || category.includes(query)) {
                    finding.classList.remove('hidden');
                } else {
                    finding.classList.add('hidden');
                }
            });
        });
    </script>
</body>
</html>
````

## File: examples/self-assessment/report-20251121-035845.md
````markdown
# ü§ñ AgentReady Assessment Report

**Repository**: agentready
**Path**: `/Users/jeder/repos/sk/agentready`
**Branch**: main
**Commit**: f968aaeb
**Date**: 2025-11-21 03:58:45

---

## üìä Summary

| Metric | Value |
|--------|-------|
| **Overall Score** | **80.0/100** |
| **Certification Level** | **Gold** |
| **Attributes Assessed** | 10/25 |
| **Attributes Skipped** | 15 |
| **Assessment Duration** | 0.2s |

### Languages Detected

- **Markdown**: 72 files
- **Python**: 36 files
- **YAML**: 9 files
- **Shell**: 5 files
- **JSON**: 3 files

### Repository Stats

- **Total Files**: 150
- **Total Lines**: 65,059

## üéñÔ∏è Certification Ladder

- üíé **Platinum** (90-100) 
- ü•á **Gold** (75-89) **‚Üí YOUR LEVEL ‚Üê**
- ü•à **Silver** (60-74) 
- ü•â **Bronze** (40-59) 
- ‚ö†Ô∏è **Needs Improvement** (0-39) 

## üìã Detailed Findings

### API Documentation

| Attribute | Tier | Status | Score |
|-----------|------|--------|-------|
| OpenAPI/Swagger Specifications | T3 | ‚äò not_applicable | ‚Äî |

### Build & Development

| Attribute | Tier | Status | Score |
|-----------|------|--------|-------|
| One-Command Build/Setup | T2 | ‚äò not_applicable | ‚Äî |
| Container/Virtualization Setup | T4 | ‚äò not_applicable | ‚Äî |

### Code Quality

| Attribute | Tier | Status | Score |
|-----------|------|--------|-------|
| Type Annotations | T1 | ‚úÖ pass | 100 |
| Cyclomatic Complexity Thresholds | T3 | ‚úÖ pass | 100 |
| Code Smell Elimination | T4 | ‚äò not_applicable | ‚Äî |

### Context Window Optimization

| Attribute | Tier | Status | Score |
|-----------|------|--------|-------|
| CLAUDE.md Configuration Files | T1 | ‚úÖ pass | 100 |
| Concise Structured Documentation | T2 | ‚äò not_applicable | ‚Äî |
| File Size Limits | T2 | ‚äò not_applicable | ‚Äî |

### Dependency Management

| Attribute | Tier | Status | Score |
|-----------|------|--------|-------|
| Lock Files for Reproducibility | T1 | ‚ùå fail | 0 |
| Dependency Freshness & Security | T2 | ‚äò not_applicable | ‚Äî |

#### ‚ùå Lock Files for Reproducibility

**Measured**: none (Threshold: at least one lock file)

**Evidence**:
- No lock files found

<details><summary><strong>üìù Remediation Steps</strong></summary>


Add lock file for dependency reproducibility

1. Use npm install, poetry lock, or equivalent to generate lock file

**Commands**:

```bash
npm install  # generates package-lock.json
```

</details>

### Documentation Standards

| Attribute | Tier | Status | Score |
|-----------|------|--------|-------|
| README Structure | T1 | ‚úÖ pass | 100 |
| Inline Documentation | T2 | ‚äò not_applicable | ‚Äî |
| Architecture Decision Records | T3 | ‚äò not_applicable | ‚Äî |

### Error Handling

| Attribute | Tier | Status | Score |
|-----------|------|--------|-------|
| Structured Logging | T3 | ‚äò not_applicable | ‚Äî |

### Git & Version Control

| Attribute | Tier | Status | Score |
|-----------|------|--------|-------|
| Conventional Commit Messages | T2 | ‚ùå fail | 0 |
| .gitignore Completeness | T2 | ‚úÖ pass | 100 |
| Issue & Pull Request Templates | T4 | ‚äò not_applicable | ‚Äî |

#### ‚ùå Conventional Commit Messages

**Measured**: not configured (Threshold: configured)

**Evidence**:
- No commitlint or husky configuration

<details><summary><strong>üìù Remediation Steps</strong></summary>


Configure conventional commits with commitlint

1. Install commitlint
2. Configure husky for commit-msg hook

**Commands**:

```bash
npm install --save-dev @commitlint/cli @commitlint/config-conventional husky
```

</details>

### Modularity

| Attribute | Tier | Status | Score |
|-----------|------|--------|-------|
| Semantic File & Directory Naming | T3 | ‚äò not_applicable | ‚Äî |

### Performance

| Attribute | Tier | Status | Score |
|-----------|------|--------|-------|
| Performance Benchmarks | T4 | ‚äò not_applicable | ‚Äî |

### Repository Structure

| Attribute | Tier | Status | Score |
|-----------|------|--------|-------|
| Standard Project Layouts | T1 | ‚úÖ pass | 100 |
| Separation of Concerns | T2 | ‚äò not_applicable | ‚Äî |

### Security

| Attribute | Tier | Status | Score |
|-----------|------|--------|-------|
| Security Scanning Automation | T4 | ‚äò not_applicable | ‚Äî |

### Testing & CI/CD

| Attribute | Tier | Status | Score |
|-----------|------|--------|-------|
| Test Coverage Requirements | T2 | ‚úÖ pass | 100 |
| Pre-commit Hooks & CI/CD Linting | T2 | ‚úÖ pass | 100 |

## üéØ Next Steps

**Priority Improvements** (highest impact first):

1. **Lock Files for Reproducibility** (Tier 1) - +10.0 points potential
   - Add lock file for dependency reproducibility
2. **Conventional Commit Messages** (Tier 2) - +3.0 points potential
   - Configure conventional commits with commitlint

---

## üìù Assessment Metadata

- **Tool Version**: AgentReady v1.0.0
- **Research Report**: Bundled version
- **Repository Snapshot**: f968aaeb4391bdd31cc815b69d0ce7d5c3d99bce
- **Assessment Duration**: 0.2s

ü§ñ Generated with [Claude Code](https://claude.com/claude-code)
````

## File: examples/self-assessment/report-20251121.html
````html
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>AgentReady Assessment - agentready</title>
    <style>
        /* Reset and Base Styles */
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif;
            line-height: 1.6;
            color: #333;
            background: #f5f5f5;
            padding: 20px;
        }

        .container {
            max-width: 1200px;
            margin: 0 auto;
            background: white;
            padding: 40px;
            border-radius: 8px;
            box-shadow: 0 2px 8px rgba(0,0,0,0.1);
        }

        /* Header */
        header {
            border-bottom: 3px solid #2563eb;
            padding-bottom: 20px;
            margin-bottom: 30px;
        }

        h1 {
            font-size: 2rem;
            color: #1e293b;
            margin-bottom: 10px;
        }

        .subtitle {
            color: #64748b;
            font-size: 1rem;
        }

        /* Summary Cards */
        .summary-grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(200px, 1fr));
            gap: 20px;
            margin: 30px 0;
        }

        .summary-card {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 20px;
            border-radius: 8px;
            text-align: center;
        }

        .summary-card.score {
            background: linear-gradient(135deg, #f093fb 0%, #f5576c 100%);
        }

        .summary-card.certification {
            background: linear-gradient(135deg, #4facfe 0%, #00f2fe 100%);
        }

        .summary-card.assessed {
            background: linear-gradient(135deg, #43e97b 0%, #38f9d7 100%);
        }

        .summary-card.duration {
            background: linear-gradient(135deg, #fa709a 0%, #fee140 100%);
        }

        .summary-card h3 {
            font-size: 0.9rem;
            font-weight: 500;
            opacity: 0.9;
            margin-bottom: 10px;
        }

        .summary-card .value {
            font-size: 2rem;
            font-weight: 700;
        }

        /* Controls */
        .controls {
            display: flex;
            flex-wrap: wrap;
            gap: 15px;
            margin: 30px 0;
            padding: 20px;
            background: #f8fafc;
            border-radius: 8px;
        }

        .control-group {
            display: flex;
            align-items: center;
            gap: 10px;
        }

        .control-group label {
            font-weight: 600;
            color: #475569;
            font-size: 0.9rem;
        }

        select, input[type="search"] {
            padding: 8px 12px;
            border: 1px solid #cbd5e1;
            border-radius: 6px;
            font-size: 0.9rem;
            background: white;
        }

        input[type="search"] {
            min-width: 250px;
        }

        .filter-btn {
            padding: 8px 16px;
            border: 2px solid #e2e8f0;
            background: white;
            border-radius: 6px;
            cursor: pointer;
            font-size: 0.9rem;
            transition: all 0.2s;
        }

        .filter-btn:hover {
            border-color: #2563eb;
            background: #eff6ff;
        }

        .filter-btn.active {
            background: #2563eb;
            color: white;
            border-color: #2563eb;
        }

        .badge {
            display: inline-block;
            padding: 2px 8px;
            border-radius: 12px;
            font-size: 0.75rem;
            font-weight: 600;
            margin-left: 5px;
        }

        /* Certification Ladder */
        .cert-ladder {
            display: flex;
            justify-content: space-between;
            margin: 30px 0;
            padding: 20px;
            background: #f8fafc;
            border-radius: 8px;
        }

        .cert-level {
            flex: 1;
            text-align: center;
            padding: 15px 10px;
            border-radius: 6px;
            margin: 0 5px;
            transition: all 0.3s;
        }

        .cert-level.platinum { background: linear-gradient(135deg, #e0e7ff, #c7d2fe); }
        .cert-level.gold { background: linear-gradient(135deg, #fef3c7, #fde68a); }
        .cert-level.silver { background: linear-gradient(135deg, #e5e7eb, #d1d5db); }
        .cert-level.bronze { background: linear-gradient(135deg, #fed7aa, #fdba74); }
        .cert-level.needs-improvement { background: linear-gradient(135deg, #fecaca, #fca5a5); }

        .cert-level.active {
            transform: scale(1.1);
            box-shadow: 0 4px 12px rgba(0,0,0,0.15);
            border: 3px solid #2563eb;
        }

        .cert-level h4 {
            font-size: 0.9rem;
            margin-bottom: 5px;
        }

        .cert-level .range {
            font-size: 0.8rem;
            opacity: 0.7;
        }

        /* Findings */
        .findings {
            margin: 30px 0;
        }

        .finding {
            border: 1px solid #e2e8f0;
            border-radius: 8px;
            margin-bottom: 15px;
            overflow: hidden;
            transition: all 0.2s;
        }

        .finding:hover {
            box-shadow: 0 4px 12px rgba(0,0,0,0.08);
        }

        .finding-header {
            padding: 15px 20px;
            background: #f8fafc;
            cursor: pointer;
            display: flex;
            justify-content: space-between;
            align-items: center;
        }

        .finding-header:hover {
            background: #f1f5f9;
        }

        .finding-title {
            display: flex;
            align-items: center;
            gap: 15px;
            flex: 1;
        }

        .status-icon {
            font-size: 1.5rem;
        }

        .finding-info h3 {
            font-size: 1rem;
            color: #1e293b;
            margin-bottom: 3px;
        }

        .finding-meta {
            font-size: 0.85rem;
            color: #64748b;
        }

        .tier-badge {
            padding: 4px 12px;
            border-radius: 12px;
            font-size: 0.75rem;
            font-weight: 600;
        }

        .tier-1 { background: #fee2e2; color: #991b1b; }
        .tier-2 { background: #fed7aa; color: #9a3412; }
        .tier-3 { background: #fef3c7; color: #92400e; }
        .tier-4 { background: #d1fae5; color: #065f46; }

        .score-display {
            font-size: 1.5rem;
            font-weight: 700;
            min-width: 80px;
            text-align: right;
        }

        .score-pass { color: #059669; }
        .score-fail { color: #dc2626; }
        .score-skip { color: #6b7280; }

        .finding-body {
            padding: 20px;
            display: none;
        }

        .finding.expanded .finding-body {
            display: block;
        }

        .finding-section {
            margin-bottom: 20px;
        }

        .finding-section h4 {
            font-size: 0.9rem;
            color: #475569;
            margin-bottom: 10px;
            text-transform: uppercase;
            letter-spacing: 0.05em;
        }

        .evidence-list, .remediation-steps {
            list-style: none;
            padding-left: 0;
        }

        .evidence-list li {
            padding: 8px 0;
            border-bottom: 1px solid #f1f5f9;
        }

        .evidence-list li:before {
            content: "‚ñ∏ ";
            color: #2563eb;
            font-weight: 700;
            margin-right: 8px;
        }

        .remediation-steps li {
            padding: 10px 0 10px 30px;
            position: relative;
        }

        .remediation-steps li:before {
            content: counter(step);
            counter-increment: step;
            position: absolute;
            left: 0;
            top: 10px;
            background: #2563eb;
            color: white;
            width: 20px;
            height: 20px;
            border-radius: 50%;
            display: flex;
            align-items: center;
            justify-content: center;
            font-size: 0.75rem;
            font-weight: 700;
        }

        .remediation-steps {
            counter-reset: step;
        }

        code {
            background: #f1f5f9;
            padding: 2px 6px;
            border-radius: 3px;
            font-family: 'Courier New', monospace;
            font-size: 0.9em;
        }

        pre {
            background: #1e293b;
            color: #e2e8f0;
            padding: 15px;
            border-radius: 6px;
            overflow-x: auto;
            margin: 10px 0;
        }

        pre code {
            background: none;
            color: inherit;
            padding: 0;
        }

        /* Footer */
        footer {
            margin-top: 50px;
            padding-top: 20px;
            border-top: 1px solid #e2e8f0;
            text-align: center;
            color: #64748b;
            font-size: 0.9rem;
        }

        /* Responsive */
        @media (max-width: 768px) {
            .container {
                padding: 20px;
            }

            .summary-grid {
                grid-template-columns: 1fr;
            }

            .controls {
                flex-direction: column;
            }

            input[type="search"] {
                min-width: 100%;
            }

            .cert-ladder {
                flex-direction: column;
            }

            .cert-level {
                margin: 5px 0;
            }
        }

        /* Hidden class */
        .hidden {
            display: none !important;
        }
    </style>
</head>
<body>
    <div class="container">
        <header>
            <h1>ü§ñ AgentReady Assessment Report</h1>
            <p class="subtitle">agentready ‚Ä¢ November 21, 2025 at 02:11</p>
        </header>

        <!-- Summary Cards -->
        <div class="summary-grid">
            <div class="summary-card score">
                <h3>Overall Score</h3>
                <div class="value">75.4</div>
            </div>
            <div class="summary-card certification">
                <h3>Certification</h3>
                <div class="value">Gold</div>
            </div>
            <div class="summary-card assessed">
                <h3>Assessed</h3>
                <div class="value">10/25</div>
            </div>
            <div class="summary-card duration">
                <h3>Duration</h3>
                <div class="value">0.2s</div>
            </div>
        </div>

        <!-- Certification Ladder -->
        <div class="cert-ladder">
            <div class="cert-level platinum ">
                <h4>üíé Platinum</h4>
                <p class="range">90-100</p>
            </div>
            <div class="cert-level gold active">
                <h4>ü•á Gold</h4>
                <p class="range">75-89</p>
            </div>
            <div class="cert-level silver ">
                <h4>ü•à Silver</h4>
                <p class="range">60-74</p>
            </div>
            <div class="cert-level bronze ">
                <h4>ü•â Bronze</h4>
                <p class="range">40-59</p>
            </div>
            <div class="cert-level needs-improvement ">
                <h4>‚ö†Ô∏è Needs Work</h4>
                <p class="range">0-39</p>
            </div>
        </div>

        <!-- Controls -->
        <div class="controls">
            <div class="control-group">
                <label>Filter:</label>
                <button class="filter-btn active" data-filter="all">All <span class="badge">25</span></button>
                <button class="filter-btn" data-filter="pass">Pass <span class="badge">7</span></button>
                <button class="filter-btn" data-filter="fail">Fail <span class="badge">3</span></button>
                <button class="filter-btn" data-filter="skipped">Skipped <span class="badge">15</span></button>
            </div>

            <div class="control-group">
                <label>Sort:</label>
                <select id="sort-select">
                    <option value="category">Category</option>
                    <option value="score-desc">Score (High to Low)</option>
                    <option value="score-asc">Score (Low to High)</option>
                    <option value="tier">Tier (Essential First)</option>
                </select>
            </div>

            <div class="control-group">
                <input type="search" id="search-box" placeholder="Search attributes...">
            </div>
        </div>

        <!-- Findings -->
        <div class="findings" id="findings-container">
            
            <div class="finding"
                 data-status="pass"
                 data-tier="1"
                 data-category="Context Window Optimization"
                 data-score="100.0"
                 data-name="claude.md configuration files">

                <div class="finding-header" onclick="toggleFinding(this)">
                    <div class="finding-title">
                        <span class="status-icon">
                            ‚úÖ
                            
                        </span>
                        <div class="finding-info">
                            <h3>CLAUDE.md Configuration Files</h3>
                            <div class="finding-meta">
                                Context Window Optimization ‚Ä¢
                                <span class="tier-badge tier-1">Tier 1</span>
                                 ‚Ä¢ present
                            </div>
                        </div>
                    </div>
                    
                    <div class="score-display score-pass">
                        100
                    </div>
                    
                </div>

                <div class="finding-body">
                    
                    <div class="finding-section">
                        <h4>Evidence</h4>
                        <ul class="evidence-list">
                            
                            <li>CLAUDE.md found at /Users/jeder/repos/sk/agentready/CLAUDE.md</li>
                            
                        </ul>
                    </div>
                    

                    

                    
                </div>
            </div>
            
            <div class="finding"
                 data-status="pass"
                 data-tier="1"
                 data-category="Documentation Standards"
                 data-score="100.0"
                 data-name="readme structure">

                <div class="finding-header" onclick="toggleFinding(this)">
                    <div class="finding-title">
                        <span class="status-icon">
                            ‚úÖ
                            
                        </span>
                        <div class="finding-info">
                            <h3>README Structure</h3>
                            <div class="finding-meta">
                                Documentation Standards ‚Ä¢
                                <span class="tier-badge tier-1">Tier 1</span>
                                 ‚Ä¢ 3/3 sections
                            </div>
                        </div>
                    </div>
                    
                    <div class="score-display score-pass">
                        100
                    </div>
                    
                </div>

                <div class="finding-body">
                    
                    <div class="finding-section">
                        <h4>Evidence</h4>
                        <ul class="evidence-list">
                            
                            <li>Found 3/3 essential sections</li>
                            
                            <li>Installation: ‚úì</li>
                            
                            <li>Usage: ‚úì</li>
                            
                            <li>Development: ‚úì</li>
                            
                        </ul>
                    </div>
                    

                    

                    
                </div>
            </div>
            
            <div class="finding"
                 data-status="pass"
                 data-tier="1"
                 data-category="Code Quality"
                 data-score="100.0"
                 data-name="type annotations">

                <div class="finding-header" onclick="toggleFinding(this)">
                    <div class="finding-title">
                        <span class="status-icon">
                            ‚úÖ
                            
                        </span>
                        <div class="finding-info">
                            <h3>Type Annotations</h3>
                            <div class="finding-meta">
                                Code Quality ‚Ä¢
                                <span class="tier-badge tier-1">Tier 1</span>
                                 ‚Ä¢ 95.9%
                            </div>
                        </div>
                    </div>
                    
                    <div class="score-display score-pass">
                        100
                    </div>
                    
                </div>

                <div class="finding-body">
                    
                    <div class="finding-section">
                        <h4>Evidence</h4>
                        <ul class="evidence-list">
                            
                            <li>Typed functions: 142/148</li>
                            
                            <li>Coverage: 95.9%</li>
                            
                        </ul>
                    </div>
                    

                    

                    
                </div>
            </div>
            
            <div class="finding"
                 data-status="pass"
                 data-tier="1"
                 data-category="Repository Structure"
                 data-score="100.0"
                 data-name="standard project layouts">

                <div class="finding-header" onclick="toggleFinding(this)">
                    <div class="finding-title">
                        <span class="status-icon">
                            ‚úÖ
                            
                        </span>
                        <div class="finding-info">
                            <h3>Standard Project Layouts</h3>
                            <div class="finding-meta">
                                Repository Structure ‚Ä¢
                                <span class="tier-badge tier-1">Tier 1</span>
                                 ‚Ä¢ 2/2 directories
                            </div>
                        </div>
                    </div>
                    
                    <div class="score-display score-pass">
                        100
                    </div>
                    
                </div>

                <div class="finding-body">
                    
                    <div class="finding-section">
                        <h4>Evidence</h4>
                        <ul class="evidence-list">
                            
                            <li>Found 2/2 standard directories</li>
                            
                            <li>src/: ‚úì</li>
                            
                            <li>tests/: ‚úì</li>
                            
                        </ul>
                    </div>
                    

                    

                    
                </div>
            </div>
            
            <div class="finding"
                 data-status="fail"
                 data-tier="1"
                 data-category="Dependency Management"
                 data-score="0"
                 data-name="lock files for reproducibility">

                <div class="finding-header" onclick="toggleFinding(this)">
                    <div class="finding-title">
                        <span class="status-icon">
                            ‚ùå
                            
                        </span>
                        <div class="finding-info">
                            <h3>Lock Files for Reproducibility</h3>
                            <div class="finding-meta">
                                Dependency Management ‚Ä¢
                                <span class="tier-badge tier-1">Tier 1</span>
                                 ‚Ä¢ none
                            </div>
                        </div>
                    </div>
                    
                    <div class="score-display score-fail">
                        0
                    </div>
                    
                </div>

                <div class="finding-body">
                    
                    <div class="finding-section">
                        <h4>Evidence</h4>
                        <ul class="evidence-list">
                            
                            <li>No lock files found</li>
                            
                        </ul>
                    </div>
                    

                    
                    <div class="finding-section">
                        <h4>Remediation</h4>
                        <p><strong>Add lock file for dependency reproducibility</strong></p>

                        
                        <ol class="remediation-steps">
                            
                            <li>Use npm install, poetry lock, or equivalent to generate lock file</li>
                            
                        </ol>
                        

                        
                        <h4 style="margin-top: 15px;">Commands</h4>
                        <pre><code>npm install  # generates package-lock.json</code></pre>
                        

                        
                    </div>
                    

                    
                </div>
            </div>
            
            <div class="finding"
                 data-status="pass"
                 data-tier="2"
                 data-category="Testing & CI/CD"
                 data-score="100.0"
                 data-name="test coverage requirements">

                <div class="finding-header" onclick="toggleFinding(this)">
                    <div class="finding-title">
                        <span class="status-icon">
                            ‚úÖ
                            
                        </span>
                        <div class="finding-info">
                            <h3>Test Coverage Requirements</h3>
                            <div class="finding-meta">
                                Testing & CI/CD ‚Ä¢
                                <span class="tier-badge tier-2">Tier 2</span>
                                 ‚Ä¢ configured
                            </div>
                        </div>
                    </div>
                    
                    <div class="score-display score-pass">
                        100
                    </div>
                    
                </div>

                <div class="finding-body">
                    
                    <div class="finding-section">
                        <h4>Evidence</h4>
                        <ul class="evidence-list">
                            
                            <li>Coverage configuration found</li>
                            
                            <li>pytest-cov dependency present</li>
                            
                        </ul>
                    </div>
                    

                    

                    
                </div>
            </div>
            
            <div class="finding"
                 data-status="fail"
                 data-tier="2"
                 data-category="Testing & CI/CD"
                 data-score="0"
                 data-name="pre-commit hooks & ci/cd linting">

                <div class="finding-header" onclick="toggleFinding(this)">
                    <div class="finding-title">
                        <span class="status-icon">
                            ‚ùå
                            
                        </span>
                        <div class="finding-info">
                            <h3>Pre-commit Hooks & CI/CD Linting</h3>
                            <div class="finding-meta">
                                Testing & CI/CD ‚Ä¢
                                <span class="tier-badge tier-2">Tier 2</span>
                                 ‚Ä¢ not configured
                            </div>
                        </div>
                    </div>
                    
                    <div class="score-display score-fail">
                        0
                    </div>
                    
                </div>

                <div class="finding-body">
                    
                    <div class="finding-section">
                        <h4>Evidence</h4>
                        <ul class="evidence-list">
                            
                            <li>.pre-commit-config.yaml not found</li>
                            
                        </ul>
                    </div>
                    

                    
                    <div class="finding-section">
                        <h4>Remediation</h4>
                        <p><strong>Configure pre-commit hooks for automated code quality checks</strong></p>

                        
                        <ol class="remediation-steps">
                            
                            <li>Install pre-commit framework</li>
                            
                            <li>Create .pre-commit-config.yaml</li>
                            
                            <li>Add hooks for linting and formatting</li>
                            
                            <li>Install hooks: pre-commit install</li>
                            
                            <li>Run on all files: pre-commit run --all-files</li>
                            
                        </ol>
                        

                        
                        <h4 style="margin-top: 15px;">Commands</h4>
                        <pre><code>pip install pre-commit
pre-commit install
pre-commit run --all-files</code></pre>
                        

                        
                        <h4 style="margin-top: 15px;">Examples</h4>
                        
                        <pre><code># .pre-commit-config.yaml
repos:
  - repo: https://github.com/pre-commit/pre-commit-hooks
    rev: v4.4.0
    hooks:
      - id: trailing-whitespace
      - id: end-of-file-fixer
      - id: check-yaml
      - id: check-added-large-files

  - repo: https://github.com/psf/black
    rev: 23.3.0
    hooks:
      - id: black

  - repo: https://github.com/pycqa/isort
    rev: 5.12.0
    hooks:
      - id: isort
</code></pre>
                        
                        
                    </div>
                    

                    
                </div>
            </div>
            
            <div class="finding"
                 data-status="fail"
                 data-tier="2"
                 data-category="Git & Version Control"
                 data-score="0"
                 data-name="conventional commit messages">

                <div class="finding-header" onclick="toggleFinding(this)">
                    <div class="finding-title">
                        <span class="status-icon">
                            ‚ùå
                            
                        </span>
                        <div class="finding-info">
                            <h3>Conventional Commit Messages</h3>
                            <div class="finding-meta">
                                Git & Version Control ‚Ä¢
                                <span class="tier-badge tier-2">Tier 2</span>
                                 ‚Ä¢ not configured
                            </div>
                        </div>
                    </div>
                    
                    <div class="score-display score-fail">
                        0
                    </div>
                    
                </div>

                <div class="finding-body">
                    
                    <div class="finding-section">
                        <h4>Evidence</h4>
                        <ul class="evidence-list">
                            
                            <li>No commitlint or husky configuration</li>
                            
                        </ul>
                    </div>
                    

                    
                    <div class="finding-section">
                        <h4>Remediation</h4>
                        <p><strong>Configure conventional commits with commitlint</strong></p>

                        
                        <ol class="remediation-steps">
                            
                            <li>Install commitlint</li>
                            
                            <li>Configure husky for commit-msg hook</li>
                            
                        </ol>
                        

                        
                        <h4 style="margin-top: 15px;">Commands</h4>
                        <pre><code>npm install --save-dev @commitlint/cli @commitlint/config-conventional husky</code></pre>
                        

                        
                    </div>
                    

                    
                </div>
            </div>
            
            <div class="finding"
                 data-status="pass"
                 data-tier="2"
                 data-category="Git & Version Control"
                 data-score="100.0"
                 data-name=".gitignore completeness">

                <div class="finding-header" onclick="toggleFinding(this)">
                    <div class="finding-title">
                        <span class="status-icon">
                            ‚úÖ
                            
                        </span>
                        <div class="finding-info">
                            <h3>.gitignore Completeness</h3>
                            <div class="finding-meta">
                                Git & Version Control ‚Ä¢
                                <span class="tier-badge tier-2">Tier 2</span>
                                 ‚Ä¢ 580 bytes
                            </div>
                        </div>
                    </div>
                    
                    <div class="score-display score-pass">
                        100
                    </div>
                    
                </div>

                <div class="finding-body">
                    
                    <div class="finding-section">
                        <h4>Evidence</h4>
                        <ul class="evidence-list">
                            
                            <li>.gitignore found (580 bytes)</li>
                            
                        </ul>
                    </div>
                    

                    

                    
                </div>
            </div>
            
            <div class="finding"
                 data-status="pass"
                 data-tier="3"
                 data-category="Code Quality"
                 data-score="100.0"
                 data-name="cyclomatic complexity thresholds">

                <div class="finding-header" onclick="toggleFinding(this)">
                    <div class="finding-title">
                        <span class="status-icon">
                            ‚úÖ
                            
                        </span>
                        <div class="finding-info">
                            <h3>Cyclomatic Complexity Thresholds</h3>
                            <div class="finding-meta">
                                Code Quality ‚Ä¢
                                <span class="tier-badge tier-3">Tier 3</span>
                                 ‚Ä¢ 3.1
                            </div>
                        </div>
                    </div>
                    
                    <div class="score-display score-pass">
                        100
                    </div>
                    
                </div>

                <div class="finding-body">
                    
                    <div class="finding-section">
                        <h4>Evidence</h4>
                        <ul class="evidence-list">
                            
                            <li>Average cyclomatic complexity: 3.1</li>
                            
                        </ul>
                    </div>
                    

                    

                    
                </div>
            </div>
            
            <div class="finding"
                 data-status="not_applicable"
                 data-tier="2"
                 data-category="Build & Development"
                 data-score="0"
                 data-name="one-command build/setup">

                <div class="finding-header" onclick="toggleFinding(this)">
                    <div class="finding-title">
                        <span class="status-icon">
                            ‚äò
                            
                        </span>
                        <div class="finding-info">
                            <h3>One-Command Build/Setup</h3>
                            <div class="finding-meta">
                                Build & Development ‚Ä¢
                                <span class="tier-badge tier-2">Tier 2</span>
                                
                            </div>
                        </div>
                    </div>
                    
                    <div class="score-display score-skip">‚Äî</div>
                    
                </div>

                <div class="finding-body">
                    
                    <div class="finding-section">
                        <h4>Evidence</h4>
                        <ul class="evidence-list">
                            
                            <li>One-Command Build/Setup assessment not yet implemented</li>
                            
                        </ul>
                    </div>
                    

                    

                    
                </div>
            </div>
            
            <div class="finding"
                 data-status="not_applicable"
                 data-tier="2"
                 data-category="Context Window Optimization"
                 data-score="0"
                 data-name="concise structured documentation">

                <div class="finding-header" onclick="toggleFinding(this)">
                    <div class="finding-title">
                        <span class="status-icon">
                            ‚äò
                            
                        </span>
                        <div class="finding-info">
                            <h3>Concise Structured Documentation</h3>
                            <div class="finding-meta">
                                Context Window Optimization ‚Ä¢
                                <span class="tier-badge tier-2">Tier 2</span>
                                
                            </div>
                        </div>
                    </div>
                    
                    <div class="score-display score-skip">‚Äî</div>
                    
                </div>

                <div class="finding-body">
                    
                    <div class="finding-section">
                        <h4>Evidence</h4>
                        <ul class="evidence-list">
                            
                            <li>Concise Structured Documentation assessment not yet implemented</li>
                            
                        </ul>
                    </div>
                    

                    

                    
                </div>
            </div>
            
            <div class="finding"
                 data-status="not_applicable"
                 data-tier="2"
                 data-category="Documentation Standards"
                 data-score="0"
                 data-name="inline documentation">

                <div class="finding-header" onclick="toggleFinding(this)">
                    <div class="finding-title">
                        <span class="status-icon">
                            ‚äò
                            
                        </span>
                        <div class="finding-info">
                            <h3>Inline Documentation</h3>
                            <div class="finding-meta">
                                Documentation Standards ‚Ä¢
                                <span class="tier-badge tier-2">Tier 2</span>
                                
                            </div>
                        </div>
                    </div>
                    
                    <div class="score-display score-skip">‚Äî</div>
                    
                </div>

                <div class="finding-body">
                    
                    <div class="finding-section">
                        <h4>Evidence</h4>
                        <ul class="evidence-list">
                            
                            <li>Inline Documentation assessment not yet implemented</li>
                            
                        </ul>
                    </div>
                    

                    

                    
                </div>
            </div>
            
            <div class="finding"
                 data-status="not_applicable"
                 data-tier="2"
                 data-category="Context Window Optimization"
                 data-score="0"
                 data-name="file size limits">

                <div class="finding-header" onclick="toggleFinding(this)">
                    <div class="finding-title">
                        <span class="status-icon">
                            ‚äò
                            
                        </span>
                        <div class="finding-info">
                            <h3>File Size Limits</h3>
                            <div class="finding-meta">
                                Context Window Optimization ‚Ä¢
                                <span class="tier-badge tier-2">Tier 2</span>
                                
                            </div>
                        </div>
                    </div>
                    
                    <div class="score-display score-skip">‚Äî</div>
                    
                </div>

                <div class="finding-body">
                    
                    <div class="finding-section">
                        <h4>Evidence</h4>
                        <ul class="evidence-list">
                            
                            <li>File Size Limits assessment not yet implemented</li>
                            
                        </ul>
                    </div>
                    

                    

                    
                </div>
            </div>
            
            <div class="finding"
                 data-status="not_applicable"
                 data-tier="2"
                 data-category="Dependency Management"
                 data-score="0"
                 data-name="dependency freshness & security">

                <div class="finding-header" onclick="toggleFinding(this)">
                    <div class="finding-title">
                        <span class="status-icon">
                            ‚äò
                            
                        </span>
                        <div class="finding-info">
                            <h3>Dependency Freshness & Security</h3>
                            <div class="finding-meta">
                                Dependency Management ‚Ä¢
                                <span class="tier-badge tier-2">Tier 2</span>
                                
                            </div>
                        </div>
                    </div>
                    
                    <div class="score-display score-skip">‚Äî</div>
                    
                </div>

                <div class="finding-body">
                    
                    <div class="finding-section">
                        <h4>Evidence</h4>
                        <ul class="evidence-list">
                            
                            <li>Dependency Freshness & Security assessment not yet implemented</li>
                            
                        </ul>
                    </div>
                    

                    

                    
                </div>
            </div>
            
            <div class="finding"
                 data-status="not_applicable"
                 data-tier="2"
                 data-category="Repository Structure"
                 data-score="0"
                 data-name="separation of concerns">

                <div class="finding-header" onclick="toggleFinding(this)">
                    <div class="finding-title">
                        <span class="status-icon">
                            ‚äò
                            
                        </span>
                        <div class="finding-info">
                            <h3>Separation of Concerns</h3>
                            <div class="finding-meta">
                                Repository Structure ‚Ä¢
                                <span class="tier-badge tier-2">Tier 2</span>
                                
                            </div>
                        </div>
                    </div>
                    
                    <div class="score-display score-skip">‚Äî</div>
                    
                </div>

                <div class="finding-body">
                    
                    <div class="finding-section">
                        <h4>Evidence</h4>
                        <ul class="evidence-list">
                            
                            <li>Separation of Concerns assessment not yet implemented</li>
                            
                        </ul>
                    </div>
                    

                    

                    
                </div>
            </div>
            
            <div class="finding"
                 data-status="not_applicable"
                 data-tier="3"
                 data-category="Error Handling"
                 data-score="0"
                 data-name="structured logging">

                <div class="finding-header" onclick="toggleFinding(this)">
                    <div class="finding-title">
                        <span class="status-icon">
                            ‚äò
                            
                        </span>
                        <div class="finding-info">
                            <h3>Structured Logging</h3>
                            <div class="finding-meta">
                                Error Handling ‚Ä¢
                                <span class="tier-badge tier-3">Tier 3</span>
                                
                            </div>
                        </div>
                    </div>
                    
                    <div class="score-display score-skip">‚Äî</div>
                    
                </div>

                <div class="finding-body">
                    
                    <div class="finding-section">
                        <h4>Evidence</h4>
                        <ul class="evidence-list">
                            
                            <li>Structured Logging assessment not yet implemented</li>
                            
                        </ul>
                    </div>
                    

                    

                    
                </div>
            </div>
            
            <div class="finding"
                 data-status="not_applicable"
                 data-tier="3"
                 data-category="API Documentation"
                 data-score="0"
                 data-name="openapi/swagger specifications">

                <div class="finding-header" onclick="toggleFinding(this)">
                    <div class="finding-title">
                        <span class="status-icon">
                            ‚äò
                            
                        </span>
                        <div class="finding-info">
                            <h3>OpenAPI/Swagger Specifications</h3>
                            <div class="finding-meta">
                                API Documentation ‚Ä¢
                                <span class="tier-badge tier-3">Tier 3</span>
                                
                            </div>
                        </div>
                    </div>
                    
                    <div class="score-display score-skip">‚Äî</div>
                    
                </div>

                <div class="finding-body">
                    
                    <div class="finding-section">
                        <h4>Evidence</h4>
                        <ul class="evidence-list">
                            
                            <li>OpenAPI/Swagger Specifications assessment not yet implemented</li>
                            
                        </ul>
                    </div>
                    

                    

                    
                </div>
            </div>
            
            <div class="finding"
                 data-status="not_applicable"
                 data-tier="3"
                 data-category="Documentation Standards"
                 data-score="0"
                 data-name="architecture decision records">

                <div class="finding-header" onclick="toggleFinding(this)">
                    <div class="finding-title">
                        <span class="status-icon">
                            ‚äò
                            
                        </span>
                        <div class="finding-info">
                            <h3>Architecture Decision Records</h3>
                            <div class="finding-meta">
                                Documentation Standards ‚Ä¢
                                <span class="tier-badge tier-3">Tier 3</span>
                                
                            </div>
                        </div>
                    </div>
                    
                    <div class="score-display score-skip">‚Äî</div>
                    
                </div>

                <div class="finding-body">
                    
                    <div class="finding-section">
                        <h4>Evidence</h4>
                        <ul class="evidence-list">
                            
                            <li>Architecture Decision Records assessment not yet implemented</li>
                            
                        </ul>
                    </div>
                    

                    

                    
                </div>
            </div>
            
            <div class="finding"
                 data-status="not_applicable"
                 data-tier="3"
                 data-category="Modularity"
                 data-score="0"
                 data-name="semantic file & directory naming">

                <div class="finding-header" onclick="toggleFinding(this)">
                    <div class="finding-title">
                        <span class="status-icon">
                            ‚äò
                            
                        </span>
                        <div class="finding-info">
                            <h3>Semantic File & Directory Naming</h3>
                            <div class="finding-meta">
                                Modularity ‚Ä¢
                                <span class="tier-badge tier-3">Tier 3</span>
                                
                            </div>
                        </div>
                    </div>
                    
                    <div class="score-display score-skip">‚Äî</div>
                    
                </div>

                <div class="finding-body">
                    
                    <div class="finding-section">
                        <h4>Evidence</h4>
                        <ul class="evidence-list">
                            
                            <li>Semantic File & Directory Naming assessment not yet implemented</li>
                            
                        </ul>
                    </div>
                    

                    

                    
                </div>
            </div>
            
            <div class="finding"
                 data-status="not_applicable"
                 data-tier="4"
                 data-category="Security"
                 data-score="0"
                 data-name="security scanning automation">

                <div class="finding-header" onclick="toggleFinding(this)">
                    <div class="finding-title">
                        <span class="status-icon">
                            ‚äò
                            
                        </span>
                        <div class="finding-info">
                            <h3>Security Scanning Automation</h3>
                            <div class="finding-meta">
                                Security ‚Ä¢
                                <span class="tier-badge tier-4">Tier 4</span>
                                
                            </div>
                        </div>
                    </div>
                    
                    <div class="score-display score-skip">‚Äî</div>
                    
                </div>

                <div class="finding-body">
                    
                    <div class="finding-section">
                        <h4>Evidence</h4>
                        <ul class="evidence-list">
                            
                            <li>Security Scanning Automation assessment not yet implemented</li>
                            
                        </ul>
                    </div>
                    

                    

                    
                </div>
            </div>
            
            <div class="finding"
                 data-status="not_applicable"
                 data-tier="4"
                 data-category="Performance"
                 data-score="0"
                 data-name="performance benchmarks">

                <div class="finding-header" onclick="toggleFinding(this)">
                    <div class="finding-title">
                        <span class="status-icon">
                            ‚äò
                            
                        </span>
                        <div class="finding-info">
                            <h3>Performance Benchmarks</h3>
                            <div class="finding-meta">
                                Performance ‚Ä¢
                                <span class="tier-badge tier-4">Tier 4</span>
                                
                            </div>
                        </div>
                    </div>
                    
                    <div class="score-display score-skip">‚Äî</div>
                    
                </div>

                <div class="finding-body">
                    
                    <div class="finding-section">
                        <h4>Evidence</h4>
                        <ul class="evidence-list">
                            
                            <li>Performance Benchmarks assessment not yet implemented</li>
                            
                        </ul>
                    </div>
                    

                    

                    
                </div>
            </div>
            
            <div class="finding"
                 data-status="not_applicable"
                 data-tier="4"
                 data-category="Code Quality"
                 data-score="0"
                 data-name="code smell elimination">

                <div class="finding-header" onclick="toggleFinding(this)">
                    <div class="finding-title">
                        <span class="status-icon">
                            ‚äò
                            
                        </span>
                        <div class="finding-info">
                            <h3>Code Smell Elimination</h3>
                            <div class="finding-meta">
                                Code Quality ‚Ä¢
                                <span class="tier-badge tier-4">Tier 4</span>
                                
                            </div>
                        </div>
                    </div>
                    
                    <div class="score-display score-skip">‚Äî</div>
                    
                </div>

                <div class="finding-body">
                    
                    <div class="finding-section">
                        <h4>Evidence</h4>
                        <ul class="evidence-list">
                            
                            <li>Code Smell Elimination assessment not yet implemented</li>
                            
                        </ul>
                    </div>
                    

                    

                    
                </div>
            </div>
            
            <div class="finding"
                 data-status="not_applicable"
                 data-tier="4"
                 data-category="Git & Version Control"
                 data-score="0"
                 data-name="issue & pull request templates">

                <div class="finding-header" onclick="toggleFinding(this)">
                    <div class="finding-title">
                        <span class="status-icon">
                            ‚äò
                            
                        </span>
                        <div class="finding-info">
                            <h3>Issue & Pull Request Templates</h3>
                            <div class="finding-meta">
                                Git & Version Control ‚Ä¢
                                <span class="tier-badge tier-4">Tier 4</span>
                                
                            </div>
                        </div>
                    </div>
                    
                    <div class="score-display score-skip">‚Äî</div>
                    
                </div>

                <div class="finding-body">
                    
                    <div class="finding-section">
                        <h4>Evidence</h4>
                        <ul class="evidence-list">
                            
                            <li>Issue & Pull Request Templates assessment not yet implemented</li>
                            
                        </ul>
                    </div>
                    

                    

                    
                </div>
            </div>
            
            <div class="finding"
                 data-status="not_applicable"
                 data-tier="4"
                 data-category="Build & Development"
                 data-score="0"
                 data-name="container/virtualization setup">

                <div class="finding-header" onclick="toggleFinding(this)">
                    <div class="finding-title">
                        <span class="status-icon">
                            ‚äò
                            
                        </span>
                        <div class="finding-info">
                            <h3>Container/Virtualization Setup</h3>
                            <div class="finding-meta">
                                Build & Development ‚Ä¢
                                <span class="tier-badge tier-4">Tier 4</span>
                                
                            </div>
                        </div>
                    </div>
                    
                    <div class="score-display score-skip">‚Äî</div>
                    
                </div>

                <div class="finding-body">
                    
                    <div class="finding-section">
                        <h4>Evidence</h4>
                        <ul class="evidence-list">
                            
                            <li>Container/Virtualization Setup assessment not yet implemented</li>
                            
                        </ul>
                    </div>
                    

                    

                    
                </div>
            </div>
            
        </div>

        <footer>
            <p>Generated by <strong>AgentReady v1.0.0</strong></p>
            <p>Repository: /Users/jeder/repos/sk/agentready ‚Ä¢ Branch: 001-agentready-scorer ‚Ä¢ Commit: adfc4c82</p>
            <p style="margin-top: 10px; font-size: 0.85rem;">
                ü§ñ Generated with <a href="https://claude.com/claude-code" style="color: #2563eb;">Claude Code</a>
            </p>
        </footer>
    </div>

    <script>
        // Embedded assessment data
        const ASSESSMENT = {"repository": {"path": "/Users/jeder/repos/sk/agentready", "name": "agentready", "url": null, "branch": "001-agentready-scorer", "commit_hash": "adfc4c823a7b25d5b1404c9b994461e290257dce", "languages": {"Markdown": 36, "Shell": 5, "Python": 31}, "total_files": 79, "total_lines": 13852}, "timestamp": "2025-11-21T02:11:05.766923", "overall_score": 75.4, "certification_level": "Gold", "attributes_assessed": 10, "attributes_skipped": 15, "attributes_total": 25, "findings": [{"attribute": {"id": "claude_md_file", "name": "CLAUDE.md Configuration Files", "category": "Context Window Optimization", "tier": 1, "description": "Project-specific configuration for Claude Code", "criteria": "CLAUDE.md file exists in repository root", "default_weight": 0.1}, "status": "pass", "score": 100.0, "measured_value": "present", "threshold": "present", "evidence": ["CLAUDE.md found at /Users/jeder/repos/sk/agentready/CLAUDE.md"], "remediation": null, "error_message": null}, {"attribute": {"id": "readme_structure", "name": "README Structure", "category": "Documentation Standards", "tier": 1, "description": "Well-structured README with key sections", "criteria": "README.md with installation, usage, and development sections", "default_weight": 0.1}, "status": "pass", "score": 100.0, "measured_value": "3/3 sections", "threshold": "3/3 sections", "evidence": ["Found 3/3 essential sections", "Installation: \u2713", "Usage: \u2713", "Development: \u2713"], "remediation": null, "error_message": null}, {"attribute": {"id": "type_annotations", "name": "Type Annotations", "category": "Code Quality", "tier": 1, "description": "Type hints in function signatures", "criteria": ">80% of functions have type annotations", "default_weight": 0.1}, "status": "pass", "score": 100.0, "measured_value": "95.9%", "threshold": "\u226580%", "evidence": ["Typed functions: 142/148", "Coverage: 95.9%"], "remediation": null, "error_message": null}, {"attribute": {"id": "standard_layout", "name": "Standard Project Layouts", "category": "Repository Structure", "tier": 1, "description": "Follows standard project structure for language", "criteria": "Standard directories (src/, tests/, docs/) present", "default_weight": 0.1}, "status": "pass", "score": 100.0, "measured_value": "2/2 directories", "threshold": "2/2 directories", "evidence": ["Found 2/2 standard directories", "src/: \u2713", "tests/: \u2713"], "remediation": null, "error_message": null}, {"attribute": {"id": "lock_files", "name": "Lock Files for Reproducibility", "category": "Dependency Management", "tier": 1, "description": "Lock files present for dependency pinning", "criteria": "package-lock.json, yarn.lock, poetry.lock, or requirements.txt with versions", "default_weight": 0.1}, "status": "fail", "score": 0.0, "measured_value": "none", "threshold": "at least one lock file", "evidence": ["No lock files found"], "remediation": {"summary": "Add lock file for dependency reproducibility", "steps": ["Use npm install, poetry lock, or equivalent to generate lock file"], "tools": [], "commands": ["npm install  # generates package-lock.json"], "examples": [], "citations": []}, "error_message": null}, {"attribute": {"id": "test_coverage", "name": "Test Coverage Requirements", "category": "Testing & CI/CD", "tier": 2, "description": "Test coverage thresholds configured and enforced", "criteria": ">80% code coverage", "default_weight": 0.03}, "status": "pass", "score": 100.0, "measured_value": "configured", "threshold": "configured with >80% threshold", "evidence": ["Coverage configuration found", "pytest-cov dependency present"], "remediation": null, "error_message": null}, {"attribute": {"id": "precommit_hooks", "name": "Pre-commit Hooks & CI/CD Linting", "category": "Testing & CI/CD", "tier": 2, "description": "Pre-commit hooks configured for linting and formatting", "criteria": ".pre-commit-config.yaml exists", "default_weight": 0.03}, "status": "fail", "score": 0.0, "measured_value": "not configured", "threshold": "configured", "evidence": [".pre-commit-config.yaml not found"], "remediation": {"summary": "Configure pre-commit hooks for automated code quality checks", "steps": ["Install pre-commit framework", "Create .pre-commit-config.yaml", "Add hooks for linting and formatting", "Install hooks: pre-commit install", "Run on all files: pre-commit run --all-files"], "tools": ["pre-commit"], "commands": ["pip install pre-commit", "pre-commit install", "pre-commit run --all-files"], "examples": ["# .pre-commit-config.yaml\nrepos:\n  - repo: https://github.com/pre-commit/pre-commit-hooks\n    rev: v4.4.0\n    hooks:\n      - id: trailing-whitespace\n      - id: end-of-file-fixer\n      - id: check-yaml\n      - id: check-added-large-files\n\n  - repo: https://github.com/psf/black\n    rev: 23.3.0\n    hooks:\n      - id: black\n\n  - repo: https://github.com/pycqa/isort\n    rev: 5.12.0\n    hooks:\n      - id: isort\n"], "citations": [{"source": "pre-commit.com", "title": "Pre-commit Framework", "url": "https://pre-commit.com/", "relevance": "Official pre-commit documentation"}]}, "error_message": null}, {"attribute": {"id": "conventional_commits", "name": "Conventional Commit Messages", "category": "Git & Version Control", "tier": 2, "description": "Follows conventional commit format", "criteria": "\u226580% of recent commits follow convention", "default_weight": 0.03}, "status": "fail", "score": 0.0, "measured_value": "not configured", "threshold": "configured", "evidence": ["No commitlint or husky configuration"], "remediation": {"summary": "Configure conventional commits with commitlint", "steps": ["Install commitlint", "Configure husky for commit-msg hook"], "tools": ["commitlint", "husky"], "commands": ["npm install --save-dev @commitlint/cli @commitlint/config-conventional husky"], "examples": [], "citations": []}, "error_message": null}, {"attribute": {"id": "gitignore_completeness", "name": ".gitignore Completeness", "category": "Git & Version Control", "tier": 2, "description": "Comprehensive .gitignore file", "criteria": ".gitignore exists and covers common patterns", "default_weight": 0.03}, "status": "pass", "score": 100.0, "measured_value": "580 bytes", "threshold": ">50 bytes", "evidence": [".gitignore found (580 bytes)"], "remediation": null, "error_message": null}, {"attribute": {"id": "cyclomatic_complexity", "name": "Cyclomatic Complexity Thresholds", "category": "Code Quality", "tier": 3, "description": "Cyclomatic complexity thresholds enforced", "criteria": "Average complexity <10, no functions >15", "default_weight": 0.03}, "status": "pass", "score": 100.0, "measured_value": "3.1", "threshold": "<10.0", "evidence": ["Average cyclomatic complexity: 3.1"], "remediation": null, "error_message": null}, {"attribute": {"id": "one_command_setup", "name": "One-Command Build/Setup", "category": "Build & Development", "tier": 2, "description": "Assessment for One-Command Build/Setup", "criteria": "To be implemented", "default_weight": 0.03}, "status": "not_applicable", "score": null, "measured_value": null, "threshold": null, "evidence": ["One-Command Build/Setup assessment not yet implemented"], "remediation": null, "error_message": null}, {"attribute": {"id": "concise_documentation", "name": "Concise Structured Documentation", "category": "Context Window Optimization", "tier": 2, "description": "Assessment for Concise Structured Documentation", "criteria": "To be implemented", "default_weight": 0.03}, "status": "not_applicable", "score": null, "measured_value": null, "threshold": null, "evidence": ["Concise Structured Documentation assessment not yet implemented"], "remediation": null, "error_message": null}, {"attribute": {"id": "inline_documentation", "name": "Inline Documentation", "category": "Documentation Standards", "tier": 2, "description": "Assessment for Inline Documentation", "criteria": "To be implemented", "default_weight": 0.03}, "status": "not_applicable", "score": null, "measured_value": null, "threshold": null, "evidence": ["Inline Documentation assessment not yet implemented"], "remediation": null, "error_message": null}, {"attribute": {"id": "file_size_limits", "name": "File Size Limits", "category": "Context Window Optimization", "tier": 2, "description": "Assessment for File Size Limits", "criteria": "To be implemented", "default_weight": 0.03}, "status": "not_applicable", "score": null, "measured_value": null, "threshold": null, "evidence": ["File Size Limits assessment not yet implemented"], "remediation": null, "error_message": null}, {"attribute": {"id": "dependency_freshness", "name": "Dependency Freshness & Security", "category": "Dependency Management", "tier": 2, "description": "Assessment for Dependency Freshness & Security", "criteria": "To be implemented", "default_weight": 0.03}, "status": "not_applicable", "score": null, "measured_value": null, "threshold": null, "evidence": ["Dependency Freshness & Security assessment not yet implemented"], "remediation": null, "error_message": null}, {"attribute": {"id": "separation_concerns", "name": "Separation of Concerns", "category": "Repository Structure", "tier": 2, "description": "Assessment for Separation of Concerns", "criteria": "To be implemented", "default_weight": 0.03}, "status": "not_applicable", "score": null, "measured_value": null, "threshold": null, "evidence": ["Separation of Concerns assessment not yet implemented"], "remediation": null, "error_message": null}, {"attribute": {"id": "structured_logging", "name": "Structured Logging", "category": "Error Handling", "tier": 3, "description": "Assessment for Structured Logging", "criteria": "To be implemented", "default_weight": 0.03}, "status": "not_applicable", "score": null, "measured_value": null, "threshold": null, "evidence": ["Structured Logging assessment not yet implemented"], "remediation": null, "error_message": null}, {"attribute": {"id": "openapi_specs", "name": "OpenAPI/Swagger Specifications", "category": "API Documentation", "tier": 3, "description": "Assessment for OpenAPI/Swagger Specifications", "criteria": "To be implemented", "default_weight": 0.03}, "status": "not_applicable", "score": null, "measured_value": null, "threshold": null, "evidence": ["OpenAPI/Swagger Specifications assessment not yet implemented"], "remediation": null, "error_message": null}, {"attribute": {"id": "architecture_decisions", "name": "Architecture Decision Records", "category": "Documentation Standards", "tier": 3, "description": "Assessment for Architecture Decision Records", "criteria": "To be implemented", "default_weight": 0.03}, "status": "not_applicable", "score": null, "measured_value": null, "threshold": null, "evidence": ["Architecture Decision Records assessment not yet implemented"], "remediation": null, "error_message": null}, {"attribute": {"id": "semantic_naming", "name": "Semantic File & Directory Naming", "category": "Modularity", "tier": 3, "description": "Assessment for Semantic File & Directory Naming", "criteria": "To be implemented", "default_weight": 0.03}, "status": "not_applicable", "score": null, "measured_value": null, "threshold": null, "evidence": ["Semantic File & Directory Naming assessment not yet implemented"], "remediation": null, "error_message": null}, {"attribute": {"id": "security_scanning", "name": "Security Scanning Automation", "category": "Security", "tier": 4, "description": "Assessment for Security Scanning Automation", "criteria": "To be implemented", "default_weight": 0.01}, "status": "not_applicable", "score": null, "measured_value": null, "threshold": null, "evidence": ["Security Scanning Automation assessment not yet implemented"], "remediation": null, "error_message": null}, {"attribute": {"id": "performance_benchmarks", "name": "Performance Benchmarks", "category": "Performance", "tier": 4, "description": "Assessment for Performance Benchmarks", "criteria": "To be implemented", "default_weight": 0.01}, "status": "not_applicable", "score": null, "measured_value": null, "threshold": null, "evidence": ["Performance Benchmarks assessment not yet implemented"], "remediation": null, "error_message": null}, {"attribute": {"id": "code_smells", "name": "Code Smell Elimination", "category": "Code Quality", "tier": 4, "description": "Assessment for Code Smell Elimination", "criteria": "To be implemented", "default_weight": 0.01}, "status": "not_applicable", "score": null, "measured_value": null, "threshold": null, "evidence": ["Code Smell Elimination assessment not yet implemented"], "remediation": null, "error_message": null}, {"attribute": {"id": "issue_pr_templates", "name": "Issue & Pull Request Templates", "category": "Git & Version Control", "tier": 4, "description": "Assessment for Issue & Pull Request Templates", "criteria": "To be implemented", "default_weight": 0.01}, "status": "not_applicable", "score": null, "measured_value": null, "threshold": null, "evidence": ["Issue & Pull Request Templates assessment not yet implemented"], "remediation": null, "error_message": null}, {"attribute": {"id": "container_setup", "name": "Container/Virtualization Setup", "category": "Build & Development", "tier": 4, "description": "Assessment for Container/Virtualization Setup", "criteria": "To be implemented", "default_weight": 0.01}, "status": "not_applicable", "score": null, "measured_value": null, "threshold": null, "evidence": ["Container/Virtualization Setup assessment not yet implemented"], "remediation": null, "error_message": null}], "config": null, "duration_seconds": 0.2};

        // Toggle finding expansion
        function toggleFinding(header) {
            const finding = header.parentElement;
            finding.classList.toggle('expanded');
        }

        // Filter functionality
        document.querySelectorAll('.filter-btn').forEach(btn => {
            btn.addEventListener('click', () => {
                // Update active button
                document.querySelectorAll('.filter-btn').forEach(b => b.classList.remove('active'));
                btn.classList.add('active');

                const filter = btn.dataset.filter;
                const findings = document.querySelectorAll('.finding');

                findings.forEach(finding => {
                    const status = finding.dataset.status;

                    if (filter === 'all') {
                        finding.classList.remove('hidden');
                    } else if (filter === 'skipped') {
                        if (status === 'skipped' || status === 'not_applicable' || status === 'error') {
                            finding.classList.remove('hidden');
                        } else {
                            finding.classList.add('hidden');
                        }
                    } else {
                        if (status === filter) {
                            finding.classList.remove('hidden');
                        } else {
                            finding.classList.add('hidden');
                        }
                    }
                });
            });
        });

        // Sort functionality
        document.getElementById('sort-select').addEventListener('change', (e) => {
            const sortBy = e.target.value;
            const container = document.getElementById('findings-container');
            const findings = Array.from(container.querySelectorAll('.finding'));

            findings.sort((a, b) => {
                if (sortBy === 'category') {
                    return a.dataset.category.localeCompare(b.dataset.category);
                } else if (sortBy === 'score-desc') {
                    return parseFloat(b.dataset.score) - parseFloat(a.dataset.score);
                } else if (sortBy === 'score-asc') {
                    return parseFloat(a.dataset.score) - parseFloat(b.dataset.score);
                } else if (sortBy === 'tier') {
                    return parseInt(a.dataset.tier) - parseInt(b.dataset.tier);
                }
            });

            findings.forEach(finding => container.appendChild(finding));
        });

        // Search functionality
        document.getElementById('search-box').addEventListener('input', (e) => {
            const query = e.target.value.toLowerCase();
            const findings = document.querySelectorAll('.finding');

            findings.forEach(finding => {
                const name = finding.dataset.name;
                const category = finding.dataset.category.toLowerCase();

                if (name.includes(query) || category.includes(query)) {
                    finding.classList.remove('hidden');
                } else {
                    finding.classList.add('hidden');
                }
            });
        });
    </script>
</body>
</html>
````

## File: scripts/backlog_to_issues.py
````python
#!/usr/bin/env python3
"""
Script to convert BACKLOG.md items into coldstart prompts.

This script:
1. Parses BACKLOG.md to extract individual items
2. Generates comprehensive coldstart prompts for each item
3. Saves prompts as markdown files in .github/coldstart-prompts/
4. Optionally creates GitHub issues via gh CLI (if --create-issues flag set)
5. Pauses after first item for user review
"""

import re
import subprocess
import sys
from pathlib import Path
from typing import List, Dict, Optional
import json


class BacklogItem:
    """Represents a single backlog item."""

    def __init__(self, title: str, priority: str, content: str, section_start: int):
        self.title = title
        self.priority = priority
        self.content = content
        self.section_start = section_start

    def __repr__(self):
        return f"BacklogItem(title={self.title}, priority={self.priority})"


def parse_backlog(backlog_path: Path) -> List[BacklogItem]:
    """Parse BACKLOG.md and extract all items."""

    with open(backlog_path, 'r') as f:
        content = f.read()

    items = []

    # Split into sections by ###
    sections = re.split(r'\n### ', content)

    for i, section in enumerate(sections[1:], 1):  # Skip first section (header)
        lines = section.split('\n')
        title = lines[0].strip()

        # Find priority in next few lines
        priority = "P4"  # default
        for line in lines[1:5]:
            if match := re.search(r'\*\*Priority\*\*:\s*(P\d)', line):
                priority = match.group(1)
                break

        # Get full content until next ### or end
        full_content = '\n'.join(lines)

        items.append(BacklogItem(
            title=title,
            priority=priority,
            content=full_content,
            section_start=i
        ))

    return items


def generate_coldstart_prompt(item: BacklogItem, repo_context: Dict) -> str:
    """Generate a comprehensive coldstart prompt for implementing the backlog item."""

    prompt = f"""# Coldstart Implementation Prompt: {item.title}

**Priority**: {item.priority}
**Repository**: agentready (https://github.com/{repo_context['owner']}/{repo_context['repo']})
**Branch Strategy**: Create feature branch from main

---

## Context

You are implementing a feature for AgentReady, a repository quality assessment tool for AI-assisted development.

### Repository Structure
```
agentready/
‚îú‚îÄ‚îÄ src/agentready/          # Source code
‚îÇ   ‚îú‚îÄ‚îÄ models/              # Data models
‚îÇ   ‚îú‚îÄ‚îÄ services/            # Scanner orchestration
‚îÇ   ‚îú‚îÄ‚îÄ assessors/           # Attribute assessments
‚îÇ   ‚îú‚îÄ‚îÄ reporters/           # Report generation (HTML, Markdown, JSON)
‚îÇ   ‚îú‚îÄ‚îÄ templates/           # Jinja2 templates
‚îÇ   ‚îî‚îÄ‚îÄ cli/                 # Click-based CLI
‚îú‚îÄ‚îÄ tests/                   # Test suite (unit + integration)
‚îú‚îÄ‚îÄ examples/                # Example reports
‚îî‚îÄ‚îÄ specs/                   # Feature specifications
```

### Key Technologies
- Python 3.11+
- Click (CLI framework)
- Jinja2 (templating)
- Pytest (testing)
- Black, isort, ruff (code quality)

### Development Workflow
1. Create feature branch: `git checkout -b NNN-feature-name`
2. Implement changes with tests
3. Run linters: `black . && isort . && ruff check .`
4. Run tests: `pytest`
5. Commit with conventional commits
6. Create PR to main

---

## Feature Requirements

{item.content}

---

## Implementation Checklist

Before you begin:
- [ ] Read CLAUDE.md for project context
- [ ] Review existing similar features (if applicable)
- [ ] Understand the data model (src/agentready/models/)
- [ ] Check acceptance criteria in feature description

Implementation steps:
- [ ] Create feature branch
- [ ] Implement core functionality
- [ ] Add unit tests (target >80% coverage)
- [ ] Add integration tests (if applicable)
- [ ] Run linters and fix any issues
- [ ] Update documentation (README.md, CLAUDE.md if needed)
- [ ] Self-test the feature end-to-end
- [ ] Create PR with descriptive title and body

Code quality requirements:
- [ ] All code formatted with black (88 char lines)
- [ ] Imports sorted with isort
- [ ] No ruff violations
- [ ] All tests passing
- [ ] Type hints where appropriate
- [ ] Docstrings for public APIs

---

## Key Files to Review

Based on this feature, you should review:
- `src/agentready/models/` - Understand Assessment, Finding, Attribute models
- `src/agentready/services/scanner.py` - Scanner orchestration
- `src/agentready/assessors/base.py` - BaseAssessor pattern
- `src/agentready/reporters/` - Report generation
- `CLAUDE.md` - Project overview and guidelines
- `BACKLOG.md` - Full context of this feature

---

## Testing Strategy

For this feature, ensure:
1. **Unit tests** for core logic (80%+ coverage)
2. **Integration tests** for end-to-end workflows
3. **Edge case tests** (empty inputs, missing files, errors)
4. **Error handling tests** (graceful degradation)

Run tests:
```bash
# All tests
pytest

# With coverage
pytest --cov=src/agentready --cov-report=html

# Specific test file
pytest tests/unit/test_feature.py -v
```

---

## Success Criteria

This feature is complete when:
- ‚úÖ All acceptance criteria from feature description are met
- ‚úÖ Tests passing with >80% coverage for new code
- ‚úÖ All linters passing (black, isort, ruff)
- ‚úÖ Documentation updated
- ‚úÖ PR created with clear description
- ‚úÖ Self-tested end-to-end

---

## Questions to Clarify (if needed)

If anything is unclear during implementation:
1. Check CLAUDE.md for project patterns
2. Review similar existing features
3. Ask for clarification in PR comments
4. Reference the original backlog item

---

## Getting Started

```bash
# Clone and setup
git clone https://github.com/{repo_context['owner']}/{repo_context['repo']}.git
cd agentready

# Create virtual environment
uv venv && source .venv/bin/activate

# Install dependencies
uv pip install -e .
uv pip install pytest black isort ruff

# Create feature branch
git checkout -b {item.section_start:03d}-{item.title.lower().replace(' ', '-')[:50]}

# Start implementing!
```

---

**Note**: This is a coldstart prompt. You have all context needed to implement this feature independently. Read the linked files, follow the patterns, and deliver high-quality code with tests.
"""

    return prompt


def create_github_issue(item: BacklogItem, prompt: str, repo_context: Dict, dry_run: bool = False) -> Optional[str]:
    """Create GitHub issue via gh CLI and attach coldstart prompt as comment."""

    # Prepare issue title
    issue_title = f"[{item.priority}] {item.title}"

    # Prepare issue body (extract description and requirements)
    # Get content up to "Implementation" section or similar
    body_parts = []
    body_parts.append(f"**Priority**: {item.priority}\n")

    # Extract description (first paragraph after Priority)
    lines = item.content.split('\n')
    in_description = False
    description_lines = []

    for line in lines:
        if '**Description**:' in line:
            in_description = True
            continue
        if in_description:
            if line.startswith('**') and ':' in line:
                break
            description_lines.append(line)

    if description_lines:
        body_parts.append("## Description\n")
        body_parts.append('\n'.join(description_lines))

    # Add link to full context
    body_parts.append("\n\n## Full Context\n")
    body_parts.append(f"See [BACKLOG.md](https://github.com/{repo_context['owner']}/{repo_context['repo']}/blob/main/BACKLOG.md) for complete requirements.\n")

    # Add acceptance criteria if present
    if '**Acceptance Criteria**:' in item.content:
        criteria_start = item.content.find('**Acceptance Criteria**:')
        criteria_section = item.content[criteria_start:criteria_start+1000]
        body_parts.append("\n## Acceptance Criteria\n")
        body_parts.append(criteria_section.split('\n\n')[0])

    issue_body = '\n'.join(body_parts)

    # Determine labels
    labels = [f"priority:{item.priority.lower()}"]

    # Add category labels based on title/content
    if 'security' in item.title.lower() or 'xss' in item.content.lower():
        labels.append('security')
    if 'bug' in item.title.lower() or 'fix' in item.title.lower():
        labels.append('bug')
    else:
        labels.append('enhancement')
    if 'test' in item.title.lower():
        labels.append('testing')
    if 'github' in item.title.lower():
        labels.append('github-integration')
    if 'report' in item.title.lower():
        labels.append('reporting')

    labels_str = ','.join(labels)

    if dry_run:
        print(f"\n{'='*80}")
        print(f"DRY RUN: Would create issue:")
        print(f"Title: {issue_title}")
        print(f"Labels: {labels_str}")
        print(f"Body preview:\n{issue_body[:500]}...")
        print(f"\nColdstart prompt would be added as first comment")
        print(f"{'='*80}\n")
        return None

    # Create issue using gh CLI
    try:
        # Create the issue
        result = subprocess.run(
            [
                'gh', 'issue', 'create',
                '--title', issue_title,
                '--body', issue_body,
                '--label', labels_str
            ],
            capture_output=True,
            text=True,
            check=True
        )

        issue_url = result.stdout.strip()
        print(f"‚úÖ Created issue: {issue_url}")

        # Extract issue number from URL
        issue_number = issue_url.split('/')[-1]

        # Add coldstart prompt as first comment
        subprocess.run(
            [
                'gh', 'issue', 'comment', issue_number,
                '--body', f"## ü§ñ Coldstart Implementation Prompt\n\n{prompt}"
            ],
            capture_output=True,
            text=True,
            check=True
        )

        print(f"‚úÖ Added coldstart prompt as comment")

        return issue_url

    except subprocess.CalledProcessError as e:
        print(f"‚ùå Failed to create issue: {e.stderr}")
        return None


def get_repo_context() -> Dict:
    """Get repository context (owner, repo name) from git remote."""
    try:
        result = subprocess.run(
            ['gh', 'repo', 'view', '--json', 'owner,name'],
            capture_output=True,
            text=True,
            check=True
        )
        data = json.loads(result.stdout)
        return {
            'owner': data['owner']['login'],
            'repo': data['name']
        }
    except Exception as e:
        # No git remote - ask user or use default
        print(f"‚ö†Ô∏è  Warning: Could not get repo context from git remote")
        print(f"    This is expected if repository not yet on GitHub")
        print(f"    Using default values for now\n")
        # For agentready, we know the intended location
        return {'owner': 'redhat', 'repo': 'agentready'}


def save_prompt_to_file(item: BacklogItem, prompt: str, output_dir: Path, item_number: int) -> Path:
    """Save coldstart prompt to markdown file."""

    # Create output directory if it doesn't exist
    output_dir.mkdir(parents=True, exist_ok=True)

    # Generate filename from item number and title
    safe_title = re.sub(r'[^\w\s-]', '', item.title.lower())
    safe_title = re.sub(r'[-\s]+', '-', safe_title)[:50]
    filename = f"{item_number:02d}-{safe_title}.md"

    filepath = output_dir / filename

    # Write prompt to file
    with open(filepath, 'w') as f:
        f.write(prompt)

    return filepath


def main():
    """Main script execution."""

    # Parse command line args
    create_issues = '--create-issues' in sys.argv
    process_all = '--all' in sys.argv

    # Get repository root
    repo_root = Path(__file__).parent.parent
    backlog_path = repo_root / 'BACKLOG.md'

    if not backlog_path.exists():
        print(f"‚ùå BACKLOG.md not found at {backlog_path}")
        sys.exit(1)

    # Create output directory
    output_dir = repo_root / '.github' / 'coldstart-prompts'

    # Get repo context
    repo_context = get_repo_context()
    print(f"üì¶ Repository: {repo_context['owner']}/{repo_context['repo']}\n")

    # Parse backlog
    print("üìñ Parsing BACKLOG.md...")
    items = parse_backlog(backlog_path)
    print(f"Found {len(items)} backlog items\n")

    # Show summary
    print("Backlog Items:")
    for i, item in enumerate(items, 1):
        print(f"  {i:2d}. [{item.priority}] {item.title}")
    print()

    # Process first item (or all if --all flag)
    items_to_process = items if process_all else [items[0]]

    for idx, item in enumerate(items_to_process, 1):
        print(f"{'='*80}")
        print(f"Processing item {idx}/{len(items_to_process)}: {item.title}")
        print(f"{'='*80}\n")

        # Generate coldstart prompt
        print("ü§ñ Generating coldstart prompt...")
        prompt = generate_coldstart_prompt(item, repo_context)
        print(f"‚úÖ Generated {len(prompt)} character prompt\n")

        # Save to file
        print("üíæ Saving prompt to file...")
        filepath = save_prompt_to_file(item, prompt, output_dir, item.section_start)
        print(f"‚úÖ Saved to: {filepath}\n")

        # Optionally create GitHub issue
        if create_issues:
            print("üêô Creating GitHub issue...")
            issue_url = create_github_issue(item, prompt, repo_context, dry_run=False)
            if issue_url:
                print(f"‚úÖ Created issue: {issue_url}\n")
            else:
                print(f"‚ùå Failed to create issue\n")

        # Pause after first item unless --all specified
        if not process_all and idx == 1:
            print(f"\n{'='*80}")
            print(f"‚úÖ FIRST PROMPT GENERATED")
            print(f"{'='*80}\n")
            print(f"Saved to: {filepath}")
            print(f"\nPlease review the prompt file.")
            print(f"Once approved, run with --all to process remaining {len(items) - 1} items:")
            print(f"  python scripts/backlog_to_issues.py --all")
            if not create_issues:
                print(f"\nTo also create GitHub issues, add --create-issues flag:")
                print(f"  python scripts/backlog_to_issues.py --all --create-issues")
            return

    # All items processed
    print(f"\n{'='*80}")
    print(f"‚úÖ PROCESSED {len(items_to_process)} ITEMS")
    print(f"{'='*80}\n")
    print(f"Coldstart prompts saved to: {output_dir}/")
    if create_issues:
        print(f"GitHub issues created (check repository)")
    print(f"\nNext steps:")
    print(f"  1. Review generated prompts in {output_dir}/")
    print(f"  2. Create GitHub issues manually, or run with --create-issues")
    print(f"  3. Start implementing features using the coldstart prompts!")


if __name__ == '__main__':
    main()
````

## File: specs/001-agentready-scorer/.remediation/A1-proportional-scoring.md
````markdown
# Remediation A1: Proportional Scoring Formula

**Issue**: FR-014 mentions proportional scoring but doesn't define the algorithm.

**Target File**: `specs/001-agentready-scorer/research.md`

**Action**: Add new section after existing technical decisions (after Decision 10)

---

## Content to Add

### Decision 11: Proportional Scoring Algorithm

**Context**: Many attributes have measurable thresholds that can be partially met (e.g., 65% test coverage when 80% is required). Linear proportional scoring provides deterministic, understandable results.

**Decision**: Use linear proportional scoring with the following formula:

```python
def calculate_proportional_score(measured_value, threshold, attribute_type):
    """
    Calculate proportional score for partial compliance.

    Args:
        measured_value: The measured value (numeric or parseable)
        threshold: The target threshold
        attribute_type: 'higher_is_better' or 'lower_is_better'

    Returns:
        Score from 0-100
    """
    if attribute_type == 'higher_is_better':
        # Example: test coverage (want higher values)
        if measured_value >= threshold:
            return 100
        elif measured_value <= 0:
            return 0
        else:
            return (measured_value / threshold) * 100

    elif attribute_type == 'lower_is_better':
        # Example: file length (want lower values)
        if measured_value <= threshold:
            return 100
        elif threshold == 0:
            return 0  # Avoid division by zero
        else:
            # Degrade linearly, cap at 0
            return max(0, 100 - ((measured_value - threshold) / threshold) * 100)
```

**Edge Cases**:
- Division by zero: Return 0 score
- Negative values: Clamp to 0
- Values exceeding 2x threshold (lower_is_better): Cap at 0
- Values exceeding 2x threshold (higher_is_better): Cap at 100

**Examples**:
- Test coverage: 65% measured, 80% threshold ‚Üí 65/80 * 100 = 81.25 score
- File length: 450 lines measured, 300 threshold ‚Üí 100 - ((450-300)/300)*100 = 50 score
- Cyclomatic complexity: 5 measured, 10 threshold ‚Üí 100 (meets threshold)

**Rationale**: Linear proportional scoring is:
- **Simple**: Easy to understand and explain to users
- **Deterministic**: Same inputs always produce same outputs
- **Fair**: Provides clear incentives for incremental improvement
- **Predictable**: Users can calculate expected score changes

Non-linear scoring (exponential penalties, sigmoid curves) was considered but rejected due to complexity and reduced transparency.

**References**:
- Spec FR-014: "Tool MUST handle repositories that partially meet attribute criteria"
- Spec FR-027: "Tool MUST calculate overall score based only on successfully evaluated attributes"
- Research report: All 25 attributes have quantifiable thresholds
````

## File: specs/001-agentready-scorer/.remediation/A2-tier-weight-distribution.md
````markdown
# Remediation A2: Tier-Based Weight Distribution

**Issue**: FR-031 references tier-based weighting but doesn't specify the distribution.

**Target File**: `specs/001-agentready-scorer/data-model.md`

**Action**: Add new section after entities section

---

## Content to Add

## Weight Distribution

### Default Tier-Based Weights

Per FR-031, attributes are weighted by tier priority with heavy penalties for missing essentials (especially CLAUDE.md). Tier 1 (Essential) attributes have 8x the impact of Tier 4 (Advanced) attributes.

| Tier | Description | Total Weight | Attributes | Weight Per Attribute |
|------|-------------|--------------|------------|---------------------|
| Tier 1 | Essential | 50% | 5 attributes | 10.0% each |
| Tier 2 | Critical | 30% | 10 attributes | 3.0% each |
| Tier 3 | Important | 15% | 5 attributes | 3.0% each |
| Tier 4 | Advanced | 5% | 5 attributes | 1.0% each |

**Total**: 100% across 25 attributes

**Penalty Philosophy**: Missing Tier 1 essentials (especially CLAUDE.md at 10%) creates significant score impact, incentivizing foundational improvements before advanced optimizations.

### Attribute-to-Tier Mapping

Based on research report tier assignments (lines 1535-1570):

**Tier 1 (Essential)** - 50% total weight (10% each):
1. CLAUDE.md Configuration Files (1.1) - **10.0%** ‚ö†Ô∏è CRITICAL
2. README Structure (2.1) - 10.0%
3. Type Annotations (3.3) - 10.0%
4. Standard Project Layouts (4.1) - 10.0%
5. Lock Files for Reproducibility (6.1) - 10.0%

**Tier 2 (Critical)** - 30% total weight (3% each):
6. Test Coverage Requirements (5.1) - 3.0%
7. Pre-commit Hooks & CI/CD Linting (5.3) - 3.0%
8. Conventional Commit Messages (7.1) - 3.0%
9. .gitignore Completeness (7.2) - 3.0%
10. One-Command Build/Setup (8.1) - 3.0%
11. Concise Structured Documentation (1.2) - 3.0%
12. Inline Documentation (2.2) - 3.0%
13. File Size Limits (1.3) - 3.0%
14. Dependency Freshness & Security (6.2) - 3.0%
15. Separation of Concerns (4.2) - 3.0%

**Tier 3 (Important)** - 15% total weight (3% each):
16. Cyclomatic Complexity Thresholds (3.1) - 3.0%
17. Structured Logging (9.2) - 3.0%
18. OpenAPI/Swagger Specifications (10.1) - 3.0%
19. Architecture Decision Records (2.3) - 3.0%
20. Semantic File & Directory Naming (11.3) - 3.0%

**Tier 4 (Advanced)** - 5% total weight (1% each):
21. Security Scanning Automation (13.1) - 1.0%
22. Performance Benchmarks (15.1) - 1.0%
23. Code Smell Elimination (3.4) - 1.0%
24. Issue & Pull Request Templates (7.3) - 1.0%
25. Container/Virtualization Setup (8.3) - 1.0%

### Custom Weight Configuration

Users can override default weights via `.agentready-config.yaml`:

```yaml
weights:
  claude_md_file: 0.15          # Increase from 10% to 15% (org prioritizes CLAUDE.md)
  test_coverage: 0.05           # Increase from 3% to 5%
  conventional_commits: 0.01    # Decrease from 3% to 1%
  # ... other attributes use defaults, rescaled to sum to 1.0
```

**Validation** (per FR-033):
- All 25 attributes must be present (explicitly or via defaults)
- Weights must be positive numbers
- Weights must sum to 1.0 (¬±0.001 tolerance for floating point)
- Missing attributes inherit rescaled tier defaults

### Score Calculation Example

```python
overall_score = sum(
    attribute_score * weight
    for attribute_score, weight in zip(scores, weights)
    if attribute_status == 'assessed'  # Exclude skipped per FR-027
)

# Normalize if some attributes skipped
total_weight_assessed = sum(
    weight for weight, status in zip(weights, statuses)
    if status == 'assessed'
)
normalized_score = (overall_score / total_weight_assessed) * 100
```

**Example Scenario**:
- Repository missing CLAUDE.md: loses 10 points immediately
- Repository with CLAUDE.md but no tests: scores 10 (CLAUDE.md) + 0 (tests)
- Perfect CLAUDE.md + perfect tests = 10 + 3 = 13 points from just 2 attributes

### Configuration Precedence

When weights exist in multiple locations:

1. **CLI flags** (highest priority) - `--weight claude_md_file=0.15`
2. **Config file** - `.agentready-config.yaml`
3. **Tier defaults** (lowest priority) - Built-in distribution

**Rationale**: Tier-based distribution with heavy penalties for missing essentials ensures:
- Users prioritize foundational improvements (CLAUDE.md, README, types)
- Missing CLAUDE.md (10% weight) has 10x impact of missing container setup (1% weight)
- Aligns with research report's "Essential ‚Üí Advanced" priority guidance
- Steep gradient incentivizes completing Tier 1 before moving to advanced features
````

## File: specs/001-agentready-scorer/.remediation/README.md
````markdown
# Specification Remediation Summary

**Analysis Date**: 2025-11-21
**Analyst**: Claude Code Analysis
**Status**: Ready for Review

---

## Overview

This directory contains 5 remediation documents addressing HIGH and MEDIUM severity specification issues identified in the analysis phase. All content has been finalized based on user decisions and is ready for integration into the specification.

## Remediation Files

### A1-proportional-scoring.md (HIGH Priority)
**Issue**: Proportional scoring algorithm undefined
**Target**: `specs/001-agentready-scorer/research.md`
**Decision**: Linear proportional scoring (user approved)
**Impact**: Resolves FR-014 ambiguity, enables deterministic partial compliance scoring

### A2-tier-weight-distribution.md (HIGH Priority)
**Issue**: Tier-based weight distribution not specified
**Target**: `specs/001-agentready-scorer/data-model.md`
**Decision**: 50/30/15/5 distribution with heavy CLAUDE.md penalty (user approved)
**Impact**: Resolves FR-031 ambiguity, defines complete 25-attribute weight mapping
**Key Points**:
- CLAUDE.md gets 10% weight (critical penalty for missing)
- Tier 1: 50% (5 attributes @ 10% each)
- Actual tier assignments extracted from research report

### U1-research-update-mechanism.md (MEDIUM Priority)
**Issue**: Research report update process underspecified
**Target**: `specs/001-agentready-scorer/plan.md`
**Decision**: Explicit opt-in via `--update-research` only (user approved)
**Impact**: Resolves FR-023, enables research report updates with safety
**Key Points**:
- Bundled version is default
- Update only on explicit command
- Users can point to custom research files
- Atomic update with rollback on failure

### U2-research-validation.md (MEDIUM Priority)
**Issue**: Research report validation criteria missing
**Target**: Create new `specs/001-agentready-scorer/contracts/research-report-schema.md`
**Decision**: Warning-only validation (user approved)
**Impact**: Resolves FR-024, enables research report quality checks
**Key Points**:
- Errors block usage (missing metadata, wrong attribute count)
- Warnings allow usage (missing examples, low reference count)
- 25 attributes, 4 tiers, 20+ references required

### U3-weight-validation.md (MEDIUM Priority)
**Issue**: Weight validation rules incomplete
**Target**: `specs/001-agentready-scorer/data-model.md`
**Decision**: User recommendation approved (partial configs with rescaling)
**Impact**: Resolves FR-033, enables safe custom weight configuration
**Key Points**:
- Completeness, positivity, sum=1.0 ¬±0.001 enforced
- Bounds (0.5%-20%) are warnings only
- Partial configs supported with automatic rescaling
- CLI > config file > tier defaults precedence

---

## Integration Steps

To integrate these remediations into the specification:

1. **Review Each Document**:
   ```bash
   cd specs/001-agentready-scorer/.remediation
   cat A1-proportional-scoring.md
   cat A2-tier-weight-distribution.md
   cat U1-research-update-mechanism.md
   cat U2-research-validation.md
   cat U3-weight-validation.md
   ```

2. **Apply Remediations**:
   - Copy content from A1 ‚Üí `research.md` (add Decision 11)
   - Copy content from A2 ‚Üí `data-model.md` (add Weight Distribution section)
   - Copy content from U1 ‚Üí `plan.md` (add to Technical Context)
   - Create new file from U2 ‚Üí `contracts/research-report-schema.md`
   - Copy content from U3 ‚Üí `data-model.md` (add Weight Validation section)

3. **Verify Integration**:
   ```bash
   # Check research.md has Decision 11
   grep "Decision 11" specs/001-agentready-scorer/research.md

   # Check data-model.md has weight sections
   grep "Weight Distribution" specs/001-agentready-scorer/data-model.md
   grep "Configuration Weight Validation" specs/001-agentready-scorer/data-model.md

   # Check plan.md has update mechanism
   grep "Research Report Update Mechanism" specs/001-agentready-scorer/plan.md

   # Check new contract file exists
   ls specs/001-agentready-scorer/contracts/research-report-schema.md
   ```

4. **Update Analysis Report**:
   - Mark A1, A2, U1, U2, U3 as RESOLVED
   - Re-run analysis to confirm no regressions
   - Update metrics (ambiguity count: 5 ‚Üí 0)

---

## User Decisions Summary

| Issue | Question | Decision |
|-------|----------|----------|
| A1 | Linear vs non-linear scoring? | Linear (simple, transparent) |
| A1 | Capping behavior? | Cap at 0/100, no negative scores |
| A2 | Weight distribution? | 50/30/15/5 across tiers |
| A2 | CLAUDE.md penalty? | Heavy (10%, highest single attribute) |
| A2 | Tier assignments? | Extract from actual research report |
| U1 | Update frequency? | Explicit `--update-research` only |
| U1 | Repository location? | `ambient-code/agentready` (future) |
| U1 | Version compatibility? | Tool + research versioned together |
| U2 | Validation strictness? | Errors block, warnings allow |
| U2 | Minimum references? | 20 citations (evidence threshold) |
| U3 | Partial configs? | Supported with automatic rescaling |
| U3 | Weight bounds? | 0.5%-20% warnings, not errors |
| U3 | Precedence? | CLI > config > tier defaults |

---

## Additional Items

### Backlog Created

**File**: `/Users/jeder/repos/sk/agentready/BACKLOG.md`

**Items Added**:
1. Bootstrap new GitHub repositories (P5)
2. Report schema versioning (P3)

**Note**: Bootstrap feature includes repository at `ambient-code/agentready` which does not yet exist. Do not push or implement until repository is created.

### Configuration Output

**Requirement**: JSON output needed in addition to HTML/Markdown
**Status**: Already specified in FR-005 and contracts/assessment-schema.json
**Action**: No changes needed (JSON already designed)

---

## Metrics After Remediation

**Before**:
- Ambiguity Count: 5
- Underspecification Count: 5
- High Severity: 3
- Medium Severity: 8

**After** (projected):
- Ambiguity Count: 0 (-5)
- Underspecification Count: 0 (-5)
- High Severity: 0 (-3)
- Medium Severity: 3 (terminology, documentation)

**Coverage**: 100% (unchanged)
**Constitution Compliance**: 100% (unchanged)

---

## Next Steps

**Immediate**:
1. Review remediation documents
2. Integrate into specification files
3. Re-run `/speckit.analyze` to verify resolution
4. Proceed to `/speckit.implement`

**Future**:
1. Create `ambient-code/agentready` GitHub repository
2. Implement bootstrap feature from backlog
3. Version report schemas per backlog item

**Status**: ‚úÖ Ready for implementation
````

## File: specs/001-agentready-scorer/.remediation/U1-research-update-mechanism.md
````markdown
# Remediation U1: Research Report Update Mechanism

**Issue**: FR-023 requires update capability but doesn't specify the mechanism.

**Target File**: `specs/001-agentready-scorer/plan.md`

**Action**: Add subsection under "## Technical Context"

---

## Content to Add

### Research Report Update Mechanism

**Per FR-023**: Tool must support updating the bundled research report.

**Versioning Philosophy**: agentready releases are bundled with a specific research report version. The tool and research report are versioned together (e.g., agentready v1.2.0 includes research report v1.2.0).

**Implementation Design**:

1. **Bundled Report**:
   - Research report shipped with tool installation in `src/agentready/data/agent-ready-codebase-attributes.md`
   - Tool version and research version always match in releases
   - Users can always rely on stable, tested research content

2. **Custom Research Reports**:
   - Users can point to custom research reports:
     ```bash
     agentready --research-file /path/to/custom-research.md
     agentready --research-file https://example.com/custom-research.md
     ```
   - Enables organizations to customize criteria for internal standards
   - Custom reports must pass validation (per FR-024)

3. **Update Command** (explicit opt-in only):
   ```bash
   agentready --update-research
   ```

4. **Update Source**:
   - Primary: `https://github.com/ambient-code/agentready/raw/main/src/agentready/data/agent-ready-codebase-attributes.md`
   - Downloads latest research report from main branch
   - User explicitly opts in to update

5. **Update Process** (atomic):
   ```
   a. Download new version to temp file
   b. Validate structure (per FR-024):
      - Check for required metadata (version, date)
      - Verify 25 attributes present
      - Validate Markdown format
      - Parse successfully with research_loader.py
   c. If valid:
      - Backup current: agent-ready-codebase-attributes.md ‚Üí .md.backup
      - Atomic rename: temp ‚Üí agent-ready-codebase-attributes.md
      - Remove backup on success
      - Display: "Updated research report from vX.Y.Z to vA.B.C"
   d. If invalid:
      - Delete temp file
      - Restore from backup if needed
      - Exit with error message and validation details
      - Keep current version intact
   ```

6. **Version Tracking**:
   - Research report includes metadata header:
     ```markdown
     ---
     version: 1.2.0
     date: 2025-11-20
     ---
     ```
   - Tool displays current research version:
     ```bash
     agentready --research-version
     # Output: Research report v1.2.0 (2025-11-20)
     ```

7. **Offline Behavior**:
   - If network unavailable: "Cannot reach update server. Using bundled version v1.0.0"
   - If update fails: "Update failed (validation error). Using current version v1.0.0"
   - Never leave tool in broken state
   - Always fall back to last known-good version

8. **Rollback**:
   ```bash
   agentready --restore-bundled-research  # Restore original from package
   ```
   - Restores research report from tool installation package
   - Useful if custom/updated research causes issues

**Error Handling**:
- Network timeout (30s): Graceful failure, keep current version
- Invalid format: Detailed validation error, rollback to previous
- Permission denied: Error with instructions to check file permissions
- Partial download: Checksum validation, retry or abort

**Testing** (integration test):
- Mock HTTP server with valid/invalid research reports
- Verify atomic updates (no partial writes)
- Test rollback on validation failure
- Confirm offline graceful degradation
- Verify version detection and display

**Philosophy**: Research report updates are explicitly opt-in only via `--update-research`. Standard workflow uses stable bundled version. Users who want latest research or custom criteria can update or override as needed.
````

## File: specs/001-agentready-scorer/.remediation/U2-research-validation.md
````markdown
# Remediation U2: Research Report Validation Criteria

**Issue**: FR-024 requires validation but doesn't define the rules.

**Target File**: Create new file `specs/001-agentready-scorer/contracts/research-report-schema.md`

---

## Content to Add (New File)

# Research Report Validation Schema

**Purpose**: Define validation criteria for agent-ready-codebase-attributes.md (per FR-024)

## Required Structure

### 1. Metadata Header (REQUIRED)

```markdown
---
version: SEMVER          # e.g., "1.2.0"
date: YYYY-MM-DD         # ISO 8601 date
---
```

**Validation**:
- ‚úÖ Version follows semantic versioning (MAJOR.MINOR.PATCH)
- ‚úÖ Date is valid ISO 8601 format
- ‚ö†Ô∏è Date is not in future (warning only)

### 2. Attribute Definitions (REQUIRED)

Must contain exactly 25 attribute sections with this structure:

```markdown
### [Number].[Subnumber] [Attribute Name]

**Definition:** [Clear definition]

**Why It Matters:** [Rationale]

**Impact on Agent Behavior:**
- [Impact point 1]
- [Impact point 2]

**Measurable Criteria:**
- [Criterion 1]
- [Criterion 2]

**Citations:**
- [Source 1]
- [Source 2]
```

**Validation**:
- ‚úÖ Exactly 25 attributes present
- ‚úÖ Attributes numbered sequentially (1.1, 1.2, ..., 15.1)
- ‚úÖ Each attribute has "Definition" section (non-empty)
- ‚úÖ Each attribute has "Measurable Criteria" section (non-empty)
- ‚úÖ At least one citation per attribute
- ‚ö†Ô∏è "Impact on Agent Behavior" section exists (warning if missing)

### 3. Tier Assignments (REQUIRED)

Research report must include tier classification section:

```markdown
## IMPLEMENTATION PRIORITIES

### Tier 1: Essential (Must-Have)
[List of attributes]

### Tier 2: Critical (Should-Have)
[List of attributes]

### Tier 3: Important (Nice-to-Have)
[List of attributes]

### Tier 4: Advanced (Optimization)
[List of attributes]
```

**Validation**:
- ‚úÖ All 4 tiers defined
- ‚úÖ All 25 attributes assigned to exactly one tier
- ‚úÖ Each tier contains at least 1 attribute
- ‚ö†Ô∏è Tier distribution roughly balanced (warning if one tier has >50% of attributes)

### 4. Category Coverage (REQUIRED)

Must include attributes across these categories:
1. Context Window Optimization
2. Documentation Standards
3. Code Quality Metrics
4. Repository Structure
5. Testing & CI/CD
6. Dependency Management
7. Git & Version Control
8. Build & Development Setup
9. Error Handling & Debugging
10. API & Interface Documentation
11. Modularity & Code Organization
12. CI/CD Integration
13. Security & Compliance

**Validation**:
- ‚úÖ At least 10 of 13 categories present
- ‚ö†Ô∏è Missing categories identified (warning only)

### 5. References Section (REQUIRED)

```markdown
## REFERENCES & CITATIONS

[Consolidated bibliography]
```

**Validation**:
- ‚úÖ References section exists
- ‚úÖ At least 20 unique citations (evidence-based design threshold)
- ‚ö†Ô∏è URLs are reachable (check via HEAD request, warn only if unreachable)

## Validation Severity

**ERRORS** (block usage - report must be fixed):
- Missing metadata header
- Invalid version/date format
- Incorrect attribute count (not 25)
- Missing "Measurable Criteria" in any attribute
- Fewer than 4 tiers defined
- Attributes not assigned to tiers

**WARNINGS** (allow usage - non-critical issues):
- Missing "Impact on Agent Behavior" sections
- Fewer than 20 references
- Unreachable citation URLs
- Unbalanced tier distribution
- Missing category coverage

## Validation Implementation

```python
class ResearchReportValidator:
    def validate(self, content: str) -> ValidationResult:
        errors = []
        warnings = []

        # Parse metadata
        metadata = self._extract_metadata(content)
        if not metadata:
            errors.append("Missing metadata header with version and date")
        elif not self._is_valid_semver(metadata.get('version')):
            errors.append(f"Invalid version format: {metadata.get('version')}")

        # Count attributes
        attributes = self._extract_attributes(content)
        if len(attributes) != 25:
            errors.append(f"Expected 25 attributes, found {len(attributes)}")

        # Check measurable criteria
        for attr in attributes:
            if not attr.measurable_criteria:
                errors.append(f"Missing measurable criteria for {attr.name}")
            if not attr.impact_on_agent:
                warnings.append(f"Missing 'Impact on Agent Behavior' for {attr.name}")

        # Check tier assignments
        tiers = self._extract_tiers(content)
        if len(tiers) < 4:
            errors.append(f"Expected 4 tiers, found {len(tiers)}")

        # Check references
        refs = self._extract_references(content)
        if len(refs) < 20:
            warnings.append(f"Only {len(refs)} references (recommend 20+ for evidence-based design)")

        return ValidationResult(
            valid=len(errors) == 0,
            errors=errors,
            warnings=warnings
        )
```

## Example Validation Output

```
‚úì Metadata header valid (v1.2.0, 2025-11-20)
‚úì 25 attributes found
‚úì All attributes have measurable criteria
‚úì 4 tiers defined with all attributes assigned
‚ö† Attribute 1.1 missing 'Impact on Agent Behavior' section
‚ö† Attribute 7.3 missing 'Impact on Agent Behavior' section
‚úì 52 references found

Validation: PASSED with 2 warnings

Warnings can be ignored - research report is usable.
```

## Usage in Tool

```python
from agentready.services.research_loader import ResearchLoader

loader = ResearchLoader()
result = loader.load_and_validate('agent-ready-codebase-attributes.md')

if not result.valid:
    print("Research report validation FAILED:")
    for error in result.errors:
        print(f"  ‚ùå {error}")
    sys.exit(1)

if result.warnings:
    print("Research report validation warnings:")
    for warning in result.warnings:
        print(f"  ‚ö†Ô∏è  {warning}")

# Proceed with valid research report
research = result.research
```
````

## File: specs/001-agentready-scorer/.remediation/U3-weight-validation.md
````markdown
# Remediation U3: Weight Validation Rules

**Issue**: FR-033 requires weight validation but doesn't define the rules.

**Target File**: `specs/001-agentready-scorer/data-model.md`

**Action**: Add subsection after the Weight Distribution section

---

## Content to Add

## Configuration Weight Validation

**Per FR-033**: Custom weights in `.agentready-config.yaml` must be validated.

### Validation Rules

#### Rule 1: Completeness
All 25 attributes must have weights (explicit or inherited):

```python
REQUIRED_ATTRIBUTES = [
    'claude_md_file', 'concise_documentation', 'file_size_limits',
    'readme_structure', 'inline_documentation', 'architecture_decisions',
    'cyclomatic_complexity', 'function_length', 'type_annotations',
    'code_smells', 'standard_layout', 'separation_concerns',
    'test_coverage', 'test_naming', 'precommit_hooks',
    'lock_files', 'dependency_freshness', 'conventional_commits',
    'gitignore_completeness', 'issue_pr_templates', 'one_command_setup',
    'dev_env_docs', 'container_setup', 'error_clarity',
    'structured_logging', 'openapi_specs', 'graphql_schemas',
    'dry_principle', 'naming_conventions', 'semantic_naming',
    'cicd_visibility', 'branch_protection', 'security_scanning',
    'secrets_management', 'performance_benchmarks'
]

def validate_completeness(config_weights, default_weights):
    missing = [
        attr for attr in REQUIRED_ATTRIBUTES
        if attr not in config_weights and attr not in default_weights
    ]
    if missing:
        raise ValidationError(f"Missing weights for: {', '.join(missing)}")
```

#### Rule 2: Positive Values
All weights must be positive numbers:

```python
def validate_positive(weights):
    invalid = {attr: w for attr, w in weights.items() if w <= 0}
    if invalid:
        raise ValidationError(f"Weights must be positive: {invalid}")
```

#### Rule 3: Sum Constraint
Weights must sum to 1.0 within floating-point tolerance:

```python
TOLERANCE = 0.001  # Allow 0.1% rounding error

def validate_sum(weights):
    total = sum(weights.values())
    if abs(total - 1.0) > TOLERANCE:
        raise ValidationError(
            f"Weights must sum to 1.0 (got {total:.4f}). "
            f"Difference: {total - 1.0:+.4f}"
        )
```

#### Rule 4: Reasonable Bounds
Individual weights should be between 0.5% and 20% (warnings only):

```python
MIN_WEIGHT = 0.005  # 0.5%
MAX_WEIGHT = 0.20   # 20%

def validate_bounds(weights):
    warnings = []
    for attr, weight in weights.items():
        if weight < MIN_WEIGHT:
            warnings.append(f"{attr}: {weight:.1%} is very low (< 0.5%)")
        if weight > MAX_WEIGHT:
            warnings.append(f"{attr}: {weight:.1%} is very high (> 20%)")
    return warnings  # Non-blocking warnings
```

### Partial Configuration Handling

Users can specify only overrides, inheriting defaults for others:

```yaml
# .agentready-config.yaml (partial configuration)
weights:
  claude_md_file: 0.15      # Override Tier 1 default (10%)
  test_coverage: 0.05       # Override Tier 2 default (3%)
  # Other 23 attributes: inherit rescaled tier defaults
```

**Rescaling Algorithm**:

```python
def merge_and_rescale(config_weights, tier_defaults):
    """
    Merge config overrides with tier defaults, then rescale to sum to 1.0.

    Args:
        config_weights: User-specified weights (partial or complete)
        tier_defaults: Default tier-based weights (complete, sums to 1.0)

    Returns:
        Final weights dictionary (complete, sums to 1.0)
    """
    # Start with tier defaults
    final_weights = tier_defaults.copy()

    # Override with config values
    final_weights.update(config_weights)

    # Rescale to sum to 1.0
    total = sum(final_weights.values())
    rescaled = {attr: w / total for attr, w in final_weights.items()}

    return rescaled

# Example:
# config = {'claude_md_file': 0.15, 'test_coverage': 0.05}
# defaults = {25 attributes summing to 1.0}
# After merge: sum = 1.07 (overrides increased total by 0.07)
# After rescale: all weights * (1.0 / 1.07) ‚âà 0.935x scaling factor
```

### Validation Command

```bash
agentready --validate-config [path/to/config.yaml]
```

**Output Example**:

```
Validating config: .agentready-config.yaml

‚úì All 25 attributes have weights
‚úì All weights are positive
‚úì Weights sum to 1.0000 (within tolerance)
‚ö† claude_md_file: 15.0% is high (default: 10.0%, tier 1 average: 10.0%)
‚ö† test_coverage: 5.0% is high (default: 3.0%, tier 2 average: 3.0%)

Configuration: VALID with 2 warnings

Effective weights after rescaling:
  claude_md_file: 14.02% (config: 15.0%, rescaled down)
  test_coverage: 4.67% (config: 5.0%, rescaled down)
  conventional_commits: 2.80% (default: 3.0%, rescaled down)
  ... (22 more attributes)

Total: 100.00%
```

### Error Messages

**Invalid Sum**:
```
ERROR: Weights must sum to 1.0 (got 0.8500). Difference: -0.1500

Your weights total 85%. This suggests missing attributes or incorrect values.

Current distribution:
  Explicitly specified: 0.6500 (13 attributes)
  Using tier defaults: 0.2000 (12 attributes)

Suggestion: Either specify all 25 attributes explicitly, or let unspecified
attributes inherit tier defaults (which will be automatically rescaled).
```

**Negative Weight**:
```
ERROR: Weights must be positive: {'test_coverage': -0.05}

Negative weights are not allowed. Did you mean to reduce other weights instead?
```

**Missing Attributes**:
```
ERROR: Missing weights for: inline_documentation, architecture_decisions

All 25 attributes must have weights. Either:
1. Specify these attributes in your config, OR
2. Ensure they exist in default-weights.yaml

To see all required attributes:
  agentready --list-attributes
```

**Tolerance Exceeded**:
```
ERROR: Weights must sum to 1.0 (got 1.0025). Difference: +0.0025

Sum exceeds tolerance (¬±0.001). Likely due to rounding errors.

Suggestion: Reduce precision or adjust values. For example:
  claude_md_file: 0.100 (instead of 0.1003)
  test_coverage: 0.050 (instead of 0.0497)
```

### Validation Integration

- **On config load**: Automatic validation (fail fast on errors)
- **On manual validate**: Detailed report with warnings
- **On assessment run**: Validation errors block execution
- **On example generation**: `--generate-config` produces valid example with comments

### Configuration Precedence

**Precedence Order** (highest to lowest):

1. **CLI flags** - `agentready --weight claude_md_file=0.15`
2. **Config file** - `.agentready-config.yaml` in current directory
3. **Config file (alternate path)** - `agentready --config /path/to/config.yaml`
4. **Tier defaults** - Built-in `src/agentready/data/default-weights.yaml`

**Merging Rules**:
- CLI flags override config file weights
- Config file overrides tier defaults
- Unspecified attributes always inherit tier defaults
- Final weights always rescaled to sum to 1.0

**Example**:
```bash
# CLI flag takes precedence
agentready --config custom.yaml --weight claude_md_file=0.20

# Precedence:
# 1. claude_md_file = 0.20 (from CLI flag)
# 2. Other weights from custom.yaml (if present)
# 3. Remaining weights from tier defaults
# 4. All rescaled to sum to 1.0
```
````

## File: specs/001-agentready-scorer/checklists/requirements.md
````markdown
# Specification Quality Checklist: AgentReady Repository Scorer

**Purpose**: Validate specification completeness and quality before proceeding to planning
**Created**: 2025-11-20
**Feature**: [spec.md](../spec.md)

## Content Quality

- [x] No implementation details (languages, frameworks, APIs)
- [x] Focused on user value and business needs
- [x] Written for non-technical stakeholders
- [x] All mandatory sections completed

## Requirement Completeness

- [x] No [NEEDS CLARIFICATION] markers remain
- [x] Requirements are testable and unambiguous
- [x] Success criteria are measurable
- [x] Success criteria are technology-agnostic (no implementation details)
- [x] All acceptance scenarios are defined
- [x] Edge cases are identified
- [x] Scope is clearly bounded
- [x] Dependencies and assumptions identified

## Feature Readiness

- [x] All functional requirements have clear acceptance criteria
- [x] User scenarios cover primary flows
- [x] Feature meets measurable outcomes defined in Success Criteria
- [x] No implementation details leak into specification

## Validation Results

### Content Quality - PASSED

‚úì **No implementation details**: Specification focuses on "what" not "how". Tool name and CLI interface are requirements, not implementation details.
‚úì **User value focused**: All user stories describe value delivery (assessment, interactive reports, remediation guidance).
‚úì **Non-technical language**: Uses domain terms (repository, assessment, attribute) accessible to stakeholders.
‚úì **Complete sections**: User Scenarios, Requirements, Success Criteria all fully populated.

### Requirement Completeness - PASSED

‚úì **No clarification markers**: All requirements are specific and concrete, no [NEEDS CLARIFICATION] markers present.
‚úì **Testable requirements**: Each FR can be verified (e.g., FR-003 "evaluate all 25 attributes" is verifiable by counting evaluated attributes).
‚úì **Measurable success criteria**: All SC include specific metrics (e.g., SC-001 "<5 minutes for <10k files", SC-002 ">95% accuracy").
‚úì **Technology-agnostic criteria**: Success criteria describe outcomes without specifying technologies (e.g., SC-008 focuses on user action time, not specific UI framework).
‚úì **Complete acceptance scenarios**: All user stories include Given/When/Then scenarios covering happy paths.
‚úì **Edge cases identified**: 8 edge cases documented covering error conditions, scale, and special repository types.
‚úì **Clear scope**: Feature bounded to assessment and reporting, not extending to automated fixes or CI/CD integration.
‚úì **Assumptions documented**: 8 assumptions listed covering access, permissions, audience, and operational constraints.

### Feature Readiness - PASSED

‚úì **Clear acceptance criteria**: Each FR maps to acceptance scenarios in user stories (e.g., FR-005 dual-format reports ‚Üí US1 acceptance scenario 5).
‚úì **Primary flows covered**: 4 user stories progress from core value (scoring) through enhanced features (interactive HTML, remediation, version control).
‚úì **Measurable outcomes**: 10 success criteria provide quantitative and qualitative validation points.
‚úì **No implementation leakage**: No mention of specific frameworks, libraries, or technical architecture.

## Notes

**Validation Status**: ALL ITEMS PASSED ‚úì

Specification is ready for `/speckit.plan` command. No updates required.

**Strengths**:
- Comprehensive coverage of 25 attributes from research report
- Clear priority ordering (P1-P4) enabling MVP-first approach
- Well-defined success criteria with specific thresholds
- Extensive edge case consideration

**Ready for next phase**: Planning can proceed to define technical approach, architecture, and implementation strategy.
````

## File: specs/001-agentready-scorer/contracts/assessment-schema.json
````json
{
  "$schema": "http://json-schema.org/draft-07/schema#",
  "title": "AgentReady Assessment",
  "description": "Complete repository assessment result with findings and scores",
  "type": "object",
  "required": [
    "repository",
    "timestamp",
    "overall_score",
    "certification_level",
    "attributes_assessed",
    "attributes_skipped",
    "attributes_total",
    "findings",
    "duration_seconds"
  ],
  "properties": {
    "repository": {
      "type": "object",
      "required": ["name", "path", "languages", "total_files"],
      "properties": {
        "name": {"type": "string"},
        "path": {"type": "string"},
        "url": {"type": ["string", "null"]},
        "branch": {"type": "string"},
        "commit_hash": {"type": "string", "pattern": "^[a-f0-9]{40}$"},
        "languages": {
          "type": "object",
          "additionalProperties": {"type": "integer", "minimum": 0}
        },
        "total_files": {"type": "integer", "minimum": 0},
        "total_lines": {"type": "integer", "minimum": 0}
      }
    },
    "timestamp": {
      "type": "string",
      "format": "date-time",
      "description": "ISO 8601 timestamp"
    },
    "overall_score": {
      "type": "number",
      "minimum": 0,
      "maximum": 100
    },
    "certification_level": {
      "type": "string",
      "enum": ["Platinum", "Gold", "Silver", "Bronze", "Needs Improvement"]
    },
    "attributes_assessed": {
      "type": "integer",
      "minimum": 0,
      "maximum": 25
    },
    "attributes_skipped": {
      "type": "integer",
      "minimum": 0,
      "maximum": 25
    },
    "attributes_total": {
      "type": "integer",
      "const": 25
    },
    "findings": {
      "type": "array",
      "minItems": 25,
      "maxItems": 25,
      "items": {
        "type": "object",
        "required": ["attribute", "status"],
        "properties": {
          "attribute": {
            "type": "object",
            "required": ["id", "name", "category", "tier"],
            "properties": {
              "id": {"type": "string"},
              "name": {"type": "string"},
              "category": {"type": "string"},
              "tier": {"type": "integer", "minimum": 1, "maximum": 4},
              "description": {"type": "string"},
              "criteria": {"type": "string"},
              "default_weight": {"type": "number", "minimum": 0, "maximum": 1}
            }
          },
          "status": {
            "type": "string",
            "enum": ["pass", "fail", "skipped", "error", "not_applicable"]
          },
          "score": {
            "type": ["number", "null"],
            "minimum": 0,
            "maximum": 100
          },
          "measured_value": {"type": ["string", "null"]},
          "threshold": {"type": ["string", "null"]},
          "evidence": {
            "type": "array",
            "items": {"type": "string"}
          },
          "remediation": {
            "type": ["object", "null"],
            "properties": {
              "summary": {"type": "string"},
              "steps": {
                "type": "array",
                "items": {"type": "string"}
              },
              "tools": {
                "type": "array",
                "items": {"type": "string"}
              },
              "commands": {
                "type": "array",
                "items": {"type": "string"}
              },
              "examples": {
                "type": "array",
                "items": {"type": "string"}
              },
              "citations": {
                "type": "array",
                "items": {
                  "type": "object",
                  "required": ["source", "title"],
                  "properties": {
                    "source": {"type": "string"},
                    "title": {"type": "string"},
                    "url": {"type": ["string", "null"], "format": "uri"},
                    "relevance": {"type": "string"}
                  }
                }
              }
            }
          },
          "error_message": {"type": ["string", "null"]}
        }
      }
    },
    "config": {
      "type": ["object", "null"],
      "properties": {
        "weights": {
          "type": "object",
          "additionalProperties": {"type": "number", "minimum": 0, "maximum": 1}
        },
        "excluded_attributes": {
          "type": "array",
          "items": {"type": "string"}
        },
        "output_dir": {"type": ["string", "null"]}
      }
    },
    "duration_seconds": {
      "type": "number",
      "minimum": 0
    }
  }
}
````

## File: specs/001-agentready-scorer/contracts/report-html-schema.md
````markdown
# HTML Report Structure Specification

**Purpose**: Define the structure, interactivity, and requirements for self-contained HTML reports generated by agentready

**Format**: Single HTML file with embedded CSS and JavaScript (no external dependencies)

**Filename**: `report-{timestamp}.html` (e.g., `report-2025-11-20T14-30-00.html`)

## Document Structure

### Overall HTML Skeleton

```html
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>AgentReady Assessment Report - {repository_name}</title>
    <style>
        /* ALL CSS EMBEDDED HERE (no external stylesheets) */
    </style>
</head>
<body>
    <div id="app">
        <!-- Report content here -->
    </div>

    <script>
        /* ALL JAVASCRIPT EMBEDDED HERE (no external scripts) */
        const ASSESSMENT_DATA = {JSON_DATA_HERE};
    </script>
</body>
</html>
```

## Required Sections

### 1. Header (Fixed Position)

```html
<header class="report-header">
    <div class="container">
        <h1>AgentReady Assessment Report</h1>
        <div class="repo-info">
            <span class="repo-name">{repository_name}</span>
            <span class="repo-path">{repository_path}</span>
        </div>
        <div class="score-display">
            <div class="score-circle" data-score="{overall_score}">
                <span class="score-number">{overall_score}</span>
                <span class="score-label">/100</span>
            </div>
            <div class="certification-badge {certification_class}">
                <span class="cert-emoji">{cert_emoji}</span>
                <span class="cert-name">{certification_level}</span>
            </div>
        </div>
        <div class="metadata">
            <span>üìÖ {timestamp}</span>
            <span>‚úÖ {assessed} assessed</span>
            <span>‚è≠Ô∏è {skipped} skipped</span>
            <span>‚è±Ô∏è {duration}s</span>
        </div>
    </div>
</header>
```

**Requirements**:
- Fixed position header (stays visible on scroll)
- Circular score display with dynamic color (red <40, yellow 40-74, green 75-100)
- Certification badge with appropriate emoji
- Metadata row with assessment stats

---

### 2. Controls/Filters (Sticky Below Header)

```html
<div class="controls-bar">
    <div class="container">
        <div class="filters">
            <button class="filter-btn active" data-filter="all">
                All ({total_attributes})
            </button>
            <button class="filter-btn" data-filter="pass">
                ‚úÖ Passing ({pass_count})
            </button>
            <button class="filter-btn" data-filter="fail">
                ‚ùå Failing ({fail_count})
            </button>
            <button class="filter-btn" data-filter="skipped">
                ‚è≠Ô∏è Skipped ({skipped_count})
            </button>
        </div>
        <div class="sort-controls">
            <label>Sort by:</label>
            <select id="sort-selector">
                <option value="category">Category</option>
                <option value="score-asc">Score (Low to High)</option>
                <option value="score-desc">Score (High to Low)</option>
                <option value="tier">Tier (Priority)</option>
            </select>
        </div>
        <div class="search">
            <input type="text" id="search-box" placeholder="Search attributes...">
        </div>
    </div>
</div>
```

**Requirements**:
- Sticky positioning (fixed below header)
- Filter buttons toggle attribute visibility
- Active filter highlighted
- Sort dropdown changes attribute display order
- Search box filters attributes by name/description (case-insensitive)
- Count badges update dynamically

---

### 3. Summary Dashboard

```html
<section class="summary-dashboard">
    <div class="container">
        <h2>Summary</h2>
        <div class="category-grid">
            <!-- One card per category -->
            <div class="category-card" data-category="{category_id}">
                <h3 class="category-name">{category_name}</h3>
                <div class="category-score">
                    <div class="progress-bar">
                        <div class="progress-fill" style="width: {score}%"></div>
                    </div>
                    <span class="score-text">{score}%</span>
                </div>
                <div class="category-stats">
                    <span>{assessed}/{total} attributes</span>
                    <span class="status-badge {status_class}">{status_text}</span>
                </div>
            </div>
        </div>
    </div>
</section>
```

**Requirements**:
- Grid layout (responsive: 1 column mobile, 2 tablet, 3 desktop)
- Each category card shows score, progress bar, attribute count
- Click category card to scroll to detailed findings
- Status badges color-coded (green/yellow/red)

---

### 4. Detailed Findings (Collapsible Sections)

```html
<section class="detailed-findings">
    <div class="container">
        <h2>Detailed Findings</h2>

        <!-- Category Section -->
        <div class="category-section" data-category="{category_id}">
            <h3 class="category-header">
                <span class="category-number">{number}</span>
                <span class="category-title">{category_name}</span>
                <span class="category-badge">{score}%</span>
            </h3>

            <!-- Attribute Findings -->
            <div class="attribute-findings">
                <details class="finding-card" data-status="{status}" data-tier="{tier}" data-attribute-id="{id}">
                    <summary class="finding-summary">
                        <div class="summary-left">
                            <span class="status-icon">{status_emoji}</span>
                            <span class="attribute-id">{number}</span>
                            <span class="attribute-name">{name}</span>
                            <span class="tier-badge">Tier {tier}</span>
                        </div>
                        <div class="summary-right">
                            <span class="score-badge">{score}/100</span>
                            <span class="expand-icon">‚ñº</span>
                        </div>
                    </summary>

                    <div class="finding-details">
                        <div class="measurement">
                            <strong>Measured:</strong> {measured_value}
                            <br>
                            <strong>Threshold:</strong> {threshold}
                        </div>

                        <div class="evidence">
                            <strong>Evidence:</strong>
                            <ul>
                                <!-- Evidence items with icons -->
                                <li><span class="evidence-icon">‚úÖ</span> {evidence_item}</li>
                            </ul>
                        </div>

                        <!-- Remediation (only for failing attributes) -->
                        <div class="remediation">
                            <h4>üîß Remediation Steps</h4>
                            <p class="remediation-summary">{summary}</p>
                            <ol class="remediation-steps">
                                <li>{step}</li>
                            </ol>
                            <div class="remediation-tools">
                                <strong>Tools:</strong> {tools_list}
                            </div>
                            <div class="remediation-commands">
                                <strong>Commands:</strong>
                                <pre><code>{commands}</code></pre>
                            </div>
                            <div class="citations">
                                <strong>References:</strong>
                                <ul>
                                    <li><a href="{url}" target="_blank">{source}: {title}</a></li>
                                </ul>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
        </div>
    </div>
</section>
```

**Requirements**:
- `<details>` / `<summary>` for native collapsible behavior
- Attribute cards sortable and filterable via JavaScript
- Status icons: ‚úÖ pass, ‚ùå fail, ‚è≠Ô∏è skipped, ‚ö†Ô∏è error, ‚ÑπÔ∏è not_applicable
- Tier badges color-coded (1=red, 2=orange, 3=yellow, 4=green)
- Remediation section only shown for failing attributes
- Code blocks with syntax highlighting (CSS-based, not JS library)
- External links open in new tab (`target="_blank"`)

---

### 5. Certification Explanation

```html
<section class="certification-info">
    <div class="container">
        <h2>Certification Level: <span class="cert-name">{certification_level}</span></h2>
        <p class="cert-description">{description}</p>

        <div class="score-ladder">
            <div class="score-tier" data-tier="platinum">
                <span class="tier-emoji">ü•á</span>
                <strong>Platinum</strong>
                <span class="tier-range">90-100</span>
                <span class="tier-desc">Exemplary</span>
            </div>
            <div class="score-tier" data-tier="gold">
                <span class="tier-emoji">ü•á</span>
                <strong>Gold</strong>
                <span class="tier-range">75-89</span>
                <span class="tier-desc">Highly Optimized</span>
            </div>
            <div class="score-tier active" data-tier="silver">
                <span class="tier-emoji">ü•à</span>
                <strong>Silver</strong>
                <span class="tier-range">60-74</span>
                <span class="tier-desc">Well-Suited (YOUR LEVEL)</span>
            </div>
            <div class="score-tier" data-tier="bronze">
                <span class="tier-emoji">ü•â</span>
                <strong>Bronze</strong>
                <span class="tier-range">40-59</span>
                <span class="tier-desc">Basic Compatibility</span>
            </div>
            <div class="score-tier" data-tier="needs-improvement">
                <span class="tier-emoji">‚õî</span>
                <strong>Needs Improvement</strong>
                <span class="tier-range">0-39</span>
                <span class="tier-desc">Significant Work Needed</span>
            </div>
        </div>

        <div class="next-steps">
            <h3>Next Steps to {next_tier}</h3>
            <ul class="improvement-list">
                <li>
                    <span class="improvement-points">+{points}</span>
                    <span class="improvement-desc">{description}</span>
                    <a href="#{attribute_id}" class="jump-link">View Details ‚Üí</a>
                </li>
            </ul>
        </div>
    </div>
</section>
```

**Requirements**:
- Visual score ladder with current level highlighted
- Next steps show top 3-5 improvements ranked by point potential
- Jump links scroll to specific attribute findings
- Active tier has distinct styling (border, background)

---

### 6. Footer

```html
<footer class="report-footer">
    <div class="container">
        <div class="footer-content">
            <div class="footer-section">
                <h4>Assessment Metadata</h4>
                <p><strong>Generated by:</strong> agentready v{version}</p>
                <p><strong>Research Report:</strong> v{research_version} ({research_date})</p>
                <p><strong>Duration:</strong> {minutes}m {seconds}s</p>
            </div>
            <div class="footer-section">
                <h4>Repository Snapshot</h4>
                <p><strong>Branch:</strong> {branch}</p>
                <p><strong>Commit:</strong> {commit_hash_short}</p>
                <p><strong>Languages:</strong> {languages_list}</p>
                <p><strong>Files:</strong> {total_files} | <strong>Lines:</strong> {total_lines}</p>
            </div>
            <div class="footer-section">
                <h4>Research Foundation</h4>
                <p>Based on evidence from 50+ authoritative sources including Anthropic, Microsoft, Google, ArXiv, and IEEE/ACM.</p>
                <p><a href="https://github.com/your-org/agentready" target="_blank">AgentReady Project</a></p>
            </div>
        </div>
        <div class="footer-attribution">
            <p><em>AgentReady - Optimizing codebases for AI-assisted development</em></p>
        </div>
    </div>
</footer>
```

---

## JavaScript Functionality

### Required Interactive Features

1. **Filtering**
```javascript
// Filter attribute cards by status
function filterByStatus(status) {
    const cards = document.querySelectorAll('.finding-card');
    cards.forEach(card => {
        if (status === 'all' || card.dataset.status === status) {
            card.style.display = 'block';
        } else {
            card.style.display = 'none';
        }
    });
    updateFilterCounts();
}
```

2. **Sorting**
```javascript
// Sort attributes by criteria
function sortAttributes(criteria) {
    const container = document.querySelector('.attribute-findings');
    const cards = Array.from(container.querySelectorAll('.finding-card'));

    cards.sort((a, b) => {
        if (criteria === 'score-asc') {
            return getScore(a) - getScore(b);
        } else if (criteria === 'score-desc') {
            return getScore(b) - getScore(a);
        } else if (criteria === 'tier') {
            return parseInt(a.dataset.tier) - parseInt(b.dataset.tier);
        }
        // Default: category order (no change)
        return 0;
    });

    cards.forEach(card => container.appendChild(card));
}
```

3. **Search**
```javascript
// Search attributes by text
function searchAttributes(query) {
    const lowercaseQuery = query.toLowerCase();
    const cards = document.querySelectorAll('.finding-card');

    cards.forEach(card => {
        const name = card.querySelector('.attribute-name').textContent.toLowerCase();
        const desc = card.querySelector('.finding-details').textContent.toLowerCase();

        if (name.includes(lowercaseQuery) || desc.includes(lowercaseQuery)) {
            card.style.display = 'block';
        } else {
            card.style.display = 'none';
        }
    });
}
```

4. **Smooth Scrolling**
```javascript
// Smooth scroll to category section
function scrollToCategory(categoryId) {
    const section = document.querySelector(`[data-category="${categoryId}"]`);
    section.scrollIntoView({ behavior: 'smooth', block: 'start' });
}
```

5. **Dynamic Progress Circles**
```javascript
// Animate score circle on page load
function animateScoreCircle(element, targetScore) {
    let currentScore = 0;
    const duration = 1000; // 1 second
    const increment = targetScore / (duration / 16); // 60 FPS

    const interval = setInterval(() => {
        currentScore += increment;
        if (currentScore >= targetScore) {
            currentScore = targetScore;
            clearInterval(interval);
        }
        updateCircle(element, currentScore);
    }, 16);
}
```

---

## CSS Requirements

### Responsive Breakpoints
```css
/* Mobile-first approach */
@media (min-width: 768px) { /* Tablet */ }
@media (min-width: 1024px) { /* Desktop */ }
```

### Color Scheme
```css
:root {
    /* Certification colors */
    --platinum: #E5E4E2;
    --gold: #FFD700;
    --silver: #C0C0C0;
    --bronze: #CD7F32;
    --needs-improvement: #D32F2F;

    /* Status colors */
    --pass: #4CAF50;
    --fail: #F44336;
    --warning: #FF9800;
    --info: #2196F3;
    --skipped: #9E9E9E;

    /* Tier colors */
    --tier-1: #D32F2F; /* Red - Essential */
    --tier-2: #FF6F00; /* Orange - Critical */
    --tier-3: #FBC02D; /* Yellow - Important */
    --tier-4: #388E3C; /* Green - Advanced */

    /* UI colors */
    --background: #FAFAFA;
    --surface: #FFFFFF;
    --text-primary: #212121;
    --text-secondary: #757575;
    --border: #E0E0E0;
}
```

### Typography
```css
body {
    font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif;
    font-size: 16px;
    line-height: 1.6;
}

code, pre {
    font-family: 'Monaco', 'Menlo', 'Ubuntu Mono', 'Consolas', monospace;
}
```

---

## File Size Requirements

- **Maximum**: 5MB (including embedded CSS/JS)
- **Target**: <2MB for typical reports
- **CSS Minification**: Required
- **JS Minification**: Required
- **Image Embedding**: None (use Unicode emoji only)

---

## Validation Checklist

- [ ] Single HTML file with no external dependencies
- [ ] All CSS embedded in `<style>` tag
- [ ] All JavaScript embedded in `<script>` tag
- [ ] Assessment data embedded as JSON constant
- [ ] Works offline (no CDN, no external resources)
- [ ] Responsive design (mobile, tablet, desktop)
- [ ] All interactive features functional (filter, sort, search)
- [ ] Smooth scroll behavior implemented
- [ ] Collapsible sections work (native `<details>` element)
- [ ] Progress bars animated on page load
- [ ] Score circle updates dynamically
- [ ] Color-coded status indicators throughout
- [ ] All links to external sites have `target="_blank"`
- [ ] Cross-browser compatible (Chrome, Firefox, Safari, Edge)
- [ ] Accessible (keyboard navigation, ARIA labels)
- [ ] Print-friendly (CSS print stylesheet)
````

## File: specs/001-agentready-scorer/contracts/report-markdown-schema.md
````markdown
# Markdown Report Structure Specification

**Purpose**: Define the structure and formatting requirements for Markdown reports generated by agentready

**Format**: GitHub-Flavored Markdown (GFM)

**Filename**: `report-{timestamp}.md` (e.g., `report-2025-11-20T14-30-00.md`)

## Document Structure

### 1. Header Section (REQUIRED)

```markdown
# AgentReady Assessment Report

**Repository**: {repository_name}
**Path**: {repository_path}
**Date**: {iso_8601_timestamp}
**Overall Score**: {score}/100 ({certification_level})
**Attributes Assessed**: {assessed}/{total} ({skipped} skipped)
```

**Requirements**:
- Level 1 heading (`#`) for title
- Bold labels (`**Label**:`) for metadata
- ISO 8601 timestamp format
- Certification level in parentheses

---

### 2. Summary Table (REQUIRED)

```markdown
## Summary

| Category | Attributes | Score | Status |
|----------|-----------|-------|--------|
| Context Window Optimization | 2/2 | 85% | ‚úÖ Good |
| Documentation Standards | 3/4 | 72% | ‚ö†Ô∏è Needs Improvement |
| Code Quality Metrics | 4/4 | 93% | ‚úÖ Excellent |
| Repository Structure | 2/2 | 80% | ‚úÖ Good |
| Testing & CI/CD | 3/3 | 67% | ‚ö†Ô∏è Needs Improvement |
| Dependency Management | 2/2 | 50% | ‚ùå Poor |
| Git & Version Control | 3/3 | 88% | ‚úÖ Good |
| Build & Development | 2/2 | 75% | ‚úÖ Good |
| Error Handling | 1/2 | 60% | ‚ö†Ô∏è Needs Improvement |
| API Documentation | 0/2 | 0% | ‚ùå Not Applicable |
| Modularity | 2/3 | 70% | ‚ö†Ô∏è Needs Improvement |
| CI/CD Integration | 2/2 | 90% | ‚úÖ Excellent |
| Security | 1/2 | 40% | ‚ùå Poor |
| **Overall** | **23/25** | **72%** | **ü•à Silver** |
```

**Requirements**:
- Level 2 heading (`##`) for "Summary"
- Table with 4 columns: Category, Attributes, Score, Status
- Attributes column shows assessed/total format
- Score column shows percentage
- Status uses emoji indicators:
  - ‚úÖ Good (70-89%)
  - ‚ö†Ô∏è Needs Improvement (40-69%)
  - ‚ùå Poor (0-39%)
  - ‚úÖ Excellent (90-100%)
  - ‚ùå Not Applicable (all attributes skipped/N/A)
- Overall row in bold
- Certification emoji: ü•á Platinum, ü•á Gold, ü•à Silver, ü•â Bronze, ‚õî Needs Improvement

---

### 3. Detailed Findings (REQUIRED)

For each category, include subsection with all attributes:

```markdown
## Detailed Findings

### 1. Context Window Optimization

#### 1.1 CLAUDE.md Configuration Files ‚úÖ

**Tier**: 1 (Essential)
**Score**: 100/100
**Status**: Pass

**Measured**: File exists (487 lines)
**Threshold**: <1000 lines with required sections

**Evidence**:
- ‚úÖ File exists: `CLAUDE.md`
- ‚úÖ File size: 487 lines (under 1000 line limit)
- ‚úÖ Contains tech stack section
- ‚úÖ Contains standard commands section

<details>
<summary>üîß Good Practices Found</summary>

- Concise format optimized for Claude context window
- Clear repository structure documentation
- Explicit security/compliance notes included
</details>

---

#### 1.2 Concise Structured Documentation ‚ö†Ô∏è

**Tier**: 1 (Essential)
**Score**: 60/100
**Status**: Needs Improvement

**Measured**: README 1247 lines
**Threshold**: <500 lines

**Evidence**:
- ‚ö†Ô∏è README exists but exceeds recommended size: `README.md` (1247 lines)
- ‚úÖ Uses standard Markdown headings
- ‚ö†Ô∏è Missing table of contents for >100 line document

<details>
<summary>üîß Remediation Steps</summary>

**Summary**: Split README into smaller, focused documents

**Steps**:
1. Identify sections that could be separate documents (API docs, architecture, deployment)
2. Create `docs/` directory for detailed documentation
3. Keep README focused on quickstart and overview
4. Add links to detailed docs from README
5. Add table of contents to README

**Tools Needed**:
- None (manual refactoring)

**Commands**:
```bash
mkdir -p docs/
# Move detailed sections to docs/api.md, docs/architecture.md, etc.
```

**References**:
- [Real Python: Documentation Best Practices](https://realpython.com/documenting-python-code/)
- [GitHub README Guide](https://docs.github.com/en/repositories/managing-your-repositorys-settings-and-features/customizing-your-repository/about-readmes)

</details>

---
```

**Requirements for Each Attribute**:
- Level 4 heading (`####`) with attribute number, name, and status emoji
- Bold labels for Tier, Score, Status, Measured, Threshold
- Tier number with description (e.g., "1 (Essential)")
- Status: "Pass", "Fail", "Needs Improvement", "Skipped", "Not Applicable"
- Evidence as bulleted list with emoji indicators
- Remediation in `<details>` block (collapsible)
- Remediation includes: Summary, Steps (ordered list), Tools, Commands (code block), References (links)
- Horizontal rule (`---`) between attributes

---

### 4. Certification Level Details (REQUIRED)

```markdown
## Certification Level: Silver (72/100)

Your repository has achieved **Silver** certification. This indicates a solid foundation for agent-ready development, with room for targeted improvements.

**Score Breakdown**:
- **Platinum** (90-100): Exemplary agent-ready codebase
- **Gold** (75-89): Highly optimized for AI-assisted development
- **Silver** (60-74): Well-suited for agent development (YOUR LEVEL)
- **Bronze** (40-59): Basic agent compatibility
- **Needs Improvement** (0-39): Significant optimization needed

**Next Steps to Gold** (need +3 points):
1. Fix README size (1.2 Concise Structured Documentation) - potential +10 points
2. Add pre-commit hooks (5.3 Pre-commit Hooks & CI/CD) - potential +8 points
3. Improve dependency freshness (6.2 Dependency Freshness) - potential +5 points
```

**Requirements**:
- Level 2 heading with certification name and score
- Paragraph explaining certification meaning
- Score breakdown showing all certification levels
- "YOUR LEVEL" marker on current level
- "Next Steps" section with actionable improvements (3-5 items)
- Each improvement shows attribute reference and potential point gain

---

### 5. Footer (REQUIRED)

```markdown
---

## Assessment Metadata

**Generated by**: agentready v1.0.0
**Research Report Version**: 1.0.0 (2025-11-20)
**Assessment Duration**: 2 minutes 7 seconds
**Research Citations**: 50+ authoritative sources (Anthropic, Microsoft, Google, ArXiv, IEEE/ACM)

**Repository Snapshot**:
- Branch: main
- Commit: abc123def456789
- Languages: Python (42 files), Markdown (12 files)
- Total Files: 54
- Total Lines: 8,432

---

*This report generated by AgentReady - optimizing codebases for AI-assisted development.*
*Based on evidence from 50+ authoritative sources including Anthropic, Microsoft, Google, ArXiv, and IEEE/ACM.*
```

**Requirements**:
- Horizontal rule before metadata section
- Level 2 heading for "Assessment Metadata"
- Bold labels for all metadata fields
- Repository snapshot with key statistics
- Footer in italics with attribution
- Research report version tracking

---

## Formatting Standards

### Emoji Usage
- ‚úÖ Pass / Good / Yes
- ‚ùå Fail / Poor / No
- ‚ö†Ô∏è Warning / Needs Improvement
- ‚ÑπÔ∏è Info / Note
- üîß Remediation / Fix
- ü•á Platinum
- ü•á Gold
- ü•à Silver
- ü•â Bronze
- ‚õî Needs Improvement

### Code Blocks
- Use triple backticks with language tag: ` ```bash `, ` ```python `, etc.
- One command per line in bash blocks
- Include comments for complex commands

### Links
- External links: `[Text](URL)`
- Internal anchors: `[Text](#section-id)` (for table of contents)
- All citation URLs must be absolute, not relative

### Tables
- Use pipe-separated format with header separator (`|---|---|`)
- Align columns for readability in source
- Keep tables simple (max 5 columns)

---

## File Size Limits

- **Maximum**: 1MB (markdown text)
- **Target**: <500KB for typical reports
- **Line Length**: No hard limit, but aim for <120 characters for readability

---

## Validation Checklist

- [ ] Header section includes all required fields
- [ ] Summary table has all categories and overall row
- [ ] Detailed findings include all 25 attributes (or clearly marked as skipped)
- [ ] Each attribute has tier, score, status, evidence
- [ ] Failing attributes have remediation steps
- [ ] Certification level section explains current level and next steps
- [ ] Footer includes assessment metadata and repository snapshot
- [ ] All emoji used correctly and consistently
- [ ] All code blocks have language tags
- [ ] All links are absolute URLs (no relative paths)
- [ ] Document renders correctly on GitHub/GitLab/Bitbucket
- [ ] No HTML tags except `<details>` and `<summary>`
````

## File: specs/001-agentready-scorer/contracts/research-report-schema.md
````markdown
# Research Report Validation Schema

**Purpose**: Define validation criteria for agent-ready-codebase-attributes.md (per FR-024)

## Required Structure

### 1. Metadata Header (REQUIRED)

```markdown
---
version: SEMVER          # e.g., "1.2.0"
date: YYYY-MM-DD         # ISO 8601 date
---
```

**Validation**:
- ‚úÖ Version follows semantic versioning (MAJOR.MINOR.PATCH)
- ‚úÖ Date is valid ISO 8601 format
- ‚ö†Ô∏è Date is not in future (warning only)

### 2. Attribute Definitions (REQUIRED)

Must contain exactly 25 attribute sections with this structure:

```markdown
### [Number].[Subnumber] [Attribute Name]

**Definition:** [Clear definition]

**Why It Matters:** [Rationale]

**Impact on Agent Behavior:**
- [Impact point 1]
- [Impact point 2]

**Measurable Criteria:**
- [Criterion 1]
- [Criterion 2]

**Citations:**
- [Source 1]
- [Source 2]
```

**Validation**:
- ‚úÖ Exactly 25 attributes present
- ‚úÖ Attributes numbered sequentially (1.1, 1.2, ..., 15.1)
- ‚úÖ Each attribute has "Definition" section (non-empty)
- ‚úÖ Each attribute has "Measurable Criteria" section (non-empty)
- ‚úÖ At least one citation per attribute
- ‚ö†Ô∏è "Impact on Agent Behavior" section exists (warning if missing)

### 3. Tier Assignments (REQUIRED)

Research report must include tier classification section:

```markdown
## IMPLEMENTATION PRIORITIES

### Tier 1: Essential (Must-Have)
[List of attributes]

### Tier 2: Critical (Should-Have)
[List of attributes]

### Tier 3: Important (Nice-to-Have)
[List of attributes]

### Tier 4: Advanced (Optimization)
[List of attributes]
```

**Validation**:
- ‚úÖ All 4 tiers defined
- ‚úÖ All 25 attributes assigned to exactly one tier
- ‚úÖ Each tier contains at least 1 attribute
- ‚ö†Ô∏è Tier distribution roughly balanced (warning if one tier has >50% of attributes)

### 4. Category Coverage (REQUIRED)

Must include attributes across these categories:
1. Context Window Optimization
2. Documentation Standards
3. Code Quality Metrics
4. Repository Structure
5. Testing & CI/CD
6. Dependency Management
7. Git & Version Control
8. Build & Development Setup
9. Error Handling & Debugging
10. API & Interface Documentation
11. Modularity & Code Organization
12. CI/CD Integration
13. Security & Compliance

**Validation**:
- ‚úÖ At least 10 of 13 categories present
- ‚ö†Ô∏è Missing categories identified (warning only)

### 5. References Section (REQUIRED)

```markdown
## REFERENCES & CITATIONS

[Consolidated bibliography]
```

**Validation**:
- ‚úÖ References section exists
- ‚úÖ At least 20 unique citations (evidence-based design threshold)
- ‚ö†Ô∏è URLs are reachable (check via HEAD request, warn only if unreachable)

## Validation Severity

**ERRORS** (block usage - report must be fixed):
- Missing metadata header
- Invalid version/date format
- Incorrect attribute count (not 25)
- Missing "Measurable Criteria" in any attribute
- Fewer than 4 tiers defined
- Attributes not assigned to tiers

**WARNINGS** (allow usage - non-critical issues):
- Missing "Impact on Agent Behavior" sections
- Fewer than 20 references
- Unreachable citation URLs
- Unbalanced tier distribution
- Missing category coverage

## Validation Implementation

```python
class ResearchReportValidator:
    def validate(self, content: str) -> ValidationResult:
        errors = []
        warnings = []

        # Parse metadata
        metadata = self._extract_metadata(content)
        if not metadata:
            errors.append("Missing metadata header with version and date")
        elif not self._is_valid_semver(metadata.get('version')):
            errors.append(f"Invalid version format: {metadata.get('version')}")

        # Count attributes
        attributes = self._extract_attributes(content)
        if len(attributes) != 25:
            errors.append(f"Expected 25 attributes, found {len(attributes)}")

        # Check measurable criteria
        for attr in attributes:
            if not attr.measurable_criteria:
                errors.append(f"Missing measurable criteria for {attr.name}")
            if not attr.impact_on_agent:
                warnings.append(f"Missing 'Impact on Agent Behavior' for {attr.name}")

        # Check tier assignments
        tiers = self._extract_tiers(content)
        if len(tiers) < 4:
            errors.append(f"Expected 4 tiers, found {len(tiers)}")

        # Check references
        refs = self._extract_references(content)
        if len(refs) < 20:
            warnings.append(f"Only {len(refs)} references (recommend 20+ for evidence-based design)")

        return ValidationResult(
            valid=len(errors) == 0,
            errors=errors,
            warnings=warnings
        )
```

## Example Validation Output

```
‚úì Metadata header valid (v1.2.0, 2025-11-20)
‚úì 25 attributes found
‚úì All attributes have measurable criteria
‚úì 4 tiers defined with all attributes assigned
‚ö† Attribute 1.1 missing 'Impact on Agent Behavior' section
‚ö† Attribute 7.3 missing 'Impact on Agent Behavior' section
‚úì 52 references found

Validation: PASSED with 2 warnings

Warnings can be ignored - research report is usable.
```

## Usage in Tool

```python
from agentready.services.research_loader import ResearchLoader

loader = ResearchLoader()
result = loader.load_and_validate('agent-ready-codebase-attributes.md')

if not result.valid:
    print("Research report validation FAILED:")
    for error in result.errors:
        print(f"  ‚ùå {error}")
    sys.exit(1)

if result.warnings:
    print("Research report validation warnings:")
    for warning in result.warnings:
        print(f"  ‚ö†Ô∏è  {warning}")

# Proceed with valid research report
research = result.research
```
````

## File: specs/001-agentready-scorer/data-model.md
````markdown
# Data Model: AgentReady Repository Scorer

**Phase**: 1 - Data Modeling
**Date**: 2025-11-20
**Purpose**: Define entities, relationships, and validation rules for the assessment system

## Overview

The data model consists of 7 primary entities organized in a hierarchical assessment workflow:
`Repository` ‚Üí `Assessment` ‚Üí `Finding[]` ‚Üí `Remediation`

All entities use Python dataclasses with type hints for clarity and validation.

## Entity Definitions

### 1. Repository

Represents the target git repository being assessed.

**Fields**:
- `path: pathlib.Path` - Absolute path to repository root
- `name: str` - Repository name (derived from path)
- `url: str | None` - Remote origin URL if available
- `branch: str` - Current branch name
- `commit_hash: str` - Current HEAD commit SHA
- `languages: dict[str, int]` - Detected languages with file counts (e.g., `{"Python": 42, "JavaScript": 18}`)
- `total_files: int` - Total files in repository (respecting .gitignore)
- `total_lines: int` - Total lines of code

**Validation Rules**:
- `path` must exist and contain `.git` directory
- `name` derived from `path.name`
- `languages` keys must be valid language names
- `total_files` and `total_lines` must be non-negative

**Relationships**:
- One Repository ‚Üí One Assessment

**Example**:
```python
Repository(
    path=Path("/home/user/myproject"),
    name="myproject",
    url="https://github.com/user/myproject.git",
    branch="main",
    commit_hash="abc123def456",
    languages={"Python": 42, "Markdown": 12},
    total_files=54,
    total_lines=8432
)
```

---

### 2. Attribute

Defines one of the 25 agent-ready quality attributes from the research report.

**Fields**:
- `id: str` - Unique identifier (e.g., "claude_md_file", "test_coverage")
- `name: str` - Human-readable name (e.g., "CLAUDE.md Configuration Files")
- `category: str` - Research report section (e.g., "Context Window Optimization")
- `tier: int` - Priority tier 1-4 (1=Essential, 4=Advanced)
- `description: str` - What this attribute measures
- `criteria: str` - Measurable criteria for passing
- `default_weight: float` - Default weight in scoring (0.0-1.0)

**Validation Rules**:
- `id` must be unique, lowercase, snake_case
- `tier` must be 1, 2, 3, or 4
- `default_weight` must be in range [0.0, 1.0]
- All default weights across 25 attributes must sum to 1.0 ¬± 0.01

**Relationships**:
- One Attribute ‚Üí Many Findings (across different assessments)

**Example**:
```python
Attribute(
    id="claude_md_file",
    name="CLAUDE.md Configuration Files",
    category="Context Window Optimization",
    tier=1,
    description="Markdown file at repository root for Claude Code context",
    criteria="File exists, <1000 lines, contains required sections",
    default_weight=0.08
)
```

---

### 3. Assessment

Represents a complete evaluation of a repository at a specific point in time.

**Fields**:
- `repository: Repository` - The repository assessed
- `timestamp: datetime` - When assessment was performed
- `overall_score: float` - Weighted average score 0-100
- `certification_level: str` - Platinum/Gold/Silver/Bronze based on score
- `attributes_assessed: int` - Number of successfully evaluated attributes
- `attributes_skipped: int` - Number of skipped attributes (tool missing, errors)
- `attributes_total: int` - Total attributes (should be 25)
- `findings: list[Finding]` - Individual attribute results
- `config: Config | None` - Custom configuration used (if any)
- `duration_seconds: float` - Time taken for assessment

**Validation Rules**:
- `overall_score` must be in range [0.0, 100.0]
- `certification_level` must be one of: "Platinum", "Gold", "Silver", "Bronze", "Needs Improvement"
- `attributes_assessed + attributes_skipped` must equal `attributes_total`
- `findings` length must equal `attributes_total`

**Certification Thresholds**:
- Platinum: 90-100
- Gold: 75-89
- Silver: 60-74
- Bronze: 40-59
- Needs Improvement: 0-39

**Relationships**:
- One Assessment ‚Üí One Repository
- One Assessment ‚Üí Many Findings (exactly 25)

**Example**:
```python
Assessment(
    repository=repo,
    timestamp=datetime(2025, 11, 20, 14, 30, 0),
    overall_score=72.5,
    certification_level="Silver",
    attributes_assessed=23,
    attributes_skipped=2,
    attributes_total=25,
    findings=[...],
    config=None,
    duration_seconds=127.3
)
```

---

### 4. Finding

Result of assessing a single attribute against a repository.

**Fields**:
- `attribute: Attribute` - The attribute being assessed
- `status: str` - "pass", "fail", "skipped", "error", "not_applicable"
- `score: float | None` - Score 0-100, or None if skipped/error
- `measured_value: str | None` - Actual measurement (e.g., "847 lines", "63% coverage")
- `threshold: str | None` - Expected threshold (e.g., "<300 lines", ">80% coverage")
- `evidence: list[str]` - Specific files/metrics supporting the finding
- `remediation: Remediation | None` - How to fix if failing
- `error_message: str | None` - Error details if status="error"

**Validation Rules**:
- `status` must be one of the defined values
- `score` required if status="pass" or status="fail", otherwise None
- `score` must be in range [0.0, 100.0] if present
- `error_message` required if status="error"

**Status Meanings**:
- `pass`: Attribute meets criteria (score typically 70-100)
- `fail`: Attribute doesn't meet criteria (score typically 0-69)
- `skipped`: Attribute couldn't be assessed (missing tool, permission error)
- `error`: Unexpected error during assessment
- `not_applicable`: Attribute doesn't apply to this repository (e.g., language-specific check)

**Relationships**:
- Many Findings ‚Üí One Assessment
- Many Findings ‚Üí One Attribute

**Example**:
```python
Finding(
    attribute=Attribute(...),
    status="fail",
    score=40.0,
    measured_value="847 lines",
    threshold="<300 lines",
    evidence=["src/large_module.py: 847 lines"],
    remediation=Remediation(...),
    error_message=None
)
```

---

### 5. Remediation

Actionable guidance for fixing a failing attribute.

**Fields**:
- `summary: str` - One-line summary of what to do
- `steps: list[str]` - Ordered steps to remediate
- `tools: list[str]` - Tools/packages needed (e.g., "black", "pytest-cov")
- `commands: list[str]` - Example commands to run
- `examples: list[str]` - Code/config examples
- `citations: list[Citation]` - Links to documentation/research

**Validation Rules**:
- `summary` must be non-empty
- `steps` should have at least one step
- Each command should be a single line

**Relationships**:
- One Remediation ‚Üí One Finding
- Many Remediation ‚Üí Many Citations

**Example**:
```python
Remediation(
    summary="Split large_module.py into smaller modules",
    steps=[
        "Identify logical groupings within large_module.py",
        "Extract each grouping into separate module",
        "Update imports in dependent files",
        "Verify tests still pass"
    ],
    tools=["radon"],
    commands=["radon cc src/large_module.py -a"],
    examples=["# Before: 847 lines\n# After: 4 modules of ~200 lines each"],
    citations=[...]
)
```

---

### 6. Citation

Reference to authoritative source from research report.

**Fields**:
- `source: str` - Source name (e.g., "Anthropic Engineering Blog")
- `title: str` - Article/paper title
- `url: str | None` - Link to source
- `relevance: str` - Why this citation supports the attribute

**Validation Rules**:
- `source` must be non-empty
- `url` must be valid URL if present

**Relationships**:
- Many Citations ‚Üí One Remediation

**Example**:
```python
Citation(
    source="Microsoft Learn",
    title="Code metrics - Cyclomatic complexity",
    url="https://learn.microsoft.com/code-metrics",
    relevance="Defines cyclomatic complexity thresholds"
)
```

---

### 7. Config

User configuration for customizing assessment behavior.

**Fields**:
- `weights: dict[str, float]` - Custom attribute weights (attribute_id ‚Üí weight)
- `excluded_attributes: list[str]` - Attributes to skip
- `language_overrides: dict[str, list[str]]` - Force language detection
- `output_dir: pathlib.Path | None` - Custom output directory

**Validation Rules**:
- `weights` keys must be valid attribute IDs
- `weights` values must be in range [0.0, 1.0]
- `weights` values must sum to 1.0 ¬± 0.01
- `excluded_attributes` must contain valid attribute IDs

**Relationships**:
- One Config ‚Üí One Assessment (optional)

**Example**:
```python
Config(
    weights={
        "claude_md_file": 0.10,  # Increase importance
        "test_coverage": 0.10,
        # ... other attributes
    },
    excluded_attributes=["performance_benchmarks"],
    language_overrides={"Python": ["*.pyx"]},
    output_dir=Path("/custom/reports")
)
```

---

## Weight Distribution

### Default Tier-Based Weights

Per FR-031, attributes are weighted by tier priority with heavy penalties for missing essentials (especially CLAUDE.md). Tier 1 (Essential) attributes have 10x the impact of Tier 4 (Advanced) attributes.

| Tier | Description | Total Weight | Attributes | Weight Per Attribute |
|------|-------------|--------------|------------|---------------------|
| Tier 1 | Essential | 50% | 5 attributes | 10.0% each |
| Tier 2 | Critical | 30% | 10 attributes | 3.0% each |
| Tier 3 | Important | 15% | 5 attributes | 3.0% each |
| Tier 4 | Advanced | 5% | 5 attributes | 1.0% each |

**Total**: 100% across 25 attributes

**Penalty Philosophy**: Missing Tier 1 essentials (especially CLAUDE.md at 10%) creates significant score impact, incentivizing foundational improvements before advanced optimizations.

### Attribute-to-Tier Mapping

Based on research report tier assignments (lines 1535-1570):

**Tier 1 (Essential)** - 50% total weight (10% each):
1. CLAUDE.md Configuration Files (1.1) - **10.0%** ‚ö†Ô∏è CRITICAL
2. README Structure (2.1) - 10.0%
3. Type Annotations (3.3) - 10.0%
4. Standard Project Layouts (4.1) - 10.0%
5. Lock Files for Reproducibility (6.1) - 10.0%

**Tier 2 (Critical)** - 30% total weight (3% each):
6. Test Coverage Requirements (5.1) - 3.0%
7. Pre-commit Hooks & CI/CD Linting (5.3) - 3.0%
8. Conventional Commit Messages (7.1) - 3.0%
9. .gitignore Completeness (7.2) - 3.0%
10. One-Command Build/Setup (8.1) - 3.0%
11. Concise Structured Documentation (1.2) - 3.0%
12. Inline Documentation (2.2) - 3.0%
13. File Size Limits (1.3) - 3.0%
14. Dependency Freshness & Security (6.2) - 3.0%
15. Separation of Concerns (4.2) - 3.0%

**Tier 3 (Important)** - 15% total weight (3% each):
16. Cyclomatic Complexity Thresholds (3.1) - 3.0%
17. Structured Logging (9.2) - 3.0%
18. OpenAPI/Swagger Specifications (10.1) - 3.0%
19. Architecture Decision Records (2.3) - 3.0%
20. Semantic File & Directory Naming (11.3) - 3.0%

**Tier 4 (Advanced)** - 5% total weight (1% each):
21. Security Scanning Automation (13.1) - 1.0%
22. Performance Benchmarks (15.1) - 1.0%
23. Code Smell Elimination (3.4) - 1.0%
24. Issue & Pull Request Templates (7.3) - 1.0%
25. Container/Virtualization Setup (8.3) - 1.0%

### Custom Weight Configuration

Users can override default weights via `.agentready-config.yaml`:

```yaml
weights:
  claude_md_file: 0.15          # Increase from 10% to 15% (org prioritizes CLAUDE.md)
  test_coverage: 0.05           # Increase from 3% to 5%
  conventional_commits: 0.01    # Decrease from 3% to 1%
  # ... other attributes use defaults, rescaled to sum to 1.0
```

**Validation** (per FR-033):
- All 25 attributes must be present (explicitly or via defaults)
- Weights must be positive numbers
- Weights must sum to 1.0 (¬±0.001 tolerance for floating point)
- Missing attributes inherit rescaled tier defaults

### Score Calculation Example

```python
overall_score = sum(
    attribute_score * weight
    for attribute_score, weight in zip(scores, weights)
    if attribute_status == 'assessed'  # Exclude skipped per FR-027
)

# Normalize if some attributes skipped
total_weight_assessed = sum(
    weight for weight, status in zip(weights, statuses)
    if status == 'assessed'
)
normalized_score = (overall_score / total_weight_assessed) * 100
```

**Example Scenario**:
- Repository missing CLAUDE.md: loses 10 points immediately
- Repository with CLAUDE.md but no tests: scores 10 (CLAUDE.md) + 0 (tests)
- Perfect CLAUDE.md + perfect tests = 10 + 3 = 13 points from just 2 attributes

### Configuration Precedence

When weights exist in multiple locations:

1. **CLI flags** (highest priority) - `--weight claude_md_file=0.15`
2. **Config file** - `.agentready-config.yaml`
3. **Tier defaults** (lowest priority) - Built-in distribution

**Rationale**: Tier-based distribution with heavy penalties for missing essentials ensures:
- Users prioritize foundational improvements (CLAUDE.md, README, types)
- Missing CLAUDE.md (10% weight) has 10x impact of missing container setup (1% weight)
- Aligns with research report's "Essential ‚Üí Advanced" priority guidance
- Steep gradient incentivizes completing Tier 1 before moving to advanced features

---

## Configuration Weight Validation

**Per FR-033**: Custom weights in `.agentready-config.yaml` must be validated.

### Validation Rules

#### Rule 1: Completeness
All 25 attributes must have weights (explicit or inherited):

```python
REQUIRED_ATTRIBUTES = [
    'claude_md_file', 'concise_documentation', 'file_size_limits',
    'readme_structure', 'inline_documentation', 'architecture_decisions',
    'cyclomatic_complexity', 'function_length', 'type_annotations',
    'code_smells', 'standard_layout', 'separation_concerns',
    'test_coverage', 'test_naming', 'precommit_hooks',
    'lock_files', 'dependency_freshness', 'conventional_commits',
    'gitignore_completeness', 'issue_pr_templates', 'one_command_setup',
    'dev_env_docs', 'container_setup', 'error_clarity',
    'structured_logging', 'openapi_specs', 'graphql_schemas',
    'dry_principle', 'naming_conventions', 'semantic_naming',
    'cicd_visibility', 'branch_protection', 'security_scanning',
    'secrets_management', 'performance_benchmarks'
]

def validate_completeness(config_weights, default_weights):
    missing = [
        attr for attr in REQUIRED_ATTRIBUTES
        if attr not in config_weights and attr not in default_weights
    ]
    if missing:
        raise ValidationError(f"Missing weights for: {', '.join(missing)}")
```

#### Rule 2: Positive Values
All weights must be positive numbers:

```python
def validate_positive(weights):
    invalid = {attr: w for attr, w in weights.items() if w <= 0}
    if invalid:
        raise ValidationError(f"Weights must be positive: {invalid}")
```

#### Rule 3: Sum Constraint
Weights must sum to 1.0 within floating-point tolerance:

```python
TOLERANCE = 0.001  # Allow 0.1% rounding error

def validate_sum(weights):
    total = sum(weights.values())
    if abs(total - 1.0) > TOLERANCE:
        raise ValidationError(
            f"Weights must sum to 1.0 (got {total:.4f}). "
            f"Difference: {total - 1.0:+.4f}"
        )
```

#### Rule 4: Reasonable Bounds
Individual weights should be between 0.5% and 20% (warnings only):

```python
MIN_WEIGHT = 0.005  # 0.5%
MAX_WEIGHT = 0.20   # 20%

def validate_bounds(weights):
    warnings = []
    for attr, weight in weights.items():
        if weight < MIN_WEIGHT:
            warnings.append(f"{attr}: {weight:.1%} is very low (< 0.5%)")
        if weight > MAX_WEIGHT:
            warnings.append(f"{attr}: {weight:.1%} is very high (> 20%)")
    return warnings  # Non-blocking warnings
```

### Partial Configuration Handling

Users can specify only overrides, inheriting defaults for others:

```yaml
# .agentready-config.yaml (partial configuration)
weights:
  claude_md_file: 0.15      # Override Tier 1 default (10%)
  test_coverage: 0.05       # Override Tier 2 default (3%)
  # Other 23 attributes: inherit rescaled tier defaults
```

**Rescaling Algorithm**:

```python
def merge_and_rescale(config_weights, tier_defaults):
    """
    Merge config overrides with tier defaults, then rescale to sum to 1.0.

    Args:
        config_weights: User-specified weights (partial or complete)
        tier_defaults: Default tier-based weights (complete, sums to 1.0)

    Returns:
        Final weights dictionary (complete, sums to 1.0)
    """
    # Start with tier defaults
    final_weights = tier_defaults.copy()

    # Override with config values
    final_weights.update(config_weights)

    # Rescale to sum to 1.0
    total = sum(final_weights.values())
    rescaled = {attr: w / total for attr, w in final_weights.items()}

    return rescaled

# Example:
# config = {'claude_md_file': 0.15, 'test_coverage': 0.05}
# defaults = {25 attributes summing to 1.0}
# After merge: sum = 1.07 (overrides increased total by 0.07)
# After rescale: all weights * (1.0 / 1.07) ‚âà 0.935x scaling factor
```

### Validation Command

```bash
agentready --validate-config [path/to/config.yaml]
```

**Output Example**:

```
Validating config: .agentready-config.yaml

‚úì All 25 attributes have weights
‚úì All weights are positive
‚úì Weights sum to 1.0000 (within tolerance)
‚ö† claude_md_file: 15.0% is high (default: 10.0%, tier 1 average: 10.0%)
‚ö† test_coverage: 5.0% is high (default: 3.0%, tier 2 average: 3.0%)

Configuration: VALID with 2 warnings

Effective weights after rescaling:
  claude_md_file: 14.02% (config: 15.0%, rescaled down)
  test_coverage: 4.67% (config: 5.0%, rescaled down)
  conventional_commits: 2.80% (default: 3.0%, rescaled down)
  ... (22 more attributes)

Total: 100.00%
```

### Error Messages

**Invalid Sum**:
```
ERROR: Weights must sum to 1.0 (got 0.8500). Difference: -0.1500

Your weights total 85%. This suggests missing attributes or incorrect values.

Current distribution:
  Explicitly specified: 0.6500 (13 attributes)
  Using tier defaults: 0.2000 (12 attributes)

Suggestion: Either specify all 25 attributes explicitly, or let unspecified
attributes inherit tier defaults (which will be automatically rescaled).
```

**Negative Weight**:
```
ERROR: Weights must be positive: {'test_coverage': -0.05}

Negative weights are not allowed. Did you mean to reduce other weights instead?
```

**Missing Attributes**:
```
ERROR: Missing weights for: inline_documentation, architecture_decisions

All 25 attributes must have weights. Either:
1. Specify these attributes in your config, OR
2. Ensure they exist in default-weights.yaml

To see all required attributes:
  agentready --list-attributes
```

**Tolerance Exceeded**:
```
ERROR: Weights must sum to 1.0 (got 1.0025). Difference: +0.0025

Sum exceeds tolerance (¬±0.001). Likely due to rounding errors.

Suggestion: Reduce precision or adjust values. For example:
  claude_md_file: 0.100 (instead of 0.1003)
  test_coverage: 0.050 (instead of 0.0497)
```

### Validation Integration

- **On config load**: Automatic validation (fail fast on errors)
- **On manual validate**: Detailed report with warnings
- **On assessment run**: Validation errors block execution
- **On example generation**: `--generate-config` produces valid example with comments

### Configuration Precedence

**Precedence Order** (highest to lowest):

1. **CLI flags** - `agentready --weight claude_md_file=0.15`
2. **Config file** - `.agentready-config.yaml` in current directory
3. **Config file (alternate path)** - `agentready --config /path/to/config.yaml`
4. **Tier defaults** - Built-in `src/agentready/data/default-weights.yaml`

**Merging Rules**:
- CLI flags override config file weights
- Config file overrides tier defaults
- Unspecified attributes always inherit tier defaults
- Final weights always rescaled to sum to 1.0

**Example**:
```bash
# CLI flag takes precedence
agentready --config custom.yaml --weight claude_md_file=0.20

# Precedence:
# 1. claude_md_file = 0.20 (from CLI flag)
# 2. Other weights from custom.yaml (if present)
# 3. Remaining weights from tier defaults
# 4. All rescaled to sum to 1.0
```

---

## State Transitions

### Assessment Workflow

```
Repository ‚Üí Validate ‚Üí Detect Languages ‚Üí Load Config ‚Üí Create Assessment

For each Attribute:
    Create Assessor ‚Üí Check Applicability

    if applicable:
        Execute Assessment ‚Üí Create Finding
    else:
        Create Finding (status=not_applicable)

Calculate Overall Score ‚Üí Determine Certification Level

Generate Reports ‚Üí Save to Disk
```

### Finding Status Flow

```
                           ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                           ‚îÇ   Start     ‚îÇ
                           ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                  ‚îÇ
                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                    ‚îÇ                            ‚îÇ
            ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê          ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
            ‚îÇ  is_applicable?‚îÇ          ‚îÇ   Execute      ‚îÇ
            ‚îÇ      No         ‚îÇ          ‚îÇ   Assessment   ‚îÇ
            ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò          ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                    ‚îÇ                            ‚îÇ
            ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
            ‚îÇ not_applicable  ‚îÇ      ‚îÇ            ‚îÇ            ‚îÇ
            ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îå‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                                ‚îÇ pass   ‚îÇ  ‚îÇ   fail    ‚îÇ ‚îÇ error  ‚îÇ
                                ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

## Serialization

All entities must support JSON serialization for:
- Report generation (HTML/Markdown use JSON as intermediate format)
- Automation integration (tools consuming assessment data)
- Caching/persistence (save assessment results)

**Serialization Strategy**:
- Use `dataclasses.asdict()` with custom JSON encoder
- Convert `datetime` to ISO 8601 strings
- Convert `Path` to strings
- Convert enums to string values
- Preserve nested structures

**Example JSON Output**:
```json
{
  "repository": {
    "name": "myproject",
    "path": "/home/user/myproject",
    "languages": {"Python": 42},
    "total_files": 54
  },
  "timestamp": "2025-11-20T14:30:00",
  "overall_score": 72.5,
  "certification_level": "Silver",
  "findings": [
    {
      "attribute": {"id": "claude_md_file", "name": "CLAUDE.md"},
      "status": "pass",
      "score": 100.0,
      "evidence": ["CLAUDE.md exists (487 lines)"]
    }
  ]
}
```

---

## Summary

Data model defines 7 entities with clear responsibilities:
- `Repository`: What is being assessed
- `Attribute`: What is being measured
- `Assessment`: Container for complete evaluation
- `Finding`: Result of one attribute check
- `Remediation`: How to fix failures
- `Citation`: Supporting research
- `Config`: User customization

All validation rules enforced at model layer (fail fast). State transitions are linear (no complex workflows). JSON serialization supports all output formats and automation integrations.
````

## File: specs/001-agentready-scorer/plan.md
````markdown
# Implementation Plan: AgentReady Repository Scorer

**Branch**: `001-agentready-scorer` | **Date**: 2025-11-20 | **Spec**: [spec.md](spec.md)
**Input**: Feature specification from `/specs/001-agentready-scorer/spec.md`

**Note**: This template is filled in by the `/speckit.plan` command. See `.specify/templates/commands/plan.md` for the execution workflow.

## Summary

Build a command-line tool called "agentready" that assesses git repositories against 25 evidence-based attributes for AI-assisted development readiness. The tool analyzes repository quality across categories like documentation, code quality, testing, structure, CI/CD, and security, then generates dual-format reports (interactive HTML + version-control-friendly Markdown) with specific findings, evidence, remediation guidance, and an overall agent-readiness score with certification level (Platinum/Gold/Silver/Bronze).

Technical approach: Library-first architecture with standalone assessment modules for each attribute category, CLI wrapper orchestrating the scan workflow, bundled research data with update capability, and self-contained HTML report generation with embedded interactivity.

## Technical Context

**Language/Version**: Python 3.11+ (only N and N-1 versions supported per constitution)
**Primary Dependencies**:
- Click (CLI framework)
- Jinja2 (HTML template rendering)
- PyYAML (configuration file parsing)
- gitpython (repository introspection)
- Language-specific analyzers: radon (Python complexity), lizard (multi-language complexity)

**Storage**: File-based (no database required)
- Bundled research report (agent-ready-codebase-attributes.md)
- Optional user configuration file (.agentready-config.yaml)
- Generated reports (.agentready/ directory)

**Testing**: pytest with pytest-cov for coverage tracking
- Unit tests for individual attribute assessors
- Integration tests for full scan workflows
- Contract tests for report schemas

**Target Platform**: Cross-platform CLI (Linux, macOS, Windows via Python)
**Project Type**: Single project (CLI tool with library modules)
**Performance Goals**:
- Complete assessment in <5 minutes for repositories with <10k files
- Deterministic scoring (consistent results across runs)
- Minimal memory footprint (<500MB for typical repositories)

**Constraints**:
- Offline-capable (internet only for research report updates)
- Self-contained HTML reports (no external CDN dependencies)
- Graceful degradation (partial results if some checks fail)
- No modification of scanned repository (read-only analysis)

**Scale/Scope**:
- 25 attribute assessors (one per research report attribute)
- Support 5+ programming languages (Python, JavaScript, TypeScript, Go, Java minimum)
- Handle repositories up to 100k files (with performance degradation warning)

### Research Report Update Mechanism

**Per FR-023**: Tool must support updating the bundled research report.

**Versioning Philosophy**: agentready releases are bundled with a specific research report version. The tool and research report are versioned together (e.g., agentready v1.2.0 includes research report v1.2.0).

**Implementation Design**:

1. **Bundled Report**:
   - Research report shipped with tool installation in `src/agentready/data/agent-ready-codebase-attributes.md`
   - Tool version and research version always match in releases
   - Users can always rely on stable, tested research content

2. **Custom Research Reports**:
   - Users can point to custom research reports:
     ```bash
     agentready --research-file /path/to/custom-research.md
     agentready --research-file https://example.com/custom-research.md
     ```
   - Enables organizations to customize criteria for internal standards
   - Custom reports must pass validation (per FR-024)

3. **Update Command** (explicit opt-in only):
   ```bash
   agentready --update-research
   ```

4. **Update Source**:
   - Primary: `https://github.com/ambient-code/agentready/raw/main/src/agentready/data/agent-ready-codebase-attributes.md`
   - Downloads latest research report from main branch
   - User explicitly opts in to update

5. **Update Process** (atomic):
   ```
   a. Download new version to temp file
   b. Validate structure (per FR-024):
      - Check for required metadata (version, date)
      - Verify 25 attributes present
      - Validate Markdown format
      - Parse successfully with research_loader.py
   c. If valid:
      - Backup current: agent-ready-codebase-attributes.md ‚Üí .md.backup
      - Atomic rename: temp ‚Üí agent-ready-codebase-attributes.md
      - Remove backup on success
      - Display: "Updated research report from vX.Y.Z to vA.B.C"
   d. If invalid:
      - Delete temp file
      - Restore from backup if needed
      - Exit with error message and validation details
      - Keep current version intact
   ```

6. **Version Tracking**:
   - Research report includes metadata header:
     ```markdown
     ---
     version: 1.2.0
     date: 2025-11-20
     ---
     ```
   - Tool displays current research version:
     ```bash
     agentready --research-version
     # Output: Research report v1.2.0 (2025-11-20)
     ```

7. **Offline Behavior**:
   - If network unavailable: "Cannot reach update server. Using bundled version v1.0.0"
   - If update fails: "Update failed (validation error). Using current version v1.0.0"
   - Never leave tool in broken state
   - Always fall back to last known-good version

8. **Rollback**:
   ```bash
   agentready --restore-bundled-research  # Restore original from package
   ```
   - Restores research report from tool installation package
   - Useful if custom/updated research causes issues

**Error Handling**:
- Network timeout (30s): Graceful failure, keep current version
- Invalid format: Detailed validation error, rollback to previous
- Permission denied: Error with instructions to check file permissions
- Partial download: Checksum validation, retry or abort

**Testing** (integration test):
- Mock HTTP server with valid/invalid research reports
- Verify atomic updates (no partial writes)
- Test rollback on validation failure
- Confirm offline graceful degradation
- Verify version detection and display

**Philosophy**: Research report updates are explicitly opt-in only via `--update-research`. Standard workflow uses stable bundled version. Users who want latest research or custom criteria can update or override as needed.

## Constitution Check

*GATE: Must pass before Phase 0 research. Re-check after Phase 1 design.*

### Principle I: Evidence-Based Design
‚úÖ **PASS** - All 25 attributes derived from agent-ready-codebase-attributes.md research report with 50+ authoritative citations. Every assessment criterion grounded in documented findings from Anthropic, Microsoft, Google, ArXiv, IEEE/ACM sources.

### Principle II: Measurable Quality
‚úÖ **PASS** - Each attribute has:
- Clear criteria (e.g., "CLAUDE.md exists and <1000 lines", "test coverage >80%")
- Automated tooling specified (radon, pytest-cov, gitpython, lizard)
- Quantifiable thresholds from research report
- Good/bad examples in research report

### Principle III: Tool-First Mindset
‚úÖ **PASS** - Architecture follows library-first approach:
- Standalone assessor modules (importable, independently testable)
- CLI wrapper as thin orchestration layer
- Text-based I/O (repository path in ‚Üí JSON/Markdown/HTML out)
- Each assessor usable without full agentready toolchain

### Principle IV: Test-Driven Development
‚úÖ **PASS** - TDD workflow planned:
- Phase 2 will generate test tasks BEFORE implementation tasks
- Contract tests for report schemas (validate structure)
- Integration tests for scan workflows
- Unit tests for each attribute assessor
- Test coverage >80% enforced

### Principle V: Structured Output
‚úÖ **PASS** - Dual-format output design:
- HTML for human consumption (interactive, visual)
- Markdown for version control/automation (parseable, diffable)
- Internal JSON representation (structured data model)
- Structured logging with progress indicators
- Error messages with context and guidance (FR-030)

### Principle VI: Incremental Delivery
‚úÖ **PASS** - User stories prioritized for MVP-first:
- P1 (MVP): Core scoring engine ‚Üí Delivers immediate value
- P2: Interactive HTML features ‚Üí Enhanced UX
- P3: Remediation guidance ‚Üí Actionable next steps
- P4: Version control integration ‚Üí Team collaboration
- Each story independently testable and deployable

### Principle VII: Documentation as Code
‚úÖ **PASS** - Documentation strategy:
- README with <5 minute quickstart (generated in Phase 1)
- CLAUDE.md with project context (to be created)
- Inline docstrings explaining "why" (assessment rationale)
- Examples in quickstart.md
- Research report citations preserved in all outputs

**GATE STATUS**: ‚úÖ ALL PRINCIPLES PASS - Proceed to Phase 0 research

## Project Structure

### Documentation (this feature)

```text
specs/001-agentready-scorer/
‚îú‚îÄ‚îÄ spec.md              # Feature specification
‚îú‚îÄ‚îÄ plan.md              # This file (/speckit.plan command output)
‚îú‚îÄ‚îÄ research.md          # Phase 0 output (/speckit.plan command)
‚îú‚îÄ‚îÄ data-model.md        # Phase 1 output (/speckit.plan command)
‚îú‚îÄ‚îÄ quickstart.md        # Phase 1 output (/speckit.plan command)
‚îú‚îÄ‚îÄ contracts/           # Phase 1 output (/speckit.plan command)
‚îÇ   ‚îú‚îÄ‚îÄ assessment-schema.json    # Assessment data model JSON schema
‚îÇ   ‚îú‚îÄ‚îÄ report-markdown-schema.md # Markdown report structure
‚îÇ   ‚îî‚îÄ‚îÄ report-html-schema.md     # HTML report structure/API
‚îî‚îÄ‚îÄ checklists/
    ‚îî‚îÄ‚îÄ requirements.md  # Validation checklist
```

### Source Code (repository root)

```text
src/
‚îú‚îÄ‚îÄ agentready/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ cli/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ main.py           # Click-based CLI entry point
‚îÇ   ‚îú‚îÄ‚îÄ assessors/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ base.py           # Base assessor interface
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ documentation.py  # Attributes 1.1-2.3 (CLAUDE.md, README, docstrings, ADRs)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ code_quality.py   # Attributes 3.1-3.4 (complexity, length, types, smells)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ structure.py      # Attributes 4.1-4.2 (layout, separation of concerns)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ testing.py        # Attributes 5.1-5.3 (coverage, naming, pre-commit)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ dependencies.py   # Attributes 6.1-6.2 (lock files, freshness/security)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ vcs.py            # Attributes 7.1-7.3 (commits, gitignore, templates)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ build.py          # Attributes 8.1-8.3 (one-command setup, docs, containers)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ errors.py         # Attributes 9.1-9.2 (error clarity, structured logging)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ api_docs.py       # Attributes 10.1-10.2 (OpenAPI, GraphQL)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ modularity.py     # Attributes 11.1-11.3 (DRY, naming, semantic files)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ cicd.py           # Attributes 12.1-12.2 (pipeline visibility, branch protection)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ security.py       # Attributes 13.1-13.2 (scanning, secrets)
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ performance.py    # Attribute 15.1 (benchmarks)
‚îÇ   ‚îú‚îÄ‚îÄ models/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ repository.py     # Repository entity
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ attribute.py      # Attribute definition
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ assessment.py     # Assessment entity
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ finding.py        # Finding entity
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ config.py         # Configuration model
‚îÇ   ‚îú‚îÄ‚îÄ services/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ scanner.py        # Orchestrates assessment workflow
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ scorer.py         # Calculates weighted scores
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ language_detector.py  # Detects repository languages
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ research_loader.py    # Loads bundled/updated research report
‚îÇ   ‚îú‚îÄ‚îÄ reporters/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ base.py           # Base reporter interface
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ markdown.py       # Markdown report generator
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ html.py           # HTML report generator (with embedded JS/CSS)
‚îÇ   ‚îú‚îÄ‚îÄ templates/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ report.html.j2    # Jinja2 template for HTML report
‚îÇ   ‚îî‚îÄ‚îÄ data/
‚îÇ       ‚îú‚îÄ‚îÄ agent-ready-codebase-attributes.md  # Bundled research report
‚îÇ       ‚îî‚îÄ‚îÄ default-weights.yaml                 # Default tier-based weights
‚îÇ
tests/
‚îú‚îÄ‚îÄ unit/
‚îÇ   ‚îú‚îÄ‚îÄ test_assessors.py     # Individual assessor tests
‚îÇ   ‚îú‚îÄ‚îÄ test_models.py        # Data model tests
‚îÇ   ‚îú‚îÄ‚îÄ test_services.py      # Service layer tests
‚îÇ   ‚îî‚îÄ‚îÄ test_reporters.py     # Report generator tests
‚îú‚îÄ‚îÄ integration/
‚îÇ   ‚îú‚îÄ‚îÄ test_scan_workflow.py      # End-to-end scan tests
‚îÇ   ‚îú‚îÄ‚îÄ test_report_generation.py  # Dual-format report tests
‚îÇ   ‚îî‚îÄ‚îÄ test_cli.py                # CLI interface tests
‚îî‚îÄ‚îÄ contract/
    ‚îú‚îÄ‚îÄ test_assessment_schema.py  # JSON schema validation
    ‚îú‚îÄ‚îÄ test_markdown_schema.py    # Markdown structure validation
    ‚îî‚îÄ‚îÄ test_html_schema.py        # HTML structure validation

pyproject.toml                # Project metadata, dependencies, build config
README.md                     # Project overview and quickstart
CLAUDE.md                     # Claude Code context file
.agentready-config.example.yaml  # Example configuration file
```

**Structure Decision**: Single project (Option 1) selected. This is a CLI tool with library modules, not a web/mobile application. The structure follows Python src-layout convention with clear separation between CLI (thin wrapper), core logic (assessors, services, models), and output (reporters). Tests mirror source structure for easy navigation.

## Complexity Tracking

> **No violations** - All constitution principles pass without requiring justification.
````

## File: specs/001-agentready-scorer/quickstart.md
````markdown
# QuickStart: AgentReady Repository Scorer

**Goal**: Get from repository clone to running your first assessment in <5 minutes

**Prerequisites**: Python 3.11+ installed

---

## Installation

### Option 1: Install from PyPI (Recommended)

```bash
# Create and activate virtual environment (recommended)
python3 -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install agentready
pip install agentready

# Verify installation
agentready --version
```

### Option 2: Install from Source

```bash
# Clone repository
git clone https://github.com/your-org/agentready.git
cd agentready

# Install with development dependencies
pip install -e ".[dev]"

# Verify installation
agentready --version
```

---

## Basic Usage

### Run Assessment on Current Directory

```bash
# Navigate to your repository
cd /path/to/your/repository

# Run assessment (uses current directory)
agentready

# Reports saved to .agentready/ directory
```

### Run Assessment on Specific Repository

```bash
# Assess a different repository
agentready /path/to/other/repository

# Or use relative paths
agentready ../my-project
```

### View Generated Reports

```bash
# Open HTML report in browser
open .agentready/report-latest.html     # macOS
xdg-open .agentready/report-latest.html # Linux
start .agentready/report-latest.html    # Windows

# View Markdown report
cat .agentready/report-latest.md

# Or open in your favorite markdown viewer
code .agentready/report-latest.md  # VS Code
```

---

## Common Options

### Verbose Output

See detailed progress during assessment:

```bash
agentready --verbose
```

Output:
```
üîç Analyzing repository: /home/user/myproject
‚úì Validated git repository
‚úì Detected languages: Python, Markdown
‚úì Loaded research report (v1.0.0)

üìä Assessing attributes:
  [1/25] ‚úÖ CLAUDE.md Configuration Files... 100/100
  [2/25] ‚ö†Ô∏è  Concise Structured Documentation... 60/100
  [3/25] ‚úÖ README Structure... 85/100
  ...
  [25/25] ‚ùå Performance Benchmarks... Skipped (no benchmarks found)

üìÑ Generating reports...
  ‚úì HTML report: .agentready/report-2025-11-20T14-30-00.html
  ‚úì Markdown report: .agentready/report-2025-11-20T14-30-00.md

‚ú® Assessment complete!
   Score: 72/100 (Silver)
   Duration: 2m 7s
```

### Custom Output Directory

Save reports to a specific location:

```bash
agentready --output-dir /custom/path
```

Reports will be saved to `/custom/path/` instead of `.agentready/`

### Update Research Report

Download latest version of the research report:

```bash
agentready --update-research
```

This updates the bundled research report with the latest findings and recommendations.

---

## Configuration File

### Create Custom Configuration

```bash
# Generate example configuration
agentready --generate-config

# This creates .agentready-config.yaml
```

### Example Configuration

```yaml
# .agentready-config.yaml

# Custom attribute weights (must sum to 1.0)
weights:
  claude_md_file: 0.10          # Increase importance (default: 0.08)
  test_coverage: 0.10           # Increase importance (default: 0.05)
  conventional_commits: 0.05    # Keep default
  # ... other attributes use defaults

# Exclude specific attributes from assessment
excluded_attributes:
  - performance_benchmarks  # Skip if not relevant to your project

# Override language detection
language_overrides:
  Python:
    - "*.pyx"    # Include Cython files as Python
  JavaScript:
    - "*.jsx"
    - "*.tsx"    # Include TypeScript/React files

# Custom output directory (overrides --output-dir flag)
output_dir: reports/agentready
```

### Use Configuration

```bash
# agentready automatically loads .agentready-config.yaml from current directory
agentready

# Or specify a different config file
agentready --config /path/to/config.yaml
```

---

## Understanding Your Results

### Certification Levels

| Level | Score | Meaning |
|-------|-------|---------|
| ü•á Platinum | 90-100 | Exemplary agent-ready codebase |
| ü•á Gold | 75-89 | Highly optimized for AI agents |
| ü•à Silver | 60-74 | Well-suited for AI development |
| ü•â Bronze | 40-59 | Basic agent compatibility |
| ‚õî Needs Improvement | 0-39 | Significant work needed |

### Prioritizing Improvements

The HTML report's "Next Steps" section shows top improvements ranked by:
1. **Point Potential**: How many points you could gain
2. **Tier Priority**: Tier 1 (Essential) improvements listed first
3. **Ease of Fix**: Quick wins highlighted

**Example Priority**:
```
Next Steps to Gold (need +3 points):
1. Fix README size (+10 potential) - Tier 1 - Easy
2. Add pre-commit hooks (+8 potential) - Tier 2 - Medium
3. Improve dependency freshness (+5 potential) - Tier 2 - Easy
```

Focus on Tier 1 and Tier 2 improvements first for maximum impact.

---

## Tracking Progress Over Time

### Compare Reports

```bash
# Run assessment
agentready

# Make improvements to your repository
# (e.g., add CLAUDE.md, improve test coverage, add pre-commit hooks)

# Run assessment again
agentready

# Compare Markdown reports (they're git-friendly!)
git diff .agentready/report-2025-11-20T14-30-00.md .agentready/report-2025-11-20T16-45-00.md
```

### Commit Reports to Version Control

```bash
# Add reports to git for historical tracking
git add .agentready/report-*.md
git commit -m "docs: add agent-ready assessment (Silver, 72/100)"

# Include in pull requests to show improvements
```

---

## Troubleshooting

### "Not a git repository" Error

```bash
Error: /path/to/directory is not a git repository (no .git directory found)
```

**Solution**: Ensure you're in a git repository root:
```bash
git init  # If needed
git status  # Verify
```

### Missing Tool Warnings

```bash
‚è≠Ô∏è Skipped: Cyclomatic Complexity (missing tool: radon)
```

**Solution**: Install missing language-specific tools:
```bash
# Python complexity analysis
pip install radon

# Multi-language complexity
pip install lizard

# Re-run assessment
agentready
```

### Slow Assessment (>5 minutes)

```bash
‚ö†Ô∏è Assessment took 8m 32s (expected <5m for <10k files)
```

**Causes**:
- Very large repository (>10k files)
- Slow disk I/O
- Many languages to analyze

**Solutions**:
- Use `--exclude` to skip vendor/generated directories
- Run on SSD for faster file scanning
- Consider assessing specific subdirectories

### Permission Errors

```bash
‚è≠Ô∏è Skipped: File Size Limits (permission denied: /protected/directory)
```

**Solution**: Ensure read access to all repository files or accept partial results (agentready continues with available data).

---

## Next Steps

1. **Review HTML Report**: Open `.agentready/report-latest.html` in browser
2. **Check Top Priorities**: Look at "Next Steps" section
3. **Make Improvements**: Start with Tier 1 (Essential) attributes
4. **Re-assess**: Run `agentready` again to see score improvement
5. **Track Progress**: Commit reports to git for historical comparison

---

## Examples

### Assess Multiple Repositories

```bash
#!/bin/bash
# assess-all.sh - Assess all repositories in a directory

for repo in ~/projects/*/; do
    echo "Assessing $repo..."
    agentready "$repo" --output-dir "$repo/.agentready"
done
```

### CI/CD Integration

```yaml
# .github/workflows/agentready.yml
name: Agent-Ready Assessment

on:
  pull_request:
    branches: [ main ]

jobs:
  assess:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'

      - name: Install agentready
        run: pip install agentready

      - name: Run assessment
        run: agentready --output-dir reports/

      - name: Upload report
        uses: actions/upload-artifact@v3
        with:
          name: agentready-report
          path: reports/report-latest.html

      - name: Comment score on PR
        run: |
          SCORE=$(cat reports/report-latest.md | grep "Overall Score" | awk '{print $3}')
          echo "Agent-Ready Score: $SCORE" >> $GITHUB_STEP_SUMMARY
```

### Custom Weighting for Organization

```yaml
# .agentready-config.yaml
# Enterprise configuration emphasizing security and testing

weights:
  # Security-focused (30% total)
  security_scanning: 0.10
  secrets_management: 0.10
  dependency_freshness: 0.10

  # Testing-focused (25% total)
  test_coverage: 0.10
  test_naming: 0.08
  pre_commit_hooks: 0.07

  # Documentation (20% total)
  claude_md_file: 0.08
  readme_structure: 0.07
  inline_documentation: 0.05

  # Code quality (15% total)
  type_annotations: 0.06
  cyclomatic_complexity: 0.05
  code_smells: 0.04

  # Remaining attributes split remaining 10%
  # (not listed = use calculated defaults from remaining weight)
```

---

## Getting Help

```bash
# Show all available commands and options
agentready --help

# Show version information
agentready --version

# Show research report version
agentready --research-version

# Generate example configuration
agentready --generate-config

# Validate existing configuration
agentready --validate-config
```

---

## Quick Reference

| Command | Purpose |
|---------|---------|
| `agentready` | Assess current directory |
| `agentready /path` | Assess specific repository |
| `agentready --verbose` | Show detailed progress |
| `agentready --output-dir DIR` | Custom output location |
| `agentready --config FILE` | Use custom configuration |
| `agentready --update-research` | Update research report |
| `agentready --generate-config` | Create example config |
| `agentready --help` | Show all options |

---

**Time from install to first report**: ~2 minutes
**Typical assessment duration**: 2-5 minutes
**Report formats**: HTML (interactive) + Markdown (version control)
**Default output**: `.agentready/` directory

üéØ **Goal Achieved**: You now have a comprehensive assessment of your repository's AI-readiness!
````

## File: specs/001-agentready-scorer/research.md
````markdown
# Research: AgentReady Repository Scorer

**Phase**: 0 - Technical Research and Best Practices
**Date**: 2025-11-20
**Purpose**: Document technical decisions, rationale, and best practices for implementation

## 1. CLI Framework Selection

### Decision: Click

**Rationale**:
- Declarative command definition with decorators (clean, readable)
- Automatic help generation and input validation
- Native support for subcommands, options, and arguments
- Excellent error handling with user-friendly messages
- Wide adoption in Python CLI tools (pytest, pip, AWS CLI v2)

**Alternatives Considered**:
- `argparse` (stdlib): Too verbose, manual help text, basic validation
- `typer`: Type-hint based, but adds FastAPI dependency (unnecessary weight)
- `docopt`: String-based interface description (less type-safe)

**Best Practices**:
- Use `click.Path(exists=True, file_okay=False)` for repository path validation
- Implement `--verbose` with `click.option('--verbose', is_flag=True)`
- Use `click.echo()` for stdout (handles encoding across platforms)
- Use `click.secho()` for colored output (certification levels, progress)
- Implement progress bars with `click.progressbar()`

## 2. HTML Report Generation

### Decision: Jinja2 + Embedded CSS/JavaScript

**Rationale**:
- Jinja2 is industry-standard Python templating (Django, Flask)
- Template-based approach separates presentation from logic
- Easy to maintain and update report layout
- Embedding CSS/JS ensures offline functionality (FR-003)
- Single-file output simplifies distribution

**Alternatives Considered**:
- Direct HTML string building: Unmaintainable, error-prone
- WeasyPrint (HTML‚ÜíPDF): Not required, HTML itself is deliverable
- React/Vue SPA: Requires build step, complex for static reports

**Best Practices**:
- Inline all CSS in `<style>` tag (avoid external stylesheets)
- Inline all JavaScript in `<script>` tag (no CDN dependencies)
- Use vanilla JavaScript (no jQuery/framework dependencies)
- Minify embedded CSS/JS for smaller file size
- Use CSS Grid/Flexbox for responsive layout (modern browser support)
- Implement collapsible sections with `<details>` and `<summary>` tags (native HTML5)
- Add `data-*` attributes for JavaScript hooks (clean separation)

**Interactive Features Implementation**:
- Filtering: JavaScript array `.filter()` on attribute findings
- Sorting: JavaScript array `.sort()` with custom comparators
- Collapsible sections: CSS `:target` pseudo-class + JavaScript event listeners
- Progress bars: CSS `width` percentage on div elements
- Color coding: CSS classes based on score thresholds

## 3. Language Detection Strategy

### Decision: File Extension Analysis + gitignore Awareness

**Rationale**:
- File extensions are reliable primary indicator (.py, .js, .ts, .go, .java)
- gitignore exclusion prevents counting generated/vendor code
- Fast scanning (no file content parsing required for detection)
- Supports polyglot repositories (multiple languages)

**Alternatives Considered**:
- GitHub Linguist: Heavyweight dependency, overkill for detection
- File content analysis (shebangs, syntax): Slow, unnecessary
- Package manager detection only: Misses standalone scripts

**Best Practices**:
- Scan repository once, cache language map
- Respect .gitignore patterns (use gitpython's `git ls-files`)
- Map extensions to languages: `.py`‚ÜíPython, `.js/.jsx`‚ÜíJavaScript, `.ts/.tsx`‚ÜíTypeScript
- Handle ambiguous extensions (`.h` could be C or C++)
- Set minimum file threshold (e.g., 5+ files to count as "using language")

## 4. Attribute Weighting System

### Decision: Tier-Based Default + YAML Configuration Override

**Rationale**:
- Research report defines 4 tiers (Essential ‚Üí Advanced)
- Default weights encode tier priorities (Tier 1 highest impact)
- YAML configuration allows organizational customization
- Weights must sum to 1.0 (percentages) for interpretable scores

**Default Weight Distribution**:
```
Tier 1 (Essential) - 5 attributes:    40% total (8% each)
Tier 2 (Critical) - 6 attributes:     30% total (5% each)
Tier 3 (Important) - 7 attributes:    20% total (~2.86% each)
Tier 4 (Advanced) - 7 attributes:     10% total (~1.43% each)
```

**Configuration File Format** (.agentready-config.yaml):
```yaml
weights:
  claude_md_file: 0.08
  conventional_commits: 0.05
  type_annotations: 0.08
  # ... all 25 attributes

# Optional: Override tier defaults
tier_multipliers:
  tier_1: 2.0  # Double importance
  tier_2: 1.5
  tier_3: 1.0
  tier_4: 0.5
```

**Best Practices**:
- Validate weights sum to 1.0 ¬± 0.01 (floating point tolerance)
- Provide clear error messages for invalid configurations
- Document weight rationale in example config file
- Allow partial overrides (missing attributes use defaults)

## 5. Assessment Architecture Pattern

### Decision: Strategy Pattern with Base Assessor Interface

**Rationale**:
- Each attribute is an independent assessment strategy
- Common interface ensures consistency across assessors
- Easy to add new attributes or modify existing ones
- Enables parallel assessment execution (future optimization)
- Supports graceful degradation (skip failed assessors)

**Base Assessor Interface**:
```python
class BaseAssessor(ABC):
    @property
    @abstractmethod
    def attribute_id(self) -> str:
        """Unique attribute identifier (e.g., 'claude_md_file')"""

    @property
    @abstractmethod
    def tier(self) -> int:
        """Tier 1-4 from research report"""

    @abstractmethod
    def assess(self, repository: Repository) -> Finding:
        """Execute assessment, return Finding with score, evidence, remediation"""

    def is_applicable(self, repository: Repository) -> bool:
        """Check if attribute applies to this repository (e.g., language-specific)"""
        return True
```

**Best Practices**:
- Keep assessors stateless (no instance variables)
- Return structured Finding objects (never raw dicts)
- Include specific evidence (file paths, line numbers, measurements)
- Generate remediation guidance from research report content
- Handle errors gracefully (return Finding with error status, not exceptions)
- Use type hints for all method signatures

## 6. Repository Analysis Tools

### Decision: gitpython + Language-Specific Analyzers

**Primary Tools**:
- **gitpython**: Repository metadata, commit history, .gitignore parsing
- **radon**: Python cyclomatic complexity, maintainability index
- **lizard**: Multi-language complexity analysis (C, C++, Java, JavaScript, etc.)
- **stdlib pathlib**: File system traversal and pattern matching

**Rationale**:
- gitpython provides Git-native operations (respects .gitignore)
- radon is Python-specific, highly accurate for Python codebases
- lizard supports multiple languages with consistent interface
- Avoid shell calls to external tools (portability, security)

**Best Practices**:
- Cache file system scans (don't traverse repository multiple times)
- Use gitpython's `git.Repo.iter_commits()` for commit analysis
- Lazy-load analysis tools (import only when language detected)
- Set reasonable limits (max files to analyze, timeout per check)
- Provide progress callbacks for long-running analyses

## 7. Error Handling and Graceful Degradation

### Decision: Try-Assess-Skip Pattern

**Rationale**:
- Partial results more valuable than complete failure
- Users can fix blockers (install missing tools) and re-run
- Clear error reporting enables self-service troubleshooting
- Maintains progress indication (X/25 attributes assessed)

**Error Handling Strategy**:
```python
for assessor in assessors:
    try:
        if not assessor.is_applicable(repo):
            findings.append(Finding.not_applicable(assessor.attribute_id))
            continue

        finding = assessor.assess(repo)
        findings.append(finding)

    except MissingToolError as e:
        findings.append(Finding.skipped(
            assessor.attribute_id,
            reason=f"Missing tool: {e.tool_name}",
            remediation=f"Install with: {e.install_command}"
        ))

    except PermissionError as e:
        findings.append(Finding.skipped(
            assessor.attribute_id,
            reason=f"Permission denied: {e.filename}"
        ))

    except Exception as e:
        findings.append(Finding.error(
            assessor.attribute_id,
            reason=f"Unexpected error: {str(e)}"
        ))
        logger.exception(f"Assessment failed for {assessor.attribute_id}")
```

**Best Practices**:
- Define custom exception types for known failure modes
- Include remediation guidance in error findings
- Log full stack traces for unexpected errors (debugging)
- Count skipped/error attributes separately from assessed
- Adjust score calculation (exclude skipped from denominator per FR-027)

## 8. Report File Naming and Versioning

### Decision: Timestamp-Based Filenames with Latest Symlink

**Rationale**:
- Timestamps enable tracking score improvements over time
- Unique filenames prevent accidental overwrites
- Symlink to latest makes it easy to find most recent report
- ISO 8601 format ensures sortable filenames

**File Naming Convention**:
```
.agentready/
‚îú‚îÄ‚îÄ report-2025-11-20T14-30-00.html
‚îú‚îÄ‚îÄ report-2025-11-20T14-30-00.md
‚îú‚îÄ‚îÄ report-latest.html -> report-2025-11-20T14-30-00.html
‚îî‚îÄ‚îÄ report-latest.md -> report-2025-11-20T14-30-00.md
```

**Best Practices**:
- Use ISO 8601 with colons replaced by hyphens (cross-platform safe)
- Create symlinks atomically (write to temp, rename)
- Offer `--output-name` flag for custom naming
- Clean up old reports (keep last N runs, configurable)
- Store assessment metadata in JSON sidecar file (for automation)

## 9. Markdown Report Structure

### Decision: GitHub-Flavored Markdown with Tables

**Rationale**:
- GFM is ubiquitous (GitHub, GitLab, Bitbucket, VS Code)
- Tables display structured data clearly
- Collapsible sections with `<details>` work in GFM
- Easy to diff in version control

**Report Structure**:
```markdown
# AgentReady Assessment Report

**Repository**: owner/repo
**Date**: 2025-11-20 14:30:00
**Score**: 72/100 (Silver)
**Attributes Assessed**: 23/25 (2 skipped)

## Summary

| Category | Score | Status |
|----------|-------|--------|
| Documentation | 85% | ‚úÖ Pass |
| Code Quality | 62% | ‚ö†Ô∏è Needs Improvement |
| ... | ... | ... |

## Detailed Findings

### 1. Context Window Optimization

#### 1.1 CLAUDE.md Configuration Files ‚úÖ

**Score**: 100/100
**Evidence**:
- File exists: `CLAUDE.md` (487 lines)
- Contains tech stack section
- Contains standard commands

<details>
<summary>Good Practices Found</summary>

- Concise format (<1000 lines)
- Clear repository structure
</details>

#### 1.2 Concise Structured Documentation ‚ö†Ô∏è

**Score**: 60/100
**Evidence**:
- README exists (1247 lines) - exceeds recommended 500 lines

**Remediation**:
- Split README into multiple docs
- Move detailed API docs to wiki
```

**Best Practices**:
- Use emoji sparingly (‚úÖ‚ùå‚ö†Ô∏è for status only)
- Include clickable table of contents for long reports
- Preserve all research report citations
- Use code blocks with language tags for remediation commands
- Keep tables simple (3-5 columns maximum)

## 10. Testing Strategy

### Decision: Pytest with Fixtures + Test Repositories

**Rationale**:
- Pytest is Python's de-facto testing framework
- Fixtures enable reusable test repositories
- Test repositories provide known-good/known-bad data
- Contract tests validate report schemas

**Test Repository Approach**:
Create fixture repositories in `tests/fixtures/repositories/`:
- `minimal-python/`: Bare Python project (most attributes fail)
- `gold-standard-python/`: Exemplary Python project (high score)
- `polyglot/`: Multi-language repository
- `edge-cases/`: Unusual structures (monorepo, submodules, etc.)

**Best Practices**:
- Use `@pytest.fixture(scope="session")` for repository setup (expensive)
- Store expected scores/findings in JSON files
- Test each assessor in isolation (unit tests)
- Test full workflow with real git repositories (integration tests)
- Use `pytest-cov` to enforce >80% coverage
- Mock external tools in unit tests, use real tools in integration tests

## 11. Proportional Scoring Algorithm

### Decision: Linear Proportional Scoring

**Context**: Many attributes have measurable thresholds that can be partially met (e.g., 65% test coverage when 80% is required). Linear proportional scoring provides deterministic, understandable results (per FR-014).

**Rationale**: Linear proportional scoring is:
- **Simple**: Easy to understand and explain to users
- **Deterministic**: Same inputs always produce same outputs
- **Fair**: Provides clear incentives for incremental improvement
- **Predictable**: Users can calculate expected score changes

**Algorithm**:
```python
def calculate_proportional_score(measured_value, threshold, attribute_type):
    """
    Calculate proportional score for partial compliance.

    Args:
        measured_value: The measured value (numeric or parseable)
        threshold: The target threshold
        attribute_type: 'higher_is_better' or 'lower_is_better'

    Returns:
        Score from 0-100
    """
    if attribute_type == 'higher_is_better':
        # Example: test coverage (want higher values)
        if measured_value >= threshold:
            return 100
        elif measured_value <= 0:
            return 0
        else:
            return (measured_value / threshold) * 100

    elif attribute_type == 'lower_is_better':
        # Example: file length (want lower values)
        if measured_value <= threshold:
            return 100
        elif threshold == 0:
            return 0  # Avoid division by zero
        else:
            # Degrade linearly, cap at 0
            return max(0, 100 - ((measured_value - threshold) / threshold) * 100)
```

**Edge Cases**:
- Division by zero: Return 0 score
- Negative values: Clamp to 0
- Values exceeding 2x threshold (lower_is_better): Cap at 0
- Values exceeding 2x threshold (higher_is_better): Cap at 100

**Examples**:
- Test coverage: 65% measured, 80% threshold ‚Üí 65/80 * 100 = 81.25 score
- File length: 450 lines measured, 300 threshold ‚Üí 100 - ((450-300)/300)*100 = 50 score
- Cyclomatic complexity: 5 measured, 10 threshold ‚Üí 100 (meets threshold)

**Alternatives Considered**:
- Exponential penalties: Too harsh for minor violations
- Sigmoid curves: Complex to explain, non-obvious behavior
- Step functions: Too coarse-grained (all-or-nothing)

**References**:
- Spec FR-014: "Tool MUST handle repositories that partially meet attribute criteria"
- Spec FR-027: "Tool MUST calculate overall score based only on successfully evaluated attributes"

## Summary

All technical decisions grounded in pragmatic Python ecosystem best practices. No research gaps or unresolved questions. Implementation can proceed directly to Phase 1 (data modeling and contracts).
````

## File: specs/001-agentready-scorer/spec.md
````markdown
# Feature Specification: AgentReady Repository Scorer

**Feature Branch**: `001-agentready-scorer`
**Created**: 2025-11-20
**Status**: Draft
**Input**: User description: "I want to build a tool that scores git repos based on the findings of this report: agent-ready-codebase-attributes.md. The goal is to prepare a git repo for introduction of agentic development patterns. The tools goal is to provide repo owners with a report and brief remediation plan for each item that requires remediation. the tool must be called agentready. use the content of the report to generate remediation plans etc. i want the report to be a single html file that is interactive. i also want a markdown version of the report to be generated. always generate both formats when the tool is run"

## Clarifications

### Session 2025-11-20

- Q: Where should the agentready tool save the generated HTML and Markdown reports? ‚Üí A: `.agentready/` directory in repository root by default, with user-specified output directory via CLI flag as option
- Q: How does the agentready tool locate the agent-ready-codebase-attributes.md research report file? ‚Üí A: Research report bundled with tool installation, with support for downloading latest version from URL/repository
- Q: What should happen if the tool cannot complete assessment for some attributes (e.g., missing analysis tools, permission errors)? ‚Üí A: Complete available attributes, clearly flag failed/skipped attributes in report with reasons
- Q: What should the tool display to stdout/stderr during execution (before reports are generated)? ‚Üí A: Summary progress by default, detailed real-time logging with verbose flag
- Q: How should attribute weights be determined for calculating the overall agent-readiness score? ‚Üí A: Fixed weights based on research report tier system by default, with user-configurable weights via configuration file

## User Scenarios & Testing *(mandatory)*

### User Story 1 - Score Repository Against Agent-Ready Criteria (Priority: P1)

A repository owner wants to assess how well their codebase is optimized for AI-assisted development. They run the agentready tool on their repository and receive a comprehensive score with specific findings across all 25 agent-ready attributes documented in the research report.

**Why this priority**: This is the core value proposition - without scoring and assessment, there is no actionable feedback. This alone provides immediate value by identifying gaps.

**Independent Test**: Can be fully tested by running the tool against any git repository and verifying it produces a score and identifies attribute compliance levels. Delivers a complete assessment without needing remediation features.

**Acceptance Scenarios**:

1. **Given** a git repository with various quality attributes, **When** user runs agentready tool on repository, **Then** tool analyzes all 25 attributes from the research report and produces an overall agent-readiness score
2. **Given** a repository missing CLAUDE.md file, **When** agentready scans the repository, **Then** tool correctly identifies this attribute as non-compliant and includes it in the assessment
3. **Given** a repository with 85% test coverage, **When** agentready evaluates test coverage attribute, **Then** tool correctly identifies this as meeting the >80% threshold and scores it positively
4. **Given** a repository with no lock files, **When** agentready checks dependency management, **Then** tool flags missing lock files and includes specific examples of what's missing
5. **Given** analysis is complete, **When** tool generates reports, **Then** both HTML and Markdown versions are created with identical content

---

### User Story 2 - View Interactive HTML Report (Priority: P2)

After running the assessment, the repository owner views an interactive HTML report that allows them to explore findings by category, filter by compliance level, and drill into specific attribute details with expandable sections.

**Why this priority**: Enhances usability and makes large amounts of assessment data digestible. Builds on P1 by improving how results are consumed.

**Independent Test**: Can be tested by generating and opening the HTML report in a browser, verifying interactive elements (collapsible sections, filters, sorting) work without requiring the scoring engine.

**Acceptance Scenarios**:

1. **Given** agentready has completed assessment, **When** user opens the HTML report in browser, **Then** report displays overall score prominently with visual indicators (color coding, progress bars)
2. **Given** HTML report is displayed, **When** user clicks on an attribute category (e.g., "Documentation Standards"), **Then** section expands to show detailed findings for all attributes in that category
3. **Given** report shows multiple failing attributes, **When** user applies "show only failures" filter, **Then** report hides passing attributes and displays only those requiring remediation
4. **Given** an attribute is marked as failing, **When** user views that attribute in report, **Then** specific evidence is shown (e.g., "File size: src/large_module.py is 847 lines, exceeds 300 line limit")

---

### User Story 3 - Receive Remediation Guidance (Priority: P3)

For each attribute that fails compliance checks, the repository owner receives specific, actionable remediation steps based on the research report's recommendations, tooling suggestions, and examples.

**Why this priority**: Transforms assessment into action. Builds on P1 (scoring) and P2 (viewing) by providing the "what to do next" guidance.

**Independent Test**: Can be tested by reviewing remediation plans for known failing attributes and verifying they include specific tools, commands, examples, and links to documentation.

**Acceptance Scenarios**:

1. **Given** repository lacks CLAUDE.md file, **When** user views remediation for this attribute, **Then** guidance includes specific template, required sections, and example content
2. **Given** repository has cyclomatic complexity >25 in multiple functions, **When** user views remediation plan, **Then** plan lists specific files/functions exceeding threshold, suggests refactoring tools (radon for Python), and provides complexity reduction strategies
3. **Given** repository missing pre-commit hooks, **When** user views remediation, **Then** plan includes installation commands, sample configuration for the detected language, and links to pre-commit framework documentation
4. **Given** multiple attributes fail, **When** user views overall remediation plan, **Then** plan prioritizes fixes by impact tier (Tier 1 Essential ‚Üí Tier 4 Advanced) as defined in research report

---

### User Story 4 - Read Markdown Report for Version Control (Priority: P4)

Repository owners want to track agent-readiness improvements over time by committing Markdown reports to version control, allowing them to compare scores across commits and share findings with team members in pull requests.

**Why this priority**: Enables continuous improvement tracking and team collaboration. Enhances existing features but not critical for initial value delivery.

**Independent Test**: Can be tested by generating Markdown report, committing it to git, and verifying it renders properly on platforms like GitHub with all tables, formatting, and links intact.

**Acceptance Scenarios**:

1. **Given** agentready completes assessment, **When** Markdown report is generated, **Then** report uses standard Markdown formatting (tables, headers, links) that renders correctly on GitHub
2. **Given** Markdown report from previous run exists, **When** user compares it with new report, **Then** differences in scores and attribute compliance are easily identifiable
3. **Given** report contains external citations, **When** user views Markdown report, **Then** all citations from research report are properly linked and attributions preserved

---

### Edge Cases

- What happens when repository is not a git repository (no .git directory)? Tool aborts with clear error message before assessment begins (validated per FR-017)
- How does tool handle extremely large repositories (>100k files)? Tool completes assessment but may exceed 5-minute target (SC-001 applies to <10k files only)
- What if repository uses multiple programming languages with different standards? Tool evaluates all detected languages and applies language-specific criteria per FR-013
- How does tool behave when file access is denied (permission errors)? Tool skips inaccessible files, continues assessment, flags permission issues in report per FR-026
- What if repository has no code files (documentation-only repos)? Tool completes assessment with many attributes marked N/A or failing, still produces valid report
- How does tool handle repositories with submodules or monorepo structures? Tool evaluates repository as-is, treating submodules as part of file structure
- What if HTML/Markdown report generation fails mid-process? Both reports fail atomically per FR-019, no partial report files left
- How does tool handle repositories with non-standard layouts that don't match any common pattern? Tool attempts best-effort pattern matching, may score lower on structure-related attributes

## Requirements *(mandatory)*

### Functional Requirements

- **FR-001**: Tool MUST be named "agentready" and executable from command line
- **FR-002**: Tool MUST accept a git repository path as input (current directory if not specified)
- **FR-003**: Tool MUST evaluate all 25 attributes defined in agent-ready-codebase-attributes.md research report
- **FR-004**: Tool MUST calculate an overall agent-readiness score (0-100) using weighted average of attribute scores (weights from FR-031/FR-032)
- **FR-005**: Tool MUST generate both HTML and Markdown format reports in a single execution
- **FR-006**: Tool MUST include specific evidence for each attribute evaluation (file names, line counts, measurements)
- **FR-007**: Tool MUST provide remediation guidance for each failing attribute, derived from research report recommendations
- **FR-008**: HTML report MUST be a single self-contained file (embedded CSS/JavaScript, no external dependencies)
- **FR-009**: HTML report MUST include interactive features (collapsible sections, filtering, sorting)
- **FR-010**: Markdown report MUST use standard GitHub-flavored Markdown for maximum compatibility
- **FR-011**: Tool MUST preserve all citations and attributions from the research report in generated reports
- **FR-012**: Tool MUST categorize attributes by tier (Tier 1 Essential through Tier 4 Advanced) as defined in research report
- **FR-013**: Tool MUST detect programming language(s) used in repository and apply language-specific criteria
- **FR-014**: Tool MUST handle repositories that partially meet attribute criteria (e.g., 50% test coverage) with proportional scoring
- **FR-015**: Remediation plans MUST include specific tooling recommendations, commands, and configuration examples from research report
- **FR-016**: Reports MUST include certification level (Platinum/Gold/Silver/Bronze) based on score ranges defined in research report
- **FR-017**: Tool MUST validate git repository before starting assessment (presence of .git directory)
- **FR-018**: Tool MUST provide progress indication showing completed attributes count during analysis (superseded by FR-028/FR-029 for specifics)
- **FR-019**: Both report formats MUST be generated atomically (both succeed or both fail)
- **FR-020**: Tool MUST save reports to `.agentready/` directory in repository root by default, creating directory if it doesn't exist
- **FR-021**: Tool MUST support `--output-dir` CLI flag allowing users to specify custom output directory for reports
- **FR-022**: Tool MUST bundle agent-ready-codebase-attributes.md research report as internal resource accessible without external dependencies
- **FR-023**: Tool MUST support updating bundled research report by downloading latest version from authoritative URL/repository
- **FR-024**: Tool MUST validate research report integrity (format, required sections) before using it for assessment
- **FR-025**: Tool MUST complete assessment for all evaluable attributes even if some attributes fail, rather than aborting entire scan
- **FR-026**: Tool MUST clearly indicate skipped or failed attributes in reports with specific failure reasons (e.g., "missing tool: radon", "permission denied: /protected/file")
- **FR-027**: Tool MUST calculate overall score based only on successfully evaluated attributes, excluding failed/skipped attributes from denominator
- **FR-028**: Tool MUST display summary progress to stdout by default (e.g., "Analyzing... [12/25 attributes complete]")
- **FR-029**: Tool MUST support `--verbose` CLI flag that enables detailed real-time logging of each file analyzed and check performed
- **FR-030**: Tool MUST output errors to stderr with clear error messages and context
- **FR-031**: Tool MUST use tier-based weighting system from research report by default (Tier 1 Essential weighted higher than Tier 4 Advanced)
- **FR-032**: Tool MUST support optional configuration file allowing users to customize attribute weights for organizational priorities
- **FR-033**: Tool MUST validate configuration file weights (all attributes present, weights are positive numbers, sum to meaningful total)

### Key Entities

- **Repository**: The target git repository being assessed, characterized by root path, detected languages, file structure, and git metadata
- **Attribute**: One of 25 agent-ready quality attributes from research report, including name, category, tier, weight, measurable criteria, and assessment method
- **Assessment**: Complete evaluation of repository, including overall score, certification level, per-attribute results, timestamp, and repository metadata
- **Finding**: Individual attribute evaluation result, including pass/fail status, measured value, threshold, evidence (specific files/metrics), and remediation guidance
- **Report**: Generated output document (HTML or Markdown), containing assessment results, formatted for specific use case (interactive viewing vs. version control)
- **Remediation Plan**: Actionable guidance for fixing failing attribute, including tools, commands, examples, and citations from research report
- **Citation**: Reference to authoritative source from research report, including author, title, publication, URL, and relevance to specific attribute

### Assumptions

- Repository owners have command-line access to their repositories
- Standard file system permissions allow reading repository files
- Repository follows conventional version control practices (not bare repository)
- Target audience has basic understanding of software quality metrics
- Internet connectivity optional for tool execution (required only for downloading updated research report)
- Reports will be viewed in modern browsers (for HTML) or platforms supporting GitHub-flavored Markdown
- Repository size is reasonable for local file system traversal (not petabyte-scale)
- Programming language detection can be performed via file extensions and common patterns

## Success Criteria *(mandatory)*

### Measurable Outcomes

- **SC-001**: Users can run complete assessment and generate both reports in under 5 minutes for repositories with <10k files
- **SC-002**: Tool correctly identifies compliance for all 25 attributes with >95% accuracy when validated against known test repositories
- **SC-003**: Generated HTML reports are fully functional without internet connectivity (no external CDN dependencies)
- **SC-004**: Markdown reports render correctly on GitHub, GitLab, and Bitbucket platforms without formatting issues
- **SC-005**: 90% of remediation plans include specific, executable commands or tool configurations that users can apply immediately
- **SC-006**: Overall agent-readiness scores align within 5 points when validated against manual evaluation by domain expert
- **SC-007**: Tool successfully handles repositories in at least 5 major programming languages (Python, JavaScript, TypeScript, Go, Java)
- **SC-008**: Users can identify top 3 priority improvements within 2 minutes of viewing report (via filtering/sorting features)
- **SC-009**: 100% of citations and attributions from research report are preserved in generated reports
- **SC-010**: Tool produces consistent scores when run multiple times on same repository (deterministic results)
````

## File: src/agentready/assessors/base.py
````python
"""Base assessor interface for attribute evaluation."""

from abc import ABC, abstractmethod

from ..models.finding import Finding
from ..models.repository import Repository


class BaseAssessor(ABC):
    """Abstract base class for all attribute assessors.

    Each assessor evaluates one or more related attributes and returns
    structured findings with evidence, scores, and remediation guidance.

    Assessors follow the strategy pattern and are stateless for easy testing
    and parallel execution.
    """

    @property
    @abstractmethod
    def attribute_id(self) -> str:
        """Unique attribute identifier (e.g., 'claude_md_file').

        Must be lowercase snake_case matching the attribute ID in the
        research report and default-weights.yaml.
        """
        pass

    @property
    @abstractmethod
    def tier(self) -> int:
        """Tier 1-4 from research report (1=Essential, 4=Advanced)."""
        pass

    @abstractmethod
    def assess(self, repository: Repository) -> Finding:
        """Execute assessment and return Finding with score, evidence, remediation.

        Args:
            repository: Repository entity with path, languages, metadata

        Returns:
            Finding with status (pass/fail/skipped/error/not_applicable),
            score (0-100 if applicable), evidence, and remediation

        Raises:
            This method should NOT raise exceptions. Handle errors gracefully
            and return Finding.error() or Finding.skipped() instead.
        """
        pass

    def is_applicable(self, repository: Repository) -> bool:
        """Check if attribute applies to this repository.

        Default implementation returns True (attribute applies to all repos).
        Override for language-specific or conditional attributes.

        Args:
            repository: Repository entity with detected languages

        Returns:
            True if attribute should be assessed, False to skip

        Examples:
            - Python-specific check: return "Python" in repository.languages
            - API check: return any openapi/swagger files exist
        """
        return True

    def calculate_proportional_score(
        self,
        measured_value: float,
        threshold: float,
        higher_is_better: bool = True,
    ) -> float:
        """Calculate proportional score for partial compliance.

        Uses linear interpolation to score values between 0 and threshold.

        Args:
            measured_value: The measured value (e.g., 65 for coverage %)
            threshold: The target threshold (e.g., 80 for coverage %)
            higher_is_better: True for metrics like coverage, False for complexity

        Returns:
            Score from 0-100

        Examples:
            - Test coverage: 65% measured, 80% threshold, higher_is_better=True
              ‚Üí 65/80 * 100 = 81.25 score
            - File length: 450 lines measured, 300 threshold, higher_is_better=False
              ‚Üí 100 - ((450-300)/300)*100 = 50 score
        """
        if higher_is_better:
            # Want higher values (e.g., test coverage, type annotation %)
            if measured_value >= threshold:
                return 100.0
            elif measured_value <= 0:
                return 0.0
            else:
                return min(100.0, (measured_value / threshold) * 100.0)
        else:
            # Want lower values (e.g., complexity, file length)
            if measured_value <= threshold:
                return 100.0
            elif threshold == 0:
                return 0.0  # Avoid division by zero
            else:
                # Degrade linearly, cap at 0
                penalty = ((measured_value - threshold) / threshold) * 100.0
                return max(0.0, 100.0 - penalty)
````

## File: src/agentready/assessors/code_quality.py
````python
"""Code quality assessors for complexity, file length, type annotations, and code smells."""

import subprocess
from pathlib import Path

from ..models.attribute import Attribute
from ..models.finding import Citation, Finding, Remediation
from ..models.repository import Repository
from ..services.scanner import MissingToolError
from .base import BaseAssessor


class TypeAnnotationsAssessor(BaseAssessor):
    """Assesses type annotation coverage in code.

    Tier 1 Essential (10% weight) - Type hints are critical for AI understanding.
    """

    @property
    def attribute_id(self) -> str:
        return "type_annotations"

    @property
    def tier(self) -> int:
        return 1  # Essential

    @property
    def attribute(self) -> Attribute:
        return Attribute(
            id=self.attribute_id,
            name="Type Annotations",
            category="Code Quality",
            tier=self.tier,
            description="Type hints in function signatures",
            criteria=">80% of functions have type annotations",
            default_weight=0.10,
        )

    def is_applicable(self, repository: Repository) -> bool:
        """Only applicable to statically-typed or type-hinted languages."""
        applicable_languages = {
            "Python",
            "TypeScript",
            "Java",
            "C#",
            "Kotlin",
            "Go",
            "Rust",
        }
        return bool(set(repository.languages.keys()) & applicable_languages)

    def assess(self, repository: Repository) -> Finding:
        """Check type annotation coverage.

        For Python: Use mypy or similar
        For TypeScript: Check tsconfig.json strict mode
        For others: Heuristic checks
        """
        if "Python" in repository.languages:
            return self._assess_python_types(repository)
        elif "TypeScript" in repository.languages:
            return self._assess_typescript_types(repository)
        else:
            # For other languages, use heuristic
            return Finding.not_applicable(
                self.attribute,
                reason=f"Type annotation check not implemented for {list(repository.languages.keys())}",
            )

    def _assess_python_types(self, repository: Repository) -> Finding:
        """Assess Python type annotations using file inspection."""
        # Simple heuristic: count functions with/without type hints
        try:
            result = subprocess.run(
                ["git", "ls-files", "*.py"],
                cwd=repository.path,
                capture_output=True,
                text=True,
                timeout=30,
                check=True,
            )
            python_files = [f for f in result.stdout.strip().split("\n") if f]
        except (subprocess.SubprocessError, FileNotFoundError):
            python_files = [
                str(f.relative_to(repository.path))
                for f in repository.path.rglob("*.py")
            ]

        total_functions = 0
        typed_functions = 0

        for file_path in python_files:
            full_path = repository.path / file_path
            try:
                with open(full_path, "r", encoding="utf-8") as f:
                    for line in f:
                        line = line.strip()
                        if line.startswith("def ") and "(" in line:
                            total_functions += 1
                            # Check for type hints (-> in signature)
                            if "->" in line or ":" in line.split("(")[1]:
                                typed_functions += 1
            except (OSError, UnicodeDecodeError):
                continue

        if total_functions == 0:
            return Finding.not_applicable(
                self.attribute, reason="No Python functions found"
            )

        coverage_percent = (typed_functions / total_functions) * 100
        score = self.calculate_proportional_score(
            measured_value=coverage_percent,
            threshold=80.0,
            higher_is_better=True,
        )

        status = "pass" if score >= 75 else "fail"

        return Finding(
            attribute=self.attribute,
            status=status,
            score=score,
            measured_value=f"{coverage_percent:.1f}%",
            threshold="‚â•80%",
            evidence=[
                f"Typed functions: {typed_functions}/{total_functions}",
                f"Coverage: {coverage_percent:.1f}%",
            ],
            remediation=self._create_remediation() if status == "fail" else None,
            error_message=None,
        )

    def _assess_typescript_types(self, repository: Repository) -> Finding:
        """Assess TypeScript type configuration."""
        tsconfig_path = repository.path / "tsconfig.json"

        if not tsconfig_path.exists():
            return Finding(
                attribute=self.attribute,
                status="fail",
                score=0.0,
                measured_value="missing tsconfig.json",
                threshold="strict mode enabled",
                evidence=["tsconfig.json not found"],
                remediation=self._create_remediation(),
                error_message=None,
            )

        try:
            import json

            with open(tsconfig_path, "r") as f:
                tsconfig = json.load(f)

            strict = tsconfig.get("compilerOptions", {}).get("strict", False)

            if strict:
                return Finding(
                    attribute=self.attribute,
                    status="pass",
                    score=100.0,
                    measured_value="strict mode enabled",
                    threshold="strict mode enabled",
                    evidence=["tsconfig.json has strict: true"],
                    remediation=None,
                    error_message=None,
                )
            else:
                return Finding(
                    attribute=self.attribute,
                    status="fail",
                    score=50.0,
                    measured_value="strict mode disabled",
                    threshold="strict mode enabled",
                    evidence=["tsconfig.json missing strict: true"],
                    remediation=self._create_remediation(),
                    error_message=None,
                )

        except (OSError, json.JSONDecodeError) as e:
            return Finding.error(
                self.attribute, reason=f"Could not parse tsconfig.json: {str(e)}"
            )

    def _create_remediation(self) -> Remediation:
        """Create remediation guidance for type annotations."""
        return Remediation(
            summary="Add type annotations to function signatures",
            steps=[
                "For Python: Add type hints to function parameters and return types",
                "For TypeScript: Enable strict mode in tsconfig.json",
                "Use mypy or pyright for Python type checking",
                "Use tsc --strict for TypeScript",
                "Add type annotations gradually to existing code",
            ],
            tools=["mypy", "pyright", "typescript"],
            commands=[
                "# Python",
                "pip install mypy",
                "mypy --strict src/",
                "",
                "# TypeScript",
                "npm install --save-dev typescript",
                'echo \'{"compilerOptions": {"strict": true}}\' > tsconfig.json',
            ],
            examples=[
                """# Python - Before
def calculate(x, y):
    return x + y

# Python - After
def calculate(x: float, y: float) -> float:
    return x + y
""",
                """// TypeScript - tsconfig.json
{
  "compilerOptions": {
    "strict": true,
    "noImplicitAny": true,
    "strictNullChecks": true
  }
}
""",
            ],
            citations=[
                Citation(
                    source="Python.org",
                    title="Type Hints",
                    url="https://docs.python.org/3/library/typing.html",
                    relevance="Official Python type hints documentation",
                ),
                Citation(
                    source="TypeScript",
                    title="TypeScript Handbook",
                    url="https://www.typescriptlang.org/docs/handbook/2/everyday-types.html",
                    relevance="TypeScript type system guide",
                ),
            ],
        )


class CyclomaticComplexityAssessor(BaseAssessor):
    """Assesses cyclomatic complexity using radon."""

    @property
    def attribute_id(self) -> str:
        return "cyclomatic_complexity"

    @property
    def tier(self) -> int:
        return 3  # Important

    @property
    def attribute(self) -> Attribute:
        return Attribute(
            id=self.attribute_id,
            name="Cyclomatic Complexity Thresholds",
            category="Code Quality",
            tier=self.tier,
            description="Cyclomatic complexity thresholds enforced",
            criteria="Average complexity <10, no functions >15",
            default_weight=0.03,
        )

    def is_applicable(self, repository: Repository) -> bool:
        """Applicable to languages supported by radon or lizard."""
        supported = {"Python", "JavaScript", "TypeScript", "C", "C++", "Java"}
        return bool(set(repository.languages.keys()) & supported)

    def assess(self, repository: Repository) -> Finding:
        """Check cyclomatic complexity using radon or lizard."""
        if "Python" in repository.languages:
            return self._assess_python_complexity(repository)
        else:
            # Use lizard for other languages
            return self._assess_with_lizard(repository)

    def _assess_python_complexity(self, repository: Repository) -> Finding:
        """Assess Python complexity using radon."""
        try:
            # Check if radon is available
            result = subprocess.run(
                ["radon", "cc", str(repository.path), "-s", "-a"],
                capture_output=True,
                text=True,
                timeout=60,
            )

            if result.returncode != 0:
                raise MissingToolError("radon", install_command="pip install radon")

            # Parse radon output for average complexity
            # Output format: "Average complexity: A (5.2)"
            output = result.stdout

            if "Average complexity:" in output:
                # Extract average value
                avg_line = [
                    l for l in output.split("\n") if "Average complexity:" in l
                ][0]
                avg_value = float(avg_line.split("(")[1].split(")")[0])

                score = self.calculate_proportional_score(
                    measured_value=avg_value,
                    threshold=10.0,
                    higher_is_better=False,
                )

                status = "pass" if score >= 75 else "fail"

                return Finding(
                    attribute=self.attribute,
                    status=status,
                    score=score,
                    measured_value=f"{avg_value:.1f}",
                    threshold="<10.0",
                    evidence=[f"Average cyclomatic complexity: {avg_value:.1f}"],
                    remediation=(
                        self._create_remediation() if status == "fail" else None
                    ),
                    error_message=None,
                )
            else:
                return Finding.not_applicable(
                    self.attribute, reason="No Python code to analyze"
                )

        except MissingToolError as e:
            raise  # Re-raise to be caught by Scanner
        except Exception as e:
            return Finding.error(
                self.attribute, reason=f"Complexity analysis failed: {str(e)}"
            )

    def _assess_with_lizard(self, repository: Repository) -> Finding:
        """Assess complexity using lizard (multi-language)."""
        try:
            result = subprocess.run(
                ["lizard", str(repository.path)],
                capture_output=True,
                text=True,
                timeout=60,
            )

            if result.returncode != 0:
                raise MissingToolError("lizard", install_command="pip install lizard")

            # Parse lizard output
            # This is simplified - production code should parse properly
            return Finding.not_applicable(
                self.attribute, reason="Lizard analysis not fully implemented"
            )

        except MissingToolError as e:
            raise
        except Exception as e:
            return Finding.error(
                self.attribute, reason=f"Complexity analysis failed: {str(e)}"
            )

    def _create_remediation(self) -> Remediation:
        """Create remediation guidance for high complexity."""
        return Remediation(
            summary="Reduce cyclomatic complexity by refactoring complex functions",
            steps=[
                "Identify functions with complexity >15",
                "Break down complex functions into smaller functions",
                "Extract conditional logic into separate functions",
                "Use early returns to reduce nesting",
                "Consider using strategy pattern for complex conditionals",
            ],
            tools=["radon", "lizard"],
            commands=[
                "# Install radon",
                "pip install radon",
                "",
                "# Check complexity",
                "radon cc src/ -s -a",
                "",
                "# Find high complexity functions",
                "radon cc src/ -n C",
            ],
            examples=[],
            citations=[
                Citation(
                    source="Microsoft",
                    title="Code Metrics - Cyclomatic Complexity",
                    url="https://learn.microsoft.com/en-us/visualstudio/code-quality/code-metrics-cyclomatic-complexity",
                    relevance="Explanation of cyclomatic complexity and thresholds",
                )
            ],
        )
````

## File: src/agentready/assessors/documentation.py
````python
"""Documentation assessor for CLAUDE.md, README, docstrings, and ADRs."""

from pathlib import Path

from ..models.attribute import Attribute
from ..models.finding import Citation, Finding, Remediation
from ..models.repository import Repository
from .base import BaseAssessor


class CLAUDEmdAssessor(BaseAssessor):
    """Assesses presence and quality of CLAUDE.md configuration file.

    CLAUDE.md is the MOST IMPORTANT attribute (10% weight - Tier 1 Essential).
    Missing this file has 10x the impact of missing advanced features.
    """

    @property
    def attribute_id(self) -> str:
        return "claude_md_file"

    @property
    def tier(self) -> int:
        return 1  # Essential

    @property
    def attribute(self) -> Attribute:
        return Attribute(
            id=self.attribute_id,
            name="CLAUDE.md Configuration Files",
            category="Context Window Optimization",
            tier=self.tier,
            description="Project-specific configuration for Claude Code",
            criteria="CLAUDE.md file exists in repository root",
            default_weight=0.10,
        )

    def assess(self, repository: Repository) -> Finding:
        """Check for CLAUDE.md file in repository root.

        Pass criteria: CLAUDE.md exists
        Scoring: Binary (100 if exists, 0 if not)
        """
        claude_md_path = repository.path / "CLAUDE.md"

        if claude_md_path.exists():
            # Check file size (should have content)
            try:
                size = claude_md_path.stat().st_size
                if size < 50:
                    # File exists but is too small
                    return Finding(
                        attribute=self.attribute,
                        status="fail",
                        score=25.0,
                        measured_value=f"{size} bytes",
                        threshold=">50 bytes",
                        evidence=[f"CLAUDE.md exists but is minimal ({size} bytes)"],
                        remediation=self._create_remediation(),
                        error_message=None,
                    )

                return Finding(
                    attribute=self.attribute,
                    status="pass",
                    score=100.0,
                    measured_value="present",
                    threshold="present",
                    evidence=[f"CLAUDE.md found at {claude_md_path}"],
                    remediation=None,
                    error_message=None,
                )

            except OSError:
                return Finding.error(
                    self.attribute, reason="Could not read CLAUDE.md file"
                )
        else:
            return Finding(
                attribute=self.attribute,
                status="fail",
                score=0.0,
                measured_value="missing",
                threshold="present",
                evidence=["CLAUDE.md not found in repository root"],
                remediation=self._create_remediation(),
                error_message=None,
            )

    def _create_remediation(self) -> Remediation:
        """Create remediation guidance for missing/inadequate CLAUDE.md."""
        return Remediation(
            summary="Create CLAUDE.md file with project-specific configuration for Claude Code",
            steps=[
                "Create CLAUDE.md file in repository root",
                "Add project overview and purpose",
                "Document key architectural patterns",
                "Specify coding standards and conventions",
                "Include build/test/deployment commands",
                "Add any project-specific context that helps AI assistants",
            ],
            tools=[],
            commands=[
                "touch CLAUDE.md",
                "# Add content describing your project",
            ],
            examples=[
                """# My Project

## Overview
Brief description of what this project does.

## Architecture
Key patterns and structure.

## Development
```bash
# Install dependencies
npm install

# Run tests
npm test

# Build
npm run build
```

## Coding Standards
- Use TypeScript strict mode
- Follow ESLint configuration
- Write tests for new features
"""
            ],
            citations=[
                Citation(
                    source="Anthropic",
                    title="Claude Code Documentation",
                    url="https://docs.anthropic.com/claude-code",
                    relevance="Official guidance on CLAUDE.md configuration",
                )
            ],
        )


class READMEAssessor(BaseAssessor):
    """Assesses README structure and completeness."""

    @property
    def attribute_id(self) -> str:
        return "readme_structure"

    @property
    def tier(self) -> int:
        return 1  # Essential

    @property
    def attribute(self) -> Attribute:
        return Attribute(
            id=self.attribute_id,
            name="README Structure",
            category="Documentation Standards",
            tier=self.tier,
            description="Well-structured README with key sections",
            criteria="README.md with installation, usage, and development sections",
            default_weight=0.10,
        )

    def assess(self, repository: Repository) -> Finding:
        """Check for README.md with required sections.

        Pass criteria: README.md exists with essential sections
        Scoring: Proportional based on section count
        """
        readme_path = repository.path / "README.md"

        if not readme_path.exists():
            return Finding(
                attribute=self.attribute,
                status="fail",
                score=0.0,
                measured_value="missing",
                threshold="present with sections",
                evidence=["README.md not found"],
                remediation=self._create_remediation(),
                error_message=None,
            )

        # Read README and check for key sections
        try:
            with open(readme_path, "r", encoding="utf-8") as f:
                content = f.read().lower()

            required_sections = {
                "installation": any(
                    keyword in content
                    for keyword in ["install", "setup", "getting started"]
                ),
                "usage": any(
                    keyword in content for keyword in ["usage", "quickstart", "example"]
                ),
                "development": any(
                    keyword in content
                    for keyword in ["development", "contributing", "build"]
                ),
            }

            found_sections = sum(required_sections.values())
            total_sections = len(required_sections)

            score = self.calculate_proportional_score(
                measured_value=found_sections,
                threshold=total_sections,
                higher_is_better=True,
            )

            status = "pass" if score >= 75 else "fail"

            evidence = [
                f"Found {found_sections}/{total_sections} essential sections",
                f"Installation: {'‚úì' if required_sections['installation'] else '‚úó'}",
                f"Usage: {'‚úì' if required_sections['usage'] else '‚úó'}",
                f"Development: {'‚úì' if required_sections['development'] else '‚úó'}",
            ]

            return Finding(
                attribute=self.attribute,
                status=status,
                score=score,
                measured_value=f"{found_sections}/{total_sections} sections",
                threshold=f"{total_sections}/{total_sections} sections",
                evidence=evidence,
                remediation=self._create_remediation() if status == "fail" else None,
                error_message=None,
            )

        except OSError as e:
            return Finding.error(
                self.attribute, reason=f"Could not read README.md: {str(e)}"
            )

    def _create_remediation(self) -> Remediation:
        """Create remediation guidance for inadequate README."""
        return Remediation(
            summary="Create or enhance README.md with essential sections",
            steps=[
                "Add project overview and description",
                "Include installation/setup instructions",
                "Document basic usage with examples",
                "Add development/contributing guidelines",
                "Include build and test commands",
            ],
            tools=[],
            commands=[],
            examples=[
                """# Project Name

## Overview
What this project does and why it exists.

## Installation
```bash
pip install -e .
```

## Usage
```bash
myproject --help
```

## Development
```bash
# Run tests
pytest

# Format code
black .
```
"""
            ],
            citations=[
                Citation(
                    source="GitHub",
                    title="About READMEs",
                    url="https://docs.github.com/en/repositories/managing-your-repositorys-settings-and-features/customizing-your-repository/about-readmes",
                    relevance="Best practices for README structure",
                )
            ],
        )
````

## File: src/agentready/assessors/stub_assessors.py
````python
"""Stub implementations for remaining assessors - minimal but functional.

These are simplified implementations to get the MVP working. Each can be
enhanced later with more sophisticated detection and scoring logic.
"""

from pathlib import Path

from ..models.attribute import Attribute
from ..models.finding import Citation, Finding, Remediation
from ..models.repository import Repository
from .base import BaseAssessor


class LockFilesAssessor(BaseAssessor):
    """Tier 1 Essential - Lock files for reproducible dependencies."""

    @property
    def attribute_id(self) -> str:
        return "lock_files"

    @property
    def tier(self) -> int:
        return 1

    @property
    def attribute(self) -> Attribute:
        return Attribute(
            id=self.attribute_id,
            name="Lock Files for Reproducibility",
            category="Dependency Management",
            tier=self.tier,
            description="Lock files present for dependency pinning",
            criteria="package-lock.json, yarn.lock, poetry.lock, or requirements.txt with versions",
            default_weight=0.10,
        )

    def assess(self, repository: Repository) -> Finding:
        lock_files = [
            "package-lock.json",
            "yarn.lock",
            "pnpm-lock.yaml",
            "poetry.lock",
            "Pipfile.lock",
            "requirements.txt",
            "Cargo.lock",
            "Gemfile.lock",
            "go.sum",
        ]

        found = [f for f in lock_files if (repository.path / f).exists()]

        if found:
            return Finding(
                attribute=self.attribute,
                status="pass",
                score=100.0,
                measured_value=", ".join(found),
                threshold="at least one lock file",
                evidence=[f"Found: {', '.join(found)}"],
                remediation=None,
                error_message=None,
            )
        else:
            return Finding(
                attribute=self.attribute,
                status="fail",
                score=0.0,
                measured_value="none",
                threshold="at least one lock file",
                evidence=["No lock files found"],
                remediation=Remediation(
                    summary="Add lock file for dependency reproducibility",
                    steps=[
                        "Use npm install, poetry lock, or equivalent to generate lock file"
                    ],
                    tools=[],
                    commands=["npm install  # generates package-lock.json"],
                    examples=[],
                    citations=[],
                ),
                error_message=None,
            )


# Tier 2 Critical Assessors (3% each)


class ConventionalCommitsAssessor(BaseAssessor):
    """Tier 2 - Conventional commit messages."""

    @property
    def attribute_id(self) -> str:
        return "conventional_commits"

    @property
    def tier(self) -> int:
        return 2

    @property
    def attribute(self) -> Attribute:
        return Attribute(
            id=self.attribute_id,
            name="Conventional Commit Messages",
            category="Git & Version Control",
            tier=self.tier,
            description="Follows conventional commit format",
            criteria="‚â•80% of recent commits follow convention",
            default_weight=0.03,
        )

    def assess(self, repository: Repository) -> Finding:
        # Simplified: Check if commitlint or husky is configured
        has_commitlint = (repository.path / ".commitlintrc.json").exists()
        has_husky = (repository.path / ".husky").exists()

        if has_commitlint or has_husky:
            return Finding(
                attribute=self.attribute,
                status="pass",
                score=100.0,
                measured_value="configured",
                threshold="configured",
                evidence=["Commit linting configured"],
                remediation=None,
                error_message=None,
            )
        else:
            return Finding(
                attribute=self.attribute,
                status="fail",
                score=0.0,
                measured_value="not configured",
                threshold="configured",
                evidence=["No commitlint or husky configuration"],
                remediation=Remediation(
                    summary="Configure conventional commits with commitlint",
                    steps=["Install commitlint", "Configure husky for commit-msg hook"],
                    tools=["commitlint", "husky"],
                    commands=[
                        "npm install --save-dev @commitlint/cli @commitlint/config-conventional husky"
                    ],
                    examples=[],
                    citations=[],
                ),
                error_message=None,
            )


class GitignoreAssessor(BaseAssessor):
    """Tier 2 - Gitignore completeness."""

    @property
    def attribute_id(self) -> str:
        return "gitignore_completeness"

    @property
    def tier(self) -> int:
        return 2

    @property
    def attribute(self) -> Attribute:
        return Attribute(
            id=self.attribute_id,
            name=".gitignore Completeness",
            category="Git & Version Control",
            tier=self.tier,
            description="Comprehensive .gitignore file",
            criteria=".gitignore exists and covers common patterns",
            default_weight=0.03,
        )

    def assess(self, repository: Repository) -> Finding:
        gitignore = repository.path / ".gitignore"

        if not gitignore.exists():
            return Finding(
                attribute=self.attribute,
                status="fail",
                score=0.0,
                measured_value="missing",
                threshold="present",
                evidence=[".gitignore not found"],
                remediation=Remediation(
                    summary="Create .gitignore file",
                    steps=["Add .gitignore with common patterns for your language"],
                    tools=[],
                    commands=["touch .gitignore"],
                    examples=[],
                    citations=[],
                ),
                error_message=None,
            )

        # Check if it has content
        try:
            size = gitignore.stat().st_size
            score = 100.0 if size > 50 else 50.0
            status = "pass" if size > 50 else "fail"

            return Finding(
                attribute=self.attribute,
                status=status,
                score=score,
                measured_value=f"{size} bytes",
                threshold=">50 bytes",
                evidence=[f".gitignore found ({size} bytes)"],
                remediation=(
                    None
                    if status == "pass"
                    else Remediation(
                        summary="Expand .gitignore coverage",
                        steps=["Add common ignore patterns"],
                        tools=[],
                        commands=[],
                        examples=[],
                        citations=[],
                    )
                ),
                error_message=None,
            )
        except OSError:
            return Finding.error(self.attribute, reason="Could not read .gitignore")


# Create stub assessors for remaining attributes
# These return "not_applicable" for now but can be enhanced later


class StubAssessor(BaseAssessor):
    """Generic stub assessor for unimplemented attributes."""

    def __init__(
        self, attr_id: str, name: str, category: str, tier: int, weight: float
    ):
        self._attr_id = attr_id
        self._name = name
        self._category = category
        self._tier = tier
        self._weight = weight

    @property
    def attribute_id(self) -> str:
        return self._attr_id

    @property
    def tier(self) -> int:
        return self._tier

    @property
    def attribute(self) -> Attribute:
        return Attribute(
            id=self._attr_id,
            name=self._name,
            category=self._category,
            tier=self._tier,
            description=f"Assessment for {self._name}",
            criteria="To be implemented",
            default_weight=self._weight,
        )

    def assess(self, repository: Repository) -> Finding:
        return Finding.not_applicable(
            self.attribute,
            reason=f"{self._name} assessment not yet implemented",
        )


# Factory function to create all stub assessors
def create_stub_assessors():
    """Create stub assessors for remaining attributes."""
    return [
        # Tier 2 Critical
        StubAssessor(
            "one_command_setup",
            "One-Command Build/Setup",
            "Build & Development",
            2,
            0.03,
        ),
        StubAssessor(
            "concise_documentation",
            "Concise Structured Documentation",
            "Context Window Optimization",
            2,
            0.03,
        ),
        StubAssessor(
            "inline_documentation",
            "Inline Documentation",
            "Documentation Standards",
            2,
            0.03,
        ),
        StubAssessor(
            "file_size_limits",
            "File Size Limits",
            "Context Window Optimization",
            2,
            0.03,
        ),
        StubAssessor(
            "dependency_freshness",
            "Dependency Freshness & Security",
            "Dependency Management",
            2,
            0.03,
        ),
        StubAssessor(
            "separation_concerns",
            "Separation of Concerns",
            "Repository Structure",
            2,
            0.03,
        ),
        # Tier 3 Important
        StubAssessor(
            "structured_logging", "Structured Logging", "Error Handling", 3, 0.03
        ),
        StubAssessor(
            "openapi_specs",
            "OpenAPI/Swagger Specifications",
            "API Documentation",
            3,
            0.03,
        ),
        StubAssessor(
            "architecture_decisions",
            "Architecture Decision Records",
            "Documentation Standards",
            3,
            0.03,
        ),
        StubAssessor(
            "semantic_naming", "Semantic File & Directory Naming", "Modularity", 3, 0.03
        ),
        # Tier 4 Advanced
        StubAssessor(
            "security_scanning", "Security Scanning Automation", "Security", 4, 0.01
        ),
        StubAssessor(
            "performance_benchmarks", "Performance Benchmarks", "Performance", 4, 0.01
        ),
        StubAssessor("code_smells", "Code Smell Elimination", "Code Quality", 4, 0.01),
        StubAssessor(
            "issue_pr_templates",
            "Issue & Pull Request Templates",
            "Git & Version Control",
            4,
            0.01,
        ),
        StubAssessor(
            "container_setup",
            "Container/Virtualization Setup",
            "Build & Development",
            4,
            0.01,
        ),
    ]
````

## File: src/agentready/assessors/testing.py
````python
"""Testing assessors for test coverage, naming conventions, and pre-commit hooks."""

import re
import subprocess
from pathlib import Path

from ..models.attribute import Attribute
from ..models.finding import Citation, Finding, Remediation
from ..models.repository import Repository
from .base import BaseAssessor


class TestCoverageAssessor(BaseAssessor):
    """Assesses test coverage requirements.

    Tier 2 Critical (3% weight) - Test coverage is important for AI-assisted refactoring.
    """

    @property
    def attribute_id(self) -> str:
        return "test_coverage"

    @property
    def tier(self) -> int:
        return 2  # Critical

    @property
    def attribute(self) -> Attribute:
        return Attribute(
            id=self.attribute_id,
            name="Test Coverage Requirements",
            category="Testing & CI/CD",
            tier=self.tier,
            description="Test coverage thresholds configured and enforced",
            criteria=">80% code coverage",
            default_weight=0.03,
        )

    def is_applicable(self, repository: Repository) -> bool:
        """Applicable if tests directory exists."""
        test_dirs = ["tests", "test", "spec", "__tests__"]
        return any((repository.path / d).exists() for d in test_dirs)

    def assess(self, repository: Repository) -> Finding:
        """Check for test coverage configuration and actual coverage.

        Looks for:
        - Python: pytest.ini, .coveragerc, pyproject.toml with coverage config
        - JavaScript: jest.config.js, package.json with coverage threshold
        """
        if "Python" in repository.languages:
            return self._assess_python_coverage(repository)
        elif any(lang in repository.languages for lang in ["JavaScript", "TypeScript"]):
            return self._assess_javascript_coverage(repository)
        else:
            return Finding.not_applicable(
                self.attribute,
                reason=f"Coverage check not implemented for {list(repository.languages.keys())}",
            )

    def _assess_python_coverage(self, repository: Repository) -> Finding:
        """Assess Python test coverage configuration."""
        # Check for coverage configuration files
        coverage_configs = [
            repository.path / ".coveragerc",
            repository.path / "pyproject.toml",
            repository.path / "setup.cfg",
        ]

        has_coverage_config = any(f.exists() for f in coverage_configs)

        # Check for pytest-cov in dependencies
        has_pytest_cov = False
        pyproject = repository.path / "pyproject.toml"
        if pyproject.exists():
            try:
                with open(pyproject, "r", encoding="utf-8") as f:
                    content = f.read()
                    has_pytest_cov = "pytest-cov" in content
            except OSError:
                pass

        # Score based on configuration presence
        if has_coverage_config and has_pytest_cov:
            score = 100.0
            status = "pass"
            evidence = [
                "Coverage configuration found",
                "pytest-cov dependency present",
            ]
        elif has_coverage_config or has_pytest_cov:
            score = 50.0
            status = "fail"
            evidence = [
                f"Coverage config: {'‚úì' if has_coverage_config else '‚úó'}",
                f"pytest-cov: {'‚úì' if has_pytest_cov else '‚úó'}",
            ]
        else:
            score = 0.0
            status = "fail"
            evidence = ["No coverage configuration found"]

        return Finding(
            attribute=self.attribute,
            status=status,
            score=score,
            measured_value="configured" if score > 50 else "not configured",
            threshold="configured with >80% threshold",
            evidence=evidence,
            remediation=self._create_remediation() if status == "fail" else None,
            error_message=None,
        )

    def _assess_javascript_coverage(self, repository: Repository) -> Finding:
        """Assess JavaScript/TypeScript test coverage configuration."""
        package_json = repository.path / "package.json"

        if not package_json.exists():
            return Finding(
                attribute=self.attribute,
                status="fail",
                score=0.0,
                measured_value="no package.json",
                threshold="configured coverage",
                evidence=["package.json not found"],
                remediation=self._create_remediation(),
                error_message=None,
            )

        try:
            import json

            with open(package_json, "r") as f:
                pkg = json.load(f)

            # Check for jest or vitest with coverage
            has_jest = "jest" in pkg.get("devDependencies", {})
            has_vitest = "vitest" in pkg.get("devDependencies", {})
            has_coverage = has_jest or has_vitest

            if has_coverage:
                return Finding(
                    attribute=self.attribute,
                    status="pass",
                    score=100.0,
                    measured_value="configured",
                    threshold="configured",
                    evidence=["Test coverage tool configured"],
                    remediation=None,
                    error_message=None,
                )
            else:
                return Finding(
                    attribute=self.attribute,
                    status="fail",
                    score=0.0,
                    measured_value="not configured",
                    threshold="configured",
                    evidence=["No test coverage tool found in devDependencies"],
                    remediation=self._create_remediation(),
                    error_message=None,
                )

        except (OSError, json.JSONDecodeError) as e:
            return Finding.error(
                self.attribute, reason=f"Could not parse package.json: {str(e)}"
            )

    def _create_remediation(self) -> Remediation:
        """Create remediation guidance for test coverage."""
        return Remediation(
            summary="Configure test coverage with ‚â•80% threshold",
            steps=[
                "Install coverage tool (pytest-cov for Python, jest for JavaScript)",
                "Configure coverage threshold in project config",
                "Add coverage reporting to CI/CD pipeline",
                "Run coverage locally before committing",
            ],
            tools=["pytest-cov", "jest", "vitest", "coverage"],
            commands=[
                "# Python",
                "pip install pytest-cov",
                "pytest --cov=src --cov-report=term-missing --cov-fail-under=80",
                "",
                "# JavaScript",
                "npm install --save-dev jest",
                "npm test -- --coverage --coverageThreshold='{\\'global\\': {\\'lines\\': 80}}'",
            ],
            examples=[
                """# Python - pyproject.toml
[tool.pytest.ini_options]
addopts = "--cov=src --cov-report=term-missing"

[tool.coverage.report]
fail_under = 80
""",
                """// JavaScript - package.json
{
  "jest": {
    "coverageThreshold": {
      "global": {
        "lines": 80,
        "statements": 80,
        "functions": 80,
        "branches": 80
      }
    }
  }
}
""",
            ],
            citations=[
                Citation(
                    source="pytest-cov",
                    title="Coverage Configuration",
                    url="https://pytest-cov.readthedocs.io/",
                    relevance="pytest-cov configuration guide",
                )
            ],
        )


class PreCommitHooksAssessor(BaseAssessor):
    """Assesses pre-commit hooks configuration."""

    @property
    def attribute_id(self) -> str:
        return "precommit_hooks"

    @property
    def tier(self) -> int:
        return 2  # Critical

    @property
    def attribute(self) -> Attribute:
        return Attribute(
            id=self.attribute_id,
            name="Pre-commit Hooks & CI/CD Linting",
            category="Testing & CI/CD",
            tier=self.tier,
            description="Pre-commit hooks configured for linting and formatting",
            criteria=".pre-commit-config.yaml exists",
            default_weight=0.03,
        )

    def assess(self, repository: Repository) -> Finding:
        """Check for pre-commit configuration."""
        precommit_config = repository.path / ".pre-commit-config.yaml"

        if precommit_config.exists():
            return Finding(
                attribute=self.attribute,
                status="pass",
                score=100.0,
                measured_value="configured",
                threshold="configured",
                evidence=[".pre-commit-config.yaml found"],
                remediation=None,
                error_message=None,
            )
        else:
            return Finding(
                attribute=self.attribute,
                status="fail",
                score=0.0,
                measured_value="not configured",
                threshold="configured",
                evidence=[".pre-commit-config.yaml not found"],
                remediation=self._create_remediation(),
                error_message=None,
            )

    def _create_remediation(self) -> Remediation:
        """Create remediation guidance for pre-commit hooks."""
        return Remediation(
            summary="Configure pre-commit hooks for automated code quality checks",
            steps=[
                "Install pre-commit framework",
                "Create .pre-commit-config.yaml",
                "Add hooks for linting and formatting",
                "Install hooks: pre-commit install",
                "Run on all files: pre-commit run --all-files",
            ],
            tools=["pre-commit"],
            commands=[
                "pip install pre-commit",
                "pre-commit install",
                "pre-commit run --all-files",
            ],
            examples=[
                """# .pre-commit-config.yaml
repos:
  - repo: https://github.com/pre-commit/pre-commit-hooks
    rev: v4.4.0
    hooks:
      - id: trailing-whitespace
      - id: end-of-file-fixer
      - id: check-yaml
      - id: check-added-large-files

  - repo: https://github.com/psf/black
    rev: 23.3.0
    hooks:
      - id: black

  - repo: https://github.com/pycqa/isort
    rev: 5.12.0
    hooks:
      - id: isort
"""
            ],
            citations=[
                Citation(
                    source="pre-commit.com",
                    title="Pre-commit Framework",
                    url="https://pre-commit.com/",
                    relevance="Official pre-commit documentation",
                )
            ],
        )
````

## File: src/agentready/cli/bootstrap.py
````python
"""Bootstrap command for setting up GitHub infrastructure."""

import sys
from pathlib import Path

import click

from ..services.bootstrap import BootstrapGenerator


@click.command()
@click.argument("repository", type=click.Path(exists=True), default=".")
@click.option(
    "--dry-run",
    is_flag=True,
    help="Preview changes without creating files",
)
@click.option(
    "--language",
    type=click.Choice(["python", "javascript", "go", "auto"], case_sensitive=False),
    default="auto",
    help="Primary language (default: auto-detect)",
)
def bootstrap(repository, dry_run, language):
    """Bootstrap repository with GitHub infrastructure and best practices.

    Creates:
    - GitHub Actions workflows (tests, AgentReady assessment, security)
    - GitHub templates (issues, pull requests, CODEOWNERS)
    - Pre-commit hooks configuration
    - Dependabot configuration
    - Contributing guidelines

    REPOSITORY: Path to git repository (default: current directory)
    """
    repo_path = Path(repository).resolve()

    # Validate git repository
    if not (repo_path / ".git").exists():
        click.echo("Error: Not a git repository", err=True)
        sys.exit(1)

    click.echo("ü§ñ AgentReady Bootstrap")
    click.echo("=" * 50)
    click.echo(f"\nRepository: {repo_path}")
    click.echo(f"Language: {language}")
    click.echo(f"Dry run: {dry_run}\n")

    # Create generator
    generator = BootstrapGenerator(repo_path, language)

    # Generate all files
    try:
        created_files = generator.generate_all(dry_run=dry_run)
    except Exception as e:
        click.echo(f"\nError during bootstrap: {str(e)}", err=True)
        sys.exit(1)

    # Report results
    click.echo("\n" + "=" * 50)
    if dry_run:
        click.echo("\nDry run complete! The following files would be created:")
    else:
        click.echo(f"\nBootstrap complete! Created {len(created_files)} files:")

    for file_path in sorted(created_files):
        rel_path = file_path.relative_to(repo_path)
        click.echo(f"  ‚úì {rel_path}")

    if not dry_run:
        click.echo("\n‚úÖ Repository bootstrapped successfully!")
        click.echo("\nNext steps:")
        click.echo("  1. Review generated files")
        click.echo(
            "  2. Commit changes: git add . && git commit -m 'chore: Bootstrap repository infrastructure'"
        )
        click.echo("  3. Push to GitHub: git push")
        click.echo("  4. Set up branch protection rules")
        click.echo("  5. Enable GitHub Actions")
    else:
        click.echo("\nRun without --dry-run to create files")
````

## File: src/agentready/data/agent-ready-codebase-attributes.md
````markdown
# Agent-Ready Codebase Attributes: Comprehensive Research
*Optimizing Codebases for Claude Code and AI-Assisted Development*

**Version:** 1.0.0
**Date:** 2025-11-20
**Focus:** Claude Code/Claude-specific optimization
**Sources:** 50+ authoritative sources including Anthropic, Microsoft, Google, ArXiv, IEEE/ACM

---

## Executive Summary

This document catalogs 25 high-impact attributes that make codebases optimal for AI-assisted development, specifically Claude Code. Each attribute includes:
- Definition and importance for AI agents
- Impact on agent behavior (context window, comprehension, task success)
- Measurable criteria and tooling
- Authoritative citations
- Good vs. bad examples

**Top 10 Critical Attributes** (highest ROI):
1. CLAUDE.md/AGENTS.md configuration files
2. Conventional commit messages
3. Type annotations (static typing)
4. Test coverage >80%
5. Standard project layouts
6. Comprehensive README
7. Dependency lock files
8. Pre-commit hooks + CI/CD enforcement
9. Structured logging
10. API specifications (OpenAPI/GraphQL)

---

## 1. CONTEXT WINDOW OPTIMIZATION

### 1.1 CLAUDE.md Configuration Files

**Definition:** Markdown file at repository root automatically ingested by Claude at conversation start.

**Why It Matters:** CLAUDE.md files are "naively dropped into context up front," providing immediate project context without repeated explanations. Reduces prompt engineering time by ~40%.

**Impact on Agent Behavior:**
- Immediate understanding of tech stack, repository structure, standard commands
- Consistent adherence to project conventions
- Reduced need for repeated context-setting
- Frames entire session with project-specific guidance

**Measurable Criteria:**
- File size: <1000 lines (concise, focused)
- Essential sections:
  - Tech stack with versions
  - Repository map/structure
  - Standard commands (build, test, lint, format)
  - Testing strategy
  - Style/lint rules
  - Branch/PR workflow
  - "Do not touch" zones
  - Security/compliance notes

**Citation:** Anthropic Engineering Blog - "Claude Code Best Practices" (2025)

**Example:**
```markdown
# Good CLAUDE.md
# Tech Stack
- Python 3.11+, pytest, black + isort

# Standard Commands
- Run tests: `pytest tests/`
- Format: `black . && isort .`
- Build: `make build`

# Repository Structure
- src/ - Main application code
- tests/ - Test files mirror src/
- docs/ - Documentation

# Boundaries
- Never modify files in legacy/
- Require approval before changing config.yaml
```

---

### 1.2 Concise, Structured Documentation

**Definition:** Documentation maximizing information density while minimizing token consumption.

**Why It Matters:** Despite expanding context windows (1M+ tokens), attention mechanisms have quadratic complexity growth. Performance drops significantly on long-context tasks: 29%‚Üí3% (Claude 3.5 Sonnet) or 70.2%‚Üí40% (Qwen2.5).

**Impact on Agent Behavior:**
- Faster information retrieval through clear headings
- Reduced context pollution
- Improved response accuracy
- Better navigation across documentation

**Measurable Criteria:**
- Use standard Markdown headings (#, ##, ###)
- README <500 lines; use wiki/docs for extensive content
- Table of contents for documents >100 lines
- Bullet points over prose paragraphs
- One concept per section

**Citations:**
- ArXiv: "LongCodeBench: Evaluating Coding LLMs at 1M Context Windows" (2025)
- IBM Research: "Why larger LLM context windows are all the rage"

---

### 1.3 File Size Limits

**Definition:** Individual source files <200-300 lines.

**Why It Matters:** Working memory handles ~4 objects simultaneously. Large files exceed cognitive capacity for both humans and AI.

**Impact on Agent Behavior:**
- More precise file selection
- Reduced irrelevant context in responses
- Safer targeted modifications
- Better understanding of module boundaries

**Measurable Criteria:**
- Target: <200-300 lines per file
- Warning threshold: 500 lines
- Exception: Generated code, data files
- Enforce via linters (e.g., pylint max-module-lines)

**Citations:**
- Stack Overflow: "At what point/range is a code file too big?"
- Medium: "Psychology of Code Readability" by Egon Elbre

---

## 2. DOCUMENTATION STANDARDS

### 2.1 README Structure

**Definition:** Standardized README with essential sections in predictable order.

**Why It Matters:** Repositories with well-structured READMEs receive more engagement (GitHub data). README serves as agent's entry point for understanding project purpose, setup, and usage.

**Impact on Agent Behavior:**
- Faster project comprehension
- Accurate answers to onboarding questions
- Better architectural understanding without exploring entire codebase
- Consistent expectations across projects

**Measurable Criteria:**
Essential sections (in order):
1. Project title and description
2. Installation/setup instructions
3. Quick start/usage examples
4. Core features
5. Dependencies and requirements
6. Testing instructions
7. Contributing guidelines
8. License

**Citations:**
- GitHub Blog: "How to write a great agents.md"
- Make a README project documentation
- Welcome to the Jungle: "Essential Sections for Better Documentation"

---

### 2.2 Inline Documentation (Docstrings/Comments)

**Definition:** Function, class, and module-level documentation using language-specific conventions (Python docstrings, JSDoc/TSDoc).

**Why It Matters:** Type hints significantly improve LLM experience. Well-typed code directs LLMs into latent space regions corresponding to higher code quality‚Äîsimilar to how LaTeX-formatted math problems get better results.

**Impact on Agent Behavior:**
- Understanding function purpose without reading implementation
- Better parameter validation suggestions
- More accurate return type predictions
- Improved test generation
- Enhanced refactoring confidence

**Measurable Criteria:**
- All public functions/methods have docstrings
- Docstrings include: description, parameters, return values, exceptions, examples
- Python: PEP 257 compliant
- JavaScript/TypeScript: JSDoc or TSDoc
- Coverage: >80% of public API documented
- Tools: pydocstyle, documentation-js

**Citations:**
- Medium: "LLM Coding Concepts: Static Typing, Structured Output, and AsyncIO"
- ArXiv: "TypyBench: Evaluating LLM Type Inference for Untyped Python Repositories"
- TypeScript Documentation: JSDoc Reference

**Example:**
```python
# Good: Comprehensive docstring
def calculate_discount(price: float, discount_percent: float) -> float:
    """
    Calculate discounted price.

    Args:
        price: Original price in USD
        discount_percent: Discount percentage (0-100)

    Returns:
        Discounted price

    Raises:
        ValueError: If discount_percent not in 0-100 range

    Example:
        >>> calculate_discount(100.0, 20.0)
        80.0
    """
    if not 0 <= discount_percent <= 100:
        raise ValueError("Discount must be 0-100")
    return price * (1 - discount_percent / 100)

# Bad: No documentation
def calc_disc(p, d):
    return p * (1 - d / 100)
```

---

### 2.3 Architecture Decision Records (ADRs)

**Definition:** Lightweight documents capturing architectural decisions with context, decision, and consequences.

**Why It Matters:** ADRs provide historical context for "why" decisions were made. When AI encounters patterns or constraints, ADRs explain rationale, preventing counter-productive suggestions.

**Impact on Agent Behavior:**
- Understanding project evolution and design philosophy
- Avoiding proposing previously rejected alternatives
- Aligning suggestions with established architectural principles
- Better context for refactoring recommendations

**Measurable Criteria:**
- Store in `docs/adr/` or `.adr/` directory
- Use consistent template (Michael Nygard or MADR)
- Each ADR includes: Title, Status, Context, Decision, Consequences
- Status values: Proposed, Accepted, Deprecated, Superseded
- One decision per ADR
- Sequential numbering (ADR-001, ADR-002...)

**Citations:**
- AWS Prescriptive Guidance: "ADR process"
- GitHub: joelparkerhenderson/architecture-decision-record
- Microsoft Azure Well-Architected Framework

**Template:**
```markdown
# ADR-001: Use PostgreSQL for Primary Database

Status: Accepted

## Context
Need persistent storage supporting ACID transactions, complex queries, and JSON data.

## Decision
Use PostgreSQL 14+ as primary database.

## Consequences
Positive:
- Strong ACID guarantees
- Rich query capabilities (joins, window functions)
- JSON support via jsonb

Negative:
- More operational complexity than managed NoSQL
- Requires schema migration planning
- Horizontal scaling more complex
```

---

## 3. CODE QUALITY METRICS

### 3.1 Cyclomatic Complexity Thresholds

**Definition:** Measurement of linearly independent paths through code, indicating decision point density.

**Why It Matters:** High cyclomatic complexity confuses both humans and AI. While not perfect (doesn't capture cognitive complexity), it correlates strongly with testing difficulty and error potential.

**Impact on Agent Behavior:**
- Functions with complexity >25 are harder to understand
- Reduced confidence in safe modifications
- More difficult to generate comprehensive tests
- Increased likelihood of introducing bugs during refactoring

**Measurable Criteria:**
- Target: Cyclomatic complexity <10 per function
- Warning threshold: 15
- Error threshold: 25
- Tools: clang-tidy (C++), radon (Python), complexity-report (JavaScript), gocyclo (Go)

**Citations:**
- Microsoft Learn: "Code metrics - Cyclomatic complexity"
- Checkstyle Documentation
- LinearB Blog: "Cyclomatic Complexity explained"

---

### 3.2 Function/Method Length Limits

**Definition:** Keeping functions/methods small (typically <50 lines, ideally <20).

**Why It Matters:** Working memory handles ~4 objects simultaneously. Long functions exceed cognitive capacity. Research on reading comprehension shows lines >50-75 characters reduce comprehension; code has higher cognitive load per line.

**Impact on Agent Behavior:**
- Easier holistic function understanding
- Better isolation for testing
- Safer modifications without unintended side effects
- Clearer single responsibility principle adherence

**Measurable Criteria:**
- Target: <20 lines per function
- Warning: 50 lines
- Hard limit: 100 lines
- Exception: Complex algorithms with extensive explanatory comments
- Tools: pylint (max-function-lines), eslint (max-lines-per-function)

**Citations:**
- Medium: "Psychology of Code Readability" by Egon Elbre
- UX Stack Exchange: Line length readability research
- Clang-Tidy: readability-function-cognitive-complexity

---

### 3.3 Type Annotations (Static Typing)

**Definition:** Explicit type declarations for variables, parameters, and return values.

**Why It Matters:** Type hints significantly improve LLM code understanding. Higher-quality codebases have type annotations, directing LLMs toward higher-quality latent space regions. Creates synergistic improvement: LLMs generate better typed code, which helps future LLM interactions.

**Impact on Agent Behavior:**
- Better input validation
- Type error detection before execution
- Structured output generation
- Improved autocomplete suggestions
- Enhanced refactoring safety

**Measurable Criteria:**
- Python: All public functions have parameter and return type hints
- TypeScript: `strict` mode enabled in tsconfig.json
- Go: Inherently typed
- Coverage: >80% of functions typed
- Tools: mypy (Python), pyright (Python), tsc --strict (TypeScript)

**Citations:**
- Medium: "LLM Coding Concepts: Static Typing, Structured Output"
- ArXiv: "Automated Type Annotation in Python Using LLMs"
- Dropbox Tech Blog: "Our journey to type checking 4 million lines of Python"

**Example:**
```python
# Good: Full type annotations
from typing import List, Optional

def find_users(
    role: str,
    active: bool = True,
    limit: Optional[int] = None
) -> List[User]:
    """Find users matching criteria."""
    query = User.query.filter_by(role=role, active=active)
    if limit:
        query = query.limit(limit)
    return query.all()

# Bad: No type hints
def find_users(role, active=True, limit=None):
    query = User.query.filter_by(role=role, active=active)
    if limit:
        query = query.limit(limit)
    return query.all()
```

---

### 3.4 Code Smell Elimination

**Definition:** Removing indicators of deeper problems: long methods, large classes, duplicate code, dead code, magic numbers.

**Why It Matters:** Research shows AI-generated code increases "code churn" (copy/paste vs. refactoring) and DRY principle violations. Clean baseline prevents AI from perpetuating anti-patterns.

**Impact on Agent Behavior:**
- Better intent understanding
- More accurate refactoring suggestions
- Avoidance of anti-pattern propagation
- Improved code quality over time

**Measurable Criteria:**
- Tools: SonarQube, PMD, Checkstyle, pylint, eslint
- Zero critical smells
- <5 major smells per 1000 lines of code
- Common smells monitored:
  - Duplicate code (DRY violations)
  - Long methods (>50 lines)
  - Large classes (>500 lines)
  - Long parameter lists (>5 params)
  - Divergent change (one class changing for multiple reasons)

**Citations:**
- GitClear: "Coding on Copilot" whitepaper
- Codacy Blog: "Code Smells and Anti-Patterns"
- ScienceDirect: "Code smells and refactoring: A tertiary systematic review"

---

## 4. REPOSITORY STRUCTURE

### 4.1 Standard Project Layouts

**Definition:** Using community-recognized directory structures for each language/framework.

**Why It Matters:** Standard layouts reduce cognitive overhead. AI models trained on open-source code recognize patterns (Python's src/, Go's cmd/ and internal/, Java's Maven structure).

**Impact on Agent Behavior:**
- Faster navigation
- Accurate location assumptions for new files
- Automatic adherence to established conventions
- Reduced confusion about file placement

**Measurable Criteria:**

**Python (src layout):**
```
project/
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îî‚îÄ‚îÄ package/
‚îÇ       ‚îú‚îÄ‚îÄ __init__.py
‚îÇ       ‚îî‚îÄ‚îÄ module.py
‚îú‚îÄ‚îÄ tests/
‚îú‚îÄ‚îÄ docs/
‚îú‚îÄ‚îÄ README.md
‚îú‚îÄ‚îÄ pyproject.toml
‚îî‚îÄ‚îÄ requirements.txt
```

**Go:**
```
project/
‚îú‚îÄ‚îÄ cmd/           # Main applications
‚îÇ   ‚îî‚îÄ‚îÄ app/
‚îÇ       ‚îî‚îÄ‚îÄ main.go
‚îú‚îÄ‚îÄ internal/      # Private code
‚îú‚îÄ‚îÄ pkg/           # Public libraries
‚îú‚îÄ‚îÄ go.mod
‚îî‚îÄ‚îÄ go.sum
```

**JavaScript/TypeScript (Node.js):**
```
project/
‚îú‚îÄ‚îÄ src/
‚îú‚îÄ‚îÄ test/
‚îú‚îÄ‚îÄ dist/
‚îú‚îÄ‚îÄ package.json
‚îú‚îÄ‚îÄ package-lock.json
‚îî‚îÄ‚îÄ tsconfig.json
```

**Citations:**
- Real Python: "Python Application Layouts"
- GitHub: golang-standards/project-layout
- Stack Overflow: "Best project structure for Python application"

---

### 4.2 Separation of Concerns

**Definition:** Organizing code so each module/file/function has single, well-defined responsibility (SOLID principles).

**Why It Matters:** 2 of 5 SOLID principles derive directly from separation of concerns. Clear boundaries improve testability, maintainability, and reduce cognitive load.

**Impact on Agent Behavior:**
- Targeted modifications without affecting unrelated code
- Better refactoring suggestions
- Clearer module purpose understanding
- Reduced side effect risk

**Measurable Criteria:**
- Each module/class has one reason to change
- High cohesion within modules (related functions together)
- Low coupling between modules (minimal dependencies)
- Organize by feature/domain, not technical layer (avoid separate "controllers", "services", "models" directories)

**Citations:**
- Wikipedia: "Separation of concerns"
- DevIQ: "Separation of Concerns"
- Medium: "Single responsibility and Separation of concerns principles"

---

## 5. TESTING & CI/CD

### 5.1 Test Coverage Requirements

**Definition:** Percentage of code executed by automated tests.

**Why It Matters:** High test coverage enables confident AI modifications. Research shows AI tools (Cursor AI) can cut test coverage time by 85% while maintaining quality‚Äîbut only when good tests exist as foundation.

**Impact on Agent Behavior:**
- Safety net enabling aggressive refactoring
- Tests document expected behavior
- Immediate feedback on breaking changes
- Higher confidence in suggested modifications

**Measurable Criteria:**
- Minimum: 70% line coverage
- Target: 80-90% line coverage
- Critical paths: 100% coverage
- Track: Statement coverage, branch coverage, function coverage
- Tools: pytest-cov (Python), Jest/Istanbul (JavaScript), go test -cover (Go)
- Coverage reports in CI/CD with failure threshold

**Citations:**
- Salesforce Engineering: "How Cursor AI Cut Legacy Code Coverage Time by 85%"
- Qodo AI Blog: "Harnessing AI to Revolutionize Test Coverage Analysis"
- Medium: "How to Improve Code Coverage using Generative AI tools"

---

### 5.2 Test Naming Conventions

**Definition:** Descriptive test names following patterns like `test_should_<expected>_when_<condition>`.

**Why It Matters:** Clear test names help AI understand intent without reading implementation. When tests fail, AI diagnoses issues faster with self-documenting names.

**Impact on Agent Behavior:**
- Generation of similar test patterns
- Faster edge case understanding
- More accurate fix proposals aligned with intent
- Better test coverage gap identification

**Measurable Criteria:**
- Pattern: `test_<method>_<scenario>_<expected_outcome>`
- Example: `test_create_user_with_invalid_email_raises_value_error`
- Avoid: `test1`, `test_edge_case`, `test_bug_fix`, `test_method_name`
- Test names should be readable as sentences

**Citations:**
- pytest documentation: Test naming best practices
- JUnit best practices
- Go testing conventions

**Example:**
```python
# Good: Self-documenting test names
def test_create_user_with_valid_data_returns_user_instance():
    user = create_user(email="test@example.com", name="Test")
    assert isinstance(user, User)

def test_create_user_with_invalid_email_raises_value_error():
    with pytest.raises(ValueError, match="Invalid email"):
        create_user(email="not-an-email", name="Test")

def test_create_user_with_duplicate_email_raises_integrity_error():
    create_user(email="test@example.com", name="Test 1")
    with pytest.raises(IntegrityError):
        create_user(email="test@example.com", name="Test 2")

# Bad: Unclear test names
def test_user1():
    user = create_user(email="test@example.com", name="Test")
    assert user

def test_user2():
    with pytest.raises(ValueError):
        create_user(email="invalid", name="Test")
```

---

### 5.3 Pre-commit Hooks & CI/CD Linting

**Definition:** Automated code quality checks before commits (pre-commit hooks) and in CI/CD pipeline.

**Why It Matters:** Pre-commit hooks provide immediate feedback but can be bypassed. Running same checks in CI/CD ensures enforcement. Linting errors prevent successful CI runs, wasting time and compute.

**Impact on Agent Behavior:**
- Ensures AI-generated code meets quality standards
- Immediate feedback loop for improvements
- Consistent code style across all contributions
- Prevents low-quality code from entering repository

**Measurable Criteria:**
- Pre-commit framework installed and configured
- Hooks include:
  - Formatters: black/autopep8 (Python), prettier (JS/TS), gofmt (Go)
  - Linters: flake8/pylint (Python), eslint (JS/TS), golint (Go)
  - Type checkers: mypy/pyright (Python), tsc (TypeScript)
- **Critical:** Same checks run in CI/CD (non-skippable)
- CI fails on any linting error
- Fast execution: <30 seconds total

**Citations:**
- Memfault Blog: "Automatically format and lint code with pre-commit"
- Medium: "Elevate Your CI: Mastering Pre-commit Hooks and GitHub Actions"
- GitHub: pre-commit/pre-commit

---

## 6. DEPENDENCY MANAGEMENT

### 6.1 Lock Files for Reproducibility

**Definition:** Pinning exact dependency versions including transitive dependencies.

**Why It Matters:** Lock files ensure reproducible builds across environments. Without them, "works on my machine" problems plague AI-generated code. Different dependency versions can break builds, fail tests, or introduce bugs.

**Impact on Agent Behavior:**
- Confident dependency-related suggestions
- Accurate compatibility issue diagnosis
- Reproducible environment recommendations
- Version-specific API usage

**Measurable Criteria:**
- Lock file committed to repository
- **npm:** package-lock.json or yarn.lock
- **Python:** requirements.txt (from pip freeze), poetry.lock, or uv.lock
- **Go:** go.sum (automatically managed)
- **Ruby:** Gemfile.lock
- Lock file updated with every dependency change
- CI/CD uses lock file for installation

**Citations:**
- npm Blog: "Why Keep package-lock.json?"
- DEV Community: "Dependency management: package.json and package-lock.json explained"
- Python Packaging User Guide

---

### 6.2 Dependency Freshness & Security Scanning

**Definition:** Regularly updating dependencies and scanning for known vulnerabilities.

**Why It Matters:** Outdated dependencies introduce security risks and compatibility issues. AI-generated code may use deprecated APIs if dependencies are stale. Security vulnerabilities in dependencies can compromise entire application.

**Impact on Agent Behavior:**
- Suggestions use modern, non-deprecated APIs
- Awareness of security considerations
- Better library feature recommendations
- Avoidance of known vulnerability patterns

**Measurable Criteria:**
- Automated dependency updates: Dependabot, Renovate, or equivalent
- Security scanning in CI/CD: Snyk, npm audit, safety (Python), govulncheck (Go)
- Update cadence:
  - Patch versions: Weekly/automated
  - Minor versions: Monthly
  - Major versions: Quarterly with testing
- Zero known high/critical vulnerabilities in production
- Vulnerability response SLA: High severity within 7 days

**Citations:**
- GitHub Dependabot documentation
- OWASP Dependency-Check
- Snyk best practices
- npm audit documentation

---

## 7. GIT & VERSION CONTROL

### 7.1 Conventional Commit Messages

**Definition:** Structured commit messages following format: `<type>(<scope>): <description>`.

**Why It Matters:** Conventional commits enable automated semantic versioning, changelog generation, and commit intent understanding. AI can parse history to understand feature evolution and impact.

**Impact on Agent Behavior:**
- Generates properly formatted commit messages
- Understands which changes are breaking
- Appropriate version bump suggestions
- Better git history comprehension
- Automated changelog contribution

**Measurable Criteria:**
- Format: `type(scope): description`
- **Types:** feat, fix, docs, style, refactor, perf, test, chore, build, ci
- **Breaking changes:** `BREAKING CHANGE:` footer or `!` after type
- **Tools:** commitlint, commitizen, semantic-release
- **Enforcement:** Pre-commit hook or CI check
- All commits follow convention (enforce in CI)

**Citations:**
- Conventional Commits specification v1.0.0
- Medium: "GIT ‚Äî Semantic versioning and conventional commits"
- CMU SEI Blog: "Versioning with Git Tags and Conventional Commits"

**Example:**
```
# Good commits
feat(auth): add OAuth2 login support
fix(api): handle null values in user response
docs(readme): update installation instructions
perf(database): add index on user_email column

# Breaking change
feat(api)!: change user endpoint from /user to /users

BREAKING CHANGE: User endpoint URL has changed from /user to /users.
Update all API clients accordingly.

# Bad commits
update stuff
fixed bug
changes
wip
asdf
```

---

### 7.2 .gitignore Completeness

**Definition:** Comprehensive .gitignore preventing sensitive files, build artifacts, and environment-specific files from version control.

**Why It Matters:** Incomplete .gitignore pollutes repository with irrelevant files, consuming context window space and creating security risks (accidentally committing .env files, credentials).

**Impact on Agent Behavior:**
- Focus on source code, not build artifacts
- Security files excluded prevent accidental exposure
- Cleaner repository navigation
- Reduced context pollution

**Measurable Criteria:**
- Use language-specific templates from github/gitignore
- **Exclude:**
  - Build artifacts (dist/, build/, *.pyc, *.class)
  - Dependencies (node_modules/, venv/, vendor/)
  - IDE files (.vscode/, .idea/, *.swp)
  - OS files (.DS_Store, Thumbs.db)
  - Environment variables (.env, .env.local)
  - Credentials (*.pem, *.key, credentials.json)
  - Logs (*.log, logs/)
- One .gitignore at repository root (avoid multiple nested)
- Review when adding new tools/frameworks

**Citations:**
- GitHub: github/gitignore template collection
- Medium: "Mastering .gitignore: A Comprehensive Guide"
- Git documentation

---

### 7.3 Issue & Pull Request Templates

**Definition:** Standardized templates for issues and PRs in .github/ directory.

**Why It Matters:** Templates provide structure for AI when creating issues or PRs. Ensures all necessary context is provided consistently.

**Impact on Agent Behavior:**
- Automatically fills templates when creating PRs
- Ensures checklist completion
- Consistent issue reporting format
- Better context for understanding existing issues/PRs

**Measurable Criteria:**
- `PULL_REQUEST_TEMPLATE.md` in .github/ or root
- Issue templates in `.github/ISSUE_TEMPLATE/`
- **PR template includes:**
  - Summary of changes
  - Related issues (Fixes #123)
  - Testing performed
  - Checklist (tests added, docs updated, etc.)
- **Issue templates for:**
  - Bug reports (with reproduction steps)
  - Feature requests (with use case)
  - Questions/discussions

**Citations:**
- GitHub Docs: "About issue and pull request templates"
- GitHub Blog: "Multiple issue and pull request templates"
- Embedded Artistry: "A GitHub Pull Request Template for Your Projects"

---

## 8. BUILD & DEVELOPMENT SETUP

### 8.1 One-Command Build/Setup

**Definition:** Single command to set up development environment from fresh clone.

**Why It Matters:** Lengthy setup documentation increases friction and errors. One-command setup enables AI to quickly reproduce environments and test changes. Reduces "works on my machine" problems.

**Impact on Agent Behavior:**
- Confident environment setup suggestions
- Quick validation of proposed changes
- Easy onboarding recommendations
- Reduced setup-related debugging

**Measurable Criteria:**
- Single command documented prominently in README
- **Examples:** `make setup`, `npm install`, `poetry install`, `./bootstrap.sh`
- **Command handles:**
  - Dependency installation
  - Virtual environment creation
  - Database setup/migrations
  - Configuration file creation (.env from .env.example)
  - Pre-commit hooks installation
- **Success criteria:** Working development environment in <5 minutes
- Idempotent (safe to run multiple times)

**Citations:**
- npm Blog: "Using Npm Scripts as a Build Tool"
- freeCodeCamp: "Want to know the easiest way to save time? Use make!"
- Medium: "Creating Reproducible Development Environments"

**Example:**
```makefile
# Good: Comprehensive Makefile
.PHONY: setup
setup:
	python -m venv venv
	. venv/bin/activate && pip install -r requirements.txt
	pre-commit install
	cp .env.example .env
	python manage.py migrate
	@echo "‚úì Setup complete! Run 'make test' to verify."

.PHONY: test
test:
	pytest tests/ -v --cov

.PHONY: lint
lint:
	black --check .
	isort --check .
	flake8 .
	mypy .

.PHONY: format
format:
	black .
	isort .
```

---

### 8.2 Development Environment Documentation

**Definition:** Clear documentation of prerequisites, environment variables, and configuration.

**Why It Matters:** Environment differences cause "works on my machine" problems. Comprehensive docs enable reproducibility and faster debugging.

**Impact on Agent Behavior:**
- Accurate environment troubleshooting
- Better setup assistance for new contributors
- Environment-specific bug diagnosis
- Configuration recommendation accuracy

**Measurable Criteria:**
- **Prerequisites documented:**
  - Language/runtime version (Python 3.11+, Node.js 18+)
  - System dependencies (PostgreSQL, Redis, etc.)
  - Operating system requirements
- **Environment variables documented:**
  - .env.example file with all variables
  - Description of each variable
  - Required vs. optional clearly marked
  - Safe default values where applicable
- **Optional but helpful:**
  - IDE/editor setup (VS Code extensions, etc.)
  - Debugging configuration
  - Performance optimization tips

**Citations:**
- Medium: "Creating Reproducible Development Environments"
- InfoQ: "Reproducible Development with Containers"
- The Turing Way: "Reproducible Environments"

---

### 8.3 Container/Virtualization Setup

**Definition:** Docker/Podman configurations for consistent development environments.

**Why It Matters:** Containers provide portable, reproducible environments across operating systems. Development containers (devcontainers) are fully functional, batteries-included environments that are shared, versioned, and self-documenting.

**Impact on Agent Behavior:**
- Dockerfile improvement suggestions
- Container debugging assistance
- Consistent build recommendations
- Cross-platform development support

**Measurable Criteria:**
- Dockerfile or Containerfile in repository root
- docker-compose.yml for multi-service setups
- .devcontainer/devcontainer.json for VS Code/GitHub Codespaces
- **Dockerfile best practices:**
  - Multi-stage builds for smaller images
  - Non-root user
  - .dockerignore file
  - Explicit version tags (not :latest)
- Documentation on running containers
- Health checks defined

**Citations:**
- InfoQ: "Reproducible Development with Containers"
- Developer.com: "Creating a Reproducible and Portable Development Environment"
- Docker best practices documentation

---

## 9. ERROR HANDLING & DEBUGGING

### 9.1 Error Message Clarity

**Definition:** Descriptive error messages with context, remediation guidance, and relevant data.

**Why It Matters:** Clear errors enable AI to diagnose issues and suggest fixes. Vague errors ("Error 500", "Something went wrong") provide no actionable information.

**Impact on Agent Behavior:**
- Accurate root cause analysis
- Targeted solution proposals
- Faster debugging cycles
- Better user error handling suggestions

**Measurable Criteria:**
- **Include in error messages:**
  - What failed (operation/function)
  - Why it failed (validation, network, etc.)
  - How to fix it (actionable guidance)
  - Context: Request IDs, user IDs, timestamps, relevant parameters
- **Avoid:**
  - Generic messages ("Invalid input", "Error occurred")
  - Exposing internal stack traces to end users
  - Sensitive information in error messages
- **Provide:** Error codes for categorization
- Consistent error format across application

**Citations:**
- Honeycomb: "Engineers Checklist: Logging Best Practices"
- Paul Serban: "Error Logging Standards: A Practical Guide"
- Stack Overflow Blog: "Best practices for writing code comments"

**Example:**
```python
# Good: Descriptive error with context and guidance
raise ValueError(
    f"Invalid discount percentage: {discount_percent}. "
    f"Expected value between 0 and 100. "
    f"Received: {discount_percent} (type: {type(discount_percent).__name__}). "
    f"Fix: Ensure discount_percent is a number in range [0, 100]."
)

# Bad: Vague error
raise ValueError("Invalid input")

# Good: API error with context
{
    "error": {
        "code": "INVALID_DISCOUNT",
        "message": "Discount percentage must be between 0 and 100",
        "details": {
            "field": "discount_percent",
            "value": 150,
            "constraint": "0 <= value <= 100"
        },
        "request_id": "req_abc123"
    }
}
```

---

### 9.2 Structured Logging

**Definition:** Logging in structured format (JSON) with consistent field names and types.

**Why It Matters:** Structured logs are machine-parseable. AI can analyze logs to diagnose issues, identify patterns, suggest optimizations, and correlate events across distributed systems.

**Impact on Agent Behavior:**
- Log query and analysis capabilities
- Event correlation across services
- Pattern identification for debugging
- Data-driven optimization suggestions
- Anomaly detection

**Measurable Criteria:**
- Use structured logging library: structlog (Python), winston (Node.js), zap (Go)
- **Standard fields across all logs:**
  - timestamp (ISO 8601 format)
  - level (DEBUG, INFO, WARNING, ERROR, CRITICAL)
  - message (human-readable)
  - context: request_id, user_id, session_id, trace_id
- Consistent naming convention (snake_case or camelCase, not both)
- Log levels used appropriately
- **Never log sensitive data:** passwords, tokens, credit cards, PII (without anonymization)
- JSON format for production

**Citations:**
- Daily.dev: "12 Logging Best Practices: Do's & Don'ts"
- Dataset Blog: "Logging Best Practices: The 13 You Should Know"
- Technogise Medium: "Logging Practices: Guidelines for Developers"

**Example:**
```python
# Good: Structured logging
import structlog

logger = structlog.get_logger()

logger.info(
    "user_login_success",
    user_id="user_123",
    request_id="req_abc",
    duration_ms=45,
    ip_address="192.168.1.1"
)

# Output:
# {"timestamp": "2025-01-20T10:30:00Z", "level": "info", "event": "user_login_success",
#  "user_id": "user_123", "request_id": "req_abc", "duration_ms": 45, "ip_address": "192.168.1.1"}

# Bad: Unstructured logging
print("User user_123 logged in from 192.168.1.1 in 45ms")
```

---

## 10. API & INTERFACE DOCUMENTATION

### 10.1 OpenAPI/Swagger Specifications

**Definition:** Machine-readable API documentation in OpenAPI format (formerly Swagger).

**Why It Matters:** OpenAPI specs define everything needed to integrate with an API: authentication, endpoints, HTTP methods, request/response schemas, error codes. AI can read specs to generate client code, tests, and integration code automatically.

**Impact on Agent Behavior:**
- Auto-generation of SDKs and client libraries
- Request/response validation
- API mocking for testing
- Contract compliance verification
- Interactive API exploration

**Measurable Criteria:**
- OpenAPI 3.0+ specification file (openapi.yaml or openapi.json)
- **All endpoints documented with:**
  - Description and purpose
  - HTTP method (GET, POST, PUT, DELETE, PATCH)
  - Parameters (path, query, header)
  - Request body schema
  - Response schemas (success and error cases)
  - Authentication requirements
  - Example requests/responses
- Validation: Use Swagger Editor or Spectral
- Auto-generate from code annotations OR keep manually in sync
- Hosted documentation (Swagger UI, ReDoc)

**Citations:**
- Swagger Blog: "API Documentation Best Practices"
- APItoolkit: "OpenAPI Specification for API Development"
- APImatic: "14 Best Practices to Write OpenAPI for Better API Consumption"

**Example:**
```yaml
# Good: Comprehensive OpenAPI spec
openapi: 3.0.0
info:
  title: User API
  version: 1.0.0
paths:
  /users/{userId}:
    get:
      summary: Get user by ID
      parameters:
        - name: userId
          in: path
          required: true
          schema:
            type: string
      responses:
        '200':
          description: User found
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/User'
        '404':
          description: User not found
components:
  schemas:
    User:
      type: object
      required:
        - id
        - email
      properties:
        id:
          type: string
        email:
          type: string
          format: email
        name:
          type: string
```

---

### 10.2 GraphQL Schemas

**Definition:** Type definitions for GraphQL APIs using Schema Definition Language (SDL).

**Why It Matters:** GraphQL schemas are self-documenting and introspectable. AI can understand available queries, mutations, types, and relationships without exploring implementation code.

**Impact on Agent Behavior:**
- Generate type-safe queries
- Schema validation
- Performance optimization suggestions (N+1 query detection)
- Type-safe client generation
- API evolution guidance

**Measurable Criteria:**
- schema.graphql file in repository
- All types, queries, mutations include descriptions
- Use directives for:
  - Deprecation (@deprecated)
  - Authorization (@auth)
  - Field resolution hints
- Schema validation in CI/CD
- SDL-first approach (schema-first, not code-first)

**Citations:**
- GraphQL documentation: "Schema Definition Language"
- Apollo GraphQL: "Schema design best practices"
- Hasura GraphQL best practices

**Example:**
```graphql
# Good: Well-documented GraphQL schema
"""
Represents a user in the system
"""
type User {
  """
  Unique identifier for the user
  """
  id: ID!

  """
  User's email address (unique)
  """
  email: String!

  """
  User's display name
  """
  name: String

  """
  Posts created by this user
  """
  posts: [Post!]!
}

type Query {
  """
  Find a user by their unique ID
  """
  user(id: ID!): User

  """
  List all users with optional filtering
  """
  users(role: String, active: Boolean): [User!]!
}
```

---

## 11. MODULARITY & CODE ORGANIZATION

### 11.1 DRY Principle (Don't Repeat Yourself)

**Definition:** Every piece of knowledge has single, authoritative representation in the system.

**Why It Matters:** Research shows AI-generated code increases code churn and DRY violations (copy-paste instead of refactoring). Enforcing DRY in codebase teaches AI to refactor rather than duplicate.

**Impact on Agent Behavior:**
- Learns to extract shared logic
- Suggests refactorings instead of duplication
- Avoids creating duplicate implementations
- Better abstraction identification

**Measurable Criteria:**
- "Three Strikes" rule: Third duplicate occurrence triggers refactoring
- Tools detect duplication: SonarQube, PMD (Java), jscpd (JavaScript), pylint (Python)
- Shared logic extracted to:
  - Utility functions/modules
  - Base classes
  - Mixins/traits
  - Libraries
- **Balance:** Avoid premature abstraction ("prefer duplication over wrong abstraction")
- Target: <5% duplicate code

**Citations:**
- Wikipedia: "Don't repeat yourself"
- The Pragmatic Programmer by Hunt & Thomas
- Medium: "The DRY Principle and Incidental Duplication"
- Sandi Metz: "The Wrong Abstraction"

---

### 11.2 Consistent Naming Conventions

**Definition:** Systematic naming patterns for variables, functions, classes, files following language/framework conventions.

**Why It Matters:** Research shows identifier style affects recall and precision. Consistency reduces cognitive load. AI models recognize naming patterns from training on open-source code.

**Impact on Agent Behavior:**
- Accurate intent inference
- Appropriate name suggestions
- Code structure understanding
- Pattern recognition

**Measurable Criteria:**
- Follow language conventions:
  - **Python:** PEP 8 (snake_case functions, PascalCase classes, UPPER_CASE constants)
  - **JavaScript/TypeScript:** camelCase functions/variables, PascalCase classes
  - **Go:** mixedCaps (exported: UpperCase, unexported: lowerCase)
  - **Java:** camelCase methods, PascalCase classes, UPPER_CASE constants
- Use paired opposites consistently: add/remove, start/stop, begin/end, open/close
- Avoid abbreviations unless widely understood (HTTP, API, URL, ID)
- Enforce via linters: pylint, eslint, golint

**Citations:**
- Wikipedia: "Naming convention (programming)"
- Microsoft Learn: "General Naming Conventions"
- PEP 8 - Style Guide for Python Code
- Google Style Guides (Java, Python, JavaScript, Go)

**Example:**
```python
# Good: Consistent naming
class UserService:
    MAX_LOGIN_ATTEMPTS = 5

    def create_user(self, email: str) -> User:
        """Create new user."""
        pass

    def delete_user(self, user_id: str) -> None:
        """Delete existing user."""
        pass

# Bad: Inconsistent naming
class userservice:
    maxLoginAttempts = 5

    def CreateUser(self, e: str) -> User:
        pass

    def removeUser(self, uid: str) -> None:
        pass
```

---

### 11.3 Semantic File & Directory Naming

**Definition:** File names and directory structures that convey purpose and content clearly.

**Why It Matters:** Semantic organization helps AI locate relevant code quickly. Clear names reduce cognitive overhead and enable predictable file location.

**Impact on Agent Behavior:**
- Faster relevant file location
- Accurate placement suggestions for new code
- Better repository organization understanding
- Reduced search time

**Measurable Criteria:**
- **Feature-based organization:** Group related files by feature/domain, not technical layer
- **Clear, descriptive names:** `user_service.py` not `us.py`
- **Avoid abbreviations** unless standard in domain
- **Mirror test structure** to source structure:
  - `src/services/user_service.py` ‚Üí `tests/services/test_user_service.py`
- **Consistent file extensions:** .py, .js, .ts, .go
- **Module files:** `__init__.py`, index.js for package entry points

**Citations:**
- GitHub: kriasoft/Folder-Structure-Conventions
- Iterators: "Comprehensive Guide on Project Codebase Organization"
- Medium: "A Front-End Application Folder Structure that Makes Sense"

**Example:**
```
# Good: Feature-based, semantic organization
src/
‚îú‚îÄ‚îÄ auth/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ login_service.py
‚îÇ   ‚îú‚îÄ‚îÄ oauth_provider.py
‚îÇ   ‚îî‚îÄ‚îÄ session_manager.py
‚îú‚îÄ‚îÄ users/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ user_model.py
‚îÇ   ‚îú‚îÄ‚îÄ user_service.py
‚îÇ   ‚îî‚îÄ‚îÄ user_repository.py
‚îî‚îÄ‚îÄ billing/
    ‚îú‚îÄ‚îÄ __init__.py
    ‚îú‚îÄ‚îÄ payment_processor.py
    ‚îî‚îÄ‚îÄ invoice_generator.py

# Bad: Technical layer organization, unclear names
src/
‚îú‚îÄ‚îÄ models/
‚îÇ   ‚îú‚îÄ‚îÄ u.py
‚îÇ   ‚îî‚îÄ‚îÄ o.py
‚îú‚îÄ‚îÄ services/
‚îÇ   ‚îú‚îÄ‚îÄ svc1.py
‚îÇ   ‚îî‚îÄ‚îÄ svc2.py
‚îî‚îÄ‚îÄ utils/
    ‚îî‚îÄ‚îÄ helpers.py
```

---

## 12. CI/CD INTEGRATION

### 12.1 CI/CD Pipeline Visibility

**Definition:** Clear, well-documented CI/CD configuration files committed to repository.

**Why It Matters:** AI can understand build/test/deploy processes by reading CI configs. When builds fail, AI can suggest targeted fixes. Visible pipelines enable collaboration and debugging.

**Impact on Agent Behavior:**
- CI improvement proposals
- Pipeline failure debugging
- Workflow optimization suggestions
- Better understanding of deployment process

**Measurable Criteria:**
- CI config file in repository:
  - GitHub Actions: `.github/workflows/`
  - GitLab CI: `.gitlab-ci.yml`
  - CircleCI: `.circleci/config.yml`
- **Clear job/step names** (not "step1", "step2")
- **Comments explaining complex logic**
- **Fast feedback:** Tests complete <10 minutes
- **Fail fast:** Stop on first failure to save compute
- **Parallelization:** Run independent jobs concurrently
- **Caching:** Dependencies, build artifacts
- **Artifacts:** Test results, coverage reports, logs

**Citations:**
- CircleCI: "Monorepo dev practices"
- GitHub Actions documentation
- GitLab CI best practices
- Martin Fowler: "Continuous Integration"

---

### 12.2 Branch Protection & Status Checks

**Definition:** Required status checks and review approvals before merging to main/production branches.

**Why It Matters:** Prevents broken code from reaching production. Provides safety net for AI-generated code. Ensures quality gates are enforced.

**Impact on Agent Behavior:**
- Understanding of merge requirements
- Awareness of quality gates
- Suggestions aligned with branch policies
- Better PR creation (ensuring checks pass)

**Measurable Criteria:**
- Branch protection enabled for main/master/production
- **Required status checks:**
  - All tests passing
  - Linting/formatting passing
  - Code coverage threshold met
  - Security scanning passing
- **Required reviews:** At least 1 approval
- **No force pushes** to protected branches
- **No direct commits** to protected branches
- **Up-to-date branch** requirement (rebase/merge before merging)

**Citations:**
- GitHub Docs: "About protected branches"
- GitLab: "Protected branches"
- Industry best practices

---

## 13. SECURITY & COMPLIANCE

### 13.1 Security Scanning Automation

**Definition:** Automated security scans for vulnerabilities, secrets, and compliance issues in CI/CD.

**Why It Matters:** AI can accidentally introduce vulnerabilities (SQL injection, XSS, etc.). Research shows LLM-generated code has security weaknesses, particularly around outdated practices. Automated scanning provides safety net.

**Impact on Agent Behavior:**
- Security pattern learning
- Vulnerability avoidance
- Secure coding practice adoption
- Failed scans provide improvement feedback

**Measurable Criteria:**
- **Dependency scanning:** Snyk, Dependabot, npm audit, safety (Python)
- **Secret scanning:** GitLeaks, TruffleHog, detect-secrets
- **Static analysis:** Semgrep, CodeQL, Bandit (Python), gosec (Go)
- **Scans run on:**
  - Every PR (pre-merge)
  - Every commit to main
  - Scheduled (weekly/nightly)
- **Zero tolerance:** No high/critical vulnerabilities allowed to merge
- **SLA:** High severity vulnerabilities fixed within 7 days

**Citations:**
- ArXiv: "Security and Quality in LLM-Generated Code"
- ArXiv: "Security Degradation in Iterative AI Code Generation"
- GitHub Advanced Security documentation
- OWASP Top 10

---

### 13.2 Secrets Management

**Definition:** Proper handling of sensitive data (API keys, passwords, tokens) using secret management tools, not hardcoded values.

**Why It Matters:** Hardcoded secrets in code create security vulnerabilities. AI might accidentally suggest or expose secrets. Proper secrets management is critical security practice.

**Impact on Agent Behavior:**
- Avoids suggesting hardcoded secrets
- Recommends environment variables
- Identifies potential secret exposure
- Suggests secure alternatives

**Measurable Criteria:**
- **No secrets in code:** Use environment variables, secret managers
- **Tools:**
  - Development: .env files (not committed), direnv
  - Production: HashiCorp Vault, AWS Secrets Manager, Azure Key Vault, GCP Secret Manager
- **.env.example** committed (without real values)
- **.env** in .gitignore
- **Secret rotation** documented and automated
- **Pre-commit hook:** Detect-secrets or similar

**Citations:**
- OWASP: "Secrets Management Cheat Sheet"
- GitHub: "Removing sensitive data from a repository"
- HashiCorp Vault documentation

---

## 14. DOCUMENTATION PHILOSOPHY

### 14.1 "Why, Not What" Comments

**Definition:** Comments explain rationale and context, not behavior (which code already shows).

**Why It Matters:** AI can read code to understand "what" it does. Comments providing "why" give context for decisions, workarounds, constraints, and edge cases that aren't obvious from code alone.

**Impact on Agent Behavior:**
- Understanding of constraints and limitations
- Avoidance of "obvious" refactorings that break assumptions
- Preservation of original intent during modifications
- Better context for debugging and optimization

**Measurable Criteria:**
- **Comments explain:**
  - Why this approach was chosen (vs. alternatives)
  - Edge cases and gotchas
  - Performance considerations
  - Historical context (why workaround exists)
  - TODOs with context and rationale
- **Avoid:**
  - Redundant comments duplicating code
  - Commented-out code (use version control)
  - Obvious statements
- **Keep comments in sync** with code during changes

**Citations:**
- Stack Overflow Blog: "Best practices for writing code comments"
- Stepsize: "The Engineer's Guide to Writing Meaningful Code Comments"
- Boot.dev: "Best Practices for Commenting Code"

**Example:**
```python
# Good: Explains "why"
# Using binary search instead of hash table because dataset is
# read-once and memory-constrained (< 100MB available).
# Hash table would require 150MB for this dataset size.
result = binary_search(sorted_data, target)

# API returns 202 Accepted for async processing, but we need
# synchronous behavior for consistency. Poll until completion.
response = api.start_job()
while response.status == 202:
    time.sleep(1)
    response = api.check_status(response.job_id)

# Bad: Redundant, explains "what"
# Search for target in sorted_data
result = binary_search(sorted_data, target)

# Call the API
response = api.start_job()
```

---

## 15. PERFORMANCE & OBSERVABILITY

### 15.1 Performance Benchmarks

**Definition:** Automated performance tests tracking metrics like response time, throughput, memory usage.

**Why It Matters:** Performance regressions can slip in unnoticed. Benchmarks provide objective measurements. AI can suggest optimizations based on benchmark results.

**Impact on Agent Behavior:**
- Performance-aware optimization suggestions
- Regression detection
- Data-driven refactoring decisions
- Bottleneck identification

**Measurable Criteria:**
- Benchmark suite in repository
- **Tools:** pytest-benchmark (Python), Benchmark.js (JavaScript), testing.B (Go)
- Run benchmarks in CI for critical paths
- Track metrics over time
- Alert on regressions (>10% slowdown)

**Citations:**
- Google: "Benchmarking Best Practices"
- Python performance benchmarking docs
- Go benchmarking documentation

---

## IMPLEMENTATION PRIORITIES

### Tier 1: Essential (Must-Have)
**Highest impact, enables basic agent functionality:**

1. **CLAUDE.md** - 40% time savings, immediate context framing
2. **README with quick start** - Entry point understanding
3. **Type annotations** - Higher quality latent space, better comprehension
4. **Standard project layout** - Faster navigation
5. **Dependency lock files** - Reproducible builds

### Tier 2: Critical (Should-Have)
**Major quality improvements, safety nets:**

6. **Test coverage >70%** - Safety for refactoring
7. **Pre-commit hooks + CI/CD** - Automated quality enforcement
8. **Conventional commits** - Semantic versioning, history understanding
9. **Complete .gitignore** - Reduced context pollution
10. **One-command setup** - Easy environment reproduction

### Tier 3: Important (Nice-to-Have)
**Significant improvements in specific areas:**

11. **Cyclomatic complexity limits** - Better code comprehension
12. **Structured logging** - Machine-parseable debugging
13. **OpenAPI/GraphQL specs** - Auto-generated clients
14. **ADRs** - Architectural context
15. **Semantic naming** - Faster code location

### Tier 4: Advanced (Optimization)
**Refinement and optimization:**

16. **Security scanning** - Vulnerability prevention
17. **Performance benchmarks** - Regression detection
18. **Code smell elimination** - Higher quality baseline
19. **PR/Issue templates** - Consistent contributions
20. **Container setup** - Reproducible environments

---

## QUICKSTART: Making Your Codebase Agent-Ready

### Week 1: Foundation Documentation
```bash
# Create CLAUDE.md
cat > CLAUDE.md << 'EOF'
# Tech Stack
- [Your language/framework with versions]

# Standard Commands
- Setup: [command]
- Test: [command]
- Lint: [command]
- Build: [command]

# Repository Structure
- src/ - [description]
- tests/ - [description]

# Boundaries
- [Any off-limits areas]
EOF

# Update README
# Add: Installation, Quick Start, Testing sections

# Create .env.example
cp .env .env.example
# Remove sensitive values, keep variable names
```

### Week 2: Quality Automation
```bash
# Install pre-commit
pip install pre-commit

# Create .pre-commit-config.yaml
pre-commit sample-config > .pre-commit-config.yaml

# Add formatters, linters for your language
# Install hooks
pre-commit install

# Add commitlint (optional but recommended)
npm install -g @commitlint/cli @commitlint/config-conventional
```

### Week 3: Testing & Dependencies
```bash
# Measure test coverage
pytest --cov  # Python
jest --coverage  # JavaScript
go test -cover  # Go

# Generate lock file
pip freeze > requirements.txt  # Python
npm install  # Generates package-lock.json
go mod tidy  # Updates go.sum

# Add Dependabot
# Create .github/dependabot.yml
```

### Week 4: Structure & Types
```bash
# Refactor to standard layout (if needed)
# Add type annotations to public APIs
mypy --install-types  # Python
tsc --init  # TypeScript

# Create PR/Issue templates
mkdir -p .github/ISSUE_TEMPLATE
# Add bug_report.md, feature_request.md
# Add PULL_REQUEST_TEMPLATE.md
```

### Ongoing Maintenance
- Update CLAUDE.md as project evolves
- Create ADRs for architectural decisions
- Monitor code quality metrics (SonarQube, CodeClimate)
- Keep dependencies updated
- Review and improve test coverage

---

## MEASUREMENT & VALIDATION

### Agent-Ready Score Formula

```
Score = (
    Documentation * 0.25 +
    Code Quality * 0.20 +
    Testing * 0.20 +
    Structure * 0.15 +
    CI/CD * 0.10 +
    Security * 0.10
) * 100

Where each category is 0.0-1.0 based on attribute completion.
```

### Certification Levels

- **Platinum (90-100):** Exemplary agent-ready codebase
- **Gold (75-89):** Highly optimized for agents
- **Silver (60-74):** Well-suited for agent development
- **Bronze (40-59):** Basic agent compatibility
- **Needs Improvement (<40):** Significant agent friction

### Validation Checklist

**Documentation (25%):**
- [ ] CLAUDE.md exists and comprehensive
- [ ] README with quick start
- [ ] Inline documentation (docstrings) >80%
- [ ] ADRs for major decisions
- [ ] API specs (OpenAPI/GraphQL)

**Code Quality (20%):**
- [ ] Type annotations >80%
- [ ] Cyclomatic complexity <10
- [ ] Function length <50 lines
- [ ] Code smells <5 per 1000 LOC
- [ ] DRY violations minimal

**Testing (20%):**
- [ ] Test coverage >70%
- [ ] Descriptive test names
- [ ] Fast test execution (<10 min)
- [ ] Tests in CI/CD

**Structure (15%):**
- [ ] Standard project layout
- [ ] Semantic file/directory names
- [ ] Separation of concerns
- [ ] .gitignore complete

**CI/CD (10%):**
- [ ] Pre-commit hooks
- [ ] CI linting/testing
- [ ] Branch protection
- [ ] Automated dependency updates

**Security (10%):**
- [ ] Dependency scanning
- [ ] Secret scanning
- [ ] No hardcoded secrets
- [ ] Security scans in CI

---

## ANTI-PATTERNS TO AVOID

### Documentation Anti-Patterns
- ‚ùå No README or minimal README
- ‚ùå Outdated documentation
- ‚ùå No inline documentation
- ‚ùå Documentation in external wiki only

### Code Anti-Patterns
- ‚ùå God objects/functions (>500 lines)
- ‚ùå No type hints
- ‚ùå Magic numbers without explanation
- ‚ùå Unclear variable names (x, tmp, data)

### Testing Anti-Patterns
- ‚ùå No tests or minimal coverage (<30%)
- ‚ùå Test names like test1, test2
- ‚ùå Slow tests (>30 min)
- ‚ùå Flaky tests

### Structure Anti-Patterns
- ‚ùå Flat file structure
- ‚ùå Mixed concerns in single file
- ‚ùå Inconsistent naming
- ‚ùå Incomplete .gitignore

### Process Anti-Patterns
- ‚ùå No CI/CD
- ‚ùå Manual quality checks
- ‚ùå No branch protection
- ‚ùå Direct commits to main

---

## REFERENCES & CITATIONS

### Anthropic
- Anthropic Engineering Blog: "Claude Code Best Practices" (2025)

### Research Papers (ArXiv)
- "LongCodeBench: Evaluating Coding LLMs at 1M Context Windows" (2025)
- "TypyBench: Evaluating LLM Type Inference for Untyped Python Repositories"
- "Automated Type Annotation in Python Using LLMs"
- "Security and Quality in LLM-Generated Code"
- "Security Degradation in Iterative AI Code Generation"

### Industry (Microsoft, Google, GitHub)
- Microsoft Learn: "Code metrics - Cyclomatic complexity"
- GitHub Blog: "How to write a great agents.md"
- GitHub: github/gitignore template collection
- Google SRE Book: Logging and monitoring best practices
- IBM Research: "Why larger LLM context windows are all the rage"

### Engineering Blogs
- Dropbox Tech Blog: "Our journey to type checking 4 million lines of Python"
- Salesforce Engineering: "How Cursor AI Cut Legacy Code Coverage Time by 85%"
- GitClear: "Coding on Copilot" whitepaper

### Standards & Specifications
- Conventional Commits specification v1.0.0
- OpenAPI Specification 3.0+
- PEP 8 - Style Guide for Python Code
- PEP 257 - Docstring Conventions

### Community Resources
- Real Python: "Python Application Layouts"
- GitHub: golang-standards/project-layout
- GitHub: joelparkerhenderson/architecture-decision-record
- GitHub: pre-commit/pre-commit

### Documentation
- Python: pytest, mypy, black, isort documentation
- JavaScript/TypeScript: ESLint, Prettier, TSDoc documentation
- Go: Official style guide, testing documentation
- Docker: Best practices documentation

---

## VERSION HISTORY

- **v1.0.0 (2025-01-20):** Initial comprehensive research compilation
  - 25 attributes identified and documented
  - 50+ authoritative sources cited
  - Measurement framework established
  - Implementation guide created

---

**Document prepared for:** agentready tool development
**Primary use case:** Scanning repositories for AI agent optimization
**Target agents:** Claude Code, Claude-based development assistants
**Methodology:** Evidence-based, cited research from authoritative sources
````

## File: src/agentready/data/default-weights.yaml
````yaml
# Default Tier-Based Weight Distribution
#
# This file defines the default weights for all 25 attributes.
# Weights are based on tier priority from research report:
#
# Tier 1 (Essential):  50% total (10% each, 5 attributes)
# Tier 2 (Critical):   30% total (3% each, 10 attributes)
# Tier 3 (Important):  15% total (3% each, 5 attributes)
# Tier 4 (Advanced):    5% total (1% each, 5 attributes)
#
# Missing essentials (especially CLAUDE.md at 10%) has 10x impact vs advanced features (1%)

# Tier 1 (Essential) - 50% total weight
claude_md_file: 0.10              # 1.1 - CLAUDE.md Configuration Files ‚ö†Ô∏è CRITICAL
readme_structure: 0.10            # 2.1 - README Structure
type_annotations: 0.10            # 3.3 - Type Annotations
standard_layout: 0.10             # 4.1 - Standard Project Layouts
lock_files: 0.10                  # 6.1 - Lock Files for Reproducibility

# Tier 2 (Critical) - 30% total weight
test_coverage: 0.03               # 5.1 - Test Coverage Requirements
precommit_hooks: 0.03             # 5.3 - Pre-commit Hooks & CI/CD Linting
conventional_commits: 0.03        # 7.1 - Conventional Commit Messages
gitignore_completeness: 0.03      # 7.2 - .gitignore Completeness
one_command_setup: 0.03           # 8.1 - One-Command Build/Setup
concise_documentation: 0.03       # 1.2 - Concise Structured Documentation
inline_documentation: 0.03        # 2.2 - Inline Documentation
file_size_limits: 0.03            # 1.3 - File Size Limits
dependency_freshness: 0.03        # 6.2 - Dependency Freshness & Security
separation_concerns: 0.03         # 4.2 - Separation of Concerns

# Tier 3 (Important) - 15% total weight
cyclomatic_complexity: 0.03       # 3.1 - Cyclomatic Complexity Thresholds
structured_logging: 0.03          # 9.2 - Structured Logging
openapi_specs: 0.03               # 10.1 - OpenAPI/Swagger Specifications
architecture_decisions: 0.03      # 2.3 - Architecture Decision Records
semantic_naming: 0.03             # 11.3 - Semantic File & Directory Naming

# Tier 4 (Advanced) - 5% total weight
security_scanning: 0.01           # 13.1 - Security Scanning Automation
performance_benchmarks: 0.01      # 15.1 - Performance Benchmarks
code_smells: 0.01                 # 3.4 - Code Smell Elimination
issue_pr_templates: 0.01          # 7.3 - Issue & Pull Request Templates
container_setup: 0.01             # 8.3 - Container/Virtualization Setup
````

## File: src/agentready/models/assessment.py
````python
"""Assessment model representing complete repository evaluation."""

from dataclasses import dataclass
from datetime import datetime

from .config import Config
from .finding import Finding
from .repository import Repository


@dataclass
class Assessment:
    """Complete evaluation of a repository at a specific point in time.

    Attributes:
        repository: The repository assessed
        timestamp: When assessment was performed
        overall_score: Weighted average score 0-100
        certification_level: Platinum/Gold/Silver/Bronze based on score
        attributes_assessed: Number of successfully evaluated attributes
        attributes_skipped: Number of skipped attributes (tool missing, errors)
        attributes_total: Total attributes (should be 25)
        findings: Individual attribute results
        config: Custom configuration used (if any)
        duration_seconds: Time taken for assessment
    """

    repository: Repository
    timestamp: datetime
    overall_score: float
    certification_level: str
    attributes_assessed: int
    attributes_skipped: int
    attributes_total: int
    findings: list[Finding]
    config: Config | None
    duration_seconds: float

    VALID_LEVELS = {"Platinum", "Gold", "Silver", "Bronze", "Needs Improvement"}

    def __post_init__(self):
        """Validate assessment data after initialization."""
        if not 0.0 <= self.overall_score <= 100.0:
            raise ValueError(
                f"Overall score must be in range [0.0, 100.0]: {self.overall_score}"
            )

        if self.certification_level not in self.VALID_LEVELS:
            raise ValueError(
                f"Certification level must be one of {self.VALID_LEVELS}: "
                f"{self.certification_level}"
            )

        if self.attributes_assessed + self.attributes_skipped != self.attributes_total:
            raise ValueError(
                f"Assessed ({self.attributes_assessed}) + skipped "
                f"({self.attributes_skipped}) must equal total "
                f"({self.attributes_total})"
            )

        if len(self.findings) != self.attributes_total:
            raise ValueError(
                f"Findings count ({len(self.findings)}) must equal "
                f"attributes total ({self.attributes_total})"
            )

    def to_dict(self) -> dict:
        """Convert to dictionary for JSON serialization."""
        return {
            "repository": self.repository.to_dict(),
            "timestamp": self.timestamp.isoformat(),
            "overall_score": self.overall_score,
            "certification_level": self.certification_level,
            "attributes_assessed": self.attributes_assessed,
            "attributes_skipped": self.attributes_skipped,
            "attributes_total": self.attributes_total,
            "findings": [f.to_dict() for f in self.findings],
            "config": self.config.to_dict() if self.config else None,
            "duration_seconds": self.duration_seconds,
        }

    @staticmethod
    def determine_certification_level(score: float) -> str:
        """Determine certification level based on overall score.

        Thresholds:
        - Platinum: 90-100
        - Gold: 75-89
        - Silver: 60-74
        - Bronze: 40-59
        - Needs Improvement: 0-39
        """
        if score >= 90:
            return "Platinum"
        elif score >= 75:
            return "Gold"
        elif score >= 60:
            return "Silver"
        elif score >= 40:
            return "Bronze"
        else:
            return "Needs Improvement"
````

## File: src/agentready/models/attribute.py
````python
"""Attribute model defining one of the 25 agent-ready quality attributes."""

from dataclasses import dataclass


@dataclass
class Attribute:
    """Defines an agent-ready quality attribute from the research report.

    Attributes:
        id: Unique identifier (e.g., "claude_md_file", "test_coverage")
        name: Human-readable name (e.g., "CLAUDE.md Configuration Files")
        category: Research report section (e.g., "Context Window Optimization")
        tier: Priority tier 1-4 (1=Essential, 4=Advanced)
        description: What this attribute measures
        criteria: Measurable criteria for passing
        default_weight: Default weight in scoring (0.0-1.0)
    """

    id: str
    name: str
    category: str
    tier: int
    description: str
    criteria: str
    default_weight: float

    def __post_init__(self):
        """Validate attribute data after initialization."""
        if not self.id.islower() or " " in self.id:
            raise ValueError(f"Attribute ID must be lowercase snake_case: {self.id}")

        if self.tier not in (1, 2, 3, 4):
            raise ValueError(f"Tier must be 1, 2, 3, or 4: {self.tier}")

        if not 0.0 <= self.default_weight <= 1.0:
            raise ValueError(
                f"Default weight must be in range [0.0, 1.0]: {self.default_weight}"
            )

    def to_dict(self) -> dict:
        """Convert to dictionary for JSON serialization."""
        return {
            "id": self.id,
            "name": self.name,
            "category": self.category,
            "tier": self.tier,
            "description": self.description,
            "criteria": self.criteria,
            "default_weight": self.default_weight,
        }
````

## File: src/agentready/models/citation.py
````python
"""Citation model for authoritative sources from research report."""

from dataclasses import dataclass


@dataclass
class Citation:
    """Reference to authoritative source from research report.

    Attributes:
        source: Source name (e.g., "Anthropic Engineering Blog")
        title: Article/paper title
        url: Link to source (optional)
        relevance: Why this citation supports the attribute
    """

    source: str
    title: str
    url: str | None
    relevance: str

    def __post_init__(self):
        """Validate citation data after initialization."""
        if not self.source:
            raise ValueError("Citation source must be non-empty")

        if not self.title:
            raise ValueError("Citation title must be non-empty")

        if not self.relevance:
            raise ValueError("Citation relevance must be non-empty")

    def to_dict(self) -> dict:
        """Convert to dictionary for JSON serialization."""
        return {
            "source": self.source,
            "title": self.title,
            "url": self.url,
            "relevance": self.relevance,
        }
````

## File: src/agentready/models/config.py
````python
"""Config model for user customization of assessment behavior."""

from dataclasses import dataclass
from pathlib import Path


@dataclass
class Config:
    """User configuration for customizing assessment behavior.

    Attributes:
        weights: Custom attribute weights (attribute_id ‚Üí weight)
        excluded_attributes: Attributes to skip
        language_overrides: Force language detection (lang ‚Üí [patterns])
        output_dir: Custom output directory (None uses default .agentready/)
    """

    weights: dict[str, float]
    excluded_attributes: list[str]
    language_overrides: dict[str, list[str]]
    output_dir: Path | None

    def __post_init__(self):
        """Validate config data after initialization."""
        # Validate weights are positive
        for attr_id, weight in self.weights.items():
            if weight <= 0:
                raise ValueError(f"Weight must be positive for {attr_id}: {weight}")
            if weight > 1.0:
                raise ValueError(f"Weight must be <= 1.0 for {attr_id}: {weight}")

        # Validate weights sum (with tolerance for floating point)
        if self.weights:
            total = sum(self.weights.values())
            tolerance = 0.001
            if abs(total - 1.0) > tolerance:
                raise ValueError(
                    f"Weights must sum to 1.0 (got {total:.4f}, "
                    f"difference: {total - 1.0:+.4f})"
                )

    def to_dict(self) -> dict:
        """Convert to dictionary for JSON serialization."""
        return {
            "weights": self.weights,
            "excluded_attributes": self.excluded_attributes,
            "language_overrides": self.language_overrides,
            "output_dir": str(self.output_dir) if self.output_dir else None,
        }

    def get_weight(self, attribute_id: str, default: float) -> float:
        """Get weight for attribute, falling back to default if not specified."""
        return self.weights.get(attribute_id, default)

    def is_excluded(self, attribute_id: str) -> bool:
        """Check if attribute is excluded from assessment."""
        return attribute_id in self.excluded_attributes
````

## File: src/agentready/models/finding.py
````python
"""Finding and Remediation models for assessment results."""

from dataclasses import dataclass

from .attribute import Attribute
from .citation import Citation


@dataclass
class Remediation:
    """Actionable guidance for fixing a failing attribute.

    Attributes:
        summary: One-line summary of what to do
        steps: Ordered steps to remediate
        tools: Tools/packages needed (e.g., "black", "pytest-cov")
        commands: Example commands to run
        examples: Code/config examples
        citations: Links to documentation/research
    """

    summary: str
    steps: list[str]
    tools: list[str]
    commands: list[str]
    examples: list[str]
    citations: list[Citation]

    def __post_init__(self):
        """Validate remediation data after initialization."""
        if not self.summary:
            raise ValueError("Remediation summary must be non-empty")

        if not self.steps:
            raise ValueError("Remediation must have at least one step")

    def to_dict(self) -> dict:
        """Convert to dictionary for JSON serialization."""
        return {
            "summary": self.summary,
            "steps": self.steps,
            "tools": self.tools,
            "commands": self.commands,
            "examples": self.examples,
            "citations": [c.to_dict() for c in self.citations],
        }


@dataclass
class Finding:
    """Result of assessing a single attribute against a repository.

    Attributes:
        attribute: The attribute being assessed
        status: One of: pass, fail, skipped, error, not_applicable
        score: Score 0-100, or None if skipped/error
        measured_value: Actual measurement (e.g., "847 lines", "63% coverage")
        threshold: Expected threshold (e.g., "<300 lines", ">80% coverage")
        evidence: Specific files/metrics supporting the finding
        remediation: How to fix if failing (None if passing)
        error_message: Error details if status="error"
    """

    attribute: Attribute
    status: str
    score: float | None
    measured_value: str | None
    threshold: str | None
    evidence: list[str]
    remediation: Remediation | None
    error_message: str | None

    VALID_STATUSES = {"pass", "fail", "skipped", "error", "not_applicable"}

    def __post_init__(self):
        """Validate finding data after initialization."""
        if self.status not in self.VALID_STATUSES:
            raise ValueError(
                f"Status must be one of {self.VALID_STATUSES}: {self.status}"
            )

        if self.status in ("pass", "fail"):
            if self.score is None:
                raise ValueError(f"Score required for status {self.status}")
            if not 0.0 <= self.score <= 100.0:
                raise ValueError(f"Score must be in range [0.0, 100.0]: {self.score}")

        if self.status == "error" and not self.error_message:
            raise ValueError("Error message required for status error")

    def to_dict(self) -> dict:
        """Convert to dictionary for JSON serialization."""
        return {
            "attribute": self.attribute.to_dict(),
            "status": self.status,
            "score": self.score,
            "measured_value": self.measured_value,
            "threshold": self.threshold,
            "evidence": self.evidence,
            "remediation": self.remediation.to_dict() if self.remediation else None,
            "error_message": self.error_message,
        }

    @classmethod
    def not_applicable(cls, attribute: Attribute, reason: str = "") -> "Finding":
        """Create a not_applicable finding for language-specific attributes."""
        evidence = [reason] if reason else []
        return cls(
            attribute=attribute,
            status="not_applicable",
            score=None,
            measured_value=None,
            threshold=None,
            evidence=evidence,
            remediation=None,
            error_message=None,
        )

    @classmethod
    def skipped(
        cls, attribute: Attribute, reason: str, remediation: str = ""
    ) -> "Finding":
        """Create a skipped finding for missing tools or permission errors."""
        rem = None
        if remediation:
            rem = Remediation(
                summary=remediation,
                steps=[remediation],
                tools=[],
                commands=[],
                examples=[],
                citations=[],
            )

        return cls(
            attribute=attribute,
            status="skipped",
            score=None,
            measured_value=None,
            threshold=None,
            evidence=[reason],
            remediation=rem,
            error_message=None,
        )

    @classmethod
    def error(cls, attribute: Attribute, reason: str) -> "Finding":
        """Create an error finding for unexpected failures."""
        return cls(
            attribute=attribute,
            status="error",
            score=None,
            measured_value=None,
            threshold=None,
            evidence=[],
            remediation=None,
            error_message=reason,
        )
````

## File: src/agentready/models/repository.py
````python
"""Repository model representing the target git repository being assessed."""

from dataclasses import dataclass
from pathlib import Path


@dataclass
class Repository:
    """Represents a git repository being assessed.

    Attributes:
        path: Absolute path to repository root
        name: Repository name (derived from path)
        url: Remote origin URL if available
        branch: Current branch name
        commit_hash: Current HEAD commit SHA
        languages: Detected languages with file counts (e.g., {"Python": 42})
        total_files: Total files in repository (respecting .gitignore)
        total_lines: Total lines of code
    """

    path: Path
    name: str
    url: str | None
    branch: str
    commit_hash: str
    languages: dict[str, int]
    total_files: int
    total_lines: int

    def __post_init__(self):
        """Validate repository data after initialization."""
        if not self.path.exists():
            raise ValueError(f"Repository path does not exist: {self.path}")

        if not (self.path / ".git").exists():
            raise ValueError(f"Not a git repository: {self.path}")

        if self.total_files < 0:
            raise ValueError(f"Total files must be non-negative: {self.total_files}")

        if self.total_lines < 0:
            raise ValueError(f"Total lines must be non-negative: {self.total_lines}")

    def to_dict(self) -> dict:
        """Convert to dictionary for JSON serialization."""
        return {
            "path": str(self.path),
            "name": self.name,
            "url": self.url,
            "branch": self.branch,
            "commit_hash": self.commit_hash,
            "languages": self.languages,
            "total_files": self.total_files,
            "total_lines": self.total_lines,
        }
````

## File: src/agentready/reporters/base.py
````python
"""Base reporter interface for generating assessment reports."""

from abc import ABC, abstractmethod
from pathlib import Path

from ..models.assessment import Assessment


class BaseReporter(ABC):
    """Abstract base class for all report generators.

    Reporters transform Assessment data into different output formats
    (HTML, Markdown, PDF, etc.) for human consumption.
    """

    @abstractmethod
    def generate(self, assessment: Assessment, output_path: Path) -> Path:
        """Generate report from assessment data.

        Args:
            assessment: Complete assessment with findings
            output_path: Path where report should be saved

        Returns:
            Path to generated report file

        Raises:
            IOError: If report cannot be written
        """
        pass
````

## File: src/agentready/services/language_detector.py
````python
"""Language detection service using file extension analysis."""

import subprocess
from collections import defaultdict
from pathlib import Path


class LanguageDetector:
    """Detects programming languages in a repository.

    Uses file extension mapping and respects .gitignore patterns
    via git ls-files for accuracy.
    """

    # Extension to language mapping
    EXTENSION_MAP = {
        ".py": "Python",
        ".pyx": "Python",
        ".pyi": "Python",
        ".js": "JavaScript",
        ".jsx": "JavaScript",
        ".mjs": "JavaScript",
        ".cjs": "JavaScript",
        ".ts": "TypeScript",
        ".tsx": "TypeScript",
        ".go": "Go",
        ".java": "Java",
        ".kt": "Kotlin",
        ".kts": "Kotlin",
        ".c": "C",
        ".h": "C",  # Ambiguous but default to C
        ".cpp": "C++",
        ".cc": "C++",
        ".cxx": "C++",
        ".hpp": "C++",
        ".hxx": "C++",
        ".rs": "Rust",
        ".rb": "Ruby",
        ".php": "PHP",
        ".swift": "Swift",
        ".r": "R",
        ".R": "R",
        ".cs": "C#",
        ".scala": "Scala",
        ".sh": "Shell",
        ".bash": "Shell",
        ".zsh": "Shell",
        ".md": "Markdown",
        ".yaml": "YAML",
        ".yml": "YAML",
        ".json": "JSON",
        ".toml": "TOML",
        ".xml": "XML",
    }

    def __init__(self, repository_path: Path):
        """Initialize language detector for repository.

        Args:
            repository_path: Path to git repository root
        """
        self.repository_path = repository_path
        self.minimum_file_threshold = 3  # Need 3+ files to count as "using language"

    def detect_languages(self) -> dict[str, int]:
        """Detect languages in repository with file counts.

        Returns:
            Dictionary mapping language name to file count
            (e.g., {"Python": 42, "JavaScript": 18})

        Only includes languages with >= minimum_file_threshold files.
        """
        language_counts = defaultdict(int)

        # Try git ls-files first (respects .gitignore)
        try:
            result = subprocess.run(
                ["git", "ls-files"],
                cwd=self.repository_path,
                capture_output=True,
                text=True,
                timeout=30,
                check=True,
            )
            files = result.stdout.strip().split("\n")
        except (subprocess.SubprocessError, FileNotFoundError):
            # Fall back to pathlib walk (less accurate)
            files = [
                str(f.relative_to(self.repository_path))
                for f in self.repository_path.rglob("*")
                if f.is_file()
            ]

        # Count files by language
        for file_path in files:
            if not file_path.strip():
                continue

            path = Path(file_path)
            suffix = path.suffix.lower()

            if suffix in self.EXTENSION_MAP:
                language = self.EXTENSION_MAP[suffix]
                language_counts[language] += 1

        # Filter by minimum threshold
        return {
            lang: count
            for lang, count in language_counts.items()
            if count >= self.minimum_file_threshold
        }

    def count_total_files(self) -> int:
        """Count total files in repository (respecting .gitignore).

        Returns:
            Total file count
        """
        try:
            result = subprocess.run(
                ["git", "ls-files"],
                cwd=self.repository_path,
                capture_output=True,
                text=True,
                timeout=30,
                check=True,
            )
            files = result.stdout.strip().split("\n")
            return len([f for f in files if f.strip()])
        except (subprocess.SubprocessError, FileNotFoundError):
            # Fall back to pathlib
            return sum(1 for _ in self.repository_path.rglob("*") if _.is_file())

    def count_total_lines(self) -> int:
        """Count total lines of code in repository.

        Returns:
            Total line count (excluding empty lines and comments)

        Note: This is a simple implementation. For production use,
        consider using a dedicated tool like cloc or tokei.
        """
        total_lines = 0

        try:
            result = subprocess.run(
                ["git", "ls-files"],
                cwd=self.repository_path,
                capture_output=True,
                text=True,
                timeout=30,
                check=True,
            )
            files = result.stdout.strip().split("\n")
        except (subprocess.SubprocessError, FileNotFoundError):
            files = [
                str(f.relative_to(self.repository_path))
                for f in self.repository_path.rglob("*")
                if f.is_file()
            ]

        for file_path in files:
            if not file_path.strip():
                continue

            full_path = self.repository_path / file_path

            # Only count text files (skip binaries)
            try:
                with open(full_path, "r", encoding="utf-8", errors="ignore") as f:
                    total_lines += sum(1 for line in f if line.strip())
            except (OSError, UnicodeDecodeError):
                # Skip binary files or unreadable files
                continue

        return total_lines
````

## File: src/agentready/services/research_loader.py
````python
"""Research loader service for loading and validating research reports."""

import re
from dataclasses import dataclass
from pathlib import Path


@dataclass
class ResearchMetadata:
    """Metadata extracted from research report."""

    version: str
    date: str
    attribute_count: int
    tier_count: int
    reference_count: int


class ResearchLoader:
    """Loads and validates bundled research report.

    Supports loading from bundled data, custom paths, and URLs.
    Validates structure per FR-024.
    """

    def __init__(self, data_dir: Path | None = None):
        """Initialize research loader.

        Args:
            data_dir: Directory containing research report
                     (defaults to package data directory)
        """
        if data_dir is None:
            # Default to package data directory
            self.data_dir = Path(__file__).parent.parent / "data"
        else:
            self.data_dir = data_dir

        self.research_file = self.data_dir / "agent-ready-codebase-attributes.md"

    def load_research_report(self) -> str:
        """Load research report content.

        Returns:
            Research report markdown content

        Raises:
            FileNotFoundError: If research report not found
        """
        if not self.research_file.exists():
            raise FileNotFoundError(f"Research report not found: {self.research_file}")

        with open(self.research_file, "r", encoding="utf-8") as f:
            return f.read()

    def extract_metadata(self, content: str) -> ResearchMetadata:
        """Extract metadata from research report.

        Args:
            content: Research report markdown content

        Returns:
            ResearchMetadata with version, date, counts

        Raises:
            ValueError: If metadata cannot be extracted
        """
        # Extract version and date from YAML frontmatter
        frontmatter_match = re.search(
            r"^---\s*\nversion:\s*([^\n]+)\s*\ndate:\s*([^\n]+)\s*\n---",
            content,
            re.MULTILINE,
        )

        if frontmatter_match:
            version = frontmatter_match.group(1).strip()
            date = frontmatter_match.group(2).strip()
        else:
            # Default version if not found
            version = "1.0.0"
            date = "unknown"

        # Count attributes (look for ### headings with numbering like "1.1", "2.3", etc.)
        attribute_pattern = r"^###\s+\d+\.\d+\s+"
        attribute_count = len(re.findall(attribute_pattern, content, re.MULTILINE))

        # Count tiers (look for tier headings)
        tier_pattern = r"^###\s+Tier\s+\d+:"
        tier_count = len(re.findall(tier_pattern, content, re.MULTILINE))

        # Count references (look for citation patterns)
        reference_pattern = r"^\d+\.\s+\[.+?\]\(.+?\)"
        reference_count = len(re.findall(reference_pattern, content, re.MULTILINE))

        return ResearchMetadata(
            version=version,
            date=date,
            attribute_count=attribute_count,
            tier_count=tier_count,
            reference_count=reference_count,
        )

    def validate_structure(self, content: str) -> tuple[bool, list[str], list[str]]:
        """Validate research report structure.

        Args:
            content: Research report markdown content

        Returns:
            Tuple of (is_valid, errors, warnings)
            - errors: Blocking issues that prevent usage
            - warnings: Non-critical issues that can be ignored

        Validation rules per FR-024:
        - Errors: Missing metadata, incorrect attribute count, missing measurable criteria
        - Warnings: Missing impact sections, fewer references than recommended
        """
        errors = []
        warnings = []

        metadata = self.extract_metadata(content)

        # Check attribute count (should be 25)
        if metadata.attribute_count != 25:
            errors.append(f"Expected 25 attributes, found {metadata.attribute_count}")

        # Check tier count (should be 4)
        if metadata.tier_count < 4:
            errors.append(f"Expected 4 tiers, found {metadata.tier_count}")

        # Check reference count (recommend 20+)
        if metadata.reference_count < 20:
            warnings.append(
                f"Only {metadata.reference_count} references "
                f"(recommend 20+ for evidence-based design)"
            )

        # Check for "Measurable Criteria" sections
        measurable_criteria_pattern = r"\*\*Measurable Criteria:\*\*"
        criteria_count = len(
            re.findall(measurable_criteria_pattern, content, re.MULTILINE)
        )

        if criteria_count < 25:
            errors.append(
                f"Missing 'Measurable Criteria' sections "
                f"(found {criteria_count}/25)"
            )

        # Check for "Impact on Agent Behavior" sections (warning only)
        impact_pattern = r"\*\*Impact on Agent Behavior:\*\*"
        impact_count = len(re.findall(impact_pattern, content, re.MULTILINE))

        if impact_count < 25:
            warnings.append(
                f"{25 - impact_count} attributes missing "
                f"'Impact on Agent Behavior' sections"
            )

        is_valid = len(errors) == 0

        return is_valid, errors, warnings

    def load_and_validate(
        self,
    ) -> tuple[str, ResearchMetadata, bool, list[str], list[str]]:
        """Load research report and validate structure.

        Returns:
            Tuple of (content, metadata, is_valid, errors, warnings)

        Raises:
            FileNotFoundError: If research report not found
        """
        content = self.load_research_report()
        metadata = self.extract_metadata(content)
        is_valid, errors, warnings = self.validate_structure(content)

        return content, metadata, is_valid, errors, warnings
````

## File: src/agentready/services/scanner.py
````python
"""Scanner service orchestrating the assessment workflow."""

import time
from datetime import datetime
from pathlib import Path

import git

from ..models.assessment import Assessment
from ..models.config import Config
from ..models.finding import Finding
from ..models.repository import Repository
from .language_detector import LanguageDetector
from .scorer import Scorer


class MissingToolError(Exception):
    """Raised when a required tool is missing."""

    def __init__(self, tool_name: str, install_command: str = ""):
        self.tool_name = tool_name
        self.install_command = install_command
        super().__init__(f"Missing tool: {tool_name}")


class Scanner:
    """Orchestrates assessment workflow with try-assess-skip error handling.

    Responsibilities:
    - Validate repository structure
    - Detect languages and metadata
    - Execute assessors with graceful degradation
    - Calculate scores and certification level
    - Track progress
    """

    def __init__(self, repository_path: Path, config: Config | None = None):
        """Initialize scanner for repository.

        Args:
            repository_path: Path to git repository root
            config: User configuration (optional)

        Raises:
            ValueError: If repository is invalid
        """
        self.repository_path = repository_path
        self.config = config
        self.scorer = Scorer()

        # Validate repository
        self._validate_repository()

    def _validate_repository(self):
        """Validate repository has .git directory per FR-017.

        Raises:
            ValueError: If not a valid git repository
        """
        if not self.repository_path.exists():
            raise ValueError(f"Repository path does not exist: {self.repository_path}")

        if not (self.repository_path / ".git").exists():
            raise ValueError(f"Not a git repository: {self.repository_path}")

    def scan(self, assessors: list, verbose: bool = False) -> Assessment:
        """Execute full assessment workflow.

        Args:
            assessors: List of assessor instances to run
            verbose: Enable detailed progress logging

        Returns:
            Complete Assessment with findings and scores

        Flow:
        1. Build Repository model (detect languages, metadata)
        2. For each assessor: try assess ‚Üí catch errors ‚Üí create finding
        3. Calculate overall score with weighted average
        4. Determine certification level
        5. Return Assessment
        """
        start_time = time.time()

        if verbose:
            print(f"Scanning repository: {self.repository_path.name}")

        # Build Repository model
        repository = self._build_repository_model(verbose)

        if verbose:
            print(f"Languages detected: {', '.join(repository.languages.keys())}")
            print(f"\nEvaluating {len(assessors)} attributes...")

        # Execute assessors with graceful degradation
        findings = []
        for assessor in assessors:
            finding = self._execute_assessor(assessor, repository, verbose)
            findings.append(finding)

        # Calculate scores
        overall_score = self.scorer.calculate_overall_score(findings, self.config)
        certification_level = self.scorer.determine_certification_level(overall_score)

        # Count assessed vs skipped
        assessed, skipped = self.scorer.count_assessed_attributes(findings)

        duration = time.time() - start_time

        if verbose:
            print(f"\nAssessment complete in {duration:.1f}s")
            print(f"Overall Score: {overall_score}/100 ({certification_level})")
            print(
                f"Attributes Assessed: {assessed}/{len(findings)} ({skipped} skipped)"
            )

        return Assessment(
            repository=repository,
            timestamp=datetime.now(),
            overall_score=overall_score,
            certification_level=certification_level,
            attributes_assessed=assessed,
            attributes_skipped=skipped,
            attributes_total=len(findings),
            findings=findings,
            config=self.config,
            duration_seconds=round(duration, 1),
        )

    def _build_repository_model(self, verbose: bool = False) -> Repository:
        """Build Repository model with metadata and language detection.

        Args:
            verbose: Enable progress logging

        Returns:
            Repository model
        """
        if verbose:
            print("Detecting languages and repository metadata...")

        # Git metadata
        repo = git.Repo(self.repository_path)
        name = self.repository_path.name
        branch = repo.active_branch.name
        commit_hash = repo.head.commit.hexsha

        # Get remote URL (if available)
        try:
            url = repo.remotes.origin.url if repo.remotes else None
        except Exception:
            url = None

        # Language detection
        detector = LanguageDetector(self.repository_path)
        languages = detector.detect_languages()
        total_files = detector.count_total_files()
        total_lines = detector.count_total_lines()

        return Repository(
            path=self.repository_path,
            name=name,
            url=url,
            branch=branch,
            commit_hash=commit_hash,
            languages=languages,
            total_files=total_files,
            total_lines=total_lines,
        )

    def _execute_assessor(
        self, assessor, repository: Repository, verbose: bool = False
    ) -> Finding:
        """Execute single assessor with error handling.

        Implements try-assess-skip pattern per research.md:
        - Check if applicable
        - Try to assess
        - Catch errors ‚Üí return Finding.skipped() or Finding.error()

        Args:
            assessor: Assessor instance
            repository: Repository model
            verbose: Enable progress logging

        Returns:
            Finding (pass/fail/skipped/error/not_applicable)
        """
        attr_id = assessor.attribute_id

        if verbose:
            print(f"  [{attr_id}] ", end="", flush=True)

        # Check if applicable (language-specific checks)
        try:
            if not assessor.is_applicable(repository):
                if verbose:
                    print("not applicable")
                return Finding.not_applicable(
                    assessor.attribute,
                    reason=f"Not applicable to {list(repository.languages.keys())}",
                )
        except Exception as e:
            if verbose:
                print(f"error (applicability check failed)")
            return Finding.error(
                assessor.attribute, reason=f"Applicability check failed: {str(e)}"
            )

        # Try to assess
        try:
            finding = assessor.assess(repository)

            if verbose:
                if finding.status == "pass":
                    print(f"pass ({finding.score:.0f})")
                elif finding.status == "fail":
                    print(f"fail ({finding.score:.0f})")
                elif finding.status == "skipped":
                    print("skipped")
                else:
                    print(finding.status)

            return finding

        except MissingToolError as e:
            if verbose:
                print(f"skipped (missing {e.tool_name})")

            return Finding.skipped(
                assessor.attribute,
                reason=f"Missing tool: {e.tool_name}",
                remediation=(
                    f"Install with: {e.install_command}" if e.install_command else ""
                ),
            )

        except PermissionError as e:
            if verbose:
                print("skipped (permission denied)")

            return Finding.skipped(
                assessor.attribute,
                reason=f"Permission denied: {getattr(e, 'filename', 'unknown')}",
            )

        except Exception as e:
            if verbose:
                print(f"error ({type(e).__name__})")

            return Finding.error(assessor.attribute, reason=str(e))
````

## File: src/agentready/services/scorer.py
````python
"""Scorer service for calculating weighted scores and certification levels."""

from pathlib import Path

import yaml

from ..models.assessment import Assessment
from ..models.config import Config
from ..models.finding import Finding


class Scorer:
    """Calculates weighted scores and determines certification levels.

    Implements tier-based weight distribution with heavy penalties for
    missing essentials (especially CLAUDE.md at 10% weight).
    """

    def __init__(self, default_weights_path: Path | None = None):
        """Initialize scorer with default weights.

        Args:
            default_weights_path: Path to default-weights.yaml
                                 (defaults to package data directory)
        """
        if default_weights_path is None:
            data_dir = Path(__file__).parent.parent / "data"
            default_weights_path = data_dir / "default-weights.yaml"

        self.default_weights = self._load_weights(default_weights_path)

    def _load_weights(self, path: Path) -> dict[str, float]:
        """Load default weights from YAML file.

        Args:
            path: Path to weights YAML file

        Returns:
            Dictionary mapping attribute ID to weight

        Raises:
            FileNotFoundError: If weights file not found
            ValueError: If weights are invalid
        """
        if not path.exists():
            raise FileNotFoundError(f"Weights file not found: {path}")

        with open(path, "r", encoding="utf-8") as f:
            weights = yaml.safe_load(f)

        # Validate weights sum to 1.0
        total = sum(weights.values())
        tolerance = 0.001

        if abs(total - 1.0) > tolerance:
            raise ValueError(
                f"Default weights must sum to 1.0 (got {total:.4f}, "
                f"difference: {total - 1.0:+.4f})"
            )

        return weights

    def merge_and_rescale_weights(self, config: Config | None) -> dict[str, float]:
        """Merge config overrides with tier defaults and rescale to sum to 1.0.

        Args:
            config: User configuration with weight overrides (or None)

        Returns:
            Final weights dictionary (complete, sums to 1.0)

        Algorithm:
        1. Start with tier defaults
        2. Override with config values
        3. Rescale to sum to 1.0
        """
        # Start with tier defaults
        final_weights = self.default_weights.copy()

        # Override with config values
        if config and config.weights:
            final_weights.update(config.weights)

        # Rescale to sum to 1.0
        total = sum(final_weights.values())
        rescaled = {attr: w / total for attr, w in final_weights.items()}

        return rescaled

    def calculate_overall_score(
        self, findings: list[Finding], config: Config | None = None
    ) -> float:
        """Calculate weighted overall score from findings.

        Args:
            findings: List of assessment findings
            config: User configuration with custom weights (optional)

        Returns:
            Overall score 0-100

        Per FR-027: Only include successfully evaluated attributes in score.
        Skipped/error attributes are excluded from denominator.
        """
        weights = self.merge_and_rescale_weights(config)

        # Calculate weighted score
        total_score = 0.0
        total_weight = 0.0

        for finding in findings:
            attr_id = finding.attribute.id

            # Skip attributes that weren't assessed
            if finding.status in ("skipped", "error", "not_applicable"):
                continue

            # Get weight for this attribute
            weight = weights.get(attr_id, finding.attribute.default_weight)

            # Add to weighted score
            if finding.score is not None:
                total_score += finding.score * weight
                total_weight += weight

        # Normalize score (divide by total weight of assessed attributes)
        if total_weight > 0:
            normalized_score = total_score / total_weight
        else:
            normalized_score = 0.0

        return round(normalized_score, 1)

    def count_assessed_attributes(self, findings: list[Finding]) -> tuple[int, int]:
        """Count assessed and skipped attributes.

        Args:
            findings: List of assessment findings

        Returns:
            Tuple of (assessed_count, skipped_count)
        """
        assessed = sum(1 for f in findings if f.status in ("pass", "fail"))

        skipped = sum(
            1 for f in findings if f.status in ("skipped", "error", "not_applicable")
        )

        return assessed, skipped

    def determine_certification_level(self, score: float) -> str:
        """Determine certification level based on score.

        Uses Assessment.determine_certification_level() for consistency.

        Args:
            score: Overall score 0-100

        Returns:
            Certification level string
        """
        return Assessment.determine_certification_level(score)
````

## File: src/agentready/templates/bootstrap/issue_templates/bug_report.md.j2
````
---
name: Bug Report
about: Create a report to help us improve
title: '[BUG] '
labels: bug
assignees: ''
---

## Bug Description

A clear and concise description of what the bug is.

## To Reproduce

Steps to reproduce the behavior:

1. Go to '...'
2. Click on '....'
3. Scroll down to '....'
4. See error

## Expected Behavior

A clear and concise description of what you expected to happen.

## Actual Behavior

A clear and concise description of what actually happened.

## Environment

- OS: [e.g. macOS 14.0, Ubuntu 22.04]
- Version: [e.g. 1.0.0]
- Python Version: [e.g. 3.11]

## Additional Context

Add any other context about the problem here. Include screenshots if applicable.

## Possible Solution

If you have suggestions on how to fix the bug, please describe them here.
````

## File: src/agentready/templates/bootstrap/issue_templates/feature_request.md.j2
````
---
name: Feature Request
about: Suggest an idea for this project
title: '[FEATURE] '
labels: enhancement
assignees: ''
---

## Problem Statement

A clear and concise description of what the problem is. Ex. I'm always frustrated when [...]

## Proposed Solution

A clear and concise description of what you want to happen.

## Alternatives Considered

A clear and concise description of any alternative solutions or features you've considered.

## Use Cases

Describe specific use cases where this feature would be valuable.

1. Use case 1
2. Use case 2
3. Use case 3

## Additional Context

Add any other context, screenshots, or mockups about the feature request here.

## Acceptance Criteria

- [ ] Criterion 1
- [ ] Criterion 2
- [ ] Criterion 3

## Priority

How important is this feature to you?

- [ ] Critical (blocking my work)
- [ ] High (significantly improves workflow)
- [ ] Medium (nice to have)
- [ ] Low (minor enhancement)
````

## File: src/agentready/templates/bootstrap/workflows/agentready-assessment.yml.j2
````
name: AgentReady Assessment

on:
  pull_request:
    types: [opened, synchronize, reopened]
  push:
    branches: [main, master]
  workflow_dispatch:

jobs:
  assess:
    runs-on: ubuntu-latest
    permissions:
      contents: read
      pull-requests: write

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install AgentReady
        run: |
          pip install agentready

      - name: Run AgentReady Assessment
        run: |
          agentready assess . --verbose

      - name: Upload Assessment Reports
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: agentready-reports
          path: .agentready/
          retention-days: 30

      - name: Comment on PR
        if: github.event_name == 'pull_request'
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            const reportPath = '.agentready/report-latest.md';

            if (!fs.existsSync(reportPath)) {
              console.log('No report found');
              return;
            }

            const report = fs.readFileSync(reportPath, 'utf8');

            // Post comment with assessment results
            await github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: report
            });
````

## File: src/agentready/templates/bootstrap/workflows/security.yml.j2
````
name: Security

on:
  push:
    branches: [main, master]
  pull_request:
  schedule:
    - cron: '0 0 * * 0'  # Weekly on Sunday
  workflow_dispatch:

permissions:
  contents: read
  security-events: write

jobs:
  codeql:
    name: CodeQL Analysis
    runs-on: ubuntu-latest

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Initialize CodeQL
        uses: github/codeql-action/init@v3
        with:
          languages: {{ 'python' if language == 'python' else 'javascript' }}

      - name: Autobuild
        uses: github/codeql-action/autobuild@v3

      - name: Perform CodeQL Analysis
        uses: github/codeql-action/analyze@v3

{% if language == 'python' %}
  safety:
    name: Dependency Security Scan
    runs-on: ubuntu-latest

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: |
          pip install safety

      - name: Run safety check
        run: |
          safety check --json || true
{% endif %}
````

## File: src/agentready/templates/bootstrap/workflows/tests-python.yml.j2
````
name: Tests

on:
  pull_request:
  push:
    branches: [main, master]
  workflow_dispatch:

jobs:
  test:
    runs-on: ubuntu-latest
    strategy:
      matrix:
        python-version: ['3.11', '3.12']

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python {{ '${{ matrix.python-version }}' }}
        uses: actions/setup-python@v5
        with:
          python-version: {{ '${{ matrix.python-version }}' }}

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -e ".[dev]"

      - name: Run black
        run: |
          black --check .

      - name: Run isort
        run: |
          isort --check .

      - name: Run ruff
        run: |
          ruff check .

      - name: Run pytest
        run: |
          pytest --cov=src --cov-report=xml --cov-report=term

      - name: Upload coverage to Codecov
        uses: codecov/codecov-action@v4
        if: matrix.python-version == '3.11'
        with:
          files: ./coverage.xml
          fail_ci_if_error: false
````

## File: src/agentready/templates/bootstrap/CODE_OF_CONDUCT.md.j2
````
# Code of Conduct

## Our Commitment

We are committed to providing a welcoming and inclusive environment for all contributors.

## Standards

Examples of behavior that contributes to a positive environment:
- Using welcoming and inclusive language
- Being respectful of differing viewpoints
- Accepting constructive criticism gracefully
- Focusing on what is best for the community

Examples of unacceptable behavior:
- Trolling or insulting comments
- Public or private harassment
- Publishing others' private information
- Other conduct which could reasonably be considered inappropriate

## Responsibilities

Project maintainers are responsible for clarifying standards of acceptable behavior and will take appropriate action in response to unacceptable behavior.

## Scope

This Code of Conduct applies within all project spaces and when representing the project in public spaces.

## Enforcement

Instances of abusive, harassing, or otherwise unacceptable behavior may be reported to the project team. All complaints will be reviewed and investigated promptly and fairly.

## Attribution

This Code of Conduct is adapted from common open source community standards.
````

## File: src/agentready/templates/bootstrap/CODEOWNERS.j2
````
# CODEOWNERS file
# Define code ownership for automatic review requests

# Global owners - these users are requested for review on all PRs
# *       @owner

# Specific paths can have different owners
# /docs/  @documentation-team
# /src/   @development-team
# /.github/ @devops-team

# Default: assign to repository owner
*       @owner
````

## File: src/agentready/templates/bootstrap/CONTRIBUTING.md.j2
````
# Contributing Guidelines

Thank you for your interest in contributing to this project! This document provides guidelines and instructions for contributing.

## Code of Conduct

Please read and follow our [Code of Conduct](CODE_OF_CONDUCT.md).

## Getting Started

### Prerequisites

{% if language == 'python' %}
- Python 3.11 or higher
- pip or uv for package management
- Git
{% elif language == 'javascript' %}
- Node.js 18 or higher
- npm or yarn
- Git
{% elif language == 'go' %}
- Go 1.21 or higher
- Git
{% endif %}

### Development Setup

1. Fork the repository
2. Clone your fork:
   ```bash
   git clone https://github.com/YOUR_USERNAME/PROJECT_NAME.git
   cd PROJECT_NAME
   ```

3. Set up your development environment:
{% if language == 'python' %}
   ```bash
   python -m venv venv
   source venv/bin/activate  # On Windows: venv\Scripts\activate
   pip install -e ".[dev]"
   ```
{% elif language == 'javascript' %}
   ```bash
   npm install
   ```
{% elif language == 'go' %}
   ```bash
   go mod download
   ```
{% endif %}

4. Install pre-commit hooks:
   ```bash
   pre-commit install
   ```

## Development Workflow

### Creating a Feature Branch

Always create a new branch for your work:

```bash
git checkout -b feature/your-feature-name
```

Use descriptive branch names:
- `feature/add-new-api`
- `fix/resolve-crash-on-startup`
- `docs/improve-installation-guide`

### Making Changes

1. Write clear, concise code
2. Follow the project's coding standards
3. Add tests for new functionality
4. Update documentation as needed
5. Run linters and tests before committing

### Running Tests

{% if language == 'python' %}
```bash
# Run all tests
pytest

# Run with coverage
pytest --cov=src --cov-report=term

# Run specific test file
pytest tests/test_specific.py
```
{% elif language == 'javascript' %}
```bash
# Run all tests
npm test

# Run with coverage
npm run test:coverage

# Run specific test
npm test -- tests/specific.test.js
```
{% elif language == 'go' %}
```bash
# Run all tests
go test ./...

# Run with coverage
go test -cover ./...

# Run specific test
go test -run TestSpecific ./...
```
{% endif %}

### Code Style

{% if language == 'python' %}
This project uses:
- **black** for code formatting
- **isort** for import sorting
- **ruff** for linting

Run formatters before committing:

```bash
black .
isort .
ruff check . --fix
```

Pre-commit hooks will automatically run these tools.
{% elif language == 'javascript' %}
This project uses:
- **Prettier** for code formatting
- **ESLint** for linting

Run formatters before committing:

```bash
npm run format
npm run lint
```
{% elif language == 'go' %}
This project uses:
- **gofmt** for code formatting
- **golangci-lint** for linting

Run formatters before committing:

```bash
go fmt ./...
golangci-lint run
```
{% endif %}

### Commit Messages

We use [Conventional Commits](https://www.conventionalcommits.org/):

```
<type>(<scope>): <subject>

<body>

<footer>
```

Types:
- `feat`: New feature
- `fix`: Bug fix
- `docs`: Documentation changes
- `style`: Code style changes (formatting, etc.)
- `refactor`: Code refactoring
- `test`: Adding or updating tests
- `chore`: Maintenance tasks

Examples:
```
feat(api): add user authentication endpoint

fix(parser): resolve crash when parsing empty files

docs(readme): update installation instructions
```

### Pull Request Process

1. **Update your branch** with the latest changes:
   ```bash
   git fetch upstream
   git rebase upstream/main
   ```

2. **Push your changes**:
   ```bash
   git push origin feature/your-feature-name
   ```

3. **Create a Pull Request**:
   - Use the PR template
   - Provide a clear description
   - Link related issues
   - Request reviews from maintainers

4. **Address feedback**:
   - Respond to review comments
   - Make requested changes
   - Push updates to your branch

5. **Wait for approval**:
   - At least one maintainer approval required
   - All CI checks must pass
   - No merge conflicts

## Testing Guidelines

- Write tests for all new functionality
- Maintain or improve code coverage
- Include both positive and negative test cases
- Test edge cases and error conditions

## Documentation

- Update README.md for user-facing changes
- Add docstrings to new functions and classes
- Update API documentation if applicable
- Add examples for new features

## Issue Reporting

When reporting issues, please include:

- Clear, descriptive title
- Steps to reproduce
- Expected vs. actual behavior
- Environment details (OS, version, etc.)
- Error messages or logs
- Screenshots if applicable

## Questions?

If you have questions:
- Check existing issues and discussions
- Ask in pull request comments
- Open a new discussion
- Contact maintainers

## License

By contributing, you agree that your contributions will be licensed under the same license as the project.

Thank you for contributing! üéâ
````

## File: src/agentready/templates/bootstrap/dependabot.yml.j2
````
version: 2
updates:
{% if language == 'python' %}
  - package-ecosystem: "pip"
    directory: "/"
    schedule:
      interval: "weekly"
    open-pull-requests-limit: 10
    labels:
      - "dependencies"
      - "python"
{% elif language == 'javascript' %}
  - package-ecosystem: "npm"
    directory: "/"
    schedule:
      interval: "weekly"
    open-pull-requests-limit: 10
    labels:
      - "dependencies"
      - "javascript"
{% elif language == 'go' %}
  - package-ecosystem: "gomod"
    directory: "/"
    schedule:
      interval: "weekly"
    open-pull-requests-limit: 10
    labels:
      - "dependencies"
      - "go"
{% endif %}

  - package-ecosystem: "github-actions"
    directory: "/"
    schedule:
      interval: "weekly"
    labels:
      - "dependencies"
      - "github-actions"
````

## File: src/agentready/templates/bootstrap/precommit-python.yaml.j2
````
repos:
  - repo: https://github.com/pre-commit/pre-commit-hooks
    rev: v4.5.0
    hooks:
      - id: trailing-whitespace
      - id: end-of-file-fixer
      - id: check-yaml
      - id: check-added-large-files
      - id: check-merge-conflict
      - id: check-toml
      - id: check-json
      - id: detect-private-key

  - repo: https://github.com/psf/black
    rev: 24.1.1
    hooks:
      - id: black
        language_version: python3.11

  - repo: https://github.com/pycqa/isort
    rev: 5.13.2
    hooks:
      - id: isort
        args: ["--profile", "black"]

  - repo: https://github.com/astral-sh/ruff-pre-commit
    rev: v0.1.14
    hooks:
      - id: ruff
        args: ["--fix"]

  - repo: https://github.com/compilerla/conventional-pre-commit
    rev: v3.0.0
    hooks:
      - id: conventional-pre-commit
        stages: [commit-msg]
````

## File: src/agentready/templates/bootstrap/PULL_REQUEST_TEMPLATE.md.j2
````
## Description

<!-- Provide a brief description of the changes in this PR -->

## Type of Change

- [ ] Bug fix (non-breaking change which fixes an issue)
- [ ] New feature (non-breaking change which adds functionality)
- [ ] Breaking change (fix or feature that would cause existing functionality to not work as expected)
- [ ] Documentation update
- [ ] Refactoring (no functional changes)
- [ ] Performance improvement
- [ ] Test coverage improvement

## Related Issues

<!-- Link related issues here using #issue_number -->

Fixes #
Relates to #

## Changes Made

<!-- Detailed list of changes made in this PR -->

-
-
-

## Testing

<!-- Describe the tests you ran to verify your changes -->

- [ ] Unit tests pass (`pytest`)
- [ ] Integration tests pass
- [ ] Manual testing performed
- [ ] No new warnings or errors

## Checklist

- [ ] My code follows the project's code style
- [ ] I have performed a self-review of my own code
- [ ] I have commented my code, particularly in hard-to-understand areas
- [ ] I have made corresponding changes to the documentation
- [ ] My changes generate no new warnings
- [ ] I have added tests that prove my fix is effective or that my feature works
- [ ] New and existing unit tests pass locally with my changes
- [ ] Any dependent changes have been merged and published

## Screenshots (if applicable)

<!-- Add screenshots to help explain your changes -->

## Additional Notes

<!-- Any additional information that reviewers should know -->
````

## File: tests/integration/__init__.py
````python
"""Integration tests for end-to-end workflows."""
````

## File: tests/integration/test_scan_workflow.py
````python
"""Integration tests for complete scan workflow."""

from pathlib import Path

import pytest

from agentready.assessors.documentation import CLAUDEmdAssessor, READMEAssessor
from agentready.reporters.html import HTMLReporter
from agentready.reporters.markdown import MarkdownReporter
from agentready.services.scanner import Scanner


class TestScanWorkflow:
    """Test end-to-end scanning workflow."""

    def test_scan_current_repository(self):
        """Test scanning the agentready repository itself."""
        repo_path = Path(__file__).parent.parent.parent

        # Create scanner
        scanner = Scanner(repo_path, config=None)

        # Use minimal assessors for faster test
        assessors = [CLAUDEmdAssessor(), READMEAssessor()]

        # Run scan
        assessment = scanner.scan(assessors, verbose=False)

        # Verify assessment structure
        assert assessment.repository.name == "agentready"
        assert assessment.overall_score >= 0.0
        assert assessment.overall_score <= 100.0
        assert assessment.attributes_total == len(assessors)
        assert assessment.certification_level in [
            "Platinum",
            "Gold",
            "Silver",
            "Bronze",
            "Needs Improvement",
        ]

    def test_html_report_generation(self, tmp_path):
        """Test HTML report generation."""
        repo_path = Path(__file__).parent.parent.parent

        scanner = Scanner(repo_path, config=None)
        assessors = [CLAUDEmdAssessor()]
        assessment = scanner.scan(assessors, verbose=False)

        # Generate HTML report
        reporter = HTMLReporter()
        output_file = tmp_path / "test_report.html"
        result = reporter.generate(assessment, output_file)

        # Verify file was created
        assert result.exists()
        assert result.stat().st_size > 0

        # Verify HTML content
        with open(result, "r") as f:
            content = f.read()
            assert "AgentReady Assessment Report" in content
            assert assessment.repository.name in content

    def test_markdown_report_generation(self, tmp_path):
        """Test Markdown report generation."""
        repo_path = Path(__file__).parent.parent.parent

        scanner = Scanner(repo_path, config=None)
        assessors = [CLAUDEmdAssessor()]
        assessment = scanner.scan(assessors, verbose=False)

        # Generate Markdown report
        reporter = MarkdownReporter()
        output_file = tmp_path / "test_report.md"
        result = reporter.generate(assessment, output_file)

        # Verify file was created
        assert result.exists()
        assert result.stat().st_size > 0

        # Verify Markdown content
        with open(result, "r") as f:
            content = f.read()
            assert "# ü§ñ AgentReady Assessment Report" in content
            assert "## üìä Summary" in content
            assert assessment.repository.name in content
````

## File: tests/unit/__init__.py
````python
"""Unit tests for individual components."""
````

## File: tests/unit/test_assessors_structure.py
````python
"""Tests for structure assessors."""

from pathlib import Path

import pytest

from agentready.assessors.structure import StandardLayoutAssessor
from agentready.models.repository import Repository


class TestStandardLayoutAssessor:
    """Test StandardLayoutAssessor."""

    def test_recognizes_tests_directory(self, tmp_path):
        """Test that assessor recognizes tests/ directory."""
        # Create repository with tests/ directory
        git_dir = tmp_path / ".git"
        git_dir.mkdir()
        (tmp_path / "src").mkdir()
        (tmp_path / "tests").mkdir()

        repo = Repository(
            path=tmp_path,
            name="test-repo",
            url=None,
            branch="main",
            commit_hash="abc123",
            languages={"Python": 100},
            total_files=10,
            total_lines=100,
        )

        assessor = StandardLayoutAssessor()
        finding = assessor.assess(repo)

        assert finding.status == "pass"
        assert finding.score == 100.0
        assert "2/2" in finding.measured_value

    def test_recognizes_test_directory(self, tmp_path):
        """Test that assessor recognizes test/ directory (not just tests/)."""
        # Create repository with test/ directory only
        git_dir = tmp_path / ".git"
        git_dir.mkdir()
        (tmp_path / "src").mkdir()
        (tmp_path / "test").mkdir()  # Note: singular 'test', not 'tests'

        repo = Repository(
            path=tmp_path,
            name="test-repo",
            url=None,
            branch="main",
            commit_hash="abc123",
            languages={"JavaScript": 100},
            total_files=10,
            total_lines=100,
        )

        assessor = StandardLayoutAssessor()
        finding = assessor.assess(repo)

        # Should pass - test/ is valid
        assert finding.status == "pass"
        assert finding.score == 100.0
        assert "2/2" in finding.measured_value

    def test_fails_without_standard_directories(self, tmp_path):
        """Test that assessor fails when standard directories missing."""
        # Create repository with no standard directories
        git_dir = tmp_path / ".git"
        git_dir.mkdir()
        (tmp_path / "lib").mkdir()  # Non-standard directory

        repo = Repository(
            path=tmp_path,
            name="test-repo",
            url=None,
            branch="main",
            commit_hash="abc123",
            languages={"Python": 100},
            total_files=10,
            total_lines=100,
        )

        assessor = StandardLayoutAssessor()
        finding = assessor.assess(repo)

        assert finding.status == "fail"
        assert finding.score == 0.0
        assert finding.remediation is not None

    def test_partial_score_with_only_src(self, tmp_path):
        """Test partial score when only src/ exists."""
        # Create repository with only src/
        git_dir = tmp_path / ".git"
        git_dir.mkdir()
        (tmp_path / "src").mkdir()

        repo = Repository(
            path=tmp_path,
            name="test-repo",
            url=None,
            branch="main",
            commit_hash="abc123",
            languages={"Python": 100},
            total_files=10,
            total_lines=100,
        )

        assessor = StandardLayoutAssessor()
        finding = assessor.assess(repo)

        # Should fail but have partial score
        assert finding.status == "fail"  # Less than 75%
        assert 0.0 < finding.score < 100.0
        assert "1/2" in finding.measured_value

    def test_evidence_shows_both_test_variants(self, tmp_path):
        """Test that evidence shows check for both tests/ and test/."""
        git_dir = tmp_path / ".git"
        git_dir.mkdir()
        (tmp_path / "src").mkdir()
        (tmp_path / "test").mkdir()

        repo = Repository(
            path=tmp_path,
            name="test-repo",
            url=None,
            branch="main",
            commit_hash="abc123",
            languages={"Python": 100},
            total_files=10,
            total_lines=100,
        )

        assessor = StandardLayoutAssessor()
        finding = assessor.assess(repo)

        # Evidence should mention tests/
        evidence_str = " ".join(finding.evidence)
        assert "tests/" in evidence_str or "test/" in evidence_str
        assert "‚úì" in evidence_str  # Should show checkmark for test dir
````

## File: tests/unit/test_models.py
````python
"""Unit tests for data models."""

from datetime import datetime
from pathlib import Path

import pytest

from agentready.models.assessment import Assessment
from agentready.models.attribute import Attribute
from agentready.models.config import Config
from agentready.models.finding import Citation, Finding, Remediation
from agentready.models.repository import Repository


class TestRepository:
    """Test Repository model."""

    def test_repository_creation(self, tmp_path):
        """Test creating a valid repository."""
        # Create a fake git repo
        git_dir = tmp_path / ".git"
        git_dir.mkdir()

        repo = Repository(
            path=tmp_path,
            name="test-repo",
            url="https://github.com/test/repo",
            branch="main",
            commit_hash="abc123",
            languages={"Python": 10},
            total_files=15,
            total_lines=500,
        )

        assert repo.name == "test-repo"
        assert repo.total_files == 15
        assert "Python" in repo.languages

    def test_repository_invalid_path(self):
        """Test repository with invalid path."""
        with pytest.raises(ValueError, match="does not exist"):
            Repository(
                path=Path("/nonexistent"),
                name="test",
                url=None,
                branch="main",
                commit_hash="abc",
                languages={},
                total_files=0,
                total_lines=0,
            )

    def test_repository_to_dict(self, tmp_path):
        """Test repository serialization."""
        git_dir = tmp_path / ".git"
        git_dir.mkdir()

        repo = Repository(
            path=tmp_path,
            name="test",
            url=None,
            branch="main",
            commit_hash="abc123",
            languages={"Python": 5},
            total_files=10,
            total_lines=200,
        )

        data = repo.to_dict()
        assert data["name"] == "test"
        assert data["languages"] == {"Python": 5}


class TestAttribute:
    """Test Attribute model."""

    def test_attribute_creation(self):
        """Test creating a valid attribute."""
        attr = Attribute(
            id="test_attr",
            name="Test Attribute",
            category="Testing",
            tier=1,
            description="A test attribute",
            criteria="Must pass test",
            default_weight=0.10,
        )

        assert attr.id == "test_attr"
        assert attr.tier == 1
        assert attr.default_weight == 0.10

    def test_attribute_invalid_tier(self):
        """Test attribute with invalid tier."""
        with pytest.raises(ValueError, match="Tier must be"):
            Attribute(
                id="test",
                name="Test",
                category="Test",
                tier=5,  # Invalid
                description="Test",
                criteria="Test",
                default_weight=0.10,
            )

    def test_attribute_invalid_weight(self):
        """Test attribute with invalid weight."""
        with pytest.raises(ValueError, match="weight must be"):
            Attribute(
                id="test",
                name="Test",
                category="Test",
                tier=1,
                description="Test",
                criteria="Test",
                default_weight=1.5,  # Invalid
            )


class TestFinding:
    """Test Finding model."""

    def test_finding_pass(self):
        """Test creating a passing finding."""
        attr = Attribute(
            id="test",
            name="Test",
            category="Test",
            tier=1,
            description="Test",
            criteria="Test",
            default_weight=0.10,
        )

        finding = Finding(
            attribute=attr,
            status="pass",
            score=100.0,
            measured_value="present",
            threshold="present",
            evidence=["File found"],
            remediation=None,
            error_message=None,
        )

        assert finding.status == "pass"
        assert finding.score == 100.0

    def test_finding_fail_with_remediation(self):
        """Test creating a failing finding with remediation."""
        attr = Attribute(
            id="test",
            name="Test",
            category="Test",
            tier=1,
            description="Test",
            criteria="Test",
            default_weight=0.10,
        )

        remediation = Remediation(
            summary="Fix the issue",
            steps=["Step 1", "Step 2"],
            tools=["tool1"],
            commands=["command1"],
            examples=["example1"],
            citations=[],
        )

        finding = Finding(
            attribute=attr,
            status="fail",
            score=0.0,
            measured_value="missing",
            threshold="present",
            evidence=["File not found"],
            remediation=remediation,
            error_message=None,
        )

        assert finding.status == "fail"
        assert finding.remediation is not None
        assert len(finding.remediation.steps) == 2

    def test_finding_invalid_status(self):
        """Test finding with invalid status."""
        attr = Attribute(
            id="test",
            name="Test",
            category="Test",
            tier=1,
            description="Test",
            criteria="Test",
            default_weight=0.10,
        )

        with pytest.raises(ValueError, match="Status must be"):
            Finding(
                attribute=attr,
                status="invalid",  # Invalid status
                score=50.0,
                measured_value="test",
                threshold="test",
                evidence=[],
                remediation=None,
                error_message=None,
            )


class TestConfig:
    """Test Config model."""

    def test_config_creation(self):
        """Test creating a valid config."""
        config = Config(
            weights={"attr1": 0.5, "attr2": 0.5},
            excluded_attributes=[],
            language_overrides={},
            output_dir=None,
        )

        assert len(config.weights) == 2
        assert config.get_weight("attr1", 0.0) == 0.5

    def test_config_invalid_weights_sum(self):
        """Test config with weights that don't sum to 1.0."""
        with pytest.raises(ValueError, match="sum to 1.0"):
            Config(
                weights={"attr1": 0.3, "attr2": 0.3},  # Only sums to 0.6
                excluded_attributes=[],
                language_overrides={},
                output_dir=None,
            )

    def test_config_is_excluded(self):
        """Test excluded attribute check."""
        config = Config(
            weights={},
            excluded_attributes=["attr1"],
            language_overrides={},
            output_dir=None,
        )

        assert config.is_excluded("attr1")
        assert not config.is_excluded("attr2")


class TestAssessment:
    """Test Assessment model."""

    def test_assessment_creation(self, tmp_path):
        """Test creating a valid assessment."""
        git_dir = tmp_path / ".git"
        git_dir.mkdir()

        repo = Repository(
            path=tmp_path,
            name="test",
            url=None,
            branch="main",
            commit_hash="abc",
            languages={},
            total_files=10,
            total_lines=100,
        )

        # Create dummy findings to match total
        attr = Attribute(
            id="test",
            name="Test",
            category="Test",
            tier=1,
            description="Test",
            criteria="Test",
            default_weight=0.04,
        )
        findings = [
            Finding(
                attribute=attr,
                status="pass",
                score=100.0,
                measured_value="test",
                threshold="test",
                evidence=[],
                remediation=None,
                error_message=None,
            )
            for _ in range(25)
        ]

        assessment = Assessment(
            repository=repo,
            timestamp=datetime.now(),
            overall_score=75.0,
            certification_level="Gold",
            attributes_assessed=20,
            attributes_skipped=5,
            attributes_total=25,
            findings=findings,
            config=None,
            duration_seconds=1.5,
        )

        assert assessment.overall_score == 75.0
        assert assessment.certification_level == "Gold"

    def test_assessment_determine_certification(self):
        """Test certification level determination."""
        assert Assessment.determine_certification_level(95.0) == "Platinum"
        assert Assessment.determine_certification_level(80.0) == "Gold"
        assert Assessment.determine_certification_level(65.0) == "Silver"
        assert Assessment.determine_certification_level(45.0) == "Bronze"
        assert Assessment.determine_certification_level(20.0) == "Needs Improvement"
````

## File: tests/unit/test_security.py
````python
"""Security tests for AgentReady."""

from datetime import datetime
from pathlib import Path

import pytest

from agentready.models.assessment import Assessment
from agentready.models.attribute import Attribute
from agentready.models.finding import Finding
from agentready.models.repository import Repository
from agentready.reporters.html import HTMLReporter


class TestXSSPrevention:
    """Test XSS prevention in HTML reports."""

    def test_malicious_repository_name_escaped(self, tmp_path):
        """Test that malicious repository names are escaped in HTML."""
        # Create repository with XSS payload in name
        git_dir = tmp_path / ".git"
        git_dir.mkdir()

        repo = Repository(
            path=tmp_path,
            name="<script>alert('xss')</script>",
            url="https://evil.com/<script>alert('xss')</script>",
            branch="main",
            commit_hash="abc123",
            languages={"Python": 100},
            total_files=10,
            total_lines=100,
        )

        # Create minimal assessment
        attr = Attribute(
            id="test",
            name="Test",
            category="Test",
            tier=1,
            description="Test",
            criteria="Test",
            default_weight=1.0,
        )

        finding = Finding(
            attribute=attr,
            status="pass",
            score=100.0,
            measured_value="test",
            threshold="test",
            evidence=["<script>alert('evidence')</script>"],
            remediation=None,
            error_message=None,
        )

        assessment = Assessment(
            repository=repo,
            timestamp=datetime.now(),
            overall_score=100.0,
            certification_level="Platinum",
            attributes_assessed=1,
            attributes_skipped=0,
            attributes_total=1,
            findings=[finding],
            config=None,
            duration_seconds=1.0,
        )

        # Generate HTML report
        reporter = HTMLReporter()
        output_file = tmp_path / "report.html"
        result = reporter.generate(assessment, output_file)

        # Read generated HTML
        html_content = result.read_text()

        # Verify XSS payloads are escaped in JavaScript context
        # The JSON.parse() line should have Unicode escapes
        assert (
            "\\u003cscript\\u003e" in html_content
            or "\u003cscript\u003e" in html_content
        )

        # Verify HTML contexts are escaped (title, body text)
        # Jinja2 autoescape should convert < to &lt; in HTML context
        title_section = html_content[
            html_content.find("<title>") : html_content.find("</title>")
        ]
        assert (
            "&lt;script&gt;" in title_section
            or "<script>alert('xss')</script>" not in title_section
        )

        # Most importantly: verify JavaScript execution context is safe
        script_section = html_content[
            html_content.find("<script>") : html_content.find("</script>")
        ]
        # JSON.parse() should have Unicode-escaped the malicious content
        assert "JSON.parse(" in script_section
        assert "\\u003c" in script_section  # Unicode escape for '<'

    def test_malicious_commit_message_escaped(self, tmp_path):
        """Test that malicious commit messages are escaped."""
        git_dir = tmp_path / ".git"
        git_dir.mkdir()

        repo = Repository(
            path=tmp_path,
            name="safe-repo",
            url=None,
            branch="<img src=x onerror=alert(1)>",
            commit_hash="<script>alert(1)</script>",
            languages={"Python": 100},
            total_files=10,
            total_lines=100,
        )

        attr = Attribute(
            id="test",
            name="Test",
            category="Test",
            tier=1,
            description="Test",
            criteria="Test",
            default_weight=1.0,
        )

        finding = Finding(
            attribute=attr,
            status="fail",
            score=0.0,
            measured_value="test",
            threshold="test",
            evidence=[],
            remediation=None,
            error_message="<script>alert('error')</script>",
        )

        assessment = Assessment(
            repository=repo,
            timestamp=datetime.now(),
            overall_score=0.0,
            certification_level="Needs Improvement",
            attributes_assessed=1,
            attributes_skipped=0,
            attributes_total=1,
            findings=[finding],
            config=None,
            duration_seconds=1.0,
        )

        reporter = HTMLReporter()
        output_file = tmp_path / "report.html"
        result = reporter.generate(assessment, output_file)

        html_content = result.read_text()

        # Verify JavaScript context is safe (JSON.parse with Unicode escapes)
        script_section = html_content[
            html_content.find("<script>") : html_content.find("</script>")
        ]
        assert "JSON.parse(" in script_section
        assert "\\u003c" in script_section  # '<' should be Unicode-escaped

        # Verify no direct script execution
        # The malicious payloads should be escaped
        assert "<img src=x onerror=alert(1)>" not in html_content
        # Script tags in data should be escaped
        assert (
            "\\u003cscript\\u003e" in html_content
            or "\u003cscript\u003e" in html_content
        )
````

## File: tests/__init__.py
````python
"""Test suite for agentready."""
````

## File: .agentready-config.example.yaml
````yaml
# AgentReady Configuration Example
#
# This file demonstrates how to customize the assessment behavior.
# Copy to .agentready-config.yaml in your repository root to use.
#
# All fields are optional - missing values use defaults from src/agentready/data/default-weights.yaml

# Custom attribute weights (must sum to 1.0)
# Tier 1 (Essential) attributes default to 10% each
# Tier 2 (Critical) attributes default to 3% each
# Tier 3 (Important) attributes default to 3% each
# Tier 4 (Advanced) attributes default to 1% each
weights:
  # Tier 1 (Essential) - 50% total
  claude_md_file: 0.10              # CLAUDE.md configuration files
  readme_structure: 0.10            # README structure and content
  type_annotations: 0.10            # Type hints in code
  standard_layout: 0.10             # Standard project layout
  lock_files: 0.10                  # Dependency lock files

  # Tier 2 (Critical) - 30% total
  test_coverage: 0.03               # Test coverage requirements
  precommit_hooks: 0.03             # Pre-commit hooks & linting
  conventional_commits: 0.03        # Conventional commit messages
  gitignore_completeness: 0.03      # Comprehensive .gitignore
  one_command_setup: 0.03           # One-command build/setup
  concise_documentation: 0.03       # Concise structured docs
  inline_documentation: 0.03        # Inline code documentation
  file_size_limits: 0.03            # File size constraints
  dependency_freshness: 0.03        # Dependency updates & security
  separation_concerns: 0.03         # Separation of concerns

  # Tier 3 (Important) - 15% total
  cyclomatic_complexity: 0.03       # Complexity thresholds
  structured_logging: 0.03          # Structured logging
  openapi_specs: 0.03               # OpenAPI/Swagger specs
  architecture_decisions: 0.03      # Architecture Decision Records
  semantic_naming: 0.03             # Semantic file naming

  # Tier 4 (Advanced) - 5% total
  security_scanning: 0.01           # Security scanning automation
  performance_benchmarks: 0.01      # Performance benchmarks
  code_smells: 0.01                 # Code smell elimination
  issue_pr_templates: 0.01          # GitHub templates
  container_setup: 0.01             # Container/virtualization

# Exclude specific attributes from assessment
excluded_attributes: []
  # - performance_benchmarks       # Skip if no benchmarks needed
  # - container_setup              # Skip if no Docker/containers

# Force language detection (override automatic detection)
language_overrides: {}
  # Python: ["*.pyx", "*.pyd"]     # Treat Cython files as Python
  # JavaScript: ["*.jsx"]          # Treat JSX as JavaScript

# Custom output directory (default: .agentready/)
# output_dir: ./custom-reports

# Example: Increase weight for CLAUDE.md and tests
# This increases CLAUDE.md from 10% to 15% and test_coverage from 3% to 5%
# Other attributes are automatically rescaled to maintain sum of 1.0
#
# weights:
#   claude_md_file: 0.15
#   test_coverage: 0.05
````

## File: .gitignore
````
# Python
__pycache__/
*.py[cod]
*$py.class
*.so
.Python
build/
develop-eggs/
dist/
downloads/
eggs/
.eggs/
lib/
lib64/
parts/
sdist/
var/
wheels/
*.egg-info/
.installed.cfg
*.egg
MANIFEST

# Virtual environments
.venv/
venv/
ENV/
env/
.virtualenv/

# Testing
.pytest_cache/
.coverage
htmlcov/
*.cover
.hypothesis/
.tox/
nosetests.xml
coverage.xml
*.coveragerc

# IDEs
.vscode/
.idea/
*.swp
*.swo
*~
.DS_Store

# Project specific
.agentready/
*.log
*.tmp

# Build artifacts
*.whl
*.tar.gz

# Configuration (exclude example)
.agentready-config.yaml
!.agentready-config.example.yaml
````

## File: .markdown-link-check.json
````json
{
  "ignorePatterns": [
    {
      "pattern": "^http://localhost"
    },
    {
      "pattern": "^https://github.com/.*/issues/new"
    },
    {
      "pattern": "\\.html"
    },
    {
      "pattern": "^https://github.com/ambient-code/agentready/discussions"
    },
    {
      "pattern": "^https://github.com/ambient-code/agentready/blob/main/LICENSE"
    }
  ],
  "timeout": "20s",
  "retryOn429": true,
  "retryCount": 3,
  "fallbackRetryDelay": "30s",
  "aliveStatusCodes": [200, 206, 301, 302, 308]
}
````

## File: .pre-commit-config.yaml
````yaml
repos:
  - repo: https://github.com/pre-commit/pre-commit-hooks
    rev: v4.5.0
    hooks:
      - id: trailing-whitespace
      - id: end-of-file-fixer
      - id: check-yaml
      - id: check-added-large-files
      - id: check-merge-conflict
      - id: check-toml
      - id: check-json
      - id: detect-private-key

  - repo: https://github.com/psf/black
    rev: 24.1.1
    hooks:
      - id: black
        language_version: python3.11

  - repo: https://github.com/pycqa/isort
    rev: 5.13.2
    hooks:
      - id: isort
        args: ["--profile", "black"]

  - repo: https://github.com/astral-sh/ruff-pre-commit
    rev: v0.1.14
    hooks:
      - id: ruff
        args: ["--fix"]

  - repo: https://github.com/compilerla/conventional-pre-commit
    rev: v3.0.0
    hooks:
      - id: conventional-pre-commit
        stages: [commit-msg]
````

## File: .releaserc.json
````json
{
  "branches": ["main"],
  "plugins": [
    "@semantic-release/commit-analyzer",
    "@semantic-release/release-notes-generator",
    [
      "@semantic-release/changelog",
      {
        "changelogFile": "CHANGELOG.md"
      }
    ],
    [
      "@semantic-release/exec",
      {
        "prepareCmd": "sed -i 's/version = \".*\"/version = \"${nextRelease.version}\"/' pyproject.toml"
      }
    ],
    [
      "@semantic-release/github",
      {
        "assets": [
          {
            "path": "dist/**",
            "label": "Distribution files"
          }
        ]
      }
    ],
    [
      "@semantic-release/git",
      {
        "assets": ["CHANGELOG.md", "pyproject.toml"],
        "message": "chore(release): ${nextRelease.version} [skip ci]\n\n${nextRelease.notes}"
      }
    ]
  ]
}
````

## File: CODE_OF_CONDUCT.md
````markdown
# Code of Conduct

## Our Commitment

We are committed to providing a welcoming and inclusive environment for all contributors.

## Standards

Examples of behavior that contributes to a positive environment:
- Using welcoming and inclusive language
- Being respectful of differing viewpoints
- Accepting constructive criticism gracefully
- Focusing on what is best for the community

Examples of unacceptable behavior:
- Trolling or insulting comments
- Public or private harassment
- Publishing others' private information
- Other conduct which could reasonably be considered inappropriate

## Responsibilities

Project maintainers are responsible for clarifying standards of acceptable behavior and will take appropriate action in response to unacceptable behavior.

## Scope

This Code of Conduct applies within all project spaces and when representing the project in public spaces.

## Enforcement

Instances of abusive, harassing, or otherwise unacceptable behavior may be reported to the project team. All complaints will be reviewed and investigated promptly and fairly.

## Attribution

This Code of Conduct is adapted from common open source community standards.
````

## File: CONTRIBUTING.md
````markdown
# Contributing Guidelines

Thank you for your interest in contributing to this project! This document provides guidelines and instructions for contributing.

## Code of Conduct

Please read and follow our [Code of Conduct](CODE_OF_CONDUCT.md).

## Getting Started

### Prerequisites


### Development Setup

1. Fork the repository
2. Clone your fork:
   ```bash
   git clone https://github.com/YOUR_USERNAME/PROJECT_NAME.git
   cd PROJECT_NAME
   ```

3. Set up your development environment:

4. Install pre-commit hooks:
   ```bash
   pre-commit install
   ```

## Development Workflow

### Creating a Feature Branch

Always create a new branch for your work:

```bash
git checkout -b feature/your-feature-name
```

Use descriptive branch names:
- `feature/add-new-api`
- `fix/resolve-crash-on-startup`
- `docs/improve-installation-guide`

### Making Changes

1. Write clear, concise code
2. Follow the project's coding standards
3. Add tests for new functionality
4. Update documentation as needed
5. Run linters and tests before committing

### Running Tests


### Code Style


### Commit Messages

We use [Conventional Commits](https://www.conventionalcommits.org/):

```
<type>(<scope>): <subject>

<body>

<footer>
```

Types:
- `feat`: New feature
- `fix`: Bug fix
- `docs`: Documentation changes
- `style`: Code style changes (formatting, etc.)
- `refactor`: Code refactoring
- `test`: Adding or updating tests
- `chore`: Maintenance tasks

Examples:
```
feat(api): add user authentication endpoint

fix(parser): resolve crash when parsing empty files

docs(readme): update installation instructions
```

### Pull Request Process

1. **Update your branch** with the latest changes:
   ```bash
   git fetch upstream
   git rebase upstream/main
   ```

2. **Push your changes**:
   ```bash
   git push origin feature/your-feature-name
   ```

3. **Create a Pull Request**:
   - Use the PR template
   - Provide a clear description
   - Link related issues
   - Request reviews from maintainers

4. **Address feedback**:
   - Respond to review comments
   - Make requested changes
   - Push updates to your branch

5. **Wait for approval**:
   - At least one maintainer approval required
   - All CI checks must pass
   - No merge conflicts

## Testing Guidelines

- Write tests for all new functionality
- Maintain or improve code coverage
- Include both positive and negative test cases
- Test edge cases and error conditions

## Documentation

- Update README.md for user-facing changes
- Add docstrings to new functions and classes
- Update API documentation if applicable
- Add examples for new features

## Issue Reporting

When reporting issues, please include:

- Clear, descriptive title
- Steps to reproduce
- Expected vs. actual behavior
- Environment details (OS, version, etc.)
- Error messages or logs
- Screenshots if applicable

## Questions?

If you have questions:
- Check existing issues and discussions
- Ask in pull request comments
- Open a new discussion
- Contact maintainers

## License

By contributing, you agree that your contributions will be licensed under the same license as the project.

Thank you for contributing! üéâ
````

## File: LICENSE
````
MIT License

Copyright (c) 2025 Jeremy Eder / Red Hat, Inc.

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.
````

## File: repomix-output.xml
````xml
This file is a merged representation of the entire codebase, combined into a single document by Repomix.

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files (if enabled)
5. Multiple file entries, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Files are sorted by Git change count (files with more changes are at the bottom)
</notes>

</file_summary>

<directory_structure>
.agentready/
  assessment-20251121-014936.json
  assessment-20251121-015331.json
  assessment-20251121-015727.json
  assessment-20251121-021105.json
  report-20251121-015331.html
  report-20251121-015727.html
  report-20251121-015727.md
  report-20251121-021105.html
  report-20251121-021105.md
.claude/
  commands/
    speckit.analyze.md
    speckit.checklist.md
    speckit.clarify.md
    speckit.constitution.md
    speckit.implement.md
    speckit.plan.md
    speckit.specify.md
    speckit.tasks.md
    speckit.taskstoissues.md
  settings.local.json
.specify/
  memory/
    constitution.md
  scripts/
    bash/
      check-prerequisites.sh
      common.sh
      create-new-feature.sh
      setup-plan.sh
      update-agent-context.sh
  templates/
    agent-file-template.md
    checklist-template.md
    plan-template.md
    spec-template.md
    tasks-template.md
src/
  agentready.egg-info/
    dependency_links.txt
    entry_points.txt
    PKG-INFO
    requires.txt
    SOURCES.txt
    top_level.txt
.coverage
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path=".agentready/assessment-20251121-014936.json">
{
  "repository": {
    "path": "/Users/jeder/repos/sk/agentready",
    "name": "agentready",
    "url": null,
    "branch": "001-agentready-scorer",
    "commit_hash": "bcc724b294fdc87a9315a7bb51cf7f88dcdeb940",
    "languages": {
      "Markdown": 15,
      "Shell": 5
    },
    "total_files": 20,
    "total_lines": 2800
  },
  "timestamp": "2025-11-21T01:49:36.916179",
  "overall_score": 67.3,
  "certification_level": "Silver",
  "attributes_assessed": 7,
  "attributes_skipped": 18,
  "attributes_total": 25,
  "findings": [
    {
      "attribute": {
        "id": "claude_md_file",
        "name": "CLAUDE.md Configuration Files",
        "category": "Context Window Optimization",
        "tier": 1,
        "description": "Project-specific configuration for Claude Code",
        "criteria": "CLAUDE.md file exists in repository root",
        "default_weight": 0.1
      },
      "status": "pass",
      "score": 100.0,
      "measured_value": "present",
      "threshold": "present",
      "evidence": [
        "CLAUDE.md found at /Users/jeder/repos/sk/agentready/CLAUDE.md"
      ],
      "remediation": null,
      "error_message": null
    },
    {
      "attribute": {
        "id": "readme_structure",
        "name": "README Structure",
        "category": "Documentation Standards",
        "tier": 1,
        "description": "Well-structured README with key sections",
        "criteria": "README.md with installation, usage, and development sections",
        "default_weight": 0.1
      },
      "status": "pass",
      "score": 100.0,
      "measured_value": "3/3 sections",
      "threshold": "3/3 sections",
      "evidence": [
        "Found 3/3 essential sections",
        "Installation: \u2713",
        "Usage: \u2713",
        "Development: \u2713"
      ],
      "remediation": null,
      "error_message": null
    },
    {
      "attribute": {
        "id": "type_annotations",
        "name": "Type Annotations",
        "category": "Code Quality",
        "tier": 1,
        "description": "Type hints in function signatures",
        "criteria": ">80% of functions have type annotations",
        "default_weight": 0.1
      },
      "status": "not_applicable",
      "score": null,
      "measured_value": null,
      "threshold": null,
      "evidence": [
        "Not applicable to ['Markdown', 'Shell']"
      ],
      "remediation": null,
      "error_message": null
    },
    {
      "attribute": {
        "id": "standard_layout",
        "name": "Standard Project Layouts",
        "category": "Repository Structure",
        "tier": 1,
        "description": "Follows standard project structure for language",
        "criteria": "Standard directories (src/, tests/, docs/) present",
        "default_weight": 0.1
      },
      "status": "pass",
      "score": 100.0,
      "measured_value": "2/2 directories",
      "threshold": "2/2 directories",
      "evidence": [
        "Found 2/2 standard directories",
        "src/: \u2713",
        "tests/: \u2713"
      ],
      "remediation": null,
      "error_message": null
    },
    {
      "attribute": {
        "id": "lock_files",
        "name": "Lock Files for Reproducibility",
        "category": "Dependency Management",
        "tier": 1,
        "description": "Lock files present for dependency pinning",
        "criteria": "package-lock.json, yarn.lock, poetry.lock, or requirements.txt with versions",
        "default_weight": 0.1
      },
      "status": "fail",
      "score": 0.0,
      "measured_value": "none",
      "threshold": "at least one lock file",
      "evidence": [
        "No lock files found"
      ],
      "remediation": {
        "summary": "Add lock file for dependency reproducibility",
        "steps": [
          "Use npm install, poetry lock, or equivalent to generate lock file"
        ],
        "tools": [],
        "commands": [
          "npm install  # generates package-lock.json"
        ],
        "examples": [],
        "citations": []
      },
      "error_message": null
    },
    {
      "attribute": {
        "id": "test_coverage",
        "name": "Test Coverage Requirements",
        "category": "Testing & CI/CD",
        "tier": 2,
        "description": "Test coverage thresholds configured and enforced",
        "criteria": ">80% code coverage",
        "default_weight": 0.03
      },
      "status": "not_applicable",
      "score": null,
      "measured_value": null,
      "threshold": null,
      "evidence": [
        "Coverage check not implemented for ['Markdown', 'Shell']"
      ],
      "remediation": null,
      "error_message": null
    },
    {
      "attribute": {
        "id": "precommit_hooks",
        "name": "Pre-commit Hooks & CI/CD Linting",
        "category": "Testing & CI/CD",
        "tier": 2,
        "description": "Pre-commit hooks configured for linting and formatting",
        "criteria": ".pre-commit-config.yaml exists",
        "default_weight": 0.03
      },
      "status": "fail",
      "score": 0.0,
      "measured_value": "not configured",
      "threshold": "configured",
      "evidence": [
        ".pre-commit-config.yaml not found"
      ],
      "remediation": {
        "summary": "Configure pre-commit hooks for automated code quality checks",
        "steps": [
          "Install pre-commit framework",
          "Create .pre-commit-config.yaml",
          "Add hooks for linting and formatting",
          "Install hooks: pre-commit install",
          "Run on all files: pre-commit run --all-files"
        ],
        "tools": [
          "pre-commit"
        ],
        "commands": [
          "pip install pre-commit",
          "pre-commit install",
          "pre-commit run --all-files"
        ],
        "examples": [
          "# .pre-commit-config.yaml\nrepos:\n  - repo: https://github.com/pre-commit/pre-commit-hooks\n    rev: v4.4.0\n    hooks:\n      - id: trailing-whitespace\n      - id: end-of-file-fixer\n      - id: check-yaml\n      - id: check-added-large-files\n\n  - repo: https://github.com/psf/black\n    rev: 23.3.0\n    hooks:\n      - id: black\n\n  - repo: https://github.com/pycqa/isort\n    rev: 5.12.0\n    hooks:\n      - id: isort\n"
        ],
        "citations": [
          {
            "source": "pre-commit.com",
            "title": "Pre-commit Framework",
            "url": "https://pre-commit.com/",
            "relevance": "Official pre-commit documentation"
          }
        ]
      },
      "error_message": null
    },
    {
      "attribute": {
        "id": "conventional_commits",
        "name": "Conventional Commit Messages",
        "category": "Git & Version Control",
        "tier": 2,
        "description": "Follows conventional commit format",
        "criteria": "\u226580% of recent commits follow convention",
        "default_weight": 0.03
      },
      "status": "fail",
      "score": 0.0,
      "measured_value": "not configured",
      "threshold": "configured",
      "evidence": [
        "No commitlint or husky configuration"
      ],
      "remediation": {
        "summary": "Configure conventional commits with commitlint",
        "steps": [
          "Install commitlint",
          "Configure husky for commit-msg hook"
        ],
        "tools": [
          "commitlint",
          "husky"
        ],
        "commands": [
          "npm install --save-dev @commitlint/cli @commitlint/config-conventional husky"
        ],
        "examples": [],
        "citations": []
      },
      "error_message": null
    },
    {
      "attribute": {
        "id": "gitignore_completeness",
        "name": ".gitignore Completeness",
        "category": "Git & Version Control",
        "tier": 2,
        "description": "Comprehensive .gitignore file",
        "criteria": ".gitignore exists and covers common patterns",
        "default_weight": 0.03
      },
      "status": "pass",
      "score": 100.0,
      "measured_value": "580 bytes",
      "threshold": ">50 bytes",
      "evidence": [
        ".gitignore found (580 bytes)"
      ],
      "remediation": null,
      "error_message": null
    },
    {
      "attribute": {
        "id": "cyclomatic_complexity",
        "name": "Cyclomatic Complexity Thresholds",
        "category": "Code Quality",
        "tier": 3,
        "description": "Cyclomatic complexity thresholds enforced",
        "criteria": "Average complexity <10, no functions >15",
        "default_weight": 0.03
      },
      "status": "not_applicable",
      "score": null,
      "measured_value": null,
      "threshold": null,
      "evidence": [
        "Not applicable to ['Markdown', 'Shell']"
      ],
      "remediation": null,
      "error_message": null
    },
    {
      "attribute": {
        "id": "one_command_setup",
        "name": "One-Command Build/Setup",
        "category": "Build & Development",
        "tier": 2,
        "description": "Assessment for One-Command Build/Setup",
        "criteria": "To be implemented",
        "default_weight": 0.03
      },
      "status": "not_applicable",
      "score": null,
      "measured_value": null,
      "threshold": null,
      "evidence": [
        "One-Command Build/Setup assessment not yet implemented"
      ],
      "remediation": null,
      "error_message": null
    },
    {
      "attribute": {
        "id": "concise_documentation",
        "name": "Concise Structured Documentation",
        "category": "Context Window Optimization",
        "tier": 2,
        "description": "Assessment for Concise Structured Documentation",
        "criteria": "To be implemented",
        "default_weight": 0.03
      },
      "status": "not_applicable",
      "score": null,
      "measured_value": null,
      "threshold": null,
      "evidence": [
        "Concise Structured Documentation assessment not yet implemented"
      ],
      "remediation": null,
      "error_message": null
    },
    {
      "attribute": {
        "id": "inline_documentation",
        "name": "Inline Documentation",
        "category": "Documentation Standards",
        "tier": 2,
        "description": "Assessment for Inline Documentation",
        "criteria": "To be implemented",
        "default_weight": 0.03
      },
      "status": "not_applicable",
      "score": null,
      "measured_value": null,
      "threshold": null,
      "evidence": [
        "Inline Documentation assessment not yet implemented"
      ],
      "remediation": null,
      "error_message": null
    },
    {
      "attribute": {
        "id": "file_size_limits",
        "name": "File Size Limits",
        "category": "Context Window Optimization",
        "tier": 2,
        "description": "Assessment for File Size Limits",
        "criteria": "To be implemented",
        "default_weight": 0.03
      },
      "status": "not_applicable",
      "score": null,
      "measured_value": null,
      "threshold": null,
      "evidence": [
        "File Size Limits assessment not yet implemented"
      ],
      "remediation": null,
      "error_message": null
    },
    {
      "attribute": {
        "id": "dependency_freshness",
        "name": "Dependency Freshness & Security",
        "category": "Dependency Management",
        "tier": 2,
        "description": "Assessment for Dependency Freshness & Security",
        "criteria": "To be implemented",
        "default_weight": 0.03
      },
      "status": "not_applicable",
      "score": null,
      "measured_value": null,
      "threshold": null,
      "evidence": [
        "Dependency Freshness & Security assessment not yet implemented"
      ],
      "remediation": null,
      "error_message": null
    },
    {
      "attribute": {
        "id": "separation_concerns",
        "name": "Separation of Concerns",
        "category": "Repository Structure",
        "tier": 2,
        "description": "Assessment for Separation of Concerns",
        "criteria": "To be implemented",
        "default_weight": 0.03
      },
      "status": "not_applicable",
      "score": null,
      "measured_value": null,
      "threshold": null,
      "evidence": [
        "Separation of Concerns assessment not yet implemented"
      ],
      "remediation": null,
      "error_message": null
    },
    {
      "attribute": {
        "id": "structured_logging",
        "name": "Structured Logging",
        "category": "Error Handling",
        "tier": 3,
        "description": "Assessment for Structured Logging",
        "criteria": "To be implemented",
        "default_weight": 0.03
      },
      "status": "not_applicable",
      "score": null,
      "measured_value": null,
      "threshold": null,
      "evidence": [
        "Structured Logging assessment not yet implemented"
      ],
      "remediation": null,
      "error_message": null
    },
    {
      "attribute": {
        "id": "openapi_specs",
        "name": "OpenAPI/Swagger Specifications",
        "category": "API Documentation",
        "tier": 3,
        "description": "Assessment for OpenAPI/Swagger Specifications",
        "criteria": "To be implemented",
        "default_weight": 0.03
      },
      "status": "not_applicable",
      "score": null,
      "measured_value": null,
      "threshold": null,
      "evidence": [
        "OpenAPI/Swagger Specifications assessment not yet implemented"
      ],
      "remediation": null,
      "error_message": null
    },
    {
      "attribute": {
        "id": "architecture_decisions",
        "name": "Architecture Decision Records",
        "category": "Documentation Standards",
        "tier": 3,
        "description": "Assessment for Architecture Decision Records",
        "criteria": "To be implemented",
        "default_weight": 0.03
      },
      "status": "not_applicable",
      "score": null,
      "measured_value": null,
      "threshold": null,
      "evidence": [
        "Architecture Decision Records assessment not yet implemented"
      ],
      "remediation": null,
      "error_message": null
    },
    {
      "attribute": {
        "id": "semantic_naming",
        "name": "Semantic File & Directory Naming",
        "category": "Modularity",
        "tier": 3,
        "description": "Assessment for Semantic File & Directory Naming",
        "criteria": "To be implemented",
        "default_weight": 0.03
      },
      "status": "not_applicable",
      "score": null,
      "measured_value": null,
      "threshold": null,
      "evidence": [
        "Semantic File & Directory Naming assessment not yet implemented"
      ],
      "remediation": null,
      "error_message": null
    },
    {
      "attribute": {
        "id": "security_scanning",
        "name": "Security Scanning Automation",
        "category": "Security",
        "tier": 4,
        "description": "Assessment for Security Scanning Automation",
        "criteria": "To be implemented",
        "default_weight": 0.01
      },
      "status": "not_applicable",
      "score": null,
      "measured_value": null,
      "threshold": null,
      "evidence": [
        "Security Scanning Automation assessment not yet implemented"
      ],
      "remediation": null,
      "error_message": null
    },
    {
      "attribute": {
        "id": "performance_benchmarks",
        "name": "Performance Benchmarks",
        "category": "Performance",
        "tier": 4,
        "description": "Assessment for Performance Benchmarks",
        "criteria": "To be implemented",
        "default_weight": 0.01
      },
      "status": "not_applicable",
      "score": null,
      "measured_value": null,
      "threshold": null,
      "evidence": [
        "Performance Benchmarks assessment not yet implemented"
      ],
      "remediation": null,
      "error_message": null
    },
    {
      "attribute": {
        "id": "code_smells",
        "name": "Code Smell Elimination",
        "category": "Code Quality",
        "tier": 4,
        "description": "Assessment for Code Smell Elimination",
        "criteria": "To be implemented",
        "default_weight": 0.01
      },
      "status": "not_applicable",
      "score": null,
      "measured_value": null,
      "threshold": null,
      "evidence": [
        "Code Smell Elimination assessment not yet implemented"
      ],
      "remediation": null,
      "error_message": null
    },
    {
      "attribute": {
        "id": "issue_pr_templates",
        "name": "Issue & Pull Request Templates",
        "category": "Git & Version Control",
        "tier": 4,
        "description": "Assessment for Issue & Pull Request Templates",
        "criteria": "To be implemented",
        "default_weight": 0.01
      },
      "status": "not_applicable",
      "score": null,
      "measured_value": null,
      "threshold": null,
      "evidence": [
        "Issue & Pull Request Templates assessment not yet implemented"
      ],
      "remediation": null,
      "error_message": null
    },
    {
      "attribute": {
        "id": "container_setup",
        "name": "Container/Virtualization Setup",
        "category": "Build & Development",
        "tier": 4,
        "description": "Assessment for Container/Virtualization Setup",
        "criteria": "To be implemented",
        "default_weight": 0.01
      },
      "status": "not_applicable",
      "score": null,
      "measured_value": null,
      "threshold": null,
      "evidence": [
        "Container/Virtualization Setup assessment not yet implemented"
      ],
      "remediation": null,
      "error_message": null
    }
  ],
  "config": null,
  "duration_seconds": 0.1
}
</file>

<file path=".agentready/assessment-20251121-015331.json">
{
  "repository": {
    "path": "/Users/jeder/repos/sk/agentready",
    "name": "agentready",
    "url": null,
    "branch": "001-agentready-scorer",
    "commit_hash": "54a96cbafc37dfc0f67dc12833b3c789ee81bc53",
    "languages": {
      "Markdown": 36,
      "Shell": 5,
      "Python": 23
    },
    "total_files": 70,
    "total_lines": 12380
  },
  "timestamp": "2025-11-21T01:53:31.339778",
  "overall_score": 75.4,
  "certification_level": "Gold",
  "attributes_assessed": 10,
  "attributes_skipped": 15,
  "attributes_total": 25,
  "findings": [
    {
      "attribute": {
        "id": "claude_md_file",
        "name": "CLAUDE.md Configuration Files",
        "category": "Context Window Optimization",
        "tier": 1,
        "description": "Project-specific configuration for Claude Code",
        "criteria": "CLAUDE.md file exists in repository root",
        "default_weight": 0.1
      },
      "status": "pass",
      "score": 100.0,
      "measured_value": "present",
      "threshold": "present",
      "evidence": [
        "CLAUDE.md found at /Users/jeder/repos/sk/agentready/CLAUDE.md"
      ],
      "remediation": null,
      "error_message": null
    },
    {
      "attribute": {
        "id": "readme_structure",
        "name": "README Structure",
        "category": "Documentation Standards",
        "tier": 1,
        "description": "Well-structured README with key sections",
        "criteria": "README.md with installation, usage, and development sections",
        "default_weight": 0.1
      },
      "status": "pass",
      "score": 100.0,
      "measured_value": "3/3 sections",
      "threshold": "3/3 sections",
      "evidence": [
        "Found 3/3 essential sections",
        "Installation: \u2713",
        "Usage: \u2713",
        "Development: \u2713"
      ],
      "remediation": null,
      "error_message": null
    },
    {
      "attribute": {
        "id": "type_annotations",
        "name": "Type Annotations",
        "category": "Code Quality",
        "tier": 1,
        "description": "Type hints in function signatures",
        "criteria": ">80% of functions have type annotations",
        "default_weight": 0.1
      },
      "status": "pass",
      "score": 100.0,
      "measured_value": "94.9%",
      "threshold": "\u226580%",
      "evidence": [
        "Typed functions: 112/118",
        "Coverage: 94.9%"
      ],
      "remediation": null,
      "error_message": null
    },
    {
      "attribute": {
        "id": "standard_layout",
        "name": "Standard Project Layouts",
        "category": "Repository Structure",
        "tier": 1,
        "description": "Follows standard project structure for language",
        "criteria": "Standard directories (src/, tests/, docs/) present",
        "default_weight": 0.1
      },
      "status": "pass",
      "score": 100.0,
      "measured_value": "2/2 directories",
      "threshold": "2/2 directories",
      "evidence": [
        "Found 2/2 standard directories",
        "src/: \u2713",
        "tests/: \u2713"
      ],
      "remediation": null,
      "error_message": null
    },
    {
      "attribute": {
        "id": "lock_files",
        "name": "Lock Files for Reproducibility",
        "category": "Dependency Management",
        "tier": 1,
        "description": "Lock files present for dependency pinning",
        "criteria": "package-lock.json, yarn.lock, poetry.lock, or requirements.txt with versions",
        "default_weight": 0.1
      },
      "status": "fail",
      "score": 0.0,
      "measured_value": "none",
      "threshold": "at least one lock file",
      "evidence": [
        "No lock files found"
      ],
      "remediation": {
        "summary": "Add lock file for dependency reproducibility",
        "steps": [
          "Use npm install, poetry lock, or equivalent to generate lock file"
        ],
        "tools": [],
        "commands": [
          "npm install  # generates package-lock.json"
        ],
        "examples": [],
        "citations": []
      },
      "error_message": null
    },
    {
      "attribute": {
        "id": "test_coverage",
        "name": "Test Coverage Requirements",
        "category": "Testing & CI/CD",
        "tier": 2,
        "description": "Test coverage thresholds configured and enforced",
        "criteria": ">80% code coverage",
        "default_weight": 0.03
      },
      "status": "pass",
      "score": 100.0,
      "measured_value": "configured",
      "threshold": "configured with >80% threshold",
      "evidence": [
        "Coverage configuration found",
        "pytest-cov dependency present"
      ],
      "remediation": null,
      "error_message": null
    },
    {
      "attribute": {
        "id": "precommit_hooks",
        "name": "Pre-commit Hooks & CI/CD Linting",
        "category": "Testing & CI/CD",
        "tier": 2,
        "description": "Pre-commit hooks configured for linting and formatting",
        "criteria": ".pre-commit-config.yaml exists",
        "default_weight": 0.03
      },
      "status": "fail",
      "score": 0.0,
      "measured_value": "not configured",
      "threshold": "configured",
      "evidence": [
        ".pre-commit-config.yaml not found"
      ],
      "remediation": {
        "summary": "Configure pre-commit hooks for automated code quality checks",
        "steps": [
          "Install pre-commit framework",
          "Create .pre-commit-config.yaml",
          "Add hooks for linting and formatting",
          "Install hooks: pre-commit install",
          "Run on all files: pre-commit run --all-files"
        ],
        "tools": [
          "pre-commit"
        ],
        "commands": [
          "pip install pre-commit",
          "pre-commit install",
          "pre-commit run --all-files"
        ],
        "examples": [
          "# .pre-commit-config.yaml\nrepos:\n  - repo: https://github.com/pre-commit/pre-commit-hooks\n    rev: v4.4.0\n    hooks:\n      - id: trailing-whitespace\n      - id: end-of-file-fixer\n      - id: check-yaml\n      - id: check-added-large-files\n\n  - repo: https://github.com/psf/black\n    rev: 23.3.0\n    hooks:\n      - id: black\n\n  - repo: https://github.com/pycqa/isort\n    rev: 5.12.0\n    hooks:\n      - id: isort\n"
        ],
        "citations": [
          {
            "source": "pre-commit.com",
            "title": "Pre-commit Framework",
            "url": "https://pre-commit.com/",
            "relevance": "Official pre-commit documentation"
          }
        ]
      },
      "error_message": null
    },
    {
      "attribute": {
        "id": "conventional_commits",
        "name": "Conventional Commit Messages",
        "category": "Git & Version Control",
        "tier": 2,
        "description": "Follows conventional commit format",
        "criteria": "\u226580% of recent commits follow convention",
        "default_weight": 0.03
      },
      "status": "fail",
      "score": 0.0,
      "measured_value": "not configured",
      "threshold": "configured",
      "evidence": [
        "No commitlint or husky configuration"
      ],
      "remediation": {
        "summary": "Configure conventional commits with commitlint",
        "steps": [
          "Install commitlint",
          "Configure husky for commit-msg hook"
        ],
        "tools": [
          "commitlint",
          "husky"
        ],
        "commands": [
          "npm install --save-dev @commitlint/cli @commitlint/config-conventional husky"
        ],
        "examples": [],
        "citations": []
      },
      "error_message": null
    },
    {
      "attribute": {
        "id": "gitignore_completeness",
        "name": ".gitignore Completeness",
        "category": "Git & Version Control",
        "tier": 2,
        "description": "Comprehensive .gitignore file",
        "criteria": ".gitignore exists and covers common patterns",
        "default_weight": 0.03
      },
      "status": "pass",
      "score": 100.0,
      "measured_value": "580 bytes",
      "threshold": ">50 bytes",
      "evidence": [
        ".gitignore found (580 bytes)"
      ],
      "remediation": null,
      "error_message": null
    },
    {
      "attribute": {
        "id": "cyclomatic_complexity",
        "name": "Cyclomatic Complexity Thresholds",
        "category": "Code Quality",
        "tier": 3,
        "description": "Cyclomatic complexity thresholds enforced",
        "criteria": "Average complexity <10, no functions >15",
        "default_weight": 0.03
      },
      "status": "pass",
      "score": 100.0,
      "measured_value": "3.0",
      "threshold": "<10.0",
      "evidence": [
        "Average cyclomatic complexity: 3.0"
      ],
      "remediation": null,
      "error_message": null
    },
    {
      "attribute": {
        "id": "one_command_setup",
        "name": "One-Command Build/Setup",
        "category": "Build & Development",
        "tier": 2,
        "description": "Assessment for One-Command Build/Setup",
        "criteria": "To be implemented",
        "default_weight": 0.03
      },
      "status": "not_applicable",
      "score": null,
      "measured_value": null,
      "threshold": null,
      "evidence": [
        "One-Command Build/Setup assessment not yet implemented"
      ],
      "remediation": null,
      "error_message": null
    },
    {
      "attribute": {
        "id": "concise_documentation",
        "name": "Concise Structured Documentation",
        "category": "Context Window Optimization",
        "tier": 2,
        "description": "Assessment for Concise Structured Documentation",
        "criteria": "To be implemented",
        "default_weight": 0.03
      },
      "status": "not_applicable",
      "score": null,
      "measured_value": null,
      "threshold": null,
      "evidence": [
        "Concise Structured Documentation assessment not yet implemented"
      ],
      "remediation": null,
      "error_message": null
    },
    {
      "attribute": {
        "id": "inline_documentation",
        "name": "Inline Documentation",
        "category": "Documentation Standards",
        "tier": 2,
        "description": "Assessment for Inline Documentation",
        "criteria": "To be implemented",
        "default_weight": 0.03
      },
      "status": "not_applicable",
      "score": null,
      "measured_value": null,
      "threshold": null,
      "evidence": [
        "Inline Documentation assessment not yet implemented"
      ],
      "remediation": null,
      "error_message": null
    },
    {
      "attribute": {
        "id": "file_size_limits",
        "name": "File Size Limits",
        "category": "Context Window Optimization",
        "tier": 2,
        "description": "Assessment for File Size Limits",
        "criteria": "To be implemented",
        "default_weight": 0.03
      },
      "status": "not_applicable",
      "score": null,
      "measured_value": null,
      "threshold": null,
      "evidence": [
        "File Size Limits assessment not yet implemented"
      ],
      "remediation": null,
      "error_message": null
    },
    {
      "attribute": {
        "id": "dependency_freshness",
        "name": "Dependency Freshness & Security",
        "category": "Dependency Management",
        "tier": 2,
        "description": "Assessment for Dependency Freshness & Security",
        "criteria": "To be implemented",
        "default_weight": 0.03
      },
      "status": "not_applicable",
      "score": null,
      "measured_value": null,
      "threshold": null,
      "evidence": [
        "Dependency Freshness & Security assessment not yet implemented"
      ],
      "remediation": null,
      "error_message": null
    },
    {
      "attribute": {
        "id": "separation_concerns",
        "name": "Separation of Concerns",
        "category": "Repository Structure",
        "tier": 2,
        "description": "Assessment for Separation of Concerns",
        "criteria": "To be implemented",
        "default_weight": 0.03
      },
      "status": "not_applicable",
      "score": null,
      "measured_value": null,
      "threshold": null,
      "evidence": [
        "Separation of Concerns assessment not yet implemented"
      ],
      "remediation": null,
      "error_message": null
    },
    {
      "attribute": {
        "id": "structured_logging",
        "name": "Structured Logging",
        "category": "Error Handling",
        "tier": 3,
        "description": "Assessment for Structured Logging",
        "criteria": "To be implemented",
        "default_weight": 0.03
      },
      "status": "not_applicable",
      "score": null,
      "measured_value": null,
      "threshold": null,
      "evidence": [
        "Structured Logging assessment not yet implemented"
      ],
      "remediation": null,
      "error_message": null
    },
    {
      "attribute": {
        "id": "openapi_specs",
        "name": "OpenAPI/Swagger Specifications",
        "category": "API Documentation",
        "tier": 3,
        "description": "Assessment for OpenAPI/Swagger Specifications",
        "criteria": "To be implemented",
        "default_weight": 0.03
      },
      "status": "not_applicable",
      "score": null,
      "measured_value": null,
      "threshold": null,
      "evidence": [
        "OpenAPI/Swagger Specifications assessment not yet implemented"
      ],
      "remediation": null,
      "error_message": null
    },
    {
      "attribute": {
        "id": "architecture_decisions",
        "name": "Architecture Decision Records",
        "category": "Documentation Standards",
        "tier": 3,
        "description": "Assessment for Architecture Decision Records",
        "criteria": "To be implemented",
        "default_weight": 0.03
      },
      "status": "not_applicable",
      "score": null,
      "measured_value": null,
      "threshold": null,
      "evidence": [
        "Architecture Decision Records assessment not yet implemented"
      ],
      "remediation": null,
      "error_message": null
    },
    {
      "attribute": {
        "id": "semantic_naming",
        "name": "Semantic File & Directory Naming",
        "category": "Modularity",
        "tier": 3,
        "description": "Assessment for Semantic File & Directory Naming",
        "criteria": "To be implemented",
        "default_weight": 0.03
      },
      "status": "not_applicable",
      "score": null,
      "measured_value": null,
      "threshold": null,
      "evidence": [
        "Semantic File & Directory Naming assessment not yet implemented"
      ],
      "remediation": null,
      "error_message": null
    },
    {
      "attribute": {
        "id": "security_scanning",
        "name": "Security Scanning Automation",
        "category": "Security",
        "tier": 4,
        "description": "Assessment for Security Scanning Automation",
        "criteria": "To be implemented",
        "default_weight": 0.01
      },
      "status": "not_applicable",
      "score": null,
      "measured_value": null,
      "threshold": null,
      "evidence": [
        "Security Scanning Automation assessment not yet implemented"
      ],
      "remediation": null,
      "error_message": null
    },
    {
      "attribute": {
        "id": "performance_benchmarks",
        "name": "Performance Benchmarks",
        "category": "Performance",
        "tier": 4,
        "description": "Assessment for Performance Benchmarks",
        "criteria": "To be implemented",
        "default_weight": 0.01
      },
      "status": "not_applicable",
      "score": null,
      "measured_value": null,
      "threshold": null,
      "evidence": [
        "Performance Benchmarks assessment not yet implemented"
      ],
      "remediation": null,
      "error_message": null
    },
    {
      "attribute": {
        "id": "code_smells",
        "name": "Code Smell Elimination",
        "category": "Code Quality",
        "tier": 4,
        "description": "Assessment for Code Smell Elimination",
        "criteria": "To be implemented",
        "default_weight": 0.01
      },
      "status": "not_applicable",
      "score": null,
      "measured_value": null,
      "threshold": null,
      "evidence": [
        "Code Smell Elimination assessment not yet implemented"
      ],
      "remediation": null,
      "error_message": null
    },
    {
      "attribute": {
        "id": "issue_pr_templates",
        "name": "Issue & Pull Request Templates",
        "category": "Git & Version Control",
        "tier": 4,
        "description": "Assessment for Issue & Pull Request Templates",
        "criteria": "To be implemented",
        "default_weight": 0.01
      },
      "status": "not_applicable",
      "score": null,
      "measured_value": null,
      "threshold": null,
      "evidence": [
        "Issue & Pull Request Templates assessment not yet implemented"
      ],
      "remediation": null,
      "error_message": null
    },
    {
      "attribute": {
        "id": "container_setup",
        "name": "Container/Virtualization Setup",
        "category": "Build & Development",
        "tier": 4,
        "description": "Assessment for Container/Virtualization Setup",
        "criteria": "To be implemented",
        "default_weight": 0.01
      },
      "status": "not_applicable",
      "score": null,
      "measured_value": null,
      "threshold": null,
      "evidence": [
        "Container/Virtualization Setup assessment not yet implemented"
      ],
      "remediation": null,
      "error_message": null
    }
  ],
  "config": null,
  "duration_seconds": 0.3
}
</file>

<file path=".agentready/assessment-20251121-015727.json">
{
  "repository": {
    "path": "/Users/jeder/repos/sk/agentready",
    "name": "agentready",
    "url": null,
    "branch": "001-agentready-scorer",
    "commit_hash": "18664eacea94e90ee2da222a09228771c67c17f0",
    "languages": {
      "Markdown": 36,
      "Shell": 5,
      "Python": 25
    },
    "total_files": 73,
    "total_lines": 13039
  },
  "timestamp": "2025-11-21T01:57:27.823509",
  "overall_score": 75.4,
  "certification_level": "Gold",
  "attributes_assessed": 10,
  "attributes_skipped": 15,
  "attributes_total": 25,
  "findings": [
    {
      "attribute": {
        "id": "claude_md_file",
        "name": "CLAUDE.md Configuration Files",
        "category": "Context Window Optimization",
        "tier": 1,
        "description": "Project-specific configuration for Claude Code",
        "criteria": "CLAUDE.md file exists in repository root",
        "default_weight": 0.1
      },
      "status": "pass",
      "score": 100.0,
      "measured_value": "present",
      "threshold": "present",
      "evidence": [
        "CLAUDE.md found at /Users/jeder/repos/sk/agentready/CLAUDE.md"
      ],
      "remediation": null,
      "error_message": null
    },
    {
      "attribute": {
        "id": "readme_structure",
        "name": "README Structure",
        "category": "Documentation Standards",
        "tier": 1,
        "description": "Well-structured README with key sections",
        "criteria": "README.md with installation, usage, and development sections",
        "default_weight": 0.1
      },
      "status": "pass",
      "score": 100.0,
      "measured_value": "3/3 sections",
      "threshold": "3/3 sections",
      "evidence": [
        "Found 3/3 essential sections",
        "Installation: \u2713",
        "Usage: \u2713",
        "Development: \u2713"
      ],
      "remediation": null,
      "error_message": null
    },
    {
      "attribute": {
        "id": "type_annotations",
        "name": "Type Annotations",
        "category": "Code Quality",
        "tier": 1,
        "description": "Type hints in function signatures",
        "criteria": ">80% of functions have type annotations",
        "default_weight": 0.1
      },
      "status": "pass",
      "score": 100.0,
      "measured_value": "95.0%",
      "threshold": "\u226580%",
      "evidence": [
        "Typed functions: 115/121",
        "Coverage: 95.0%"
      ],
      "remediation": null,
      "error_message": null
    },
    {
      "attribute": {
        "id": "standard_layout",
        "name": "Standard Project Layouts",
        "category": "Repository Structure",
        "tier": 1,
        "description": "Follows standard project structure for language",
        "criteria": "Standard directories (src/, tests/, docs/) present",
        "default_weight": 0.1
      },
      "status": "pass",
      "score": 100.0,
      "measured_value": "2/2 directories",
      "threshold": "2/2 directories",
      "evidence": [
        "Found 2/2 standard directories",
        "src/: \u2713",
        "tests/: \u2713"
      ],
      "remediation": null,
      "error_message": null
    },
    {
      "attribute": {
        "id": "lock_files",
        "name": "Lock Files for Reproducibility",
        "category": "Dependency Management",
        "tier": 1,
        "description": "Lock files present for dependency pinning",
        "criteria": "package-lock.json, yarn.lock, poetry.lock, or requirements.txt with versions",
        "default_weight": 0.1
      },
      "status": "fail",
      "score": 0.0,
      "measured_value": "none",
      "threshold": "at least one lock file",
      "evidence": [
        "No lock files found"
      ],
      "remediation": {
        "summary": "Add lock file for dependency reproducibility",
        "steps": [
          "Use npm install, poetry lock, or equivalent to generate lock file"
        ],
        "tools": [],
        "commands": [
          "npm install  # generates package-lock.json"
        ],
        "examples": [],
        "citations": []
      },
      "error_message": null
    },
    {
      "attribute": {
        "id": "test_coverage",
        "name": "Test Coverage Requirements",
        "category": "Testing & CI/CD",
        "tier": 2,
        "description": "Test coverage thresholds configured and enforced",
        "criteria": ">80% code coverage",
        "default_weight": 0.03
      },
      "status": "pass",
      "score": 100.0,
      "measured_value": "configured",
      "threshold": "configured with >80% threshold",
      "evidence": [
        "Coverage configuration found",
        "pytest-cov dependency present"
      ],
      "remediation": null,
      "error_message": null
    },
    {
      "attribute": {
        "id": "precommit_hooks",
        "name": "Pre-commit Hooks & CI/CD Linting",
        "category": "Testing & CI/CD",
        "tier": 2,
        "description": "Pre-commit hooks configured for linting and formatting",
        "criteria": ".pre-commit-config.yaml exists",
        "default_weight": 0.03
      },
      "status": "fail",
      "score": 0.0,
      "measured_value": "not configured",
      "threshold": "configured",
      "evidence": [
        ".pre-commit-config.yaml not found"
      ],
      "remediation": {
        "summary": "Configure pre-commit hooks for automated code quality checks",
        "steps": [
          "Install pre-commit framework",
          "Create .pre-commit-config.yaml",
          "Add hooks for linting and formatting",
          "Install hooks: pre-commit install",
          "Run on all files: pre-commit run --all-files"
        ],
        "tools": [
          "pre-commit"
        ],
        "commands": [
          "pip install pre-commit",
          "pre-commit install",
          "pre-commit run --all-files"
        ],
        "examples": [
          "# .pre-commit-config.yaml\nrepos:\n  - repo: https://github.com/pre-commit/pre-commit-hooks\n    rev: v4.4.0\n    hooks:\n      - id: trailing-whitespace\n      - id: end-of-file-fixer\n      - id: check-yaml\n      - id: check-added-large-files\n\n  - repo: https://github.com/psf/black\n    rev: 23.3.0\n    hooks:\n      - id: black\n\n  - repo: https://github.com/pycqa/isort\n    rev: 5.12.0\n    hooks:\n      - id: isort\n"
        ],
        "citations": [
          {
            "source": "pre-commit.com",
            "title": "Pre-commit Framework",
            "url": "https://pre-commit.com/",
            "relevance": "Official pre-commit documentation"
          }
        ]
      },
      "error_message": null
    },
    {
      "attribute": {
        "id": "conventional_commits",
        "name": "Conventional Commit Messages",
        "category": "Git & Version Control",
        "tier": 2,
        "description": "Follows conventional commit format",
        "criteria": "\u226580% of recent commits follow convention",
        "default_weight": 0.03
      },
      "status": "fail",
      "score": 0.0,
      "measured_value": "not configured",
      "threshold": "configured",
      "evidence": [
        "No commitlint or husky configuration"
      ],
      "remediation": {
        "summary": "Configure conventional commits with commitlint",
        "steps": [
          "Install commitlint",
          "Configure husky for commit-msg hook"
        ],
        "tools": [
          "commitlint",
          "husky"
        ],
        "commands": [
          "npm install --save-dev @commitlint/cli @commitlint/config-conventional husky"
        ],
        "examples": [],
        "citations": []
      },
      "error_message": null
    },
    {
      "attribute": {
        "id": "gitignore_completeness",
        "name": ".gitignore Completeness",
        "category": "Git & Version Control",
        "tier": 2,
        "description": "Comprehensive .gitignore file",
        "criteria": ".gitignore exists and covers common patterns",
        "default_weight": 0.03
      },
      "status": "pass",
      "score": 100.0,
      "measured_value": "580 bytes",
      "threshold": ">50 bytes",
      "evidence": [
        ".gitignore found (580 bytes)"
      ],
      "remediation": null,
      "error_message": null
    },
    {
      "attribute": {
        "id": "cyclomatic_complexity",
        "name": "Cyclomatic Complexity Thresholds",
        "category": "Code Quality",
        "tier": 3,
        "description": "Cyclomatic complexity thresholds enforced",
        "criteria": "Average complexity <10, no functions >15",
        "default_weight": 0.03
      },
      "status": "pass",
      "score": 100.0,
      "measured_value": "3.0",
      "threshold": "<10.0",
      "evidence": [
        "Average cyclomatic complexity: 3.0"
      ],
      "remediation": null,
      "error_message": null
    },
    {
      "attribute": {
        "id": "one_command_setup",
        "name": "One-Command Build/Setup",
        "category": "Build & Development",
        "tier": 2,
        "description": "Assessment for One-Command Build/Setup",
        "criteria": "To be implemented",
        "default_weight": 0.03
      },
      "status": "not_applicable",
      "score": null,
      "measured_value": null,
      "threshold": null,
      "evidence": [
        "One-Command Build/Setup assessment not yet implemented"
      ],
      "remediation": null,
      "error_message": null
    },
    {
      "attribute": {
        "id": "concise_documentation",
        "name": "Concise Structured Documentation",
        "category": "Context Window Optimization",
        "tier": 2,
        "description": "Assessment for Concise Structured Documentation",
        "criteria": "To be implemented",
        "default_weight": 0.03
      },
      "status": "not_applicable",
      "score": null,
      "measured_value": null,
      "threshold": null,
      "evidence": [
        "Concise Structured Documentation assessment not yet implemented"
      ],
      "remediation": null,
      "error_message": null
    },
    {
      "attribute": {
        "id": "inline_documentation",
        "name": "Inline Documentation",
        "category": "Documentation Standards",
        "tier": 2,
        "description": "Assessment for Inline Documentation",
        "criteria": "To be implemented",
        "default_weight": 0.03
      },
      "status": "not_applicable",
      "score": null,
      "measured_value": null,
      "threshold": null,
      "evidence": [
        "Inline Documentation assessment not yet implemented"
      ],
      "remediation": null,
      "error_message": null
    },
    {
      "attribute": {
        "id": "file_size_limits",
        "name": "File Size Limits",
        "category": "Context Window Optimization",
        "tier": 2,
        "description": "Assessment for File Size Limits",
        "criteria": "To be implemented",
        "default_weight": 0.03
      },
      "status": "not_applicable",
      "score": null,
      "measured_value": null,
      "threshold": null,
      "evidence": [
        "File Size Limits assessment not yet implemented"
      ],
      "remediation": null,
      "error_message": null
    },
    {
      "attribute": {
        "id": "dependency_freshness",
        "name": "Dependency Freshness & Security",
        "category": "Dependency Management",
        "tier": 2,
        "description": "Assessment for Dependency Freshness & Security",
        "criteria": "To be implemented",
        "default_weight": 0.03
      },
      "status": "not_applicable",
      "score": null,
      "measured_value": null,
      "threshold": null,
      "evidence": [
        "Dependency Freshness & Security assessment not yet implemented"
      ],
      "remediation": null,
      "error_message": null
    },
    {
      "attribute": {
        "id": "separation_concerns",
        "name": "Separation of Concerns",
        "category": "Repository Structure",
        "tier": 2,
        "description": "Assessment for Separation of Concerns",
        "criteria": "To be implemented",
        "default_weight": 0.03
      },
      "status": "not_applicable",
      "score": null,
      "measured_value": null,
      "threshold": null,
      "evidence": [
        "Separation of Concerns assessment not yet implemented"
      ],
      "remediation": null,
      "error_message": null
    },
    {
      "attribute": {
        "id": "structured_logging",
        "name": "Structured Logging",
        "category": "Error Handling",
        "tier": 3,
        "description": "Assessment for Structured Logging",
        "criteria": "To be implemented",
        "default_weight": 0.03
      },
      "status": "not_applicable",
      "score": null,
      "measured_value": null,
      "threshold": null,
      "evidence": [
        "Structured Logging assessment not yet implemented"
      ],
      "remediation": null,
      "error_message": null
    },
    {
      "attribute": {
        "id": "openapi_specs",
        "name": "OpenAPI/Swagger Specifications",
        "category": "API Documentation",
        "tier": 3,
        "description": "Assessment for OpenAPI/Swagger Specifications",
        "criteria": "To be implemented",
        "default_weight": 0.03
      },
      "status": "not_applicable",
      "score": null,
      "measured_value": null,
      "threshold": null,
      "evidence": [
        "OpenAPI/Swagger Specifications assessment not yet implemented"
      ],
      "remediation": null,
      "error_message": null
    },
    {
      "attribute": {
        "id": "architecture_decisions",
        "name": "Architecture Decision Records",
        "category": "Documentation Standards",
        "tier": 3,
        "description": "Assessment for Architecture Decision Records",
        "criteria": "To be implemented",
        "default_weight": 0.03
      },
      "status": "not_applicable",
      "score": null,
      "measured_value": null,
      "threshold": null,
      "evidence": [
        "Architecture Decision Records assessment not yet implemented"
      ],
      "remediation": null,
      "error_message": null
    },
    {
      "attribute": {
        "id": "semantic_naming",
        "name": "Semantic File & Directory Naming",
        "category": "Modularity",
        "tier": 3,
        "description": "Assessment for Semantic File & Directory Naming",
        "criteria": "To be implemented",
        "default_weight": 0.03
      },
      "status": "not_applicable",
      "score": null,
      "measured_value": null,
      "threshold": null,
      "evidence": [
        "Semantic File & Directory Naming assessment not yet implemented"
      ],
      "remediation": null,
      "error_message": null
    },
    {
      "attribute": {
        "id": "security_scanning",
        "name": "Security Scanning Automation",
        "category": "Security",
        "tier": 4,
        "description": "Assessment for Security Scanning Automation",
        "criteria": "To be implemented",
        "default_weight": 0.01
      },
      "status": "not_applicable",
      "score": null,
      "measured_value": null,
      "threshold": null,
      "evidence": [
        "Security Scanning Automation assessment not yet implemented"
      ],
      "remediation": null,
      "error_message": null
    },
    {
      "attribute": {
        "id": "performance_benchmarks",
        "name": "Performance Benchmarks",
        "category": "Performance",
        "tier": 4,
        "description": "Assessment for Performance Benchmarks",
        "criteria": "To be implemented",
        "default_weight": 0.01
      },
      "status": "not_applicable",
      "score": null,
      "measured_value": null,
      "threshold": null,
      "evidence": [
        "Performance Benchmarks assessment not yet implemented"
      ],
      "remediation": null,
      "error_message": null
    },
    {
      "attribute": {
        "id": "code_smells",
        "name": "Code Smell Elimination",
        "category": "Code Quality",
        "tier": 4,
        "description": "Assessment for Code Smell Elimination",
        "criteria": "To be implemented",
        "default_weight": 0.01
      },
      "status": "not_applicable",
      "score": null,
      "measured_value": null,
      "threshold": null,
      "evidence": [
        "Code Smell Elimination assessment not yet implemented"
      ],
      "remediation": null,
      "error_message": null
    },
    {
      "attribute": {
        "id": "issue_pr_templates",
        "name": "Issue & Pull Request Templates",
        "category": "Git & Version Control",
        "tier": 4,
        "description": "Assessment for Issue & Pull Request Templates",
        "criteria": "To be implemented",
        "default_weight": 0.01
      },
      "status": "not_applicable",
      "score": null,
      "measured_value": null,
      "threshold": null,
      "evidence": [
        "Issue & Pull Request Templates assessment not yet implemented"
      ],
      "remediation": null,
      "error_message": null
    },
    {
      "attribute": {
        "id": "container_setup",
        "name": "Container/Virtualization Setup",
        "category": "Build & Development",
        "tier": 4,
        "description": "Assessment for Container/Virtualization Setup",
        "criteria": "To be implemented",
        "default_weight": 0.01
      },
      "status": "not_applicable",
      "score": null,
      "measured_value": null,
      "threshold": null,
      "evidence": [
        "Container/Virtualization Setup assessment not yet implemented"
      ],
      "remediation": null,
      "error_message": null
    }
  ],
  "config": null,
  "duration_seconds": 0.1
}
</file>

<file path=".agentready/assessment-20251121-021105.json">
{
  "repository": {
    "path": "/Users/jeder/repos/sk/agentready",
    "name": "agentready",
    "url": null,
    "branch": "001-agentready-scorer",
    "commit_hash": "adfc4c823a7b25d5b1404c9b994461e290257dce",
    "languages": {
      "Markdown": 36,
      "Shell": 5,
      "Python": 31
    },
    "total_files": 79,
    "total_lines": 13852
  },
  "timestamp": "2025-11-21T02:11:05.766923",
  "overall_score": 75.4,
  "certification_level": "Gold",
  "attributes_assessed": 10,
  "attributes_skipped": 15,
  "attributes_total": 25,
  "findings": [
    {
      "attribute": {
        "id": "claude_md_file",
        "name": "CLAUDE.md Configuration Files",
        "category": "Context Window Optimization",
        "tier": 1,
        "description": "Project-specific configuration for Claude Code",
        "criteria": "CLAUDE.md file exists in repository root",
        "default_weight": 0.1
      },
      "status": "pass",
      "score": 100.0,
      "measured_value": "present",
      "threshold": "present",
      "evidence": [
        "CLAUDE.md found at /Users/jeder/repos/sk/agentready/CLAUDE.md"
      ],
      "remediation": null,
      "error_message": null
    },
    {
      "attribute": {
        "id": "readme_structure",
        "name": "README Structure",
        "category": "Documentation Standards",
        "tier": 1,
        "description": "Well-structured README with key sections",
        "criteria": "README.md with installation, usage, and development sections",
        "default_weight": 0.1
      },
      "status": "pass",
      "score": 100.0,
      "measured_value": "3/3 sections",
      "threshold": "3/3 sections",
      "evidence": [
        "Found 3/3 essential sections",
        "Installation: \u2713",
        "Usage: \u2713",
        "Development: \u2713"
      ],
      "remediation": null,
      "error_message": null
    },
    {
      "attribute": {
        "id": "type_annotations",
        "name": "Type Annotations",
        "category": "Code Quality",
        "tier": 1,
        "description": "Type hints in function signatures",
        "criteria": ">80% of functions have type annotations",
        "default_weight": 0.1
      },
      "status": "pass",
      "score": 100.0,
      "measured_value": "95.9%",
      "threshold": "\u226580%",
      "evidence": [
        "Typed functions: 142/148",
        "Coverage: 95.9%"
      ],
      "remediation": null,
      "error_message": null
    },
    {
      "attribute": {
        "id": "standard_layout",
        "name": "Standard Project Layouts",
        "category": "Repository Structure",
        "tier": 1,
        "description": "Follows standard project structure for language",
        "criteria": "Standard directories (src/, tests/, docs/) present",
        "default_weight": 0.1
      },
      "status": "pass",
      "score": 100.0,
      "measured_value": "2/2 directories",
      "threshold": "2/2 directories",
      "evidence": [
        "Found 2/2 standard directories",
        "src/: \u2713",
        "tests/: \u2713"
      ],
      "remediation": null,
      "error_message": null
    },
    {
      "attribute": {
        "id": "lock_files",
        "name": "Lock Files for Reproducibility",
        "category": "Dependency Management",
        "tier": 1,
        "description": "Lock files present for dependency pinning",
        "criteria": "package-lock.json, yarn.lock, poetry.lock, or requirements.txt with versions",
        "default_weight": 0.1
      },
      "status": "fail",
      "score": 0.0,
      "measured_value": "none",
      "threshold": "at least one lock file",
      "evidence": [
        "No lock files found"
      ],
      "remediation": {
        "summary": "Add lock file for dependency reproducibility",
        "steps": [
          "Use npm install, poetry lock, or equivalent to generate lock file"
        ],
        "tools": [],
        "commands": [
          "npm install  # generates package-lock.json"
        ],
        "examples": [],
        "citations": []
      },
      "error_message": null
    },
    {
      "attribute": {
        "id": "test_coverage",
        "name": "Test Coverage Requirements",
        "category": "Testing & CI/CD",
        "tier": 2,
        "description": "Test coverage thresholds configured and enforced",
        "criteria": ">80% code coverage",
        "default_weight": 0.03
      },
      "status": "pass",
      "score": 100.0,
      "measured_value": "configured",
      "threshold": "configured with >80% threshold",
      "evidence": [
        "Coverage configuration found",
        "pytest-cov dependency present"
      ],
      "remediation": null,
      "error_message": null
    },
    {
      "attribute": {
        "id": "precommit_hooks",
        "name": "Pre-commit Hooks & CI/CD Linting",
        "category": "Testing & CI/CD",
        "tier": 2,
        "description": "Pre-commit hooks configured for linting and formatting",
        "criteria": ".pre-commit-config.yaml exists",
        "default_weight": 0.03
      },
      "status": "fail",
      "score": 0.0,
      "measured_value": "not configured",
      "threshold": "configured",
      "evidence": [
        ".pre-commit-config.yaml not found"
      ],
      "remediation": {
        "summary": "Configure pre-commit hooks for automated code quality checks",
        "steps": [
          "Install pre-commit framework",
          "Create .pre-commit-config.yaml",
          "Add hooks for linting and formatting",
          "Install hooks: pre-commit install",
          "Run on all files: pre-commit run --all-files"
        ],
        "tools": [
          "pre-commit"
        ],
        "commands": [
          "pip install pre-commit",
          "pre-commit install",
          "pre-commit run --all-files"
        ],
        "examples": [
          "# .pre-commit-config.yaml\nrepos:\n  - repo: https://github.com/pre-commit/pre-commit-hooks\n    rev: v4.4.0\n    hooks:\n      - id: trailing-whitespace\n      - id: end-of-file-fixer\n      - id: check-yaml\n      - id: check-added-large-files\n\n  - repo: https://github.com/psf/black\n    rev: 23.3.0\n    hooks:\n      - id: black\n\n  - repo: https://github.com/pycqa/isort\n    rev: 5.12.0\n    hooks:\n      - id: isort\n"
        ],
        "citations": [
          {
            "source": "pre-commit.com",
            "title": "Pre-commit Framework",
            "url": "https://pre-commit.com/",
            "relevance": "Official pre-commit documentation"
          }
        ]
      },
      "error_message": null
    },
    {
      "attribute": {
        "id": "conventional_commits",
        "name": "Conventional Commit Messages",
        "category": "Git & Version Control",
        "tier": 2,
        "description": "Follows conventional commit format",
        "criteria": "\u226580% of recent commits follow convention",
        "default_weight": 0.03
      },
      "status": "fail",
      "score": 0.0,
      "measured_value": "not configured",
      "threshold": "configured",
      "evidence": [
        "No commitlint or husky configuration"
      ],
      "remediation": {
        "summary": "Configure conventional commits with commitlint",
        "steps": [
          "Install commitlint",
          "Configure husky for commit-msg hook"
        ],
        "tools": [
          "commitlint",
          "husky"
        ],
        "commands": [
          "npm install --save-dev @commitlint/cli @commitlint/config-conventional husky"
        ],
        "examples": [],
        "citations": []
      },
      "error_message": null
    },
    {
      "attribute": {
        "id": "gitignore_completeness",
        "name": ".gitignore Completeness",
        "category": "Git & Version Control",
        "tier": 2,
        "description": "Comprehensive .gitignore file",
        "criteria": ".gitignore exists and covers common patterns",
        "default_weight": 0.03
      },
      "status": "pass",
      "score": 100.0,
      "measured_value": "580 bytes",
      "threshold": ">50 bytes",
      "evidence": [
        ".gitignore found (580 bytes)"
      ],
      "remediation": null,
      "error_message": null
    },
    {
      "attribute": {
        "id": "cyclomatic_complexity",
        "name": "Cyclomatic Complexity Thresholds",
        "category": "Code Quality",
        "tier": 3,
        "description": "Cyclomatic complexity thresholds enforced",
        "criteria": "Average complexity <10, no functions >15",
        "default_weight": 0.03
      },
      "status": "pass",
      "score": 100.0,
      "measured_value": "3.1",
      "threshold": "<10.0",
      "evidence": [
        "Average cyclomatic complexity: 3.1"
      ],
      "remediation": null,
      "error_message": null
    },
    {
      "attribute": {
        "id": "one_command_setup",
        "name": "One-Command Build/Setup",
        "category": "Build & Development",
        "tier": 2,
        "description": "Assessment for One-Command Build/Setup",
        "criteria": "To be implemented",
        "default_weight": 0.03
      },
      "status": "not_applicable",
      "score": null,
      "measured_value": null,
      "threshold": null,
      "evidence": [
        "One-Command Build/Setup assessment not yet implemented"
      ],
      "remediation": null,
      "error_message": null
    },
    {
      "attribute": {
        "id": "concise_documentation",
        "name": "Concise Structured Documentation",
        "category": "Context Window Optimization",
        "tier": 2,
        "description": "Assessment for Concise Structured Documentation",
        "criteria": "To be implemented",
        "default_weight": 0.03
      },
      "status": "not_applicable",
      "score": null,
      "measured_value": null,
      "threshold": null,
      "evidence": [
        "Concise Structured Documentation assessment not yet implemented"
      ],
      "remediation": null,
      "error_message": null
    },
    {
      "attribute": {
        "id": "inline_documentation",
        "name": "Inline Documentation",
        "category": "Documentation Standards",
        "tier": 2,
        "description": "Assessment for Inline Documentation",
        "criteria": "To be implemented",
        "default_weight": 0.03
      },
      "status": "not_applicable",
      "score": null,
      "measured_value": null,
      "threshold": null,
      "evidence": [
        "Inline Documentation assessment not yet implemented"
      ],
      "remediation": null,
      "error_message": null
    },
    {
      "attribute": {
        "id": "file_size_limits",
        "name": "File Size Limits",
        "category": "Context Window Optimization",
        "tier": 2,
        "description": "Assessment for File Size Limits",
        "criteria": "To be implemented",
        "default_weight": 0.03
      },
      "status": "not_applicable",
      "score": null,
      "measured_value": null,
      "threshold": null,
      "evidence": [
        "File Size Limits assessment not yet implemented"
      ],
      "remediation": null,
      "error_message": null
    },
    {
      "attribute": {
        "id": "dependency_freshness",
        "name": "Dependency Freshness & Security",
        "category": "Dependency Management",
        "tier": 2,
        "description": "Assessment for Dependency Freshness & Security",
        "criteria": "To be implemented",
        "default_weight": 0.03
      },
      "status": "not_applicable",
      "score": null,
      "measured_value": null,
      "threshold": null,
      "evidence": [
        "Dependency Freshness & Security assessment not yet implemented"
      ],
      "remediation": null,
      "error_message": null
    },
    {
      "attribute": {
        "id": "separation_concerns",
        "name": "Separation of Concerns",
        "category": "Repository Structure",
        "tier": 2,
        "description": "Assessment for Separation of Concerns",
        "criteria": "To be implemented",
        "default_weight": 0.03
      },
      "status": "not_applicable",
      "score": null,
      "measured_value": null,
      "threshold": null,
      "evidence": [
        "Separation of Concerns assessment not yet implemented"
      ],
      "remediation": null,
      "error_message": null
    },
    {
      "attribute": {
        "id": "structured_logging",
        "name": "Structured Logging",
        "category": "Error Handling",
        "tier": 3,
        "description": "Assessment for Structured Logging",
        "criteria": "To be implemented",
        "default_weight": 0.03
      },
      "status": "not_applicable",
      "score": null,
      "measured_value": null,
      "threshold": null,
      "evidence": [
        "Structured Logging assessment not yet implemented"
      ],
      "remediation": null,
      "error_message": null
    },
    {
      "attribute": {
        "id": "openapi_specs",
        "name": "OpenAPI/Swagger Specifications",
        "category": "API Documentation",
        "tier": 3,
        "description": "Assessment for OpenAPI/Swagger Specifications",
        "criteria": "To be implemented",
        "default_weight": 0.03
      },
      "status": "not_applicable",
      "score": null,
      "measured_value": null,
      "threshold": null,
      "evidence": [
        "OpenAPI/Swagger Specifications assessment not yet implemented"
      ],
      "remediation": null,
      "error_message": null
    },
    {
      "attribute": {
        "id": "architecture_decisions",
        "name": "Architecture Decision Records",
        "category": "Documentation Standards",
        "tier": 3,
        "description": "Assessment for Architecture Decision Records",
        "criteria": "To be implemented",
        "default_weight": 0.03
      },
      "status": "not_applicable",
      "score": null,
      "measured_value": null,
      "threshold": null,
      "evidence": [
        "Architecture Decision Records assessment not yet implemented"
      ],
      "remediation": null,
      "error_message": null
    },
    {
      "attribute": {
        "id": "semantic_naming",
        "name": "Semantic File & Directory Naming",
        "category": "Modularity",
        "tier": 3,
        "description": "Assessment for Semantic File & Directory Naming",
        "criteria": "To be implemented",
        "default_weight": 0.03
      },
      "status": "not_applicable",
      "score": null,
      "measured_value": null,
      "threshold": null,
      "evidence": [
        "Semantic File & Directory Naming assessment not yet implemented"
      ],
      "remediation": null,
      "error_message": null
    },
    {
      "attribute": {
        "id": "security_scanning",
        "name": "Security Scanning Automation",
        "category": "Security",
        "tier": 4,
        "description": "Assessment for Security Scanning Automation",
        "criteria": "To be implemented",
        "default_weight": 0.01
      },
      "status": "not_applicable",
      "score": null,
      "measured_value": null,
      "threshold": null,
      "evidence": [
        "Security Scanning Automation assessment not yet implemented"
      ],
      "remediation": null,
      "error_message": null
    },
    {
      "attribute": {
        "id": "performance_benchmarks",
        "name": "Performance Benchmarks",
        "category": "Performance",
        "tier": 4,
        "description": "Assessment for Performance Benchmarks",
        "criteria": "To be implemented",
        "default_weight": 0.01
      },
      "status": "not_applicable",
      "score": null,
      "measured_value": null,
      "threshold": null,
      "evidence": [
        "Performance Benchmarks assessment not yet implemented"
      ],
      "remediation": null,
      "error_message": null
    },
    {
      "attribute": {
        "id": "code_smells",
        "name": "Code Smell Elimination",
        "category": "Code Quality",
        "tier": 4,
        "description": "Assessment for Code Smell Elimination",
        "criteria": "To be implemented",
        "default_weight": 0.01
      },
      "status": "not_applicable",
      "score": null,
      "measured_value": null,
      "threshold": null,
      "evidence": [
        "Code Smell Elimination assessment not yet implemented"
      ],
      "remediation": null,
      "error_message": null
    },
    {
      "attribute": {
        "id": "issue_pr_templates",
        "name": "Issue & Pull Request Templates",
        "category": "Git & Version Control",
        "tier": 4,
        "description": "Assessment for Issue & Pull Request Templates",
        "criteria": "To be implemented",
        "default_weight": 0.01
      },
      "status": "not_applicable",
      "score": null,
      "measured_value": null,
      "threshold": null,
      "evidence": [
        "Issue & Pull Request Templates assessment not yet implemented"
      ],
      "remediation": null,
      "error_message": null
    },
    {
      "attribute": {
        "id": "container_setup",
        "name": "Container/Virtualization Setup",
        "category": "Build & Development",
        "tier": 4,
        "description": "Assessment for Container/Virtualization Setup",
        "criteria": "To be implemented",
        "default_weight": 0.01
      },
      "status": "not_applicable",
      "score": null,
      "measured_value": null,
      "threshold": null,
      "evidence": [
        "Container/Virtualization Setup assessment not yet implemented"
      ],
      "remediation": null,
      "error_message": null
    }
  ],
  "config": null,
  "duration_seconds": 0.2
}
</file>

<file path=".agentready/report-20251121-015331.html">
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>AgentReady Assessment - agentready</title>
    <style>
        /* Reset and Base Styles */
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif;
            line-height: 1.6;
            color: #333;
            background: #f5f5f5;
            padding: 20px;
        }

        .container {
            max-width: 1200px;
            margin: 0 auto;
            background: white;
            padding: 40px;
            border-radius: 8px;
            box-shadow: 0 2px 8px rgba(0,0,0,0.1);
        }

        /* Header */
        header {
            border-bottom: 3px solid #2563eb;
            padding-bottom: 20px;
            margin-bottom: 30px;
        }

        h1 {
            font-size: 2rem;
            color: #1e293b;
            margin-bottom: 10px;
        }

        .subtitle {
            color: #64748b;
            font-size: 1rem;
        }

        /* Summary Cards */
        .summary-grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(200px, 1fr));
            gap: 20px;
            margin: 30px 0;
        }

        .summary-card {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 20px;
            border-radius: 8px;
            text-align: center;
        }

        .summary-card.score {
            background: linear-gradient(135deg, #f093fb 0%, #f5576c 100%);
        }

        .summary-card.certification {
            background: linear-gradient(135deg, #4facfe 0%, #00f2fe 100%);
        }

        .summary-card.assessed {
            background: linear-gradient(135deg, #43e97b 0%, #38f9d7 100%);
        }

        .summary-card.duration {
            background: linear-gradient(135deg, #fa709a 0%, #fee140 100%);
        }

        .summary-card h3 {
            font-size: 0.9rem;
            font-weight: 500;
            opacity: 0.9;
            margin-bottom: 10px;
        }

        .summary-card .value {
            font-size: 2rem;
            font-weight: 700;
        }

        /* Controls */
        .controls {
            display: flex;
            flex-wrap: wrap;
            gap: 15px;
            margin: 30px 0;
            padding: 20px;
            background: #f8fafc;
            border-radius: 8px;
        }

        .control-group {
            display: flex;
            align-items: center;
            gap: 10px;
        }

        .control-group label {
            font-weight: 600;
            color: #475569;
            font-size: 0.9rem;
        }

        select, input[type="search"] {
            padding: 8px 12px;
            border: 1px solid #cbd5e1;
            border-radius: 6px;
            font-size: 0.9rem;
            background: white;
        }

        input[type="search"] {
            min-width: 250px;
        }

        .filter-btn {
            padding: 8px 16px;
            border: 2px solid #e2e8f0;
            background: white;
            border-radius: 6px;
            cursor: pointer;
            font-size: 0.9rem;
            transition: all 0.2s;
        }

        .filter-btn:hover {
            border-color: #2563eb;
            background: #eff6ff;
        }

        .filter-btn.active {
            background: #2563eb;
            color: white;
            border-color: #2563eb;
        }

        .badge {
            display: inline-block;
            padding: 2px 8px;
            border-radius: 12px;
            font-size: 0.75rem;
            font-weight: 600;
            margin-left: 5px;
        }

        /* Certification Ladder */
        .cert-ladder {
            display: flex;
            justify-content: space-between;
            margin: 30px 0;
            padding: 20px;
            background: #f8fafc;
            border-radius: 8px;
        }

        .cert-level {
            flex: 1;
            text-align: center;
            padding: 15px 10px;
            border-radius: 6px;
            margin: 0 5px;
            transition: all 0.3s;
        }

        .cert-level.platinum { background: linear-gradient(135deg, #e0e7ff, #c7d2fe); }
        .cert-level.gold { background: linear-gradient(135deg, #fef3c7, #fde68a); }
        .cert-level.silver { background: linear-gradient(135deg, #e5e7eb, #d1d5db); }
        .cert-level.bronze { background: linear-gradient(135deg, #fed7aa, #fdba74); }
        .cert-level.needs-improvement { background: linear-gradient(135deg, #fecaca, #fca5a5); }

        .cert-level.active {
            transform: scale(1.1);
            box-shadow: 0 4px 12px rgba(0,0,0,0.15);
            border: 3px solid #2563eb;
        }

        .cert-level h4 {
            font-size: 0.9rem;
            margin-bottom: 5px;
        }

        .cert-level .range {
            font-size: 0.8rem;
            opacity: 0.7;
        }

        /* Findings */
        .findings {
            margin: 30px 0;
        }

        .finding {
            border: 1px solid #e2e8f0;
            border-radius: 8px;
            margin-bottom: 15px;
            overflow: hidden;
            transition: all 0.2s;
        }

        .finding:hover {
            box-shadow: 0 4px 12px rgba(0,0,0,0.08);
        }

        .finding-header {
            padding: 15px 20px;
            background: #f8fafc;
            cursor: pointer;
            display: flex;
            justify-content: space-between;
            align-items: center;
        }

        .finding-header:hover {
            background: #f1f5f9;
        }

        .finding-title {
            display: flex;
            align-items: center;
            gap: 15px;
            flex: 1;
        }

        .status-icon {
            font-size: 1.5rem;
        }

        .finding-info h3 {
            font-size: 1rem;
            color: #1e293b;
            margin-bottom: 3px;
        }

        .finding-meta {
            font-size: 0.85rem;
            color: #64748b;
        }

        .tier-badge {
            padding: 4px 12px;
            border-radius: 12px;
            font-size: 0.75rem;
            font-weight: 600;
        }

        .tier-1 { background: #fee2e2; color: #991b1b; }
        .tier-2 { background: #fed7aa; color: #9a3412; }
        .tier-3 { background: #fef3c7; color: #92400e; }
        .tier-4 { background: #d1fae5; color: #065f46; }

        .score-display {
            font-size: 1.5rem;
            font-weight: 700;
            min-width: 80px;
            text-align: right;
        }

        .score-pass { color: #059669; }
        .score-fail { color: #dc2626; }
        .score-skip { color: #6b7280; }

        .finding-body {
            padding: 20px;
            display: none;
        }

        .finding.expanded .finding-body {
            display: block;
        }

        .finding-section {
            margin-bottom: 20px;
        }

        .finding-section h4 {
            font-size: 0.9rem;
            color: #475569;
            margin-bottom: 10px;
            text-transform: uppercase;
            letter-spacing: 0.05em;
        }

        .evidence-list, .remediation-steps {
            list-style: none;
            padding-left: 0;
        }

        .evidence-list li {
            padding: 8px 0;
            border-bottom: 1px solid #f1f5f9;
        }

        .evidence-list li:before {
            content: "‚ñ∏ ";
            color: #2563eb;
            font-weight: 700;
            margin-right: 8px;
        }

        .remediation-steps li {
            padding: 10px 0 10px 30px;
            position: relative;
        }

        .remediation-steps li:before {
            content: counter(step);
            counter-increment: step;
            position: absolute;
            left: 0;
            top: 10px;
            background: #2563eb;
            color: white;
            width: 20px;
            height: 20px;
            border-radius: 50%;
            display: flex;
            align-items: center;
            justify-content: center;
            font-size: 0.75rem;
            font-weight: 700;
        }

        .remediation-steps {
            counter-reset: step;
        }

        code {
            background: #f1f5f9;
            padding: 2px 6px;
            border-radius: 3px;
            font-family: 'Courier New', monospace;
            font-size: 0.9em;
        }

        pre {
            background: #1e293b;
            color: #e2e8f0;
            padding: 15px;
            border-radius: 6px;
            overflow-x: auto;
            margin: 10px 0;
        }

        pre code {
            background: none;
            color: inherit;
            padding: 0;
        }

        /* Footer */
        footer {
            margin-top: 50px;
            padding-top: 20px;
            border-top: 1px solid #e2e8f0;
            text-align: center;
            color: #64748b;
            font-size: 0.9rem;
        }

        /* Responsive */
        @media (max-width: 768px) {
            .container {
                padding: 20px;
            }

            .summary-grid {
                grid-template-columns: 1fr;
            }

            .controls {
                flex-direction: column;
            }

            input[type="search"] {
                min-width: 100%;
            }

            .cert-ladder {
                flex-direction: column;
            }

            .cert-level {
                margin: 5px 0;
            }
        }

        /* Hidden class */
        .hidden {
            display: none !important;
        }
    </style>
</head>
<body>
    <div class="container">
        <header>
            <h1>ü§ñ AgentReady Assessment Report</h1>
            <p class="subtitle">agentready ‚Ä¢ November 21, 2025 at 01:53</p>
        </header>

        <!-- Summary Cards -->
        <div class="summary-grid">
            <div class="summary-card score">
                <h3>Overall Score</h3>
                <div class="value">75.4</div>
            </div>
            <div class="summary-card certification">
                <h3>Certification</h3>
                <div class="value">Gold</div>
            </div>
            <div class="summary-card assessed">
                <h3>Assessed</h3>
                <div class="value">10/25</div>
            </div>
            <div class="summary-card duration">
                <h3>Duration</h3>
                <div class="value">0.3s</div>
            </div>
        </div>

        <!-- Certification Ladder -->
        <div class="cert-ladder">
            <div class="cert-level platinum ">
                <h4>üíé Platinum</h4>
                <p class="range">90-100</p>
            </div>
            <div class="cert-level gold active">
                <h4>ü•á Gold</h4>
                <p class="range">75-89</p>
            </div>
            <div class="cert-level silver ">
                <h4>ü•à Silver</h4>
                <p class="range">60-74</p>
            </div>
            <div class="cert-level bronze ">
                <h4>ü•â Bronze</h4>
                <p class="range">40-59</p>
            </div>
            <div class="cert-level needs-improvement ">
                <h4>‚ö†Ô∏è Needs Work</h4>
                <p class="range">0-39</p>
            </div>
        </div>

        <!-- Controls -->
        <div class="controls">
            <div class="control-group">
                <label>Filter:</label>
                <button class="filter-btn active" data-filter="all">All <span class="badge">25</span></button>
                <button class="filter-btn" data-filter="pass">Pass <span class="badge">7</span></button>
                <button class="filter-btn" data-filter="fail">Fail <span class="badge">3</span></button>
                <button class="filter-btn" data-filter="skipped">Skipped <span class="badge">15</span></button>
            </div>

            <div class="control-group">
                <label>Sort:</label>
                <select id="sort-select">
                    <option value="category">Category</option>
                    <option value="score-desc">Score (High to Low)</option>
                    <option value="score-asc">Score (Low to High)</option>
                    <option value="tier">Tier (Essential First)</option>
                </select>
            </div>

            <div class="control-group">
                <input type="search" id="search-box" placeholder="Search attributes...">
            </div>
        </div>

        <!-- Findings -->
        <div class="findings" id="findings-container">
            
            <div class="finding"
                 data-status="pass"
                 data-tier="1"
                 data-category="Context Window Optimization"
                 data-score="100.0"
                 data-name="claude.md configuration files">

                <div class="finding-header" onclick="toggleFinding(this)">
                    <div class="finding-title">
                        <span class="status-icon">
                            ‚úÖ
                            
                        </span>
                        <div class="finding-info">
                            <h3>CLAUDE.md Configuration Files</h3>
                            <div class="finding-meta">
                                Context Window Optimization ‚Ä¢
                                <span class="tier-badge tier-1">Tier 1</span>
                                 ‚Ä¢ present
                            </div>
                        </div>
                    </div>
                    
                    <div class="score-display score-pass">
                        100
                    </div>
                    
                </div>

                <div class="finding-body">
                    
                    <div class="finding-section">
                        <h4>Evidence</h4>
                        <ul class="evidence-list">
                            
                            <li>CLAUDE.md found at /Users/jeder/repos/sk/agentready/CLAUDE.md</li>
                            
                        </ul>
                    </div>
                    

                    

                    
                </div>
            </div>
            
            <div class="finding"
                 data-status="pass"
                 data-tier="1"
                 data-category="Documentation Standards"
                 data-score="100.0"
                 data-name="readme structure">

                <div class="finding-header" onclick="toggleFinding(this)">
                    <div class="finding-title">
                        <span class="status-icon">
                            ‚úÖ
                            
                        </span>
                        <div class="finding-info">
                            <h3>README Structure</h3>
                            <div class="finding-meta">
                                Documentation Standards ‚Ä¢
                                <span class="tier-badge tier-1">Tier 1</span>
                                 ‚Ä¢ 3/3 sections
                            </div>
                        </div>
                    </div>
                    
                    <div class="score-display score-pass">
                        100
                    </div>
                    
                </div>

                <div class="finding-body">
                    
                    <div class="finding-section">
                        <h4>Evidence</h4>
                        <ul class="evidence-list">
                            
                            <li>Found 3/3 essential sections</li>
                            
                            <li>Installation: ‚úì</li>
                            
                            <li>Usage: ‚úì</li>
                            
                            <li>Development: ‚úì</li>
                            
                        </ul>
                    </div>
                    

                    

                    
                </div>
            </div>
            
            <div class="finding"
                 data-status="pass"
                 data-tier="1"
                 data-category="Code Quality"
                 data-score="100.0"
                 data-name="type annotations">

                <div class="finding-header" onclick="toggleFinding(this)">
                    <div class="finding-title">
                        <span class="status-icon">
                            ‚úÖ
                            
                        </span>
                        <div class="finding-info">
                            <h3>Type Annotations</h3>
                            <div class="finding-meta">
                                Code Quality ‚Ä¢
                                <span class="tier-badge tier-1">Tier 1</span>
                                 ‚Ä¢ 94.9%
                            </div>
                        </div>
                    </div>
                    
                    <div class="score-display score-pass">
                        100
                    </div>
                    
                </div>

                <div class="finding-body">
                    
                    <div class="finding-section">
                        <h4>Evidence</h4>
                        <ul class="evidence-list">
                            
                            <li>Typed functions: 112/118</li>
                            
                            <li>Coverage: 94.9%</li>
                            
                        </ul>
                    </div>
                    

                    

                    
                </div>
            </div>
            
            <div class="finding"
                 data-status="pass"
                 data-tier="1"
                 data-category="Repository Structure"
                 data-score="100.0"
                 data-name="standard project layouts">

                <div class="finding-header" onclick="toggleFinding(this)">
                    <div class="finding-title">
                        <span class="status-icon">
                            ‚úÖ
                            
                        </span>
                        <div class="finding-info">
                            <h3>Standard Project Layouts</h3>
                            <div class="finding-meta">
                                Repository Structure ‚Ä¢
                                <span class="tier-badge tier-1">Tier 1</span>
                                 ‚Ä¢ 2/2 directories
                            </div>
                        </div>
                    </div>
                    
                    <div class="score-display score-pass">
                        100
                    </div>
                    
                </div>

                <div class="finding-body">
                    
                    <div class="finding-section">
                        <h4>Evidence</h4>
                        <ul class="evidence-list">
                            
                            <li>Found 2/2 standard directories</li>
                            
                            <li>src/: ‚úì</li>
                            
                            <li>tests/: ‚úì</li>
                            
                        </ul>
                    </div>
                    

                    

                    
                </div>
            </div>
            
            <div class="finding"
                 data-status="fail"
                 data-tier="1"
                 data-category="Dependency Management"
                 data-score="0"
                 data-name="lock files for reproducibility">

                <div class="finding-header" onclick="toggleFinding(this)">
                    <div class="finding-title">
                        <span class="status-icon">
                            ‚ùå
                            
                        </span>
                        <div class="finding-info">
                            <h3>Lock Files for Reproducibility</h3>
                            <div class="finding-meta">
                                Dependency Management ‚Ä¢
                                <span class="tier-badge tier-1">Tier 1</span>
                                 ‚Ä¢ none
                            </div>
                        </div>
                    </div>
                    
                    <div class="score-display score-fail">
                        0
                    </div>
                    
                </div>

                <div class="finding-body">
                    
                    <div class="finding-section">
                        <h4>Evidence</h4>
                        <ul class="evidence-list">
                            
                            <li>No lock files found</li>
                            
                        </ul>
                    </div>
                    

                    
                    <div class="finding-section">
                        <h4>Remediation</h4>
                        <p><strong>Add lock file for dependency reproducibility</strong></p>

                        
                        <ol class="remediation-steps">
                            
                            <li>Use npm install, poetry lock, or equivalent to generate lock file</li>
                            
                        </ol>
                        

                        
                        <h4 style="margin-top: 15px;">Commands</h4>
                        <pre><code>npm install  # generates package-lock.json</code></pre>
                        

                        
                    </div>
                    

                    
                </div>
            </div>
            
            <div class="finding"
                 data-status="pass"
                 data-tier="2"
                 data-category="Testing & CI/CD"
                 data-score="100.0"
                 data-name="test coverage requirements">

                <div class="finding-header" onclick="toggleFinding(this)">
                    <div class="finding-title">
                        <span class="status-icon">
                            ‚úÖ
                            
                        </span>
                        <div class="finding-info">
                            <h3>Test Coverage Requirements</h3>
                            <div class="finding-meta">
                                Testing & CI/CD ‚Ä¢
                                <span class="tier-badge tier-2">Tier 2</span>
                                 ‚Ä¢ configured
                            </div>
                        </div>
                    </div>
                    
                    <div class="score-display score-pass">
                        100
                    </div>
                    
                </div>

                <div class="finding-body">
                    
                    <div class="finding-section">
                        <h4>Evidence</h4>
                        <ul class="evidence-list">
                            
                            <li>Coverage configuration found</li>
                            
                            <li>pytest-cov dependency present</li>
                            
                        </ul>
                    </div>
                    

                    

                    
                </div>
            </div>
            
            <div class="finding"
                 data-status="fail"
                 data-tier="2"
                 data-category="Testing & CI/CD"
                 data-score="0"
                 data-name="pre-commit hooks & ci/cd linting">

                <div class="finding-header" onclick="toggleFinding(this)">
                    <div class="finding-title">
                        <span class="status-icon">
                            ‚ùå
                            
                        </span>
                        <div class="finding-info">
                            <h3>Pre-commit Hooks & CI/CD Linting</h3>
                            <div class="finding-meta">
                                Testing & CI/CD ‚Ä¢
                                <span class="tier-badge tier-2">Tier 2</span>
                                 ‚Ä¢ not configured
                            </div>
                        </div>
                    </div>
                    
                    <div class="score-display score-fail">
                        0
                    </div>
                    
                </div>

                <div class="finding-body">
                    
                    <div class="finding-section">
                        <h4>Evidence</h4>
                        <ul class="evidence-list">
                            
                            <li>.pre-commit-config.yaml not found</li>
                            
                        </ul>
                    </div>
                    

                    
                    <div class="finding-section">
                        <h4>Remediation</h4>
                        <p><strong>Configure pre-commit hooks for automated code quality checks</strong></p>

                        
                        <ol class="remediation-steps">
                            
                            <li>Install pre-commit framework</li>
                            
                            <li>Create .pre-commit-config.yaml</li>
                            
                            <li>Add hooks for linting and formatting</li>
                            
                            <li>Install hooks: pre-commit install</li>
                            
                            <li>Run on all files: pre-commit run --all-files</li>
                            
                        </ol>
                        

                        
                        <h4 style="margin-top: 15px;">Commands</h4>
                        <pre><code>pip install pre-commit
pre-commit install
pre-commit run --all-files</code></pre>
                        

                        
                        <h4 style="margin-top: 15px;">Examples</h4>
                        
                        <pre><code># .pre-commit-config.yaml
repos:
  - repo: https://github.com/pre-commit/pre-commit-hooks
    rev: v4.4.0
    hooks:
      - id: trailing-whitespace
      - id: end-of-file-fixer
      - id: check-yaml
      - id: check-added-large-files

  - repo: https://github.com/psf/black
    rev: 23.3.0
    hooks:
      - id: black

  - repo: https://github.com/pycqa/isort
    rev: 5.12.0
    hooks:
      - id: isort
</code></pre>
                        
                        
                    </div>
                    

                    
                </div>
            </div>
            
            <div class="finding"
                 data-status="fail"
                 data-tier="2"
                 data-category="Git & Version Control"
                 data-score="0"
                 data-name="conventional commit messages">

                <div class="finding-header" onclick="toggleFinding(this)">
                    <div class="finding-title">
                        <span class="status-icon">
                            ‚ùå
                            
                        </span>
                        <div class="finding-info">
                            <h3>Conventional Commit Messages</h3>
                            <div class="finding-meta">
                                Git & Version Control ‚Ä¢
                                <span class="tier-badge tier-2">Tier 2</span>
                                 ‚Ä¢ not configured
                            </div>
                        </div>
                    </div>
                    
                    <div class="score-display score-fail">
                        0
                    </div>
                    
                </div>

                <div class="finding-body">
                    
                    <div class="finding-section">
                        <h4>Evidence</h4>
                        <ul class="evidence-list">
                            
                            <li>No commitlint or husky configuration</li>
                            
                        </ul>
                    </div>
                    

                    
                    <div class="finding-section">
                        <h4>Remediation</h4>
                        <p><strong>Configure conventional commits with commitlint</strong></p>

                        
                        <ol class="remediation-steps">
                            
                            <li>Install commitlint</li>
                            
                            <li>Configure husky for commit-msg hook</li>
                            
                        </ol>
                        

                        
                        <h4 style="margin-top: 15px;">Commands</h4>
                        <pre><code>npm install --save-dev @commitlint/cli @commitlint/config-conventional husky</code></pre>
                        

                        
                    </div>
                    

                    
                </div>
            </div>
            
            <div class="finding"
                 data-status="pass"
                 data-tier="2"
                 data-category="Git & Version Control"
                 data-score="100.0"
                 data-name=".gitignore completeness">

                <div class="finding-header" onclick="toggleFinding(this)">
                    <div class="finding-title">
                        <span class="status-icon">
                            ‚úÖ
                            
                        </span>
                        <div class="finding-info">
                            <h3>.gitignore Completeness</h3>
                            <div class="finding-meta">
                                Git & Version Control ‚Ä¢
                                <span class="tier-badge tier-2">Tier 2</span>
                                 ‚Ä¢ 580 bytes
                            </div>
                        </div>
                    </div>
                    
                    <div class="score-display score-pass">
                        100
                    </div>
                    
                </div>

                <div class="finding-body">
                    
                    <div class="finding-section">
                        <h4>Evidence</h4>
                        <ul class="evidence-list">
                            
                            <li>.gitignore found (580 bytes)</li>
                            
                        </ul>
                    </div>
                    

                    

                    
                </div>
            </div>
            
            <div class="finding"
                 data-status="pass"
                 data-tier="3"
                 data-category="Code Quality"
                 data-score="100.0"
                 data-name="cyclomatic complexity thresholds">

                <div class="finding-header" onclick="toggleFinding(this)">
                    <div class="finding-title">
                        <span class="status-icon">
                            ‚úÖ
                            
                        </span>
                        <div class="finding-info">
                            <h3>Cyclomatic Complexity Thresholds</h3>
                            <div class="finding-meta">
                                Code Quality ‚Ä¢
                                <span class="tier-badge tier-3">Tier 3</span>
                                 ‚Ä¢ 3.0
                            </div>
                        </div>
                    </div>
                    
                    <div class="score-display score-pass">
                        100
                    </div>
                    
                </div>

                <div class="finding-body">
                    
                    <div class="finding-section">
                        <h4>Evidence</h4>
                        <ul class="evidence-list">
                            
                            <li>Average cyclomatic complexity: 3.0</li>
                            
                        </ul>
                    </div>
                    

                    

                    
                </div>
            </div>
            
            <div class="finding"
                 data-status="not_applicable"
                 data-tier="2"
                 data-category="Build & Development"
                 data-score="0"
                 data-name="one-command build/setup">

                <div class="finding-header" onclick="toggleFinding(this)">
                    <div class="finding-title">
                        <span class="status-icon">
                            ‚äò
                            
                        </span>
                        <div class="finding-info">
                            <h3>One-Command Build/Setup</h3>
                            <div class="finding-meta">
                                Build & Development ‚Ä¢
                                <span class="tier-badge tier-2">Tier 2</span>
                                
                            </div>
                        </div>
                    </div>
                    
                    <div class="score-display score-skip">‚Äî</div>
                    
                </div>

                <div class="finding-body">
                    
                    <div class="finding-section">
                        <h4>Evidence</h4>
                        <ul class="evidence-list">
                            
                            <li>One-Command Build/Setup assessment not yet implemented</li>
                            
                        </ul>
                    </div>
                    

                    

                    
                </div>
            </div>
            
            <div class="finding"
                 data-status="not_applicable"
                 data-tier="2"
                 data-category="Context Window Optimization"
                 data-score="0"
                 data-name="concise structured documentation">

                <div class="finding-header" onclick="toggleFinding(this)">
                    <div class="finding-title">
                        <span class="status-icon">
                            ‚äò
                            
                        </span>
                        <div class="finding-info">
                            <h3>Concise Structured Documentation</h3>
                            <div class="finding-meta">
                                Context Window Optimization ‚Ä¢
                                <span class="tier-badge tier-2">Tier 2</span>
                                
                            </div>
                        </div>
                    </div>
                    
                    <div class="score-display score-skip">‚Äî</div>
                    
                </div>

                <div class="finding-body">
                    
                    <div class="finding-section">
                        <h4>Evidence</h4>
                        <ul class="evidence-list">
                            
                            <li>Concise Structured Documentation assessment not yet implemented</li>
                            
                        </ul>
                    </div>
                    

                    

                    
                </div>
            </div>
            
            <div class="finding"
                 data-status="not_applicable"
                 data-tier="2"
                 data-category="Documentation Standards"
                 data-score="0"
                 data-name="inline documentation">

                <div class="finding-header" onclick="toggleFinding(this)">
                    <div class="finding-title">
                        <span class="status-icon">
                            ‚äò
                            
                        </span>
                        <div class="finding-info">
                            <h3>Inline Documentation</h3>
                            <div class="finding-meta">
                                Documentation Standards ‚Ä¢
                                <span class="tier-badge tier-2">Tier 2</span>
                                
                            </div>
                        </div>
                    </div>
                    
                    <div class="score-display score-skip">‚Äî</div>
                    
                </div>

                <div class="finding-body">
                    
                    <div class="finding-section">
                        <h4>Evidence</h4>
                        <ul class="evidence-list">
                            
                            <li>Inline Documentation assessment not yet implemented</li>
                            
                        </ul>
                    </div>
                    

                    

                    
                </div>
            </div>
            
            <div class="finding"
                 data-status="not_applicable"
                 data-tier="2"
                 data-category="Context Window Optimization"
                 data-score="0"
                 data-name="file size limits">

                <div class="finding-header" onclick="toggleFinding(this)">
                    <div class="finding-title">
                        <span class="status-icon">
                            ‚äò
                            
                        </span>
                        <div class="finding-info">
                            <h3>File Size Limits</h3>
                            <div class="finding-meta">
                                Context Window Optimization ‚Ä¢
                                <span class="tier-badge tier-2">Tier 2</span>
                                
                            </div>
                        </div>
                    </div>
                    
                    <div class="score-display score-skip">‚Äî</div>
                    
                </div>

                <div class="finding-body">
                    
                    <div class="finding-section">
                        <h4>Evidence</h4>
                        <ul class="evidence-list">
                            
                            <li>File Size Limits assessment not yet implemented</li>
                            
                        </ul>
                    </div>
                    

                    

                    
                </div>
            </div>
            
            <div class="finding"
                 data-status="not_applicable"
                 data-tier="2"
                 data-category="Dependency Management"
                 data-score="0"
                 data-name="dependency freshness & security">

                <div class="finding-header" onclick="toggleFinding(this)">
                    <div class="finding-title">
                        <span class="status-icon">
                            ‚äò
                            
                        </span>
                        <div class="finding-info">
                            <h3>Dependency Freshness & Security</h3>
                            <div class="finding-meta">
                                Dependency Management ‚Ä¢
                                <span class="tier-badge tier-2">Tier 2</span>
                                
                            </div>
                        </div>
                    </div>
                    
                    <div class="score-display score-skip">‚Äî</div>
                    
                </div>

                <div class="finding-body">
                    
                    <div class="finding-section">
                        <h4>Evidence</h4>
                        <ul class="evidence-list">
                            
                            <li>Dependency Freshness & Security assessment not yet implemented</li>
                            
                        </ul>
                    </div>
                    

                    

                    
                </div>
            </div>
            
            <div class="finding"
                 data-status="not_applicable"
                 data-tier="2"
                 data-category="Repository Structure"
                 data-score="0"
                 data-name="separation of concerns">

                <div class="finding-header" onclick="toggleFinding(this)">
                    <div class="finding-title">
                        <span class="status-icon">
                            ‚äò
                            
                        </span>
                        <div class="finding-info">
                            <h3>Separation of Concerns</h3>
                            <div class="finding-meta">
                                Repository Structure ‚Ä¢
                                <span class="tier-badge tier-2">Tier 2</span>
                                
                            </div>
                        </div>
                    </div>
                    
                    <div class="score-display score-skip">‚Äî</div>
                    
                </div>

                <div class="finding-body">
                    
                    <div class="finding-section">
                        <h4>Evidence</h4>
                        <ul class="evidence-list">
                            
                            <li>Separation of Concerns assessment not yet implemented</li>
                            
                        </ul>
                    </div>
                    

                    

                    
                </div>
            </div>
            
            <div class="finding"
                 data-status="not_applicable"
                 data-tier="3"
                 data-category="Error Handling"
                 data-score="0"
                 data-name="structured logging">

                <div class="finding-header" onclick="toggleFinding(this)">
                    <div class="finding-title">
                        <span class="status-icon">
                            ‚äò
                            
                        </span>
                        <div class="finding-info">
                            <h3>Structured Logging</h3>
                            <div class="finding-meta">
                                Error Handling ‚Ä¢
                                <span class="tier-badge tier-3">Tier 3</span>
                                
                            </div>
                        </div>
                    </div>
                    
                    <div class="score-display score-skip">‚Äî</div>
                    
                </div>

                <div class="finding-body">
                    
                    <div class="finding-section">
                        <h4>Evidence</h4>
                        <ul class="evidence-list">
                            
                            <li>Structured Logging assessment not yet implemented</li>
                            
                        </ul>
                    </div>
                    

                    

                    
                </div>
            </div>
            
            <div class="finding"
                 data-status="not_applicable"
                 data-tier="3"
                 data-category="API Documentation"
                 data-score="0"
                 data-name="openapi/swagger specifications">

                <div class="finding-header" onclick="toggleFinding(this)">
                    <div class="finding-title">
                        <span class="status-icon">
                            ‚äò
                            
                        </span>
                        <div class="finding-info">
                            <h3>OpenAPI/Swagger Specifications</h3>
                            <div class="finding-meta">
                                API Documentation ‚Ä¢
                                <span class="tier-badge tier-3">Tier 3</span>
                                
                            </div>
                        </div>
                    </div>
                    
                    <div class="score-display score-skip">‚Äî</div>
                    
                </div>

                <div class="finding-body">
                    
                    <div class="finding-section">
                        <h4>Evidence</h4>
                        <ul class="evidence-list">
                            
                            <li>OpenAPI/Swagger Specifications assessment not yet implemented</li>
                            
                        </ul>
                    </div>
                    

                    

                    
                </div>
            </div>
            
            <div class="finding"
                 data-status="not_applicable"
                 data-tier="3"
                 data-category="Documentation Standards"
                 data-score="0"
                 data-name="architecture decision records">

                <div class="finding-header" onclick="toggleFinding(this)">
                    <div class="finding-title">
                        <span class="status-icon">
                            ‚äò
                            
                        </span>
                        <div class="finding-info">
                            <h3>Architecture Decision Records</h3>
                            <div class="finding-meta">
                                Documentation Standards ‚Ä¢
                                <span class="tier-badge tier-3">Tier 3</span>
                                
                            </div>
                        </div>
                    </div>
                    
                    <div class="score-display score-skip">‚Äî</div>
                    
                </div>

                <div class="finding-body">
                    
                    <div class="finding-section">
                        <h4>Evidence</h4>
                        <ul class="evidence-list">
                            
                            <li>Architecture Decision Records assessment not yet implemented</li>
                            
                        </ul>
                    </div>
                    

                    

                    
                </div>
            </div>
            
            <div class="finding"
                 data-status="not_applicable"
                 data-tier="3"
                 data-category="Modularity"
                 data-score="0"
                 data-name="semantic file & directory naming">

                <div class="finding-header" onclick="toggleFinding(this)">
                    <div class="finding-title">
                        <span class="status-icon">
                            ‚äò
                            
                        </span>
                        <div class="finding-info">
                            <h3>Semantic File & Directory Naming</h3>
                            <div class="finding-meta">
                                Modularity ‚Ä¢
                                <span class="tier-badge tier-3">Tier 3</span>
                                
                            </div>
                        </div>
                    </div>
                    
                    <div class="score-display score-skip">‚Äî</div>
                    
                </div>

                <div class="finding-body">
                    
                    <div class="finding-section">
                        <h4>Evidence</h4>
                        <ul class="evidence-list">
                            
                            <li>Semantic File & Directory Naming assessment not yet implemented</li>
                            
                        </ul>
                    </div>
                    

                    

                    
                </div>
            </div>
            
            <div class="finding"
                 data-status="not_applicable"
                 data-tier="4"
                 data-category="Security"
                 data-score="0"
                 data-name="security scanning automation">

                <div class="finding-header" onclick="toggleFinding(this)">
                    <div class="finding-title">
                        <span class="status-icon">
                            ‚äò
                            
                        </span>
                        <div class="finding-info">
                            <h3>Security Scanning Automation</h3>
                            <div class="finding-meta">
                                Security ‚Ä¢
                                <span class="tier-badge tier-4">Tier 4</span>
                                
                            </div>
                        </div>
                    </div>
                    
                    <div class="score-display score-skip">‚Äî</div>
                    
                </div>

                <div class="finding-body">
                    
                    <div class="finding-section">
                        <h4>Evidence</h4>
                        <ul class="evidence-list">
                            
                            <li>Security Scanning Automation assessment not yet implemented</li>
                            
                        </ul>
                    </div>
                    

                    

                    
                </div>
            </div>
            
            <div class="finding"
                 data-status="not_applicable"
                 data-tier="4"
                 data-category="Performance"
                 data-score="0"
                 data-name="performance benchmarks">

                <div class="finding-header" onclick="toggleFinding(this)">
                    <div class="finding-title">
                        <span class="status-icon">
                            ‚äò
                            
                        </span>
                        <div class="finding-info">
                            <h3>Performance Benchmarks</h3>
                            <div class="finding-meta">
                                Performance ‚Ä¢
                                <span class="tier-badge tier-4">Tier 4</span>
                                
                            </div>
                        </div>
                    </div>
                    
                    <div class="score-display score-skip">‚Äî</div>
                    
                </div>

                <div class="finding-body">
                    
                    <div class="finding-section">
                        <h4>Evidence</h4>
                        <ul class="evidence-list">
                            
                            <li>Performance Benchmarks assessment not yet implemented</li>
                            
                        </ul>
                    </div>
                    

                    

                    
                </div>
            </div>
            
            <div class="finding"
                 data-status="not_applicable"
                 data-tier="4"
                 data-category="Code Quality"
                 data-score="0"
                 data-name="code smell elimination">

                <div class="finding-header" onclick="toggleFinding(this)">
                    <div class="finding-title">
                        <span class="status-icon">
                            ‚äò
                            
                        </span>
                        <div class="finding-info">
                            <h3>Code Smell Elimination</h3>
                            <div class="finding-meta">
                                Code Quality ‚Ä¢
                                <span class="tier-badge tier-4">Tier 4</span>
                                
                            </div>
                        </div>
                    </div>
                    
                    <div class="score-display score-skip">‚Äî</div>
                    
                </div>

                <div class="finding-body">
                    
                    <div class="finding-section">
                        <h4>Evidence</h4>
                        <ul class="evidence-list">
                            
                            <li>Code Smell Elimination assessment not yet implemented</li>
                            
                        </ul>
                    </div>
                    

                    

                    
                </div>
            </div>
            
            <div class="finding"
                 data-status="not_applicable"
                 data-tier="4"
                 data-category="Git & Version Control"
                 data-score="0"
                 data-name="issue & pull request templates">

                <div class="finding-header" onclick="toggleFinding(this)">
                    <div class="finding-title">
                        <span class="status-icon">
                            ‚äò
                            
                        </span>
                        <div class="finding-info">
                            <h3>Issue & Pull Request Templates</h3>
                            <div class="finding-meta">
                                Git & Version Control ‚Ä¢
                                <span class="tier-badge tier-4">Tier 4</span>
                                
                            </div>
                        </div>
                    </div>
                    
                    <div class="score-display score-skip">‚Äî</div>
                    
                </div>

                <div class="finding-body">
                    
                    <div class="finding-section">
                        <h4>Evidence</h4>
                        <ul class="evidence-list">
                            
                            <li>Issue & Pull Request Templates assessment not yet implemented</li>
                            
                        </ul>
                    </div>
                    

                    

                    
                </div>
            </div>
            
            <div class="finding"
                 data-status="not_applicable"
                 data-tier="4"
                 data-category="Build & Development"
                 data-score="0"
                 data-name="container/virtualization setup">

                <div class="finding-header" onclick="toggleFinding(this)">
                    <div class="finding-title">
                        <span class="status-icon">
                            ‚äò
                            
                        </span>
                        <div class="finding-info">
                            <h3>Container/Virtualization Setup</h3>
                            <div class="finding-meta">
                                Build & Development ‚Ä¢
                                <span class="tier-badge tier-4">Tier 4</span>
                                
                            </div>
                        </div>
                    </div>
                    
                    <div class="score-display score-skip">‚Äî</div>
                    
                </div>

                <div class="finding-body">
                    
                    <div class="finding-section">
                        <h4>Evidence</h4>
                        <ul class="evidence-list">
                            
                            <li>Container/Virtualization Setup assessment not yet implemented</li>
                            
                        </ul>
                    </div>
                    

                    

                    
                </div>
            </div>
            
        </div>

        <footer>
            <p>Generated by <strong>AgentReady v1.0.0</strong></p>
            <p>Repository: /Users/jeder/repos/sk/agentready ‚Ä¢ Branch: 001-agentready-scorer ‚Ä¢ Commit: 54a96cba</p>
            <p style="margin-top: 10px; font-size: 0.85rem;">
                ü§ñ Generated with <a href="https://claude.com/claude-code" style="color: #2563eb;">Claude Code</a>
            </p>
        </footer>
    </div>

    <script>
        // Embedded assessment data
        const ASSESSMENT = {"repository": {"path": "/Users/jeder/repos/sk/agentready", "name": "agentready", "url": null, "branch": "001-agentready-scorer", "commit_hash": "54a96cbafc37dfc0f67dc12833b3c789ee81bc53", "languages": {"Markdown": 36, "Shell": 5, "Python": 23}, "total_files": 70, "total_lines": 12380}, "timestamp": "2025-11-21T01:53:31.339778", "overall_score": 75.4, "certification_level": "Gold", "attributes_assessed": 10, "attributes_skipped": 15, "attributes_total": 25, "findings": [{"attribute": {"id": "claude_md_file", "name": "CLAUDE.md Configuration Files", "category": "Context Window Optimization", "tier": 1, "description": "Project-specific configuration for Claude Code", "criteria": "CLAUDE.md file exists in repository root", "default_weight": 0.1}, "status": "pass", "score": 100.0, "measured_value": "present", "threshold": "present", "evidence": ["CLAUDE.md found at /Users/jeder/repos/sk/agentready/CLAUDE.md"], "remediation": null, "error_message": null}, {"attribute": {"id": "readme_structure", "name": "README Structure", "category": "Documentation Standards", "tier": 1, "description": "Well-structured README with key sections", "criteria": "README.md with installation, usage, and development sections", "default_weight": 0.1}, "status": "pass", "score": 100.0, "measured_value": "3/3 sections", "threshold": "3/3 sections", "evidence": ["Found 3/3 essential sections", "Installation: \u2713", "Usage: \u2713", "Development: \u2713"], "remediation": null, "error_message": null}, {"attribute": {"id": "type_annotations", "name": "Type Annotations", "category": "Code Quality", "tier": 1, "description": "Type hints in function signatures", "criteria": ">80% of functions have type annotations", "default_weight": 0.1}, "status": "pass", "score": 100.0, "measured_value": "94.9%", "threshold": "\u226580%", "evidence": ["Typed functions: 112/118", "Coverage: 94.9%"], "remediation": null, "error_message": null}, {"attribute": {"id": "standard_layout", "name": "Standard Project Layouts", "category": "Repository Structure", "tier": 1, "description": "Follows standard project structure for language", "criteria": "Standard directories (src/, tests/, docs/) present", "default_weight": 0.1}, "status": "pass", "score": 100.0, "measured_value": "2/2 directories", "threshold": "2/2 directories", "evidence": ["Found 2/2 standard directories", "src/: \u2713", "tests/: \u2713"], "remediation": null, "error_message": null}, {"attribute": {"id": "lock_files", "name": "Lock Files for Reproducibility", "category": "Dependency Management", "tier": 1, "description": "Lock files present for dependency pinning", "criteria": "package-lock.json, yarn.lock, poetry.lock, or requirements.txt with versions", "default_weight": 0.1}, "status": "fail", "score": 0.0, "measured_value": "none", "threshold": "at least one lock file", "evidence": ["No lock files found"], "remediation": {"summary": "Add lock file for dependency reproducibility", "steps": ["Use npm install, poetry lock, or equivalent to generate lock file"], "tools": [], "commands": ["npm install  # generates package-lock.json"], "examples": [], "citations": []}, "error_message": null}, {"attribute": {"id": "test_coverage", "name": "Test Coverage Requirements", "category": "Testing & CI/CD", "tier": 2, "description": "Test coverage thresholds configured and enforced", "criteria": ">80% code coverage", "default_weight": 0.03}, "status": "pass", "score": 100.0, "measured_value": "configured", "threshold": "configured with >80% threshold", "evidence": ["Coverage configuration found", "pytest-cov dependency present"], "remediation": null, "error_message": null}, {"attribute": {"id": "precommit_hooks", "name": "Pre-commit Hooks & CI/CD Linting", "category": "Testing & CI/CD", "tier": 2, "description": "Pre-commit hooks configured for linting and formatting", "criteria": ".pre-commit-config.yaml exists", "default_weight": 0.03}, "status": "fail", "score": 0.0, "measured_value": "not configured", "threshold": "configured", "evidence": [".pre-commit-config.yaml not found"], "remediation": {"summary": "Configure pre-commit hooks for automated code quality checks", "steps": ["Install pre-commit framework", "Create .pre-commit-config.yaml", "Add hooks for linting and formatting", "Install hooks: pre-commit install", "Run on all files: pre-commit run --all-files"], "tools": ["pre-commit"], "commands": ["pip install pre-commit", "pre-commit install", "pre-commit run --all-files"], "examples": ["# .pre-commit-config.yaml\nrepos:\n  - repo: https://github.com/pre-commit/pre-commit-hooks\n    rev: v4.4.0\n    hooks:\n      - id: trailing-whitespace\n      - id: end-of-file-fixer\n      - id: check-yaml\n      - id: check-added-large-files\n\n  - repo: https://github.com/psf/black\n    rev: 23.3.0\n    hooks:\n      - id: black\n\n  - repo: https://github.com/pycqa/isort\n    rev: 5.12.0\n    hooks:\n      - id: isort\n"], "citations": [{"source": "pre-commit.com", "title": "Pre-commit Framework", "url": "https://pre-commit.com/", "relevance": "Official pre-commit documentation"}]}, "error_message": null}, {"attribute": {"id": "conventional_commits", "name": "Conventional Commit Messages", "category": "Git & Version Control", "tier": 2, "description": "Follows conventional commit format", "criteria": "\u226580% of recent commits follow convention", "default_weight": 0.03}, "status": "fail", "score": 0.0, "measured_value": "not configured", "threshold": "configured", "evidence": ["No commitlint or husky configuration"], "remediation": {"summary": "Configure conventional commits with commitlint", "steps": ["Install commitlint", "Configure husky for commit-msg hook"], "tools": ["commitlint", "husky"], "commands": ["npm install --save-dev @commitlint/cli @commitlint/config-conventional husky"], "examples": [], "citations": []}, "error_message": null}, {"attribute": {"id": "gitignore_completeness", "name": ".gitignore Completeness", "category": "Git & Version Control", "tier": 2, "description": "Comprehensive .gitignore file", "criteria": ".gitignore exists and covers common patterns", "default_weight": 0.03}, "status": "pass", "score": 100.0, "measured_value": "580 bytes", "threshold": ">50 bytes", "evidence": [".gitignore found (580 bytes)"], "remediation": null, "error_message": null}, {"attribute": {"id": "cyclomatic_complexity", "name": "Cyclomatic Complexity Thresholds", "category": "Code Quality", "tier": 3, "description": "Cyclomatic complexity thresholds enforced", "criteria": "Average complexity <10, no functions >15", "default_weight": 0.03}, "status": "pass", "score": 100.0, "measured_value": "3.0", "threshold": "<10.0", "evidence": ["Average cyclomatic complexity: 3.0"], "remediation": null, "error_message": null}, {"attribute": {"id": "one_command_setup", "name": "One-Command Build/Setup", "category": "Build & Development", "tier": 2, "description": "Assessment for One-Command Build/Setup", "criteria": "To be implemented", "default_weight": 0.03}, "status": "not_applicable", "score": null, "measured_value": null, "threshold": null, "evidence": ["One-Command Build/Setup assessment not yet implemented"], "remediation": null, "error_message": null}, {"attribute": {"id": "concise_documentation", "name": "Concise Structured Documentation", "category": "Context Window Optimization", "tier": 2, "description": "Assessment for Concise Structured Documentation", "criteria": "To be implemented", "default_weight": 0.03}, "status": "not_applicable", "score": null, "measured_value": null, "threshold": null, "evidence": ["Concise Structured Documentation assessment not yet implemented"], "remediation": null, "error_message": null}, {"attribute": {"id": "inline_documentation", "name": "Inline Documentation", "category": "Documentation Standards", "tier": 2, "description": "Assessment for Inline Documentation", "criteria": "To be implemented", "default_weight": 0.03}, "status": "not_applicable", "score": null, "measured_value": null, "threshold": null, "evidence": ["Inline Documentation assessment not yet implemented"], "remediation": null, "error_message": null}, {"attribute": {"id": "file_size_limits", "name": "File Size Limits", "category": "Context Window Optimization", "tier": 2, "description": "Assessment for File Size Limits", "criteria": "To be implemented", "default_weight": 0.03}, "status": "not_applicable", "score": null, "measured_value": null, "threshold": null, "evidence": ["File Size Limits assessment not yet implemented"], "remediation": null, "error_message": null}, {"attribute": {"id": "dependency_freshness", "name": "Dependency Freshness & Security", "category": "Dependency Management", "tier": 2, "description": "Assessment for Dependency Freshness & Security", "criteria": "To be implemented", "default_weight": 0.03}, "status": "not_applicable", "score": null, "measured_value": null, "threshold": null, "evidence": ["Dependency Freshness & Security assessment not yet implemented"], "remediation": null, "error_message": null}, {"attribute": {"id": "separation_concerns", "name": "Separation of Concerns", "category": "Repository Structure", "tier": 2, "description": "Assessment for Separation of Concerns", "criteria": "To be implemented", "default_weight": 0.03}, "status": "not_applicable", "score": null, "measured_value": null, "threshold": null, "evidence": ["Separation of Concerns assessment not yet implemented"], "remediation": null, "error_message": null}, {"attribute": {"id": "structured_logging", "name": "Structured Logging", "category": "Error Handling", "tier": 3, "description": "Assessment for Structured Logging", "criteria": "To be implemented", "default_weight": 0.03}, "status": "not_applicable", "score": null, "measured_value": null, "threshold": null, "evidence": ["Structured Logging assessment not yet implemented"], "remediation": null, "error_message": null}, {"attribute": {"id": "openapi_specs", "name": "OpenAPI/Swagger Specifications", "category": "API Documentation", "tier": 3, "description": "Assessment for OpenAPI/Swagger Specifications", "criteria": "To be implemented", "default_weight": 0.03}, "status": "not_applicable", "score": null, "measured_value": null, "threshold": null, "evidence": ["OpenAPI/Swagger Specifications assessment not yet implemented"], "remediation": null, "error_message": null}, {"attribute": {"id": "architecture_decisions", "name": "Architecture Decision Records", "category": "Documentation Standards", "tier": 3, "description": "Assessment for Architecture Decision Records", "criteria": "To be implemented", "default_weight": 0.03}, "status": "not_applicable", "score": null, "measured_value": null, "threshold": null, "evidence": ["Architecture Decision Records assessment not yet implemented"], "remediation": null, "error_message": null}, {"attribute": {"id": "semantic_naming", "name": "Semantic File & Directory Naming", "category": "Modularity", "tier": 3, "description": "Assessment for Semantic File & Directory Naming", "criteria": "To be implemented", "default_weight": 0.03}, "status": "not_applicable", "score": null, "measured_value": null, "threshold": null, "evidence": ["Semantic File & Directory Naming assessment not yet implemented"], "remediation": null, "error_message": null}, {"attribute": {"id": "security_scanning", "name": "Security Scanning Automation", "category": "Security", "tier": 4, "description": "Assessment for Security Scanning Automation", "criteria": "To be implemented", "default_weight": 0.01}, "status": "not_applicable", "score": null, "measured_value": null, "threshold": null, "evidence": ["Security Scanning Automation assessment not yet implemented"], "remediation": null, "error_message": null}, {"attribute": {"id": "performance_benchmarks", "name": "Performance Benchmarks", "category": "Performance", "tier": 4, "description": "Assessment for Performance Benchmarks", "criteria": "To be implemented", "default_weight": 0.01}, "status": "not_applicable", "score": null, "measured_value": null, "threshold": null, "evidence": ["Performance Benchmarks assessment not yet implemented"], "remediation": null, "error_message": null}, {"attribute": {"id": "code_smells", "name": "Code Smell Elimination", "category": "Code Quality", "tier": 4, "description": "Assessment for Code Smell Elimination", "criteria": "To be implemented", "default_weight": 0.01}, "status": "not_applicable", "score": null, "measured_value": null, "threshold": null, "evidence": ["Code Smell Elimination assessment not yet implemented"], "remediation": null, "error_message": null}, {"attribute": {"id": "issue_pr_templates", "name": "Issue & Pull Request Templates", "category": "Git & Version Control", "tier": 4, "description": "Assessment for Issue & Pull Request Templates", "criteria": "To be implemented", "default_weight": 0.01}, "status": "not_applicable", "score": null, "measured_value": null, "threshold": null, "evidence": ["Issue & Pull Request Templates assessment not yet implemented"], "remediation": null, "error_message": null}, {"attribute": {"id": "container_setup", "name": "Container/Virtualization Setup", "category": "Build & Development", "tier": 4, "description": "Assessment for Container/Virtualization Setup", "criteria": "To be implemented", "default_weight": 0.01}, "status": "not_applicable", "score": null, "measured_value": null, "threshold": null, "evidence": ["Container/Virtualization Setup assessment not yet implemented"], "remediation": null, "error_message": null}], "config": null, "duration_seconds": 0.3};

        // Toggle finding expansion
        function toggleFinding(header) {
            const finding = header.parentElement;
            finding.classList.toggle('expanded');
        }

        // Filter functionality
        document.querySelectorAll('.filter-btn').forEach(btn => {
            btn.addEventListener('click', () => {
                // Update active button
                document.querySelectorAll('.filter-btn').forEach(b => b.classList.remove('active'));
                btn.classList.add('active');

                const filter = btn.dataset.filter;
                const findings = document.querySelectorAll('.finding');

                findings.forEach(finding => {
                    const status = finding.dataset.status;

                    if (filter === 'all') {
                        finding.classList.remove('hidden');
                    } else if (filter === 'skipped') {
                        if (status === 'skipped' || status === 'not_applicable' || status === 'error') {
                            finding.classList.remove('hidden');
                        } else {
                            finding.classList.add('hidden');
                        }
                    } else {
                        if (status === filter) {
                            finding.classList.remove('hidden');
                        } else {
                            finding.classList.add('hidden');
                        }
                    }
                });
            });
        });

        // Sort functionality
        document.getElementById('sort-select').addEventListener('change', (e) => {
            const sortBy = e.target.value;
            const container = document.getElementById('findings-container');
            const findings = Array.from(container.querySelectorAll('.finding'));

            findings.sort((a, b) => {
                if (sortBy === 'category') {
                    return a.dataset.category.localeCompare(b.dataset.category);
                } else if (sortBy === 'score-desc') {
                    return parseFloat(b.dataset.score) - parseFloat(a.dataset.score);
                } else if (sortBy === 'score-asc') {
                    return parseFloat(a.dataset.score) - parseFloat(b.dataset.score);
                } else if (sortBy === 'tier') {
                    return parseInt(a.dataset.tier) - parseInt(b.dataset.tier);
                }
            });

            findings.forEach(finding => container.appendChild(finding));
        });

        // Search functionality
        document.getElementById('search-box').addEventListener('input', (e) => {
            const query = e.target.value.toLowerCase();
            const findings = document.querySelectorAll('.finding');

            findings.forEach(finding => {
                const name = finding.dataset.name;
                const category = finding.dataset.category.toLowerCase();

                if (name.includes(query) || category.includes(query)) {
                    finding.classList.remove('hidden');
                } else {
                    finding.classList.add('hidden');
                }
            });
        });
    </script>
</body>
</html>
</file>

<file path=".agentready/report-20251121-015727.html">
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>AgentReady Assessment - agentready</title>
    <style>
        /* Reset and Base Styles */
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif;
            line-height: 1.6;
            color: #333;
            background: #f5f5f5;
            padding: 20px;
        }

        .container {
            max-width: 1200px;
            margin: 0 auto;
            background: white;
            padding: 40px;
            border-radius: 8px;
            box-shadow: 0 2px 8px rgba(0,0,0,0.1);
        }

        /* Header */
        header {
            border-bottom: 3px solid #2563eb;
            padding-bottom: 20px;
            margin-bottom: 30px;
        }

        h1 {
            font-size: 2rem;
            color: #1e293b;
            margin-bottom: 10px;
        }

        .subtitle {
            color: #64748b;
            font-size: 1rem;
        }

        /* Summary Cards */
        .summary-grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(200px, 1fr));
            gap: 20px;
            margin: 30px 0;
        }

        .summary-card {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 20px;
            border-radius: 8px;
            text-align: center;
        }

        .summary-card.score {
            background: linear-gradient(135deg, #f093fb 0%, #f5576c 100%);
        }

        .summary-card.certification {
            background: linear-gradient(135deg, #4facfe 0%, #00f2fe 100%);
        }

        .summary-card.assessed {
            background: linear-gradient(135deg, #43e97b 0%, #38f9d7 100%);
        }

        .summary-card.duration {
            background: linear-gradient(135deg, #fa709a 0%, #fee140 100%);
        }

        .summary-card h3 {
            font-size: 0.9rem;
            font-weight: 500;
            opacity: 0.9;
            margin-bottom: 10px;
        }

        .summary-card .value {
            font-size: 2rem;
            font-weight: 700;
        }

        /* Controls */
        .controls {
            display: flex;
            flex-wrap: wrap;
            gap: 15px;
            margin: 30px 0;
            padding: 20px;
            background: #f8fafc;
            border-radius: 8px;
        }

        .control-group {
            display: flex;
            align-items: center;
            gap: 10px;
        }

        .control-group label {
            font-weight: 600;
            color: #475569;
            font-size: 0.9rem;
        }

        select, input[type="search"] {
            padding: 8px 12px;
            border: 1px solid #cbd5e1;
            border-radius: 6px;
            font-size: 0.9rem;
            background: white;
        }

        input[type="search"] {
            min-width: 250px;
        }

        .filter-btn {
            padding: 8px 16px;
            border: 2px solid #e2e8f0;
            background: white;
            border-radius: 6px;
            cursor: pointer;
            font-size: 0.9rem;
            transition: all 0.2s;
        }

        .filter-btn:hover {
            border-color: #2563eb;
            background: #eff6ff;
        }

        .filter-btn.active {
            background: #2563eb;
            color: white;
            border-color: #2563eb;
        }

        .badge {
            display: inline-block;
            padding: 2px 8px;
            border-radius: 12px;
            font-size: 0.75rem;
            font-weight: 600;
            margin-left: 5px;
        }

        /* Certification Ladder */
        .cert-ladder {
            display: flex;
            justify-content: space-between;
            margin: 30px 0;
            padding: 20px;
            background: #f8fafc;
            border-radius: 8px;
        }

        .cert-level {
            flex: 1;
            text-align: center;
            padding: 15px 10px;
            border-radius: 6px;
            margin: 0 5px;
            transition: all 0.3s;
        }

        .cert-level.platinum { background: linear-gradient(135deg, #e0e7ff, #c7d2fe); }
        .cert-level.gold { background: linear-gradient(135deg, #fef3c7, #fde68a); }
        .cert-level.silver { background: linear-gradient(135deg, #e5e7eb, #d1d5db); }
        .cert-level.bronze { background: linear-gradient(135deg, #fed7aa, #fdba74); }
        .cert-level.needs-improvement { background: linear-gradient(135deg, #fecaca, #fca5a5); }

        .cert-level.active {
            transform: scale(1.1);
            box-shadow: 0 4px 12px rgba(0,0,0,0.15);
            border: 3px solid #2563eb;
        }

        .cert-level h4 {
            font-size: 0.9rem;
            margin-bottom: 5px;
        }

        .cert-level .range {
            font-size: 0.8rem;
            opacity: 0.7;
        }

        /* Findings */
        .findings {
            margin: 30px 0;
        }

        .finding {
            border: 1px solid #e2e8f0;
            border-radius: 8px;
            margin-bottom: 15px;
            overflow: hidden;
            transition: all 0.2s;
        }

        .finding:hover {
            box-shadow: 0 4px 12px rgba(0,0,0,0.08);
        }

        .finding-header {
            padding: 15px 20px;
            background: #f8fafc;
            cursor: pointer;
            display: flex;
            justify-content: space-between;
            align-items: center;
        }

        .finding-header:hover {
            background: #f1f5f9;
        }

        .finding-title {
            display: flex;
            align-items: center;
            gap: 15px;
            flex: 1;
        }

        .status-icon {
            font-size: 1.5rem;
        }

        .finding-info h3 {
            font-size: 1rem;
            color: #1e293b;
            margin-bottom: 3px;
        }

        .finding-meta {
            font-size: 0.85rem;
            color: #64748b;
        }

        .tier-badge {
            padding: 4px 12px;
            border-radius: 12px;
            font-size: 0.75rem;
            font-weight: 600;
        }

        .tier-1 { background: #fee2e2; color: #991b1b; }
        .tier-2 { background: #fed7aa; color: #9a3412; }
        .tier-3 { background: #fef3c7; color: #92400e; }
        .tier-4 { background: #d1fae5; color: #065f46; }

        .score-display {
            font-size: 1.5rem;
            font-weight: 700;
            min-width: 80px;
            text-align: right;
        }

        .score-pass { color: #059669; }
        .score-fail { color: #dc2626; }
        .score-skip { color: #6b7280; }

        .finding-body {
            padding: 20px;
            display: none;
        }

        .finding.expanded .finding-body {
            display: block;
        }

        .finding-section {
            margin-bottom: 20px;
        }

        .finding-section h4 {
            font-size: 0.9rem;
            color: #475569;
            margin-bottom: 10px;
            text-transform: uppercase;
            letter-spacing: 0.05em;
        }

        .evidence-list, .remediation-steps {
            list-style: none;
            padding-left: 0;
        }

        .evidence-list li {
            padding: 8px 0;
            border-bottom: 1px solid #f1f5f9;
        }

        .evidence-list li:before {
            content: "‚ñ∏ ";
            color: #2563eb;
            font-weight: 700;
            margin-right: 8px;
        }

        .remediation-steps li {
            padding: 10px 0 10px 30px;
            position: relative;
        }

        .remediation-steps li:before {
            content: counter(step);
            counter-increment: step;
            position: absolute;
            left: 0;
            top: 10px;
            background: #2563eb;
            color: white;
            width: 20px;
            height: 20px;
            border-radius: 50%;
            display: flex;
            align-items: center;
            justify-content: center;
            font-size: 0.75rem;
            font-weight: 700;
        }

        .remediation-steps {
            counter-reset: step;
        }

        code {
            background: #f1f5f9;
            padding: 2px 6px;
            border-radius: 3px;
            font-family: 'Courier New', monospace;
            font-size: 0.9em;
        }

        pre {
            background: #1e293b;
            color: #e2e8f0;
            padding: 15px;
            border-radius: 6px;
            overflow-x: auto;
            margin: 10px 0;
        }

        pre code {
            background: none;
            color: inherit;
            padding: 0;
        }

        /* Footer */
        footer {
            margin-top: 50px;
            padding-top: 20px;
            border-top: 1px solid #e2e8f0;
            text-align: center;
            color: #64748b;
            font-size: 0.9rem;
        }

        /* Responsive */
        @media (max-width: 768px) {
            .container {
                padding: 20px;
            }

            .summary-grid {
                grid-template-columns: 1fr;
            }

            .controls {
                flex-direction: column;
            }

            input[type="search"] {
                min-width: 100%;
            }

            .cert-ladder {
                flex-direction: column;
            }

            .cert-level {
                margin: 5px 0;
            }
        }

        /* Hidden class */
        .hidden {
            display: none !important;
        }
    </style>
</head>
<body>
    <div class="container">
        <header>
            <h1>ü§ñ AgentReady Assessment Report</h1>
            <p class="subtitle">agentready ‚Ä¢ November 21, 2025 at 01:57</p>
        </header>

        <!-- Summary Cards -->
        <div class="summary-grid">
            <div class="summary-card score">
                <h3>Overall Score</h3>
                <div class="value">75.4</div>
            </div>
            <div class="summary-card certification">
                <h3>Certification</h3>
                <div class="value">Gold</div>
            </div>
            <div class="summary-card assessed">
                <h3>Assessed</h3>
                <div class="value">10/25</div>
            </div>
            <div class="summary-card duration">
                <h3>Duration</h3>
                <div class="value">0.1s</div>
            </div>
        </div>

        <!-- Certification Ladder -->
        <div class="cert-ladder">
            <div class="cert-level platinum ">
                <h4>üíé Platinum</h4>
                <p class="range">90-100</p>
            </div>
            <div class="cert-level gold active">
                <h4>ü•á Gold</h4>
                <p class="range">75-89</p>
            </div>
            <div class="cert-level silver ">
                <h4>ü•à Silver</h4>
                <p class="range">60-74</p>
            </div>
            <div class="cert-level bronze ">
                <h4>ü•â Bronze</h4>
                <p class="range">40-59</p>
            </div>
            <div class="cert-level needs-improvement ">
                <h4>‚ö†Ô∏è Needs Work</h4>
                <p class="range">0-39</p>
            </div>
        </div>

        <!-- Controls -->
        <div class="controls">
            <div class="control-group">
                <label>Filter:</label>
                <button class="filter-btn active" data-filter="all">All <span class="badge">25</span></button>
                <button class="filter-btn" data-filter="pass">Pass <span class="badge">7</span></button>
                <button class="filter-btn" data-filter="fail">Fail <span class="badge">3</span></button>
                <button class="filter-btn" data-filter="skipped">Skipped <span class="badge">15</span></button>
            </div>

            <div class="control-group">
                <label>Sort:</label>
                <select id="sort-select">
                    <option value="category">Category</option>
                    <option value="score-desc">Score (High to Low)</option>
                    <option value="score-asc">Score (Low to High)</option>
                    <option value="tier">Tier (Essential First)</option>
                </select>
            </div>

            <div class="control-group">
                <input type="search" id="search-box" placeholder="Search attributes...">
            </div>
        </div>

        <!-- Findings -->
        <div class="findings" id="findings-container">
            
            <div class="finding"
                 data-status="pass"
                 data-tier="1"
                 data-category="Context Window Optimization"
                 data-score="100.0"
                 data-name="claude.md configuration files">

                <div class="finding-header" onclick="toggleFinding(this)">
                    <div class="finding-title">
                        <span class="status-icon">
                            ‚úÖ
                            
                        </span>
                        <div class="finding-info">
                            <h3>CLAUDE.md Configuration Files</h3>
                            <div class="finding-meta">
                                Context Window Optimization ‚Ä¢
                                <span class="tier-badge tier-1">Tier 1</span>
                                 ‚Ä¢ present
                            </div>
                        </div>
                    </div>
                    
                    <div class="score-display score-pass">
                        100
                    </div>
                    
                </div>

                <div class="finding-body">
                    
                    <div class="finding-section">
                        <h4>Evidence</h4>
                        <ul class="evidence-list">
                            
                            <li>CLAUDE.md found at /Users/jeder/repos/sk/agentready/CLAUDE.md</li>
                            
                        </ul>
                    </div>
                    

                    

                    
                </div>
            </div>
            
            <div class="finding"
                 data-status="pass"
                 data-tier="1"
                 data-category="Documentation Standards"
                 data-score="100.0"
                 data-name="readme structure">

                <div class="finding-header" onclick="toggleFinding(this)">
                    <div class="finding-title">
                        <span class="status-icon">
                            ‚úÖ
                            
                        </span>
                        <div class="finding-info">
                            <h3>README Structure</h3>
                            <div class="finding-meta">
                                Documentation Standards ‚Ä¢
                                <span class="tier-badge tier-1">Tier 1</span>
                                 ‚Ä¢ 3/3 sections
                            </div>
                        </div>
                    </div>
                    
                    <div class="score-display score-pass">
                        100
                    </div>
                    
                </div>

                <div class="finding-body">
                    
                    <div class="finding-section">
                        <h4>Evidence</h4>
                        <ul class="evidence-list">
                            
                            <li>Found 3/3 essential sections</li>
                            
                            <li>Installation: ‚úì</li>
                            
                            <li>Usage: ‚úì</li>
                            
                            <li>Development: ‚úì</li>
                            
                        </ul>
                    </div>
                    

                    

                    
                </div>
            </div>
            
            <div class="finding"
                 data-status="pass"
                 data-tier="1"
                 data-category="Code Quality"
                 data-score="100.0"
                 data-name="type annotations">

                <div class="finding-header" onclick="toggleFinding(this)">
                    <div class="finding-title">
                        <span class="status-icon">
                            ‚úÖ
                            
                        </span>
                        <div class="finding-info">
                            <h3>Type Annotations</h3>
                            <div class="finding-meta">
                                Code Quality ‚Ä¢
                                <span class="tier-badge tier-1">Tier 1</span>
                                 ‚Ä¢ 95.0%
                            </div>
                        </div>
                    </div>
                    
                    <div class="score-display score-pass">
                        100
                    </div>
                    
                </div>

                <div class="finding-body">
                    
                    <div class="finding-section">
                        <h4>Evidence</h4>
                        <ul class="evidence-list">
                            
                            <li>Typed functions: 115/121</li>
                            
                            <li>Coverage: 95.0%</li>
                            
                        </ul>
                    </div>
                    

                    

                    
                </div>
            </div>
            
            <div class="finding"
                 data-status="pass"
                 data-tier="1"
                 data-category="Repository Structure"
                 data-score="100.0"
                 data-name="standard project layouts">

                <div class="finding-header" onclick="toggleFinding(this)">
                    <div class="finding-title">
                        <span class="status-icon">
                            ‚úÖ
                            
                        </span>
                        <div class="finding-info">
                            <h3>Standard Project Layouts</h3>
                            <div class="finding-meta">
                                Repository Structure ‚Ä¢
                                <span class="tier-badge tier-1">Tier 1</span>
                                 ‚Ä¢ 2/2 directories
                            </div>
                        </div>
                    </div>
                    
                    <div class="score-display score-pass">
                        100
                    </div>
                    
                </div>

                <div class="finding-body">
                    
                    <div class="finding-section">
                        <h4>Evidence</h4>
                        <ul class="evidence-list">
                            
                            <li>Found 2/2 standard directories</li>
                            
                            <li>src/: ‚úì</li>
                            
                            <li>tests/: ‚úì</li>
                            
                        </ul>
                    </div>
                    

                    

                    
                </div>
            </div>
            
            <div class="finding"
                 data-status="fail"
                 data-tier="1"
                 data-category="Dependency Management"
                 data-score="0"
                 data-name="lock files for reproducibility">

                <div class="finding-header" onclick="toggleFinding(this)">
                    <div class="finding-title">
                        <span class="status-icon">
                            ‚ùå
                            
                        </span>
                        <div class="finding-info">
                            <h3>Lock Files for Reproducibility</h3>
                            <div class="finding-meta">
                                Dependency Management ‚Ä¢
                                <span class="tier-badge tier-1">Tier 1</span>
                                 ‚Ä¢ none
                            </div>
                        </div>
                    </div>
                    
                    <div class="score-display score-fail">
                        0
                    </div>
                    
                </div>

                <div class="finding-body">
                    
                    <div class="finding-section">
                        <h4>Evidence</h4>
                        <ul class="evidence-list">
                            
                            <li>No lock files found</li>
                            
                        </ul>
                    </div>
                    

                    
                    <div class="finding-section">
                        <h4>Remediation</h4>
                        <p><strong>Add lock file for dependency reproducibility</strong></p>

                        
                        <ol class="remediation-steps">
                            
                            <li>Use npm install, poetry lock, or equivalent to generate lock file</li>
                            
                        </ol>
                        

                        
                        <h4 style="margin-top: 15px;">Commands</h4>
                        <pre><code>npm install  # generates package-lock.json</code></pre>
                        

                        
                    </div>
                    

                    
                </div>
            </div>
            
            <div class="finding"
                 data-status="pass"
                 data-tier="2"
                 data-category="Testing & CI/CD"
                 data-score="100.0"
                 data-name="test coverage requirements">

                <div class="finding-header" onclick="toggleFinding(this)">
                    <div class="finding-title">
                        <span class="status-icon">
                            ‚úÖ
                            
                        </span>
                        <div class="finding-info">
                            <h3>Test Coverage Requirements</h3>
                            <div class="finding-meta">
                                Testing & CI/CD ‚Ä¢
                                <span class="tier-badge tier-2">Tier 2</span>
                                 ‚Ä¢ configured
                            </div>
                        </div>
                    </div>
                    
                    <div class="score-display score-pass">
                        100
                    </div>
                    
                </div>

                <div class="finding-body">
                    
                    <div class="finding-section">
                        <h4>Evidence</h4>
                        <ul class="evidence-list">
                            
                            <li>Coverage configuration found</li>
                            
                            <li>pytest-cov dependency present</li>
                            
                        </ul>
                    </div>
                    

                    

                    
                </div>
            </div>
            
            <div class="finding"
                 data-status="fail"
                 data-tier="2"
                 data-category="Testing & CI/CD"
                 data-score="0"
                 data-name="pre-commit hooks & ci/cd linting">

                <div class="finding-header" onclick="toggleFinding(this)">
                    <div class="finding-title">
                        <span class="status-icon">
                            ‚ùå
                            
                        </span>
                        <div class="finding-info">
                            <h3>Pre-commit Hooks & CI/CD Linting</h3>
                            <div class="finding-meta">
                                Testing & CI/CD ‚Ä¢
                                <span class="tier-badge tier-2">Tier 2</span>
                                 ‚Ä¢ not configured
                            </div>
                        </div>
                    </div>
                    
                    <div class="score-display score-fail">
                        0
                    </div>
                    
                </div>

                <div class="finding-body">
                    
                    <div class="finding-section">
                        <h4>Evidence</h4>
                        <ul class="evidence-list">
                            
                            <li>.pre-commit-config.yaml not found</li>
                            
                        </ul>
                    </div>
                    

                    
                    <div class="finding-section">
                        <h4>Remediation</h4>
                        <p><strong>Configure pre-commit hooks for automated code quality checks</strong></p>

                        
                        <ol class="remediation-steps">
                            
                            <li>Install pre-commit framework</li>
                            
                            <li>Create .pre-commit-config.yaml</li>
                            
                            <li>Add hooks for linting and formatting</li>
                            
                            <li>Install hooks: pre-commit install</li>
                            
                            <li>Run on all files: pre-commit run --all-files</li>
                            
                        </ol>
                        

                        
                        <h4 style="margin-top: 15px;">Commands</h4>
                        <pre><code>pip install pre-commit
pre-commit install
pre-commit run --all-files</code></pre>
                        

                        
                        <h4 style="margin-top: 15px;">Examples</h4>
                        
                        <pre><code># .pre-commit-config.yaml
repos:
  - repo: https://github.com/pre-commit/pre-commit-hooks
    rev: v4.4.0
    hooks:
      - id: trailing-whitespace
      - id: end-of-file-fixer
      - id: check-yaml
      - id: check-added-large-files

  - repo: https://github.com/psf/black
    rev: 23.3.0
    hooks:
      - id: black

  - repo: https://github.com/pycqa/isort
    rev: 5.12.0
    hooks:
      - id: isort
</code></pre>
                        
                        
                    </div>
                    

                    
                </div>
            </div>
            
            <div class="finding"
                 data-status="fail"
                 data-tier="2"
                 data-category="Git & Version Control"
                 data-score="0"
                 data-name="conventional commit messages">

                <div class="finding-header" onclick="toggleFinding(this)">
                    <div class="finding-title">
                        <span class="status-icon">
                            ‚ùå
                            
                        </span>
                        <div class="finding-info">
                            <h3>Conventional Commit Messages</h3>
                            <div class="finding-meta">
                                Git & Version Control ‚Ä¢
                                <span class="tier-badge tier-2">Tier 2</span>
                                 ‚Ä¢ not configured
                            </div>
                        </div>
                    </div>
                    
                    <div class="score-display score-fail">
                        0
                    </div>
                    
                </div>

                <div class="finding-body">
                    
                    <div class="finding-section">
                        <h4>Evidence</h4>
                        <ul class="evidence-list">
                            
                            <li>No commitlint or husky configuration</li>
                            
                        </ul>
                    </div>
                    

                    
                    <div class="finding-section">
                        <h4>Remediation</h4>
                        <p><strong>Configure conventional commits with commitlint</strong></p>

                        
                        <ol class="remediation-steps">
                            
                            <li>Install commitlint</li>
                            
                            <li>Configure husky for commit-msg hook</li>
                            
                        </ol>
                        

                        
                        <h4 style="margin-top: 15px;">Commands</h4>
                        <pre><code>npm install --save-dev @commitlint/cli @commitlint/config-conventional husky</code></pre>
                        

                        
                    </div>
                    

                    
                </div>
            </div>
            
            <div class="finding"
                 data-status="pass"
                 data-tier="2"
                 data-category="Git & Version Control"
                 data-score="100.0"
                 data-name=".gitignore completeness">

                <div class="finding-header" onclick="toggleFinding(this)">
                    <div class="finding-title">
                        <span class="status-icon">
                            ‚úÖ
                            
                        </span>
                        <div class="finding-info">
                            <h3>.gitignore Completeness</h3>
                            <div class="finding-meta">
                                Git & Version Control ‚Ä¢
                                <span class="tier-badge tier-2">Tier 2</span>
                                 ‚Ä¢ 580 bytes
                            </div>
                        </div>
                    </div>
                    
                    <div class="score-display score-pass">
                        100
                    </div>
                    
                </div>

                <div class="finding-body">
                    
                    <div class="finding-section">
                        <h4>Evidence</h4>
                        <ul class="evidence-list">
                            
                            <li>.gitignore found (580 bytes)</li>
                            
                        </ul>
                    </div>
                    

                    

                    
                </div>
            </div>
            
            <div class="finding"
                 data-status="pass"
                 data-tier="3"
                 data-category="Code Quality"
                 data-score="100.0"
                 data-name="cyclomatic complexity thresholds">

                <div class="finding-header" onclick="toggleFinding(this)">
                    <div class="finding-title">
                        <span class="status-icon">
                            ‚úÖ
                            
                        </span>
                        <div class="finding-info">
                            <h3>Cyclomatic Complexity Thresholds</h3>
                            <div class="finding-meta">
                                Code Quality ‚Ä¢
                                <span class="tier-badge tier-3">Tier 3</span>
                                 ‚Ä¢ 3.0
                            </div>
                        </div>
                    </div>
                    
                    <div class="score-display score-pass">
                        100
                    </div>
                    
                </div>

                <div class="finding-body">
                    
                    <div class="finding-section">
                        <h4>Evidence</h4>
                        <ul class="evidence-list">
                            
                            <li>Average cyclomatic complexity: 3.0</li>
                            
                        </ul>
                    </div>
                    

                    

                    
                </div>
            </div>
            
            <div class="finding"
                 data-status="not_applicable"
                 data-tier="2"
                 data-category="Build & Development"
                 data-score="0"
                 data-name="one-command build/setup">

                <div class="finding-header" onclick="toggleFinding(this)">
                    <div class="finding-title">
                        <span class="status-icon">
                            ‚äò
                            
                        </span>
                        <div class="finding-info">
                            <h3>One-Command Build/Setup</h3>
                            <div class="finding-meta">
                                Build & Development ‚Ä¢
                                <span class="tier-badge tier-2">Tier 2</span>
                                
                            </div>
                        </div>
                    </div>
                    
                    <div class="score-display score-skip">‚Äî</div>
                    
                </div>

                <div class="finding-body">
                    
                    <div class="finding-section">
                        <h4>Evidence</h4>
                        <ul class="evidence-list">
                            
                            <li>One-Command Build/Setup assessment not yet implemented</li>
                            
                        </ul>
                    </div>
                    

                    

                    
                </div>
            </div>
            
            <div class="finding"
                 data-status="not_applicable"
                 data-tier="2"
                 data-category="Context Window Optimization"
                 data-score="0"
                 data-name="concise structured documentation">

                <div class="finding-header" onclick="toggleFinding(this)">
                    <div class="finding-title">
                        <span class="status-icon">
                            ‚äò
                            
                        </span>
                        <div class="finding-info">
                            <h3>Concise Structured Documentation</h3>
                            <div class="finding-meta">
                                Context Window Optimization ‚Ä¢
                                <span class="tier-badge tier-2">Tier 2</span>
                                
                            </div>
                        </div>
                    </div>
                    
                    <div class="score-display score-skip">‚Äî</div>
                    
                </div>

                <div class="finding-body">
                    
                    <div class="finding-section">
                        <h4>Evidence</h4>
                        <ul class="evidence-list">
                            
                            <li>Concise Structured Documentation assessment not yet implemented</li>
                            
                        </ul>
                    </div>
                    

                    

                    
                </div>
            </div>
            
            <div class="finding"
                 data-status="not_applicable"
                 data-tier="2"
                 data-category="Documentation Standards"
                 data-score="0"
                 data-name="inline documentation">

                <div class="finding-header" onclick="toggleFinding(this)">
                    <div class="finding-title">
                        <span class="status-icon">
                            ‚äò
                            
                        </span>
                        <div class="finding-info">
                            <h3>Inline Documentation</h3>
                            <div class="finding-meta">
                                Documentation Standards ‚Ä¢
                                <span class="tier-badge tier-2">Tier 2</span>
                                
                            </div>
                        </div>
                    </div>
                    
                    <div class="score-display score-skip">‚Äî</div>
                    
                </div>

                <div class="finding-body">
                    
                    <div class="finding-section">
                        <h4>Evidence</h4>
                        <ul class="evidence-list">
                            
                            <li>Inline Documentation assessment not yet implemented</li>
                            
                        </ul>
                    </div>
                    

                    

                    
                </div>
            </div>
            
            <div class="finding"
                 data-status="not_applicable"
                 data-tier="2"
                 data-category="Context Window Optimization"
                 data-score="0"
                 data-name="file size limits">

                <div class="finding-header" onclick="toggleFinding(this)">
                    <div class="finding-title">
                        <span class="status-icon">
                            ‚äò
                            
                        </span>
                        <div class="finding-info">
                            <h3>File Size Limits</h3>
                            <div class="finding-meta">
                                Context Window Optimization ‚Ä¢
                                <span class="tier-badge tier-2">Tier 2</span>
                                
                            </div>
                        </div>
                    </div>
                    
                    <div class="score-display score-skip">‚Äî</div>
                    
                </div>

                <div class="finding-body">
                    
                    <div class="finding-section">
                        <h4>Evidence</h4>
                        <ul class="evidence-list">
                            
                            <li>File Size Limits assessment not yet implemented</li>
                            
                        </ul>
                    </div>
                    

                    

                    
                </div>
            </div>
            
            <div class="finding"
                 data-status="not_applicable"
                 data-tier="2"
                 data-category="Dependency Management"
                 data-score="0"
                 data-name="dependency freshness & security">

                <div class="finding-header" onclick="toggleFinding(this)">
                    <div class="finding-title">
                        <span class="status-icon">
                            ‚äò
                            
                        </span>
                        <div class="finding-info">
                            <h3>Dependency Freshness & Security</h3>
                            <div class="finding-meta">
                                Dependency Management ‚Ä¢
                                <span class="tier-badge tier-2">Tier 2</span>
                                
                            </div>
                        </div>
                    </div>
                    
                    <div class="score-display score-skip">‚Äî</div>
                    
                </div>

                <div class="finding-body">
                    
                    <div class="finding-section">
                        <h4>Evidence</h4>
                        <ul class="evidence-list">
                            
                            <li>Dependency Freshness & Security assessment not yet implemented</li>
                            
                        </ul>
                    </div>
                    

                    

                    
                </div>
            </div>
            
            <div class="finding"
                 data-status="not_applicable"
                 data-tier="2"
                 data-category="Repository Structure"
                 data-score="0"
                 data-name="separation of concerns">

                <div class="finding-header" onclick="toggleFinding(this)">
                    <div class="finding-title">
                        <span class="status-icon">
                            ‚äò
                            
                        </span>
                        <div class="finding-info">
                            <h3>Separation of Concerns</h3>
                            <div class="finding-meta">
                                Repository Structure ‚Ä¢
                                <span class="tier-badge tier-2">Tier 2</span>
                                
                            </div>
                        </div>
                    </div>
                    
                    <div class="score-display score-skip">‚Äî</div>
                    
                </div>

                <div class="finding-body">
                    
                    <div class="finding-section">
                        <h4>Evidence</h4>
                        <ul class="evidence-list">
                            
                            <li>Separation of Concerns assessment not yet implemented</li>
                            
                        </ul>
                    </div>
                    

                    

                    
                </div>
            </div>
            
            <div class="finding"
                 data-status="not_applicable"
                 data-tier="3"
                 data-category="Error Handling"
                 data-score="0"
                 data-name="structured logging">

                <div class="finding-header" onclick="toggleFinding(this)">
                    <div class="finding-title">
                        <span class="status-icon">
                            ‚äò
                            
                        </span>
                        <div class="finding-info">
                            <h3>Structured Logging</h3>
                            <div class="finding-meta">
                                Error Handling ‚Ä¢
                                <span class="tier-badge tier-3">Tier 3</span>
                                
                            </div>
                        </div>
                    </div>
                    
                    <div class="score-display score-skip">‚Äî</div>
                    
                </div>

                <div class="finding-body">
                    
                    <div class="finding-section">
                        <h4>Evidence</h4>
                        <ul class="evidence-list">
                            
                            <li>Structured Logging assessment not yet implemented</li>
                            
                        </ul>
                    </div>
                    

                    

                    
                </div>
            </div>
            
            <div class="finding"
                 data-status="not_applicable"
                 data-tier="3"
                 data-category="API Documentation"
                 data-score="0"
                 data-name="openapi/swagger specifications">

                <div class="finding-header" onclick="toggleFinding(this)">
                    <div class="finding-title">
                        <span class="status-icon">
                            ‚äò
                            
                        </span>
                        <div class="finding-info">
                            <h3>OpenAPI/Swagger Specifications</h3>
                            <div class="finding-meta">
                                API Documentation ‚Ä¢
                                <span class="tier-badge tier-3">Tier 3</span>
                                
                            </div>
                        </div>
                    </div>
                    
                    <div class="score-display score-skip">‚Äî</div>
                    
                </div>

                <div class="finding-body">
                    
                    <div class="finding-section">
                        <h4>Evidence</h4>
                        <ul class="evidence-list">
                            
                            <li>OpenAPI/Swagger Specifications assessment not yet implemented</li>
                            
                        </ul>
                    </div>
                    

                    

                    
                </div>
            </div>
            
            <div class="finding"
                 data-status="not_applicable"
                 data-tier="3"
                 data-category="Documentation Standards"
                 data-score="0"
                 data-name="architecture decision records">

                <div class="finding-header" onclick="toggleFinding(this)">
                    <div class="finding-title">
                        <span class="status-icon">
                            ‚äò
                            
                        </span>
                        <div class="finding-info">
                            <h3>Architecture Decision Records</h3>
                            <div class="finding-meta">
                                Documentation Standards ‚Ä¢
                                <span class="tier-badge tier-3">Tier 3</span>
                                
                            </div>
                        </div>
                    </div>
                    
                    <div class="score-display score-skip">‚Äî</div>
                    
                </div>

                <div class="finding-body">
                    
                    <div class="finding-section">
                        <h4>Evidence</h4>
                        <ul class="evidence-list">
                            
                            <li>Architecture Decision Records assessment not yet implemented</li>
                            
                        </ul>
                    </div>
                    

                    

                    
                </div>
            </div>
            
            <div class="finding"
                 data-status="not_applicable"
                 data-tier="3"
                 data-category="Modularity"
                 data-score="0"
                 data-name="semantic file & directory naming">

                <div class="finding-header" onclick="toggleFinding(this)">
                    <div class="finding-title">
                        <span class="status-icon">
                            ‚äò
                            
                        </span>
                        <div class="finding-info">
                            <h3>Semantic File & Directory Naming</h3>
                            <div class="finding-meta">
                                Modularity ‚Ä¢
                                <span class="tier-badge tier-3">Tier 3</span>
                                
                            </div>
                        </div>
                    </div>
                    
                    <div class="score-display score-skip">‚Äî</div>
                    
                </div>

                <div class="finding-body">
                    
                    <div class="finding-section">
                        <h4>Evidence</h4>
                        <ul class="evidence-list">
                            
                            <li>Semantic File & Directory Naming assessment not yet implemented</li>
                            
                        </ul>
                    </div>
                    

                    

                    
                </div>
            </div>
            
            <div class="finding"
                 data-status="not_applicable"
                 data-tier="4"
                 data-category="Security"
                 data-score="0"
                 data-name="security scanning automation">

                <div class="finding-header" onclick="toggleFinding(this)">
                    <div class="finding-title">
                        <span class="status-icon">
                            ‚äò
                            
                        </span>
                        <div class="finding-info">
                            <h3>Security Scanning Automation</h3>
                            <div class="finding-meta">
                                Security ‚Ä¢
                                <span class="tier-badge tier-4">Tier 4</span>
                                
                            </div>
                        </div>
                    </div>
                    
                    <div class="score-display score-skip">‚Äî</div>
                    
                </div>

                <div class="finding-body">
                    
                    <div class="finding-section">
                        <h4>Evidence</h4>
                        <ul class="evidence-list">
                            
                            <li>Security Scanning Automation assessment not yet implemented</li>
                            
                        </ul>
                    </div>
                    

                    

                    
                </div>
            </div>
            
            <div class="finding"
                 data-status="not_applicable"
                 data-tier="4"
                 data-category="Performance"
                 data-score="0"
                 data-name="performance benchmarks">

                <div class="finding-header" onclick="toggleFinding(this)">
                    <div class="finding-title">
                        <span class="status-icon">
                            ‚äò
                            
                        </span>
                        <div class="finding-info">
                            <h3>Performance Benchmarks</h3>
                            <div class="finding-meta">
                                Performance ‚Ä¢
                                <span class="tier-badge tier-4">Tier 4</span>
                                
                            </div>
                        </div>
                    </div>
                    
                    <div class="score-display score-skip">‚Äî</div>
                    
                </div>

                <div class="finding-body">
                    
                    <div class="finding-section">
                        <h4>Evidence</h4>
                        <ul class="evidence-list">
                            
                            <li>Performance Benchmarks assessment not yet implemented</li>
                            
                        </ul>
                    </div>
                    

                    

                    
                </div>
            </div>
            
            <div class="finding"
                 data-status="not_applicable"
                 data-tier="4"
                 data-category="Code Quality"
                 data-score="0"
                 data-name="code smell elimination">

                <div class="finding-header" onclick="toggleFinding(this)">
                    <div class="finding-title">
                        <span class="status-icon">
                            ‚äò
                            
                        </span>
                        <div class="finding-info">
                            <h3>Code Smell Elimination</h3>
                            <div class="finding-meta">
                                Code Quality ‚Ä¢
                                <span class="tier-badge tier-4">Tier 4</span>
                                
                            </div>
                        </div>
                    </div>
                    
                    <div class="score-display score-skip">‚Äî</div>
                    
                </div>

                <div class="finding-body">
                    
                    <div class="finding-section">
                        <h4>Evidence</h4>
                        <ul class="evidence-list">
                            
                            <li>Code Smell Elimination assessment not yet implemented</li>
                            
                        </ul>
                    </div>
                    

                    

                    
                </div>
            </div>
            
            <div class="finding"
                 data-status="not_applicable"
                 data-tier="4"
                 data-category="Git & Version Control"
                 data-score="0"
                 data-name="issue & pull request templates">

                <div class="finding-header" onclick="toggleFinding(this)">
                    <div class="finding-title">
                        <span class="status-icon">
                            ‚äò
                            
                        </span>
                        <div class="finding-info">
                            <h3>Issue & Pull Request Templates</h3>
                            <div class="finding-meta">
                                Git & Version Control ‚Ä¢
                                <span class="tier-badge tier-4">Tier 4</span>
                                
                            </div>
                        </div>
                    </div>
                    
                    <div class="score-display score-skip">‚Äî</div>
                    
                </div>

                <div class="finding-body">
                    
                    <div class="finding-section">
                        <h4>Evidence</h4>
                        <ul class="evidence-list">
                            
                            <li>Issue & Pull Request Templates assessment not yet implemented</li>
                            
                        </ul>
                    </div>
                    

                    

                    
                </div>
            </div>
            
            <div class="finding"
                 data-status="not_applicable"
                 data-tier="4"
                 data-category="Build & Development"
                 data-score="0"
                 data-name="container/virtualization setup">

                <div class="finding-header" onclick="toggleFinding(this)">
                    <div class="finding-title">
                        <span class="status-icon">
                            ‚äò
                            
                        </span>
                        <div class="finding-info">
                            <h3>Container/Virtualization Setup</h3>
                            <div class="finding-meta">
                                Build & Development ‚Ä¢
                                <span class="tier-badge tier-4">Tier 4</span>
                                
                            </div>
                        </div>
                    </div>
                    
                    <div class="score-display score-skip">‚Äî</div>
                    
                </div>

                <div class="finding-body">
                    
                    <div class="finding-section">
                        <h4>Evidence</h4>
                        <ul class="evidence-list">
                            
                            <li>Container/Virtualization Setup assessment not yet implemented</li>
                            
                        </ul>
                    </div>
                    

                    

                    
                </div>
            </div>
            
        </div>

        <footer>
            <p>Generated by <strong>AgentReady v1.0.0</strong></p>
            <p>Repository: /Users/jeder/repos/sk/agentready ‚Ä¢ Branch: 001-agentready-scorer ‚Ä¢ Commit: 18664eac</p>
            <p style="margin-top: 10px; font-size: 0.85rem;">
                ü§ñ Generated with <a href="https://claude.com/claude-code" style="color: #2563eb;">Claude Code</a>
            </p>
        </footer>
    </div>

    <script>
        // Embedded assessment data
        const ASSESSMENT = {"repository": {"path": "/Users/jeder/repos/sk/agentready", "name": "agentready", "url": null, "branch": "001-agentready-scorer", "commit_hash": "18664eacea94e90ee2da222a09228771c67c17f0", "languages": {"Markdown": 36, "Shell": 5, "Python": 25}, "total_files": 73, "total_lines": 13039}, "timestamp": "2025-11-21T01:57:27.823509", "overall_score": 75.4, "certification_level": "Gold", "attributes_assessed": 10, "attributes_skipped": 15, "attributes_total": 25, "findings": [{"attribute": {"id": "claude_md_file", "name": "CLAUDE.md Configuration Files", "category": "Context Window Optimization", "tier": 1, "description": "Project-specific configuration for Claude Code", "criteria": "CLAUDE.md file exists in repository root", "default_weight": 0.1}, "status": "pass", "score": 100.0, "measured_value": "present", "threshold": "present", "evidence": ["CLAUDE.md found at /Users/jeder/repos/sk/agentready/CLAUDE.md"], "remediation": null, "error_message": null}, {"attribute": {"id": "readme_structure", "name": "README Structure", "category": "Documentation Standards", "tier": 1, "description": "Well-structured README with key sections", "criteria": "README.md with installation, usage, and development sections", "default_weight": 0.1}, "status": "pass", "score": 100.0, "measured_value": "3/3 sections", "threshold": "3/3 sections", "evidence": ["Found 3/3 essential sections", "Installation: \u2713", "Usage: \u2713", "Development: \u2713"], "remediation": null, "error_message": null}, {"attribute": {"id": "type_annotations", "name": "Type Annotations", "category": "Code Quality", "tier": 1, "description": "Type hints in function signatures", "criteria": ">80% of functions have type annotations", "default_weight": 0.1}, "status": "pass", "score": 100.0, "measured_value": "95.0%", "threshold": "\u226580%", "evidence": ["Typed functions: 115/121", "Coverage: 95.0%"], "remediation": null, "error_message": null}, {"attribute": {"id": "standard_layout", "name": "Standard Project Layouts", "category": "Repository Structure", "tier": 1, "description": "Follows standard project structure for language", "criteria": "Standard directories (src/, tests/, docs/) present", "default_weight": 0.1}, "status": "pass", "score": 100.0, "measured_value": "2/2 directories", "threshold": "2/2 directories", "evidence": ["Found 2/2 standard directories", "src/: \u2713", "tests/: \u2713"], "remediation": null, "error_message": null}, {"attribute": {"id": "lock_files", "name": "Lock Files for Reproducibility", "category": "Dependency Management", "tier": 1, "description": "Lock files present for dependency pinning", "criteria": "package-lock.json, yarn.lock, poetry.lock, or requirements.txt with versions", "default_weight": 0.1}, "status": "fail", "score": 0.0, "measured_value": "none", "threshold": "at least one lock file", "evidence": ["No lock files found"], "remediation": {"summary": "Add lock file for dependency reproducibility", "steps": ["Use npm install, poetry lock, or equivalent to generate lock file"], "tools": [], "commands": ["npm install  # generates package-lock.json"], "examples": [], "citations": []}, "error_message": null}, {"attribute": {"id": "test_coverage", "name": "Test Coverage Requirements", "category": "Testing & CI/CD", "tier": 2, "description": "Test coverage thresholds configured and enforced", "criteria": ">80% code coverage", "default_weight": 0.03}, "status": "pass", "score": 100.0, "measured_value": "configured", "threshold": "configured with >80% threshold", "evidence": ["Coverage configuration found", "pytest-cov dependency present"], "remediation": null, "error_message": null}, {"attribute": {"id": "precommit_hooks", "name": "Pre-commit Hooks & CI/CD Linting", "category": "Testing & CI/CD", "tier": 2, "description": "Pre-commit hooks configured for linting and formatting", "criteria": ".pre-commit-config.yaml exists", "default_weight": 0.03}, "status": "fail", "score": 0.0, "measured_value": "not configured", "threshold": "configured", "evidence": [".pre-commit-config.yaml not found"], "remediation": {"summary": "Configure pre-commit hooks for automated code quality checks", "steps": ["Install pre-commit framework", "Create .pre-commit-config.yaml", "Add hooks for linting and formatting", "Install hooks: pre-commit install", "Run on all files: pre-commit run --all-files"], "tools": ["pre-commit"], "commands": ["pip install pre-commit", "pre-commit install", "pre-commit run --all-files"], "examples": ["# .pre-commit-config.yaml\nrepos:\n  - repo: https://github.com/pre-commit/pre-commit-hooks\n    rev: v4.4.0\n    hooks:\n      - id: trailing-whitespace\n      - id: end-of-file-fixer\n      - id: check-yaml\n      - id: check-added-large-files\n\n  - repo: https://github.com/psf/black\n    rev: 23.3.0\n    hooks:\n      - id: black\n\n  - repo: https://github.com/pycqa/isort\n    rev: 5.12.0\n    hooks:\n      - id: isort\n"], "citations": [{"source": "pre-commit.com", "title": "Pre-commit Framework", "url": "https://pre-commit.com/", "relevance": "Official pre-commit documentation"}]}, "error_message": null}, {"attribute": {"id": "conventional_commits", "name": "Conventional Commit Messages", "category": "Git & Version Control", "tier": 2, "description": "Follows conventional commit format", "criteria": "\u226580% of recent commits follow convention", "default_weight": 0.03}, "status": "fail", "score": 0.0, "measured_value": "not configured", "threshold": "configured", "evidence": ["No commitlint or husky configuration"], "remediation": {"summary": "Configure conventional commits with commitlint", "steps": ["Install commitlint", "Configure husky for commit-msg hook"], "tools": ["commitlint", "husky"], "commands": ["npm install --save-dev @commitlint/cli @commitlint/config-conventional husky"], "examples": [], "citations": []}, "error_message": null}, {"attribute": {"id": "gitignore_completeness", "name": ".gitignore Completeness", "category": "Git & Version Control", "tier": 2, "description": "Comprehensive .gitignore file", "criteria": ".gitignore exists and covers common patterns", "default_weight": 0.03}, "status": "pass", "score": 100.0, "measured_value": "580 bytes", "threshold": ">50 bytes", "evidence": [".gitignore found (580 bytes)"], "remediation": null, "error_message": null}, {"attribute": {"id": "cyclomatic_complexity", "name": "Cyclomatic Complexity Thresholds", "category": "Code Quality", "tier": 3, "description": "Cyclomatic complexity thresholds enforced", "criteria": "Average complexity <10, no functions >15", "default_weight": 0.03}, "status": "pass", "score": 100.0, "measured_value": "3.0", "threshold": "<10.0", "evidence": ["Average cyclomatic complexity: 3.0"], "remediation": null, "error_message": null}, {"attribute": {"id": "one_command_setup", "name": "One-Command Build/Setup", "category": "Build & Development", "tier": 2, "description": "Assessment for One-Command Build/Setup", "criteria": "To be implemented", "default_weight": 0.03}, "status": "not_applicable", "score": null, "measured_value": null, "threshold": null, "evidence": ["One-Command Build/Setup assessment not yet implemented"], "remediation": null, "error_message": null}, {"attribute": {"id": "concise_documentation", "name": "Concise Structured Documentation", "category": "Context Window Optimization", "tier": 2, "description": "Assessment for Concise Structured Documentation", "criteria": "To be implemented", "default_weight": 0.03}, "status": "not_applicable", "score": null, "measured_value": null, "threshold": null, "evidence": ["Concise Structured Documentation assessment not yet implemented"], "remediation": null, "error_message": null}, {"attribute": {"id": "inline_documentation", "name": "Inline Documentation", "category": "Documentation Standards", "tier": 2, "description": "Assessment for Inline Documentation", "criteria": "To be implemented", "default_weight": 0.03}, "status": "not_applicable", "score": null, "measured_value": null, "threshold": null, "evidence": ["Inline Documentation assessment not yet implemented"], "remediation": null, "error_message": null}, {"attribute": {"id": "file_size_limits", "name": "File Size Limits", "category": "Context Window Optimization", "tier": 2, "description": "Assessment for File Size Limits", "criteria": "To be implemented", "default_weight": 0.03}, "status": "not_applicable", "score": null, "measured_value": null, "threshold": null, "evidence": ["File Size Limits assessment not yet implemented"], "remediation": null, "error_message": null}, {"attribute": {"id": "dependency_freshness", "name": "Dependency Freshness & Security", "category": "Dependency Management", "tier": 2, "description": "Assessment for Dependency Freshness & Security", "criteria": "To be implemented", "default_weight": 0.03}, "status": "not_applicable", "score": null, "measured_value": null, "threshold": null, "evidence": ["Dependency Freshness & Security assessment not yet implemented"], "remediation": null, "error_message": null}, {"attribute": {"id": "separation_concerns", "name": "Separation of Concerns", "category": "Repository Structure", "tier": 2, "description": "Assessment for Separation of Concerns", "criteria": "To be implemented", "default_weight": 0.03}, "status": "not_applicable", "score": null, "measured_value": null, "threshold": null, "evidence": ["Separation of Concerns assessment not yet implemented"], "remediation": null, "error_message": null}, {"attribute": {"id": "structured_logging", "name": "Structured Logging", "category": "Error Handling", "tier": 3, "description": "Assessment for Structured Logging", "criteria": "To be implemented", "default_weight": 0.03}, "status": "not_applicable", "score": null, "measured_value": null, "threshold": null, "evidence": ["Structured Logging assessment not yet implemented"], "remediation": null, "error_message": null}, {"attribute": {"id": "openapi_specs", "name": "OpenAPI/Swagger Specifications", "category": "API Documentation", "tier": 3, "description": "Assessment for OpenAPI/Swagger Specifications", "criteria": "To be implemented", "default_weight": 0.03}, "status": "not_applicable", "score": null, "measured_value": null, "threshold": null, "evidence": ["OpenAPI/Swagger Specifications assessment not yet implemented"], "remediation": null, "error_message": null}, {"attribute": {"id": "architecture_decisions", "name": "Architecture Decision Records", "category": "Documentation Standards", "tier": 3, "description": "Assessment for Architecture Decision Records", "criteria": "To be implemented", "default_weight": 0.03}, "status": "not_applicable", "score": null, "measured_value": null, "threshold": null, "evidence": ["Architecture Decision Records assessment not yet implemented"], "remediation": null, "error_message": null}, {"attribute": {"id": "semantic_naming", "name": "Semantic File & Directory Naming", "category": "Modularity", "tier": 3, "description": "Assessment for Semantic File & Directory Naming", "criteria": "To be implemented", "default_weight": 0.03}, "status": "not_applicable", "score": null, "measured_value": null, "threshold": null, "evidence": ["Semantic File & Directory Naming assessment not yet implemented"], "remediation": null, "error_message": null}, {"attribute": {"id": "security_scanning", "name": "Security Scanning Automation", "category": "Security", "tier": 4, "description": "Assessment for Security Scanning Automation", "criteria": "To be implemented", "default_weight": 0.01}, "status": "not_applicable", "score": null, "measured_value": null, "threshold": null, "evidence": ["Security Scanning Automation assessment not yet implemented"], "remediation": null, "error_message": null}, {"attribute": {"id": "performance_benchmarks", "name": "Performance Benchmarks", "category": "Performance", "tier": 4, "description": "Assessment for Performance Benchmarks", "criteria": "To be implemented", "default_weight": 0.01}, "status": "not_applicable", "score": null, "measured_value": null, "threshold": null, "evidence": ["Performance Benchmarks assessment not yet implemented"], "remediation": null, "error_message": null}, {"attribute": {"id": "code_smells", "name": "Code Smell Elimination", "category": "Code Quality", "tier": 4, "description": "Assessment for Code Smell Elimination", "criteria": "To be implemented", "default_weight": 0.01}, "status": "not_applicable", "score": null, "measured_value": null, "threshold": null, "evidence": ["Code Smell Elimination assessment not yet implemented"], "remediation": null, "error_message": null}, {"attribute": {"id": "issue_pr_templates", "name": "Issue & Pull Request Templates", "category": "Git & Version Control", "tier": 4, "description": "Assessment for Issue & Pull Request Templates", "criteria": "To be implemented", "default_weight": 0.01}, "status": "not_applicable", "score": null, "measured_value": null, "threshold": null, "evidence": ["Issue & Pull Request Templates assessment not yet implemented"], "remediation": null, "error_message": null}, {"attribute": {"id": "container_setup", "name": "Container/Virtualization Setup", "category": "Build & Development", "tier": 4, "description": "Assessment for Container/Virtualization Setup", "criteria": "To be implemented", "default_weight": 0.01}, "status": "not_applicable", "score": null, "measured_value": null, "threshold": null, "evidence": ["Container/Virtualization Setup assessment not yet implemented"], "remediation": null, "error_message": null}], "config": null, "duration_seconds": 0.1};

        // Toggle finding expansion
        function toggleFinding(header) {
            const finding = header.parentElement;
            finding.classList.toggle('expanded');
        }

        // Filter functionality
        document.querySelectorAll('.filter-btn').forEach(btn => {
            btn.addEventListener('click', () => {
                // Update active button
                document.querySelectorAll('.filter-btn').forEach(b => b.classList.remove('active'));
                btn.classList.add('active');

                const filter = btn.dataset.filter;
                const findings = document.querySelectorAll('.finding');

                findings.forEach(finding => {
                    const status = finding.dataset.status;

                    if (filter === 'all') {
                        finding.classList.remove('hidden');
                    } else if (filter === 'skipped') {
                        if (status === 'skipped' || status === 'not_applicable' || status === 'error') {
                            finding.classList.remove('hidden');
                        } else {
                            finding.classList.add('hidden');
                        }
                    } else {
                        if (status === filter) {
                            finding.classList.remove('hidden');
                        } else {
                            finding.classList.add('hidden');
                        }
                    }
                });
            });
        });

        // Sort functionality
        document.getElementById('sort-select').addEventListener('change', (e) => {
            const sortBy = e.target.value;
            const container = document.getElementById('findings-container');
            const findings = Array.from(container.querySelectorAll('.finding'));

            findings.sort((a, b) => {
                if (sortBy === 'category') {
                    return a.dataset.category.localeCompare(b.dataset.category);
                } else if (sortBy === 'score-desc') {
                    return parseFloat(b.dataset.score) - parseFloat(a.dataset.score);
                } else if (sortBy === 'score-asc') {
                    return parseFloat(a.dataset.score) - parseFloat(b.dataset.score);
                } else if (sortBy === 'tier') {
                    return parseInt(a.dataset.tier) - parseInt(b.dataset.tier);
                }
            });

            findings.forEach(finding => container.appendChild(finding));
        });

        // Search functionality
        document.getElementById('search-box').addEventListener('input', (e) => {
            const query = e.target.value.toLowerCase();
            const findings = document.querySelectorAll('.finding');

            findings.forEach(finding => {
                const name = finding.dataset.name;
                const category = finding.dataset.category.toLowerCase();

                if (name.includes(query) || category.includes(query)) {
                    finding.classList.remove('hidden');
                } else {
                    finding.classList.add('hidden');
                }
            });
        });
    </script>
</body>
</html>
</file>

<file path=".agentready/report-20251121-015727.md">
# ü§ñ AgentReady Assessment Report

**Repository**: agentready
**Path**: `/Users/jeder/repos/sk/agentready`
**Branch**: 001-agentready-scorer
**Commit**: 18664eac
**Date**: 2025-11-21 01:57:27

---

## üìä Summary

| Metric | Value |
|--------|-------|
| **Overall Score** | **75.4/100** |
| **Certification Level** | **Gold** |
| **Attributes Assessed** | 10/25 |
| **Attributes Skipped** | 15 |
| **Assessment Duration** | 0.1s |

### Languages Detected

- **Markdown**: 36 files
- **Python**: 25 files
- **Shell**: 5 files

### Repository Stats

- **Total Files**: 73
- **Total Lines**: 13,039

## üéñÔ∏è Certification Ladder

- üíé **Platinum** (90-100) 
- ü•á **Gold** (75-89) **‚Üí YOUR LEVEL ‚Üê**
- ü•à **Silver** (60-74) 
- ü•â **Bronze** (40-59) 
- ‚ö†Ô∏è **Needs Improvement** (0-39) 

## üìã Detailed Findings

### API Documentation

| Attribute | Tier | Status | Score |
|-----------|------|--------|-------|
| OpenAPI/Swagger Specifications | T3 | ‚äò not_applicable | ‚Äî |

### Build & Development

| Attribute | Tier | Status | Score |
|-----------|------|--------|-------|
| One-Command Build/Setup | T2 | ‚äò not_applicable | ‚Äî |
| Container/Virtualization Setup | T4 | ‚äò not_applicable | ‚Äî |

### Code Quality

| Attribute | Tier | Status | Score |
|-----------|------|--------|-------|
| Type Annotations | T1 | ‚úÖ pass | 100 |
| Cyclomatic Complexity Thresholds | T3 | ‚úÖ pass | 100 |
| Code Smell Elimination | T4 | ‚äò not_applicable | ‚Äî |

### Context Window Optimization

| Attribute | Tier | Status | Score |
|-----------|------|--------|-------|
| CLAUDE.md Configuration Files | T1 | ‚úÖ pass | 100 |
| Concise Structured Documentation | T2 | ‚äò not_applicable | ‚Äî |
| File Size Limits | T2 | ‚äò not_applicable | ‚Äî |

### Dependency Management

| Attribute | Tier | Status | Score |
|-----------|------|--------|-------|
| Lock Files for Reproducibility | T1 | ‚ùå fail | 0 |
| Dependency Freshness & Security | T2 | ‚äò not_applicable | ‚Äî |

#### ‚ùå Lock Files for Reproducibility

**Measured**: none (Threshold: at least one lock file)

**Evidence**:
- No lock files found

<details><summary><strong>üìù Remediation Steps</strong></summary>


Add lock file for dependency reproducibility

1. Use npm install, poetry lock, or equivalent to generate lock file

**Commands**:

```bash
npm install  # generates package-lock.json
```

</details>

### Documentation Standards

| Attribute | Tier | Status | Score |
|-----------|------|--------|-------|
| README Structure | T1 | ‚úÖ pass | 100 |
| Inline Documentation | T2 | ‚äò not_applicable | ‚Äî |
| Architecture Decision Records | T3 | ‚äò not_applicable | ‚Äî |

### Error Handling

| Attribute | Tier | Status | Score |
|-----------|------|--------|-------|
| Structured Logging | T3 | ‚äò not_applicable | ‚Äî |

### Git & Version Control

| Attribute | Tier | Status | Score |
|-----------|------|--------|-------|
| Conventional Commit Messages | T2 | ‚ùå fail | 0 |
| .gitignore Completeness | T2 | ‚úÖ pass | 100 |
| Issue & Pull Request Templates | T4 | ‚äò not_applicable | ‚Äî |

#### ‚ùå Conventional Commit Messages

**Measured**: not configured (Threshold: configured)

**Evidence**:
- No commitlint or husky configuration

<details><summary><strong>üìù Remediation Steps</strong></summary>


Configure conventional commits with commitlint

1. Install commitlint
2. Configure husky for commit-msg hook

**Commands**:

```bash
npm install --save-dev @commitlint/cli @commitlint/config-conventional husky
```

</details>

### Modularity

| Attribute | Tier | Status | Score |
|-----------|------|--------|-------|
| Semantic File & Directory Naming | T3 | ‚äò not_applicable | ‚Äî |

### Performance

| Attribute | Tier | Status | Score |
|-----------|------|--------|-------|
| Performance Benchmarks | T4 | ‚äò not_applicable | ‚Äî |

### Repository Structure

| Attribute | Tier | Status | Score |
|-----------|------|--------|-------|
| Standard Project Layouts | T1 | ‚úÖ pass | 100 |
| Separation of Concerns | T2 | ‚äò not_applicable | ‚Äî |

### Security

| Attribute | Tier | Status | Score |
|-----------|------|--------|-------|
| Security Scanning Automation | T4 | ‚äò not_applicable | ‚Äî |

### Testing & CI/CD

| Attribute | Tier | Status | Score |
|-----------|------|--------|-------|
| Test Coverage Requirements | T2 | ‚úÖ pass | 100 |
| Pre-commit Hooks & CI/CD Linting | T2 | ‚ùå fail | 0 |

#### ‚ùå Pre-commit Hooks & CI/CD Linting

**Measured**: not configured (Threshold: configured)

**Evidence**:
- .pre-commit-config.yaml not found

<details><summary><strong>üìù Remediation Steps</strong></summary>


Configure pre-commit hooks for automated code quality checks

1. Install pre-commit framework
2. Create .pre-commit-config.yaml
3. Add hooks for linting and formatting
4. Install hooks: pre-commit install
5. Run on all files: pre-commit run --all-files

**Commands**:

```bash
pip install pre-commit
pre-commit install
pre-commit run --all-files
```

**Examples**:

```
# .pre-commit-config.yaml
repos:
  - repo: https://github.com/pre-commit/pre-commit-hooks
    rev: v4.4.0
    hooks:
      - id: trailing-whitespace
      - id: end-of-file-fixer
      - id: check-yaml
      - id: check-added-large-files

  - repo: https://github.com/psf/black
    rev: 23.3.0
    hooks:
      - id: black

  - repo: https://github.com/pycqa/isort
    rev: 5.12.0
    hooks:
      - id: isort

```

</details>

## üéØ Next Steps

**Priority Improvements** (highest impact first):

1. **Lock Files for Reproducibility** (Tier 1) - +10.0 points potential
   - Add lock file for dependency reproducibility
2. **Pre-commit Hooks & CI/CD Linting** (Tier 2) - +3.0 points potential
   - Configure pre-commit hooks for automated code quality checks
3. **Conventional Commit Messages** (Tier 2) - +3.0 points potential
   - Configure conventional commits with commitlint

---

## üìù Assessment Metadata

- **Tool Version**: AgentReady v1.0.0
- **Research Report**: Bundled version
- **Repository Snapshot**: 18664eacea94e90ee2da222a09228771c67c17f0
- **Assessment Duration**: 0.1s

ü§ñ Generated with [Claude Code](https://claude.com/claude-code)
</file>

<file path=".agentready/report-20251121-021105.html">
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>AgentReady Assessment - agentready</title>
    <style>
        /* Reset and Base Styles */
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif;
            line-height: 1.6;
            color: #333;
            background: #f5f5f5;
            padding: 20px;
        }

        .container {
            max-width: 1200px;
            margin: 0 auto;
            background: white;
            padding: 40px;
            border-radius: 8px;
            box-shadow: 0 2px 8px rgba(0,0,0,0.1);
        }

        /* Header */
        header {
            border-bottom: 3px solid #2563eb;
            padding-bottom: 20px;
            margin-bottom: 30px;
        }

        h1 {
            font-size: 2rem;
            color: #1e293b;
            margin-bottom: 10px;
        }

        .subtitle {
            color: #64748b;
            font-size: 1rem;
        }

        /* Summary Cards */
        .summary-grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(200px, 1fr));
            gap: 20px;
            margin: 30px 0;
        }

        .summary-card {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 20px;
            border-radius: 8px;
            text-align: center;
        }

        .summary-card.score {
            background: linear-gradient(135deg, #f093fb 0%, #f5576c 100%);
        }

        .summary-card.certification {
            background: linear-gradient(135deg, #4facfe 0%, #00f2fe 100%);
        }

        .summary-card.assessed {
            background: linear-gradient(135deg, #43e97b 0%, #38f9d7 100%);
        }

        .summary-card.duration {
            background: linear-gradient(135deg, #fa709a 0%, #fee140 100%);
        }

        .summary-card h3 {
            font-size: 0.9rem;
            font-weight: 500;
            opacity: 0.9;
            margin-bottom: 10px;
        }

        .summary-card .value {
            font-size: 2rem;
            font-weight: 700;
        }

        /* Controls */
        .controls {
            display: flex;
            flex-wrap: wrap;
            gap: 15px;
            margin: 30px 0;
            padding: 20px;
            background: #f8fafc;
            border-radius: 8px;
        }

        .control-group {
            display: flex;
            align-items: center;
            gap: 10px;
        }

        .control-group label {
            font-weight: 600;
            color: #475569;
            font-size: 0.9rem;
        }

        select, input[type="search"] {
            padding: 8px 12px;
            border: 1px solid #cbd5e1;
            border-radius: 6px;
            font-size: 0.9rem;
            background: white;
        }

        input[type="search"] {
            min-width: 250px;
        }

        .filter-btn {
            padding: 8px 16px;
            border: 2px solid #e2e8f0;
            background: white;
            border-radius: 6px;
            cursor: pointer;
            font-size: 0.9rem;
            transition: all 0.2s;
        }

        .filter-btn:hover {
            border-color: #2563eb;
            background: #eff6ff;
        }

        .filter-btn.active {
            background: #2563eb;
            color: white;
            border-color: #2563eb;
        }

        .badge {
            display: inline-block;
            padding: 2px 8px;
            border-radius: 12px;
            font-size: 0.75rem;
            font-weight: 600;
            margin-left: 5px;
        }

        /* Certification Ladder */
        .cert-ladder {
            display: flex;
            justify-content: space-between;
            margin: 30px 0;
            padding: 20px;
            background: #f8fafc;
            border-radius: 8px;
        }

        .cert-level {
            flex: 1;
            text-align: center;
            padding: 15px 10px;
            border-radius: 6px;
            margin: 0 5px;
            transition: all 0.3s;
        }

        .cert-level.platinum { background: linear-gradient(135deg, #e0e7ff, #c7d2fe); }
        .cert-level.gold { background: linear-gradient(135deg, #fef3c7, #fde68a); }
        .cert-level.silver { background: linear-gradient(135deg, #e5e7eb, #d1d5db); }
        .cert-level.bronze { background: linear-gradient(135deg, #fed7aa, #fdba74); }
        .cert-level.needs-improvement { background: linear-gradient(135deg, #fecaca, #fca5a5); }

        .cert-level.active {
            transform: scale(1.1);
            box-shadow: 0 4px 12px rgba(0,0,0,0.15);
            border: 3px solid #2563eb;
        }

        .cert-level h4 {
            font-size: 0.9rem;
            margin-bottom: 5px;
        }

        .cert-level .range {
            font-size: 0.8rem;
            opacity: 0.7;
        }

        /* Findings */
        .findings {
            margin: 30px 0;
        }

        .finding {
            border: 1px solid #e2e8f0;
            border-radius: 8px;
            margin-bottom: 15px;
            overflow: hidden;
            transition: all 0.2s;
        }

        .finding:hover {
            box-shadow: 0 4px 12px rgba(0,0,0,0.08);
        }

        .finding-header {
            padding: 15px 20px;
            background: #f8fafc;
            cursor: pointer;
            display: flex;
            justify-content: space-between;
            align-items: center;
        }

        .finding-header:hover {
            background: #f1f5f9;
        }

        .finding-title {
            display: flex;
            align-items: center;
            gap: 15px;
            flex: 1;
        }

        .status-icon {
            font-size: 1.5rem;
        }

        .finding-info h3 {
            font-size: 1rem;
            color: #1e293b;
            margin-bottom: 3px;
        }

        .finding-meta {
            font-size: 0.85rem;
            color: #64748b;
        }

        .tier-badge {
            padding: 4px 12px;
            border-radius: 12px;
            font-size: 0.75rem;
            font-weight: 600;
        }

        .tier-1 { background: #fee2e2; color: #991b1b; }
        .tier-2 { background: #fed7aa; color: #9a3412; }
        .tier-3 { background: #fef3c7; color: #92400e; }
        .tier-4 { background: #d1fae5; color: #065f46; }

        .score-display {
            font-size: 1.5rem;
            font-weight: 700;
            min-width: 80px;
            text-align: right;
        }

        .score-pass { color: #059669; }
        .score-fail { color: #dc2626; }
        .score-skip { color: #6b7280; }

        .finding-body {
            padding: 20px;
            display: none;
        }

        .finding.expanded .finding-body {
            display: block;
        }

        .finding-section {
            margin-bottom: 20px;
        }

        .finding-section h4 {
            font-size: 0.9rem;
            color: #475569;
            margin-bottom: 10px;
            text-transform: uppercase;
            letter-spacing: 0.05em;
        }

        .evidence-list, .remediation-steps {
            list-style: none;
            padding-left: 0;
        }

        .evidence-list li {
            padding: 8px 0;
            border-bottom: 1px solid #f1f5f9;
        }

        .evidence-list li:before {
            content: "‚ñ∏ ";
            color: #2563eb;
            font-weight: 700;
            margin-right: 8px;
        }

        .remediation-steps li {
            padding: 10px 0 10px 30px;
            position: relative;
        }

        .remediation-steps li:before {
            content: counter(step);
            counter-increment: step;
            position: absolute;
            left: 0;
            top: 10px;
            background: #2563eb;
            color: white;
            width: 20px;
            height: 20px;
            border-radius: 50%;
            display: flex;
            align-items: center;
            justify-content: center;
            font-size: 0.75rem;
            font-weight: 700;
        }

        .remediation-steps {
            counter-reset: step;
        }

        code {
            background: #f1f5f9;
            padding: 2px 6px;
            border-radius: 3px;
            font-family: 'Courier New', monospace;
            font-size: 0.9em;
        }

        pre {
            background: #1e293b;
            color: #e2e8f0;
            padding: 15px;
            border-radius: 6px;
            overflow-x: auto;
            margin: 10px 0;
        }

        pre code {
            background: none;
            color: inherit;
            padding: 0;
        }

        /* Footer */
        footer {
            margin-top: 50px;
            padding-top: 20px;
            border-top: 1px solid #e2e8f0;
            text-align: center;
            color: #64748b;
            font-size: 0.9rem;
        }

        /* Responsive */
        @media (max-width: 768px) {
            .container {
                padding: 20px;
            }

            .summary-grid {
                grid-template-columns: 1fr;
            }

            .controls {
                flex-direction: column;
            }

            input[type="search"] {
                min-width: 100%;
            }

            .cert-ladder {
                flex-direction: column;
            }

            .cert-level {
                margin: 5px 0;
            }
        }

        /* Hidden class */
        .hidden {
            display: none !important;
        }
    </style>
</head>
<body>
    <div class="container">
        <header>
            <h1>ü§ñ AgentReady Assessment Report</h1>
            <p class="subtitle">agentready ‚Ä¢ November 21, 2025 at 02:11</p>
        </header>

        <!-- Summary Cards -->
        <div class="summary-grid">
            <div class="summary-card score">
                <h3>Overall Score</h3>
                <div class="value">75.4</div>
            </div>
            <div class="summary-card certification">
                <h3>Certification</h3>
                <div class="value">Gold</div>
            </div>
            <div class="summary-card assessed">
                <h3>Assessed</h3>
                <div class="value">10/25</div>
            </div>
            <div class="summary-card duration">
                <h3>Duration</h3>
                <div class="value">0.2s</div>
            </div>
        </div>

        <!-- Certification Ladder -->
        <div class="cert-ladder">
            <div class="cert-level platinum ">
                <h4>üíé Platinum</h4>
                <p class="range">90-100</p>
            </div>
            <div class="cert-level gold active">
                <h4>ü•á Gold</h4>
                <p class="range">75-89</p>
            </div>
            <div class="cert-level silver ">
                <h4>ü•à Silver</h4>
                <p class="range">60-74</p>
            </div>
            <div class="cert-level bronze ">
                <h4>ü•â Bronze</h4>
                <p class="range">40-59</p>
            </div>
            <div class="cert-level needs-improvement ">
                <h4>‚ö†Ô∏è Needs Work</h4>
                <p class="range">0-39</p>
            </div>
        </div>

        <!-- Controls -->
        <div class="controls">
            <div class="control-group">
                <label>Filter:</label>
                <button class="filter-btn active" data-filter="all">All <span class="badge">25</span></button>
                <button class="filter-btn" data-filter="pass">Pass <span class="badge">7</span></button>
                <button class="filter-btn" data-filter="fail">Fail <span class="badge">3</span></button>
                <button class="filter-btn" data-filter="skipped">Skipped <span class="badge">15</span></button>
            </div>

            <div class="control-group">
                <label>Sort:</label>
                <select id="sort-select">
                    <option value="category">Category</option>
                    <option value="score-desc">Score (High to Low)</option>
                    <option value="score-asc">Score (Low to High)</option>
                    <option value="tier">Tier (Essential First)</option>
                </select>
            </div>

            <div class="control-group">
                <input type="search" id="search-box" placeholder="Search attributes...">
            </div>
        </div>

        <!-- Findings -->
        <div class="findings" id="findings-container">
            
            <div class="finding"
                 data-status="pass"
                 data-tier="1"
                 data-category="Context Window Optimization"
                 data-score="100.0"
                 data-name="claude.md configuration files">

                <div class="finding-header" onclick="toggleFinding(this)">
                    <div class="finding-title">
                        <span class="status-icon">
                            ‚úÖ
                            
                        </span>
                        <div class="finding-info">
                            <h3>CLAUDE.md Configuration Files</h3>
                            <div class="finding-meta">
                                Context Window Optimization ‚Ä¢
                                <span class="tier-badge tier-1">Tier 1</span>
                                 ‚Ä¢ present
                            </div>
                        </div>
                    </div>
                    
                    <div class="score-display score-pass">
                        100
                    </div>
                    
                </div>

                <div class="finding-body">
                    
                    <div class="finding-section">
                        <h4>Evidence</h4>
                        <ul class="evidence-list">
                            
                            <li>CLAUDE.md found at /Users/jeder/repos/sk/agentready/CLAUDE.md</li>
                            
                        </ul>
                    </div>
                    

                    

                    
                </div>
            </div>
            
            <div class="finding"
                 data-status="pass"
                 data-tier="1"
                 data-category="Documentation Standards"
                 data-score="100.0"
                 data-name="readme structure">

                <div class="finding-header" onclick="toggleFinding(this)">
                    <div class="finding-title">
                        <span class="status-icon">
                            ‚úÖ
                            
                        </span>
                        <div class="finding-info">
                            <h3>README Structure</h3>
                            <div class="finding-meta">
                                Documentation Standards ‚Ä¢
                                <span class="tier-badge tier-1">Tier 1</span>
                                 ‚Ä¢ 3/3 sections
                            </div>
                        </div>
                    </div>
                    
                    <div class="score-display score-pass">
                        100
                    </div>
                    
                </div>

                <div class="finding-body">
                    
                    <div class="finding-section">
                        <h4>Evidence</h4>
                        <ul class="evidence-list">
                            
                            <li>Found 3/3 essential sections</li>
                            
                            <li>Installation: ‚úì</li>
                            
                            <li>Usage: ‚úì</li>
                            
                            <li>Development: ‚úì</li>
                            
                        </ul>
                    </div>
                    

                    

                    
                </div>
            </div>
            
            <div class="finding"
                 data-status="pass"
                 data-tier="1"
                 data-category="Code Quality"
                 data-score="100.0"
                 data-name="type annotations">

                <div class="finding-header" onclick="toggleFinding(this)">
                    <div class="finding-title">
                        <span class="status-icon">
                            ‚úÖ
                            
                        </span>
                        <div class="finding-info">
                            <h3>Type Annotations</h3>
                            <div class="finding-meta">
                                Code Quality ‚Ä¢
                                <span class="tier-badge tier-1">Tier 1</span>
                                 ‚Ä¢ 95.9%
                            </div>
                        </div>
                    </div>
                    
                    <div class="score-display score-pass">
                        100
                    </div>
                    
                </div>

                <div class="finding-body">
                    
                    <div class="finding-section">
                        <h4>Evidence</h4>
                        <ul class="evidence-list">
                            
                            <li>Typed functions: 142/148</li>
                            
                            <li>Coverage: 95.9%</li>
                            
                        </ul>
                    </div>
                    

                    

                    
                </div>
            </div>
            
            <div class="finding"
                 data-status="pass"
                 data-tier="1"
                 data-category="Repository Structure"
                 data-score="100.0"
                 data-name="standard project layouts">

                <div class="finding-header" onclick="toggleFinding(this)">
                    <div class="finding-title">
                        <span class="status-icon">
                            ‚úÖ
                            
                        </span>
                        <div class="finding-info">
                            <h3>Standard Project Layouts</h3>
                            <div class="finding-meta">
                                Repository Structure ‚Ä¢
                                <span class="tier-badge tier-1">Tier 1</span>
                                 ‚Ä¢ 2/2 directories
                            </div>
                        </div>
                    </div>
                    
                    <div class="score-display score-pass">
                        100
                    </div>
                    
                </div>

                <div class="finding-body">
                    
                    <div class="finding-section">
                        <h4>Evidence</h4>
                        <ul class="evidence-list">
                            
                            <li>Found 2/2 standard directories</li>
                            
                            <li>src/: ‚úì</li>
                            
                            <li>tests/: ‚úì</li>
                            
                        </ul>
                    </div>
                    

                    

                    
                </div>
            </div>
            
            <div class="finding"
                 data-status="fail"
                 data-tier="1"
                 data-category="Dependency Management"
                 data-score="0"
                 data-name="lock files for reproducibility">

                <div class="finding-header" onclick="toggleFinding(this)">
                    <div class="finding-title">
                        <span class="status-icon">
                            ‚ùå
                            
                        </span>
                        <div class="finding-info">
                            <h3>Lock Files for Reproducibility</h3>
                            <div class="finding-meta">
                                Dependency Management ‚Ä¢
                                <span class="tier-badge tier-1">Tier 1</span>
                                 ‚Ä¢ none
                            </div>
                        </div>
                    </div>
                    
                    <div class="score-display score-fail">
                        0
                    </div>
                    
                </div>

                <div class="finding-body">
                    
                    <div class="finding-section">
                        <h4>Evidence</h4>
                        <ul class="evidence-list">
                            
                            <li>No lock files found</li>
                            
                        </ul>
                    </div>
                    

                    
                    <div class="finding-section">
                        <h4>Remediation</h4>
                        <p><strong>Add lock file for dependency reproducibility</strong></p>

                        
                        <ol class="remediation-steps">
                            
                            <li>Use npm install, poetry lock, or equivalent to generate lock file</li>
                            
                        </ol>
                        

                        
                        <h4 style="margin-top: 15px;">Commands</h4>
                        <pre><code>npm install  # generates package-lock.json</code></pre>
                        

                        
                    </div>
                    

                    
                </div>
            </div>
            
            <div class="finding"
                 data-status="pass"
                 data-tier="2"
                 data-category="Testing & CI/CD"
                 data-score="100.0"
                 data-name="test coverage requirements">

                <div class="finding-header" onclick="toggleFinding(this)">
                    <div class="finding-title">
                        <span class="status-icon">
                            ‚úÖ
                            
                        </span>
                        <div class="finding-info">
                            <h3>Test Coverage Requirements</h3>
                            <div class="finding-meta">
                                Testing & CI/CD ‚Ä¢
                                <span class="tier-badge tier-2">Tier 2</span>
                                 ‚Ä¢ configured
                            </div>
                        </div>
                    </div>
                    
                    <div class="score-display score-pass">
                        100
                    </div>
                    
                </div>

                <div class="finding-body">
                    
                    <div class="finding-section">
                        <h4>Evidence</h4>
                        <ul class="evidence-list">
                            
                            <li>Coverage configuration found</li>
                            
                            <li>pytest-cov dependency present</li>
                            
                        </ul>
                    </div>
                    

                    

                    
                </div>
            </div>
            
            <div class="finding"
                 data-status="fail"
                 data-tier="2"
                 data-category="Testing & CI/CD"
                 data-score="0"
                 data-name="pre-commit hooks & ci/cd linting">

                <div class="finding-header" onclick="toggleFinding(this)">
                    <div class="finding-title">
                        <span class="status-icon">
                            ‚ùå
                            
                        </span>
                        <div class="finding-info">
                            <h3>Pre-commit Hooks & CI/CD Linting</h3>
                            <div class="finding-meta">
                                Testing & CI/CD ‚Ä¢
                                <span class="tier-badge tier-2">Tier 2</span>
                                 ‚Ä¢ not configured
                            </div>
                        </div>
                    </div>
                    
                    <div class="score-display score-fail">
                        0
                    </div>
                    
                </div>

                <div class="finding-body">
                    
                    <div class="finding-section">
                        <h4>Evidence</h4>
                        <ul class="evidence-list">
                            
                            <li>.pre-commit-config.yaml not found</li>
                            
                        </ul>
                    </div>
                    

                    
                    <div class="finding-section">
                        <h4>Remediation</h4>
                        <p><strong>Configure pre-commit hooks for automated code quality checks</strong></p>

                        
                        <ol class="remediation-steps">
                            
                            <li>Install pre-commit framework</li>
                            
                            <li>Create .pre-commit-config.yaml</li>
                            
                            <li>Add hooks for linting and formatting</li>
                            
                            <li>Install hooks: pre-commit install</li>
                            
                            <li>Run on all files: pre-commit run --all-files</li>
                            
                        </ol>
                        

                        
                        <h4 style="margin-top: 15px;">Commands</h4>
                        <pre><code>pip install pre-commit
pre-commit install
pre-commit run --all-files</code></pre>
                        

                        
                        <h4 style="margin-top: 15px;">Examples</h4>
                        
                        <pre><code># .pre-commit-config.yaml
repos:
  - repo: https://github.com/pre-commit/pre-commit-hooks
    rev: v4.4.0
    hooks:
      - id: trailing-whitespace
      - id: end-of-file-fixer
      - id: check-yaml
      - id: check-added-large-files

  - repo: https://github.com/psf/black
    rev: 23.3.0
    hooks:
      - id: black

  - repo: https://github.com/pycqa/isort
    rev: 5.12.0
    hooks:
      - id: isort
</code></pre>
                        
                        
                    </div>
                    

                    
                </div>
            </div>
            
            <div class="finding"
                 data-status="fail"
                 data-tier="2"
                 data-category="Git & Version Control"
                 data-score="0"
                 data-name="conventional commit messages">

                <div class="finding-header" onclick="toggleFinding(this)">
                    <div class="finding-title">
                        <span class="status-icon">
                            ‚ùå
                            
                        </span>
                        <div class="finding-info">
                            <h3>Conventional Commit Messages</h3>
                            <div class="finding-meta">
                                Git & Version Control ‚Ä¢
                                <span class="tier-badge tier-2">Tier 2</span>
                                 ‚Ä¢ not configured
                            </div>
                        </div>
                    </div>
                    
                    <div class="score-display score-fail">
                        0
                    </div>
                    
                </div>

                <div class="finding-body">
                    
                    <div class="finding-section">
                        <h4>Evidence</h4>
                        <ul class="evidence-list">
                            
                            <li>No commitlint or husky configuration</li>
                            
                        </ul>
                    </div>
                    

                    
                    <div class="finding-section">
                        <h4>Remediation</h4>
                        <p><strong>Configure conventional commits with commitlint</strong></p>

                        
                        <ol class="remediation-steps">
                            
                            <li>Install commitlint</li>
                            
                            <li>Configure husky for commit-msg hook</li>
                            
                        </ol>
                        

                        
                        <h4 style="margin-top: 15px;">Commands</h4>
                        <pre><code>npm install --save-dev @commitlint/cli @commitlint/config-conventional husky</code></pre>
                        

                        
                    </div>
                    

                    
                </div>
            </div>
            
            <div class="finding"
                 data-status="pass"
                 data-tier="2"
                 data-category="Git & Version Control"
                 data-score="100.0"
                 data-name=".gitignore completeness">

                <div class="finding-header" onclick="toggleFinding(this)">
                    <div class="finding-title">
                        <span class="status-icon">
                            ‚úÖ
                            
                        </span>
                        <div class="finding-info">
                            <h3>.gitignore Completeness</h3>
                            <div class="finding-meta">
                                Git & Version Control ‚Ä¢
                                <span class="tier-badge tier-2">Tier 2</span>
                                 ‚Ä¢ 580 bytes
                            </div>
                        </div>
                    </div>
                    
                    <div class="score-display score-pass">
                        100
                    </div>
                    
                </div>

                <div class="finding-body">
                    
                    <div class="finding-section">
                        <h4>Evidence</h4>
                        <ul class="evidence-list">
                            
                            <li>.gitignore found (580 bytes)</li>
                            
                        </ul>
                    </div>
                    

                    

                    
                </div>
            </div>
            
            <div class="finding"
                 data-status="pass"
                 data-tier="3"
                 data-category="Code Quality"
                 data-score="100.0"
                 data-name="cyclomatic complexity thresholds">

                <div class="finding-header" onclick="toggleFinding(this)">
                    <div class="finding-title">
                        <span class="status-icon">
                            ‚úÖ
                            
                        </span>
                        <div class="finding-info">
                            <h3>Cyclomatic Complexity Thresholds</h3>
                            <div class="finding-meta">
                                Code Quality ‚Ä¢
                                <span class="tier-badge tier-3">Tier 3</span>
                                 ‚Ä¢ 3.1
                            </div>
                        </div>
                    </div>
                    
                    <div class="score-display score-pass">
                        100
                    </div>
                    
                </div>

                <div class="finding-body">
                    
                    <div class="finding-section">
                        <h4>Evidence</h4>
                        <ul class="evidence-list">
                            
                            <li>Average cyclomatic complexity: 3.1</li>
                            
                        </ul>
                    </div>
                    

                    

                    
                </div>
            </div>
            
            <div class="finding"
                 data-status="not_applicable"
                 data-tier="2"
                 data-category="Build & Development"
                 data-score="0"
                 data-name="one-command build/setup">

                <div class="finding-header" onclick="toggleFinding(this)">
                    <div class="finding-title">
                        <span class="status-icon">
                            ‚äò
                            
                        </span>
                        <div class="finding-info">
                            <h3>One-Command Build/Setup</h3>
                            <div class="finding-meta">
                                Build & Development ‚Ä¢
                                <span class="tier-badge tier-2">Tier 2</span>
                                
                            </div>
                        </div>
                    </div>
                    
                    <div class="score-display score-skip">‚Äî</div>
                    
                </div>

                <div class="finding-body">
                    
                    <div class="finding-section">
                        <h4>Evidence</h4>
                        <ul class="evidence-list">
                            
                            <li>One-Command Build/Setup assessment not yet implemented</li>
                            
                        </ul>
                    </div>
                    

                    

                    
                </div>
            </div>
            
            <div class="finding"
                 data-status="not_applicable"
                 data-tier="2"
                 data-category="Context Window Optimization"
                 data-score="0"
                 data-name="concise structured documentation">

                <div class="finding-header" onclick="toggleFinding(this)">
                    <div class="finding-title">
                        <span class="status-icon">
                            ‚äò
                            
                        </span>
                        <div class="finding-info">
                            <h3>Concise Structured Documentation</h3>
                            <div class="finding-meta">
                                Context Window Optimization ‚Ä¢
                                <span class="tier-badge tier-2">Tier 2</span>
                                
                            </div>
                        </div>
                    </div>
                    
                    <div class="score-display score-skip">‚Äî</div>
                    
                </div>

                <div class="finding-body">
                    
                    <div class="finding-section">
                        <h4>Evidence</h4>
                        <ul class="evidence-list">
                            
                            <li>Concise Structured Documentation assessment not yet implemented</li>
                            
                        </ul>
                    </div>
                    

                    

                    
                </div>
            </div>
            
            <div class="finding"
                 data-status="not_applicable"
                 data-tier="2"
                 data-category="Documentation Standards"
                 data-score="0"
                 data-name="inline documentation">

                <div class="finding-header" onclick="toggleFinding(this)">
                    <div class="finding-title">
                        <span class="status-icon">
                            ‚äò
                            
                        </span>
                        <div class="finding-info">
                            <h3>Inline Documentation</h3>
                            <div class="finding-meta">
                                Documentation Standards ‚Ä¢
                                <span class="tier-badge tier-2">Tier 2</span>
                                
                            </div>
                        </div>
                    </div>
                    
                    <div class="score-display score-skip">‚Äî</div>
                    
                </div>

                <div class="finding-body">
                    
                    <div class="finding-section">
                        <h4>Evidence</h4>
                        <ul class="evidence-list">
                            
                            <li>Inline Documentation assessment not yet implemented</li>
                            
                        </ul>
                    </div>
                    

                    

                    
                </div>
            </div>
            
            <div class="finding"
                 data-status="not_applicable"
                 data-tier="2"
                 data-category="Context Window Optimization"
                 data-score="0"
                 data-name="file size limits">

                <div class="finding-header" onclick="toggleFinding(this)">
                    <div class="finding-title">
                        <span class="status-icon">
                            ‚äò
                            
                        </span>
                        <div class="finding-info">
                            <h3>File Size Limits</h3>
                            <div class="finding-meta">
                                Context Window Optimization ‚Ä¢
                                <span class="tier-badge tier-2">Tier 2</span>
                                
                            </div>
                        </div>
                    </div>
                    
                    <div class="score-display score-skip">‚Äî</div>
                    
                </div>

                <div class="finding-body">
                    
                    <div class="finding-section">
                        <h4>Evidence</h4>
                        <ul class="evidence-list">
                            
                            <li>File Size Limits assessment not yet implemented</li>
                            
                        </ul>
                    </div>
                    

                    

                    
                </div>
            </div>
            
            <div class="finding"
                 data-status="not_applicable"
                 data-tier="2"
                 data-category="Dependency Management"
                 data-score="0"
                 data-name="dependency freshness & security">

                <div class="finding-header" onclick="toggleFinding(this)">
                    <div class="finding-title">
                        <span class="status-icon">
                            ‚äò
                            
                        </span>
                        <div class="finding-info">
                            <h3>Dependency Freshness & Security</h3>
                            <div class="finding-meta">
                                Dependency Management ‚Ä¢
                                <span class="tier-badge tier-2">Tier 2</span>
                                
                            </div>
                        </div>
                    </div>
                    
                    <div class="score-display score-skip">‚Äî</div>
                    
                </div>

                <div class="finding-body">
                    
                    <div class="finding-section">
                        <h4>Evidence</h4>
                        <ul class="evidence-list">
                            
                            <li>Dependency Freshness & Security assessment not yet implemented</li>
                            
                        </ul>
                    </div>
                    

                    

                    
                </div>
            </div>
            
            <div class="finding"
                 data-status="not_applicable"
                 data-tier="2"
                 data-category="Repository Structure"
                 data-score="0"
                 data-name="separation of concerns">

                <div class="finding-header" onclick="toggleFinding(this)">
                    <div class="finding-title">
                        <span class="status-icon">
                            ‚äò
                            
                        </span>
                        <div class="finding-info">
                            <h3>Separation of Concerns</h3>
                            <div class="finding-meta">
                                Repository Structure ‚Ä¢
                                <span class="tier-badge tier-2">Tier 2</span>
                                
                            </div>
                        </div>
                    </div>
                    
                    <div class="score-display score-skip">‚Äî</div>
                    
                </div>

                <div class="finding-body">
                    
                    <div class="finding-section">
                        <h4>Evidence</h4>
                        <ul class="evidence-list">
                            
                            <li>Separation of Concerns assessment not yet implemented</li>
                            
                        </ul>
                    </div>
                    

                    

                    
                </div>
            </div>
            
            <div class="finding"
                 data-status="not_applicable"
                 data-tier="3"
                 data-category="Error Handling"
                 data-score="0"
                 data-name="structured logging">

                <div class="finding-header" onclick="toggleFinding(this)">
                    <div class="finding-title">
                        <span class="status-icon">
                            ‚äò
                            
                        </span>
                        <div class="finding-info">
                            <h3>Structured Logging</h3>
                            <div class="finding-meta">
                                Error Handling ‚Ä¢
                                <span class="tier-badge tier-3">Tier 3</span>
                                
                            </div>
                        </div>
                    </div>
                    
                    <div class="score-display score-skip">‚Äî</div>
                    
                </div>

                <div class="finding-body">
                    
                    <div class="finding-section">
                        <h4>Evidence</h4>
                        <ul class="evidence-list">
                            
                            <li>Structured Logging assessment not yet implemented</li>
                            
                        </ul>
                    </div>
                    

                    

                    
                </div>
            </div>
            
            <div class="finding"
                 data-status="not_applicable"
                 data-tier="3"
                 data-category="API Documentation"
                 data-score="0"
                 data-name="openapi/swagger specifications">

                <div class="finding-header" onclick="toggleFinding(this)">
                    <div class="finding-title">
                        <span class="status-icon">
                            ‚äò
                            
                        </span>
                        <div class="finding-info">
                            <h3>OpenAPI/Swagger Specifications</h3>
                            <div class="finding-meta">
                                API Documentation ‚Ä¢
                                <span class="tier-badge tier-3">Tier 3</span>
                                
                            </div>
                        </div>
                    </div>
                    
                    <div class="score-display score-skip">‚Äî</div>
                    
                </div>

                <div class="finding-body">
                    
                    <div class="finding-section">
                        <h4>Evidence</h4>
                        <ul class="evidence-list">
                            
                            <li>OpenAPI/Swagger Specifications assessment not yet implemented</li>
                            
                        </ul>
                    </div>
                    

                    

                    
                </div>
            </div>
            
            <div class="finding"
                 data-status="not_applicable"
                 data-tier="3"
                 data-category="Documentation Standards"
                 data-score="0"
                 data-name="architecture decision records">

                <div class="finding-header" onclick="toggleFinding(this)">
                    <div class="finding-title">
                        <span class="status-icon">
                            ‚äò
                            
                        </span>
                        <div class="finding-info">
                            <h3>Architecture Decision Records</h3>
                            <div class="finding-meta">
                                Documentation Standards ‚Ä¢
                                <span class="tier-badge tier-3">Tier 3</span>
                                
                            </div>
                        </div>
                    </div>
                    
                    <div class="score-display score-skip">‚Äî</div>
                    
                </div>

                <div class="finding-body">
                    
                    <div class="finding-section">
                        <h4>Evidence</h4>
                        <ul class="evidence-list">
                            
                            <li>Architecture Decision Records assessment not yet implemented</li>
                            
                        </ul>
                    </div>
                    

                    

                    
                </div>
            </div>
            
            <div class="finding"
                 data-status="not_applicable"
                 data-tier="3"
                 data-category="Modularity"
                 data-score="0"
                 data-name="semantic file & directory naming">

                <div class="finding-header" onclick="toggleFinding(this)">
                    <div class="finding-title">
                        <span class="status-icon">
                            ‚äò
                            
                        </span>
                        <div class="finding-info">
                            <h3>Semantic File & Directory Naming</h3>
                            <div class="finding-meta">
                                Modularity ‚Ä¢
                                <span class="tier-badge tier-3">Tier 3</span>
                                
                            </div>
                        </div>
                    </div>
                    
                    <div class="score-display score-skip">‚Äî</div>
                    
                </div>

                <div class="finding-body">
                    
                    <div class="finding-section">
                        <h4>Evidence</h4>
                        <ul class="evidence-list">
                            
                            <li>Semantic File & Directory Naming assessment not yet implemented</li>
                            
                        </ul>
                    </div>
                    

                    

                    
                </div>
            </div>
            
            <div class="finding"
                 data-status="not_applicable"
                 data-tier="4"
                 data-category="Security"
                 data-score="0"
                 data-name="security scanning automation">

                <div class="finding-header" onclick="toggleFinding(this)">
                    <div class="finding-title">
                        <span class="status-icon">
                            ‚äò
                            
                        </span>
                        <div class="finding-info">
                            <h3>Security Scanning Automation</h3>
                            <div class="finding-meta">
                                Security ‚Ä¢
                                <span class="tier-badge tier-4">Tier 4</span>
                                
                            </div>
                        </div>
                    </div>
                    
                    <div class="score-display score-skip">‚Äî</div>
                    
                </div>

                <div class="finding-body">
                    
                    <div class="finding-section">
                        <h4>Evidence</h4>
                        <ul class="evidence-list">
                            
                            <li>Security Scanning Automation assessment not yet implemented</li>
                            
                        </ul>
                    </div>
                    

                    

                    
                </div>
            </div>
            
            <div class="finding"
                 data-status="not_applicable"
                 data-tier="4"
                 data-category="Performance"
                 data-score="0"
                 data-name="performance benchmarks">

                <div class="finding-header" onclick="toggleFinding(this)">
                    <div class="finding-title">
                        <span class="status-icon">
                            ‚äò
                            
                        </span>
                        <div class="finding-info">
                            <h3>Performance Benchmarks</h3>
                            <div class="finding-meta">
                                Performance ‚Ä¢
                                <span class="tier-badge tier-4">Tier 4</span>
                                
                            </div>
                        </div>
                    </div>
                    
                    <div class="score-display score-skip">‚Äî</div>
                    
                </div>

                <div class="finding-body">
                    
                    <div class="finding-section">
                        <h4>Evidence</h4>
                        <ul class="evidence-list">
                            
                            <li>Performance Benchmarks assessment not yet implemented</li>
                            
                        </ul>
                    </div>
                    

                    

                    
                </div>
            </div>
            
            <div class="finding"
                 data-status="not_applicable"
                 data-tier="4"
                 data-category="Code Quality"
                 data-score="0"
                 data-name="code smell elimination">

                <div class="finding-header" onclick="toggleFinding(this)">
                    <div class="finding-title">
                        <span class="status-icon">
                            ‚äò
                            
                        </span>
                        <div class="finding-info">
                            <h3>Code Smell Elimination</h3>
                            <div class="finding-meta">
                                Code Quality ‚Ä¢
                                <span class="tier-badge tier-4">Tier 4</span>
                                
                            </div>
                        </div>
                    </div>
                    
                    <div class="score-display score-skip">‚Äî</div>
                    
                </div>

                <div class="finding-body">
                    
                    <div class="finding-section">
                        <h4>Evidence</h4>
                        <ul class="evidence-list">
                            
                            <li>Code Smell Elimination assessment not yet implemented</li>
                            
                        </ul>
                    </div>
                    

                    

                    
                </div>
            </div>
            
            <div class="finding"
                 data-status="not_applicable"
                 data-tier="4"
                 data-category="Git & Version Control"
                 data-score="0"
                 data-name="issue & pull request templates">

                <div class="finding-header" onclick="toggleFinding(this)">
                    <div class="finding-title">
                        <span class="status-icon">
                            ‚äò
                            
                        </span>
                        <div class="finding-info">
                            <h3>Issue & Pull Request Templates</h3>
                            <div class="finding-meta">
                                Git & Version Control ‚Ä¢
                                <span class="tier-badge tier-4">Tier 4</span>
                                
                            </div>
                        </div>
                    </div>
                    
                    <div class="score-display score-skip">‚Äî</div>
                    
                </div>

                <div class="finding-body">
                    
                    <div class="finding-section">
                        <h4>Evidence</h4>
                        <ul class="evidence-list">
                            
                            <li>Issue & Pull Request Templates assessment not yet implemented</li>
                            
                        </ul>
                    </div>
                    

                    

                    
                </div>
            </div>
            
            <div class="finding"
                 data-status="not_applicable"
                 data-tier="4"
                 data-category="Build & Development"
                 data-score="0"
                 data-name="container/virtualization setup">

                <div class="finding-header" onclick="toggleFinding(this)">
                    <div class="finding-title">
                        <span class="status-icon">
                            ‚äò
                            
                        </span>
                        <div class="finding-info">
                            <h3>Container/Virtualization Setup</h3>
                            <div class="finding-meta">
                                Build & Development ‚Ä¢
                                <span class="tier-badge tier-4">Tier 4</span>
                                
                            </div>
                        </div>
                    </div>
                    
                    <div class="score-display score-skip">‚Äî</div>
                    
                </div>

                <div class="finding-body">
                    
                    <div class="finding-section">
                        <h4>Evidence</h4>
                        <ul class="evidence-list">
                            
                            <li>Container/Virtualization Setup assessment not yet implemented</li>
                            
                        </ul>
                    </div>
                    

                    

                    
                </div>
            </div>
            
        </div>

        <footer>
            <p>Generated by <strong>AgentReady v1.0.0</strong></p>
            <p>Repository: /Users/jeder/repos/sk/agentready ‚Ä¢ Branch: 001-agentready-scorer ‚Ä¢ Commit: adfc4c82</p>
            <p style="margin-top: 10px; font-size: 0.85rem;">
                ü§ñ Generated with <a href="https://claude.com/claude-code" style="color: #2563eb;">Claude Code</a>
            </p>
        </footer>
    </div>

    <script>
        // Embedded assessment data
        const ASSESSMENT = {"repository": {"path": "/Users/jeder/repos/sk/agentready", "name": "agentready", "url": null, "branch": "001-agentready-scorer", "commit_hash": "adfc4c823a7b25d5b1404c9b994461e290257dce", "languages": {"Markdown": 36, "Shell": 5, "Python": 31}, "total_files": 79, "total_lines": 13852}, "timestamp": "2025-11-21T02:11:05.766923", "overall_score": 75.4, "certification_level": "Gold", "attributes_assessed": 10, "attributes_skipped": 15, "attributes_total": 25, "findings": [{"attribute": {"id": "claude_md_file", "name": "CLAUDE.md Configuration Files", "category": "Context Window Optimization", "tier": 1, "description": "Project-specific configuration for Claude Code", "criteria": "CLAUDE.md file exists in repository root", "default_weight": 0.1}, "status": "pass", "score": 100.0, "measured_value": "present", "threshold": "present", "evidence": ["CLAUDE.md found at /Users/jeder/repos/sk/agentready/CLAUDE.md"], "remediation": null, "error_message": null}, {"attribute": {"id": "readme_structure", "name": "README Structure", "category": "Documentation Standards", "tier": 1, "description": "Well-structured README with key sections", "criteria": "README.md with installation, usage, and development sections", "default_weight": 0.1}, "status": "pass", "score": 100.0, "measured_value": "3/3 sections", "threshold": "3/3 sections", "evidence": ["Found 3/3 essential sections", "Installation: \u2713", "Usage: \u2713", "Development: \u2713"], "remediation": null, "error_message": null}, {"attribute": {"id": "type_annotations", "name": "Type Annotations", "category": "Code Quality", "tier": 1, "description": "Type hints in function signatures", "criteria": ">80% of functions have type annotations", "default_weight": 0.1}, "status": "pass", "score": 100.0, "measured_value": "95.9%", "threshold": "\u226580%", "evidence": ["Typed functions: 142/148", "Coverage: 95.9%"], "remediation": null, "error_message": null}, {"attribute": {"id": "standard_layout", "name": "Standard Project Layouts", "category": "Repository Structure", "tier": 1, "description": "Follows standard project structure for language", "criteria": "Standard directories (src/, tests/, docs/) present", "default_weight": 0.1}, "status": "pass", "score": 100.0, "measured_value": "2/2 directories", "threshold": "2/2 directories", "evidence": ["Found 2/2 standard directories", "src/: \u2713", "tests/: \u2713"], "remediation": null, "error_message": null}, {"attribute": {"id": "lock_files", "name": "Lock Files for Reproducibility", "category": "Dependency Management", "tier": 1, "description": "Lock files present for dependency pinning", "criteria": "package-lock.json, yarn.lock, poetry.lock, or requirements.txt with versions", "default_weight": 0.1}, "status": "fail", "score": 0.0, "measured_value": "none", "threshold": "at least one lock file", "evidence": ["No lock files found"], "remediation": {"summary": "Add lock file for dependency reproducibility", "steps": ["Use npm install, poetry lock, or equivalent to generate lock file"], "tools": [], "commands": ["npm install  # generates package-lock.json"], "examples": [], "citations": []}, "error_message": null}, {"attribute": {"id": "test_coverage", "name": "Test Coverage Requirements", "category": "Testing & CI/CD", "tier": 2, "description": "Test coverage thresholds configured and enforced", "criteria": ">80% code coverage", "default_weight": 0.03}, "status": "pass", "score": 100.0, "measured_value": "configured", "threshold": "configured with >80% threshold", "evidence": ["Coverage configuration found", "pytest-cov dependency present"], "remediation": null, "error_message": null}, {"attribute": {"id": "precommit_hooks", "name": "Pre-commit Hooks & CI/CD Linting", "category": "Testing & CI/CD", "tier": 2, "description": "Pre-commit hooks configured for linting and formatting", "criteria": ".pre-commit-config.yaml exists", "default_weight": 0.03}, "status": "fail", "score": 0.0, "measured_value": "not configured", "threshold": "configured", "evidence": [".pre-commit-config.yaml not found"], "remediation": {"summary": "Configure pre-commit hooks for automated code quality checks", "steps": ["Install pre-commit framework", "Create .pre-commit-config.yaml", "Add hooks for linting and formatting", "Install hooks: pre-commit install", "Run on all files: pre-commit run --all-files"], "tools": ["pre-commit"], "commands": ["pip install pre-commit", "pre-commit install", "pre-commit run --all-files"], "examples": ["# .pre-commit-config.yaml\nrepos:\n  - repo: https://github.com/pre-commit/pre-commit-hooks\n    rev: v4.4.0\n    hooks:\n      - id: trailing-whitespace\n      - id: end-of-file-fixer\n      - id: check-yaml\n      - id: check-added-large-files\n\n  - repo: https://github.com/psf/black\n    rev: 23.3.0\n    hooks:\n      - id: black\n\n  - repo: https://github.com/pycqa/isort\n    rev: 5.12.0\n    hooks:\n      - id: isort\n"], "citations": [{"source": "pre-commit.com", "title": "Pre-commit Framework", "url": "https://pre-commit.com/", "relevance": "Official pre-commit documentation"}]}, "error_message": null}, {"attribute": {"id": "conventional_commits", "name": "Conventional Commit Messages", "category": "Git & Version Control", "tier": 2, "description": "Follows conventional commit format", "criteria": "\u226580% of recent commits follow convention", "default_weight": 0.03}, "status": "fail", "score": 0.0, "measured_value": "not configured", "threshold": "configured", "evidence": ["No commitlint or husky configuration"], "remediation": {"summary": "Configure conventional commits with commitlint", "steps": ["Install commitlint", "Configure husky for commit-msg hook"], "tools": ["commitlint", "husky"], "commands": ["npm install --save-dev @commitlint/cli @commitlint/config-conventional husky"], "examples": [], "citations": []}, "error_message": null}, {"attribute": {"id": "gitignore_completeness", "name": ".gitignore Completeness", "category": "Git & Version Control", "tier": 2, "description": "Comprehensive .gitignore file", "criteria": ".gitignore exists and covers common patterns", "default_weight": 0.03}, "status": "pass", "score": 100.0, "measured_value": "580 bytes", "threshold": ">50 bytes", "evidence": [".gitignore found (580 bytes)"], "remediation": null, "error_message": null}, {"attribute": {"id": "cyclomatic_complexity", "name": "Cyclomatic Complexity Thresholds", "category": "Code Quality", "tier": 3, "description": "Cyclomatic complexity thresholds enforced", "criteria": "Average complexity <10, no functions >15", "default_weight": 0.03}, "status": "pass", "score": 100.0, "measured_value": "3.1", "threshold": "<10.0", "evidence": ["Average cyclomatic complexity: 3.1"], "remediation": null, "error_message": null}, {"attribute": {"id": "one_command_setup", "name": "One-Command Build/Setup", "category": "Build & Development", "tier": 2, "description": "Assessment for One-Command Build/Setup", "criteria": "To be implemented", "default_weight": 0.03}, "status": "not_applicable", "score": null, "measured_value": null, "threshold": null, "evidence": ["One-Command Build/Setup assessment not yet implemented"], "remediation": null, "error_message": null}, {"attribute": {"id": "concise_documentation", "name": "Concise Structured Documentation", "category": "Context Window Optimization", "tier": 2, "description": "Assessment for Concise Structured Documentation", "criteria": "To be implemented", "default_weight": 0.03}, "status": "not_applicable", "score": null, "measured_value": null, "threshold": null, "evidence": ["Concise Structured Documentation assessment not yet implemented"], "remediation": null, "error_message": null}, {"attribute": {"id": "inline_documentation", "name": "Inline Documentation", "category": "Documentation Standards", "tier": 2, "description": "Assessment for Inline Documentation", "criteria": "To be implemented", "default_weight": 0.03}, "status": "not_applicable", "score": null, "measured_value": null, "threshold": null, "evidence": ["Inline Documentation assessment not yet implemented"], "remediation": null, "error_message": null}, {"attribute": {"id": "file_size_limits", "name": "File Size Limits", "category": "Context Window Optimization", "tier": 2, "description": "Assessment for File Size Limits", "criteria": "To be implemented", "default_weight": 0.03}, "status": "not_applicable", "score": null, "measured_value": null, "threshold": null, "evidence": ["File Size Limits assessment not yet implemented"], "remediation": null, "error_message": null}, {"attribute": {"id": "dependency_freshness", "name": "Dependency Freshness & Security", "category": "Dependency Management", "tier": 2, "description": "Assessment for Dependency Freshness & Security", "criteria": "To be implemented", "default_weight": 0.03}, "status": "not_applicable", "score": null, "measured_value": null, "threshold": null, "evidence": ["Dependency Freshness & Security assessment not yet implemented"], "remediation": null, "error_message": null}, {"attribute": {"id": "separation_concerns", "name": "Separation of Concerns", "category": "Repository Structure", "tier": 2, "description": "Assessment for Separation of Concerns", "criteria": "To be implemented", "default_weight": 0.03}, "status": "not_applicable", "score": null, "measured_value": null, "threshold": null, "evidence": ["Separation of Concerns assessment not yet implemented"], "remediation": null, "error_message": null}, {"attribute": {"id": "structured_logging", "name": "Structured Logging", "category": "Error Handling", "tier": 3, "description": "Assessment for Structured Logging", "criteria": "To be implemented", "default_weight": 0.03}, "status": "not_applicable", "score": null, "measured_value": null, "threshold": null, "evidence": ["Structured Logging assessment not yet implemented"], "remediation": null, "error_message": null}, {"attribute": {"id": "openapi_specs", "name": "OpenAPI/Swagger Specifications", "category": "API Documentation", "tier": 3, "description": "Assessment for OpenAPI/Swagger Specifications", "criteria": "To be implemented", "default_weight": 0.03}, "status": "not_applicable", "score": null, "measured_value": null, "threshold": null, "evidence": ["OpenAPI/Swagger Specifications assessment not yet implemented"], "remediation": null, "error_message": null}, {"attribute": {"id": "architecture_decisions", "name": "Architecture Decision Records", "category": "Documentation Standards", "tier": 3, "description": "Assessment for Architecture Decision Records", "criteria": "To be implemented", "default_weight": 0.03}, "status": "not_applicable", "score": null, "measured_value": null, "threshold": null, "evidence": ["Architecture Decision Records assessment not yet implemented"], "remediation": null, "error_message": null}, {"attribute": {"id": "semantic_naming", "name": "Semantic File & Directory Naming", "category": "Modularity", "tier": 3, "description": "Assessment for Semantic File & Directory Naming", "criteria": "To be implemented", "default_weight": 0.03}, "status": "not_applicable", "score": null, "measured_value": null, "threshold": null, "evidence": ["Semantic File & Directory Naming assessment not yet implemented"], "remediation": null, "error_message": null}, {"attribute": {"id": "security_scanning", "name": "Security Scanning Automation", "category": "Security", "tier": 4, "description": "Assessment for Security Scanning Automation", "criteria": "To be implemented", "default_weight": 0.01}, "status": "not_applicable", "score": null, "measured_value": null, "threshold": null, "evidence": ["Security Scanning Automation assessment not yet implemented"], "remediation": null, "error_message": null}, {"attribute": {"id": "performance_benchmarks", "name": "Performance Benchmarks", "category": "Performance", "tier": 4, "description": "Assessment for Performance Benchmarks", "criteria": "To be implemented", "default_weight": 0.01}, "status": "not_applicable", "score": null, "measured_value": null, "threshold": null, "evidence": ["Performance Benchmarks assessment not yet implemented"], "remediation": null, "error_message": null}, {"attribute": {"id": "code_smells", "name": "Code Smell Elimination", "category": "Code Quality", "tier": 4, "description": "Assessment for Code Smell Elimination", "criteria": "To be implemented", "default_weight": 0.01}, "status": "not_applicable", "score": null, "measured_value": null, "threshold": null, "evidence": ["Code Smell Elimination assessment not yet implemented"], "remediation": null, "error_message": null}, {"attribute": {"id": "issue_pr_templates", "name": "Issue & Pull Request Templates", "category": "Git & Version Control", "tier": 4, "description": "Assessment for Issue & Pull Request Templates", "criteria": "To be implemented", "default_weight": 0.01}, "status": "not_applicable", "score": null, "measured_value": null, "threshold": null, "evidence": ["Issue & Pull Request Templates assessment not yet implemented"], "remediation": null, "error_message": null}, {"attribute": {"id": "container_setup", "name": "Container/Virtualization Setup", "category": "Build & Development", "tier": 4, "description": "Assessment for Container/Virtualization Setup", "criteria": "To be implemented", "default_weight": 0.01}, "status": "not_applicable", "score": null, "measured_value": null, "threshold": null, "evidence": ["Container/Virtualization Setup assessment not yet implemented"], "remediation": null, "error_message": null}], "config": null, "duration_seconds": 0.2};

        // Toggle finding expansion
        function toggleFinding(header) {
            const finding = header.parentElement;
            finding.classList.toggle('expanded');
        }

        // Filter functionality
        document.querySelectorAll('.filter-btn').forEach(btn => {
            btn.addEventListener('click', () => {
                // Update active button
                document.querySelectorAll('.filter-btn').forEach(b => b.classList.remove('active'));
                btn.classList.add('active');

                const filter = btn.dataset.filter;
                const findings = document.querySelectorAll('.finding');

                findings.forEach(finding => {
                    const status = finding.dataset.status;

                    if (filter === 'all') {
                        finding.classList.remove('hidden');
                    } else if (filter === 'skipped') {
                        if (status === 'skipped' || status === 'not_applicable' || status === 'error') {
                            finding.classList.remove('hidden');
                        } else {
                            finding.classList.add('hidden');
                        }
                    } else {
                        if (status === filter) {
                            finding.classList.remove('hidden');
                        } else {
                            finding.classList.add('hidden');
                        }
                    }
                });
            });
        });

        // Sort functionality
        document.getElementById('sort-select').addEventListener('change', (e) => {
            const sortBy = e.target.value;
            const container = document.getElementById('findings-container');
            const findings = Array.from(container.querySelectorAll('.finding'));

            findings.sort((a, b) => {
                if (sortBy === 'category') {
                    return a.dataset.category.localeCompare(b.dataset.category);
                } else if (sortBy === 'score-desc') {
                    return parseFloat(b.dataset.score) - parseFloat(a.dataset.score);
                } else if (sortBy === 'score-asc') {
                    return parseFloat(a.dataset.score) - parseFloat(b.dataset.score);
                } else if (sortBy === 'tier') {
                    return parseInt(a.dataset.tier) - parseInt(b.dataset.tier);
                }
            });

            findings.forEach(finding => container.appendChild(finding));
        });

        // Search functionality
        document.getElementById('search-box').addEventListener('input', (e) => {
            const query = e.target.value.toLowerCase();
            const findings = document.querySelectorAll('.finding');

            findings.forEach(finding => {
                const name = finding.dataset.name;
                const category = finding.dataset.category.toLowerCase();

                if (name.includes(query) || category.includes(query)) {
                    finding.classList.remove('hidden');
                } else {
                    finding.classList.add('hidden');
                }
            });
        });
    </script>
</body>
</html>
</file>

<file path=".agentready/report-20251121-021105.md">
# ü§ñ AgentReady Assessment Report

**Repository**: agentready
**Path**: `/Users/jeder/repos/sk/agentready`
**Branch**: 001-agentready-scorer
**Commit**: adfc4c82
**Date**: 2025-11-21 02:11:05

---

## üìä Summary

| Metric | Value |
|--------|-------|
| **Overall Score** | **75.4/100** |
| **Certification Level** | **Gold** |
| **Attributes Assessed** | 10/25 |
| **Attributes Skipped** | 15 |
| **Assessment Duration** | 0.2s |

### Languages Detected

- **Markdown**: 36 files
- **Python**: 31 files
- **Shell**: 5 files

### Repository Stats

- **Total Files**: 79
- **Total Lines**: 13,852

## üéñÔ∏è Certification Ladder

- üíé **Platinum** (90-100) 
- ü•á **Gold** (75-89) **‚Üí YOUR LEVEL ‚Üê**
- ü•à **Silver** (60-74) 
- ü•â **Bronze** (40-59) 
- ‚ö†Ô∏è **Needs Improvement** (0-39) 

## üìã Detailed Findings

### API Documentation

| Attribute | Tier | Status | Score |
|-----------|------|--------|-------|
| OpenAPI/Swagger Specifications | T3 | ‚äò not_applicable | ‚Äî |

### Build & Development

| Attribute | Tier | Status | Score |
|-----------|------|--------|-------|
| One-Command Build/Setup | T2 | ‚äò not_applicable | ‚Äî |
| Container/Virtualization Setup | T4 | ‚äò not_applicable | ‚Äî |

### Code Quality

| Attribute | Tier | Status | Score |
|-----------|------|--------|-------|
| Type Annotations | T1 | ‚úÖ pass | 100 |
| Cyclomatic Complexity Thresholds | T3 | ‚úÖ pass | 100 |
| Code Smell Elimination | T4 | ‚äò not_applicable | ‚Äî |

### Context Window Optimization

| Attribute | Tier | Status | Score |
|-----------|------|--------|-------|
| CLAUDE.md Configuration Files | T1 | ‚úÖ pass | 100 |
| Concise Structured Documentation | T2 | ‚äò not_applicable | ‚Äî |
| File Size Limits | T2 | ‚äò not_applicable | ‚Äî |

### Dependency Management

| Attribute | Tier | Status | Score |
|-----------|------|--------|-------|
| Lock Files for Reproducibility | T1 | ‚ùå fail | 0 |
| Dependency Freshness & Security | T2 | ‚äò not_applicable | ‚Äî |

#### ‚ùå Lock Files for Reproducibility

**Measured**: none (Threshold: at least one lock file)

**Evidence**:
- No lock files found

<details><summary><strong>üìù Remediation Steps</strong></summary>


Add lock file for dependency reproducibility

1. Use npm install, poetry lock, or equivalent to generate lock file

**Commands**:

```bash
npm install  # generates package-lock.json
```

</details>

### Documentation Standards

| Attribute | Tier | Status | Score |
|-----------|------|--------|-------|
| README Structure | T1 | ‚úÖ pass | 100 |
| Inline Documentation | T2 | ‚äò not_applicable | ‚Äî |
| Architecture Decision Records | T3 | ‚äò not_applicable | ‚Äî |

### Error Handling

| Attribute | Tier | Status | Score |
|-----------|------|--------|-------|
| Structured Logging | T3 | ‚äò not_applicable | ‚Äî |

### Git & Version Control

| Attribute | Tier | Status | Score |
|-----------|------|--------|-------|
| Conventional Commit Messages | T2 | ‚ùå fail | 0 |
| .gitignore Completeness | T2 | ‚úÖ pass | 100 |
| Issue & Pull Request Templates | T4 | ‚äò not_applicable | ‚Äî |

#### ‚ùå Conventional Commit Messages

**Measured**: not configured (Threshold: configured)

**Evidence**:
- No commitlint or husky configuration

<details><summary><strong>üìù Remediation Steps</strong></summary>


Configure conventional commits with commitlint

1. Install commitlint
2. Configure husky for commit-msg hook

**Commands**:

```bash
npm install --save-dev @commitlint/cli @commitlint/config-conventional husky
```

</details>

### Modularity

| Attribute | Tier | Status | Score |
|-----------|------|--------|-------|
| Semantic File & Directory Naming | T3 | ‚äò not_applicable | ‚Äî |

### Performance

| Attribute | Tier | Status | Score |
|-----------|------|--------|-------|
| Performance Benchmarks | T4 | ‚äò not_applicable | ‚Äî |

### Repository Structure

| Attribute | Tier | Status | Score |
|-----------|------|--------|-------|
| Standard Project Layouts | T1 | ‚úÖ pass | 100 |
| Separation of Concerns | T2 | ‚äò not_applicable | ‚Äî |

### Security

| Attribute | Tier | Status | Score |
|-----------|------|--------|-------|
| Security Scanning Automation | T4 | ‚äò not_applicable | ‚Äî |

### Testing & CI/CD

| Attribute | Tier | Status | Score |
|-----------|------|--------|-------|
| Test Coverage Requirements | T2 | ‚úÖ pass | 100 |
| Pre-commit Hooks & CI/CD Linting | T2 | ‚ùå fail | 0 |

#### ‚ùå Pre-commit Hooks & CI/CD Linting

**Measured**: not configured (Threshold: configured)

**Evidence**:
- .pre-commit-config.yaml not found

<details><summary><strong>üìù Remediation Steps</strong></summary>


Configure pre-commit hooks for automated code quality checks

1. Install pre-commit framework
2. Create .pre-commit-config.yaml
3. Add hooks for linting and formatting
4. Install hooks: pre-commit install
5. Run on all files: pre-commit run --all-files

**Commands**:

```bash
pip install pre-commit
pre-commit install
pre-commit run --all-files
```

**Examples**:

```
# .pre-commit-config.yaml
repos:
  - repo: https://github.com/pre-commit/pre-commit-hooks
    rev: v4.4.0
    hooks:
      - id: trailing-whitespace
      - id: end-of-file-fixer
      - id: check-yaml
      - id: check-added-large-files

  - repo: https://github.com/psf/black
    rev: 23.3.0
    hooks:
      - id: black

  - repo: https://github.com/pycqa/isort
    rev: 5.12.0
    hooks:
      - id: isort

```

</details>

## üéØ Next Steps

**Priority Improvements** (highest impact first):

1. **Lock Files for Reproducibility** (Tier 1) - +10.0 points potential
   - Add lock file for dependency reproducibility
2. **Pre-commit Hooks & CI/CD Linting** (Tier 2) - +3.0 points potential
   - Configure pre-commit hooks for automated code quality checks
3. **Conventional Commit Messages** (Tier 2) - +3.0 points potential
   - Configure conventional commits with commitlint

---

## üìù Assessment Metadata

- **Tool Version**: AgentReady v1.0.0
- **Research Report**: Bundled version
- **Repository Snapshot**: adfc4c823a7b25d5b1404c9b994461e290257dce
- **Assessment Duration**: 0.2s

ü§ñ Generated with [Claude Code](https://claude.com/claude-code)
</file>

<file path=".claude/settings.local.json">
{
  "permissions": {
    "allow": [
      "Bash(tree:*)",
      "Bash(agentready bootstrap:*)",
      "Bash(agentready bootstrap --help:*)"
    ],
    "deny": [],
    "ask": []
  }
}
</file>

<file path="src/agentready.egg-info/dependency_links.txt">

</file>

<file path="src/agentready.egg-info/entry_points.txt">
[console_scripts]
agentready = agentready.cli.main:cli
</file>

<file path="src/agentready.egg-info/PKG-INFO">
Metadata-Version: 2.4
Name: agentready
Version: 1.0.0
Summary: Assess git repositories against 25 evidence-based attributes for AI-assisted development readiness
Author-email: Jeremy Eder <jeder@redhat.com>
License: MIT
Classifier: Development Status :: 4 - Beta
Classifier: Intended Audience :: Developers
Classifier: License :: OSI Approved :: MIT License
Classifier: Programming Language :: Python :: 3.11
Classifier: Programming Language :: Python :: 3.12
Classifier: Topic :: Software Development :: Quality Assurance
Requires-Python: >=3.11
Description-Content-Type: text/markdown
Requires-Dist: click>=8.1.0
Requires-Dist: jinja2>=3.1.0
Requires-Dist: pyyaml>=6.0
Requires-Dist: gitpython>=3.1.0
Requires-Dist: radon>=6.0.0
Requires-Dist: lizard>=1.17.0
Provides-Extra: dev
Requires-Dist: pytest>=7.4.0; extra == "dev"
Requires-Dist: pytest-cov>=4.1.0; extra == "dev"
Requires-Dist: black>=23.0.0; extra == "dev"
Requires-Dist: isort>=5.12.0; extra == "dev"
Requires-Dist: flake8>=6.0.0; extra == "dev"

# AgentReady Repository Scorer

Assess git repositories against 25 evidence-based attributes for AI-assisted development readiness.

## Overview

AgentReady evaluates your repository across multiple dimensions of code quality, documentation, testing, and infrastructure to determine how well-suited it is for AI-assisted development workflows. The tool generates comprehensive reports with:

- **Overall Score & Certification**: Platinum/Gold/Silver/Bronze based on 25 attributes
- **Interactive HTML Reports**: Filter, sort, and explore findings with embedded guidance
- **Version-Control-Friendly Markdown**: Track progress over time with git-diffable reports
- **Actionable Remediation**: Specific tools, commands, and examples to improve each attribute

## Quick Start

### Installation

```bash
# Clone the repository
git clone https://github.com/yourusername/agentready.git
cd agentready

# Create virtual environment
python3 -m venv .venv
source .venv/bin/activate  # On Windows: .venv\Scripts\activate

# Install dependencies
pip install -e ".[dev]"
```

### Basic Usage

```bash
# Assess current repository
agentready .

# Assess another repository
agentready /path/to/your/repo

# Specify custom configuration
agentready /path/to/repo --config my-config.yaml

# Custom output directory
agentready /path/to/repo --output-dir ./reports
```

### Example Output

```
Assessing repository: myproject
Repository: /Users/username/myproject
Languages detected: Python (42 files), JavaScript (18 files)

Evaluating 25 attributes...
[‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë] 23/25 (2 skipped)

Overall Score: 72.5/100 (Silver)
Attributes Assessed: 23/25
Duration: 2m 7s

Reports generated:
  HTML: .agentready/report-latest.html
  Markdown: .agentready/report-latest.md
```

## Features

### 25 Evidence-Based Attributes

Evaluated across 13 categories:

1. **Context Window Optimization**: CLAUDE.md files, concise docs, file size limits
2. **Documentation Standards**: README structure, inline docs, ADRs
3. **Code Quality**: Cyclomatic complexity, file length, type annotations, code smells
4. **Repository Structure**: Standard layouts, separation of concerns
5. **Testing & CI/CD**: Coverage, test naming, pre-commit hooks
6. **Dependency Management**: Lock files, freshness, security
7. **Git & Version Control**: Conventional commits, gitignore, templates
8. **Build & Development**: One-command setup, dev docs, containers
9. **Error Handling**: Clear messages, structured logging
10. **API Documentation**: OpenAPI/Swagger specs
11. **Modularity**: DRY principle, naming conventions
12. **CI/CD Integration**: Pipeline visibility, branch protection
13. **Security**: Scanning automation, secrets management

### Tier-Based Scoring

Attributes are weighted by importance:

- **Tier 1 (Essential)**: 50% of total score - CLAUDE.md, README, types, layouts, lock files
- **Tier 2 (Critical)**: 30% of total score - Tests, commits, build setup
- **Tier 3 (Important)**: 15% of total score - Complexity, logging, API docs
- **Tier 4 (Advanced)**: 5% of total score - Security scanning, performance benchmarks

Missing essential attributes (especially CLAUDE.md at 10% weight) has 10x the impact of missing advanced features.

### Interactive HTML Reports

- Filter by status (Pass/Fail/Skipped)
- Sort by score, tier, or category
- Search attributes by name
- Collapsible sections with detailed evidence
- Color-coded score indicators
- Certification ladder visualization
- Works offline (no CDN dependencies)

### Customization

Create `.agentready-config.yaml` to customize weights:

```yaml
weights:
  claude_md_file: 0.15      # Increase importance (default: 0.10)
  test_coverage: 0.05       # Increase importance (default: 0.03)
  conventional_commits: 0.01  # Decrease importance (default: 0.03)
  # Other 22 attributes use defaults, rescaled to sum to 1.0

excluded_attributes:
  - performance_benchmarks  # Skip this attribute

output_dir: ./custom-reports
```

## CLI Reference

```bash
# Assessment commands
agentready PATH                          # Assess repository at PATH
agentready PATH --verbose                # Show detailed progress
agentready PATH --config FILE            # Use custom configuration
agentready PATH --output-dir DIR         # Custom report location

# Configuration commands
agentready --validate-config FILE        # Validate configuration
agentready --generate-config             # Create example config

# Research report management
agentready --research-version            # Show bundled research version
agentready --update-research             # Update to latest research
agentready --restore-bundled-research    # Restore original research

# Utility commands
agentready --version                     # Show tool version
agentready --help                        # Show help message
```

## Architecture

AgentReady follows a library-first design:

- **Models**: Data entities (Repository, Assessment, Finding, Attribute)
- **Assessors**: Independent evaluators for each attribute category
- **Services**: Scanner (orchestration), Scorer (calculation), LanguageDetector
- **Reporters**: HTML and Markdown report generators
- **CLI**: Thin wrapper orchestrating assessment workflow

## Development

### Run Tests

```bash
# Run all tests with coverage
pytest

# Run specific test suite
pytest tests/unit/
pytest tests/integration/
pytest tests/contract/

# Run with verbose output
pytest -v -s
```

### Code Quality

```bash
# Format code
black src/ tests/

# Sort imports
isort src/ tests/

# Lint code
flake8 src/ tests/ --ignore=E501

# Run all checks
black . && isort . && flake8 .
```

### Project Structure

```
src/agentready/
‚îú‚îÄ‚îÄ cli/              # Click-based CLI entry point
‚îú‚îÄ‚îÄ assessors/        # Attribute evaluators (13 categories)
‚îú‚îÄ‚îÄ models/           # Data entities
‚îú‚îÄ‚îÄ services/         # Core logic (Scanner, Scorer)
‚îú‚îÄ‚îÄ reporters/        # HTML and Markdown generators
‚îú‚îÄ‚îÄ templates/        # Jinja2 HTML template
‚îî‚îÄ‚îÄ data/             # Bundled research report and defaults

tests/
‚îú‚îÄ‚îÄ unit/             # Unit tests for individual components
‚îú‚îÄ‚îÄ integration/      # End-to-end workflow tests
‚îú‚îÄ‚îÄ contract/         # Schema validation tests
‚îî‚îÄ‚îÄ fixtures/         # Test repositories
```

## Research Foundation

All 25 attributes are derived from evidence-based research with 50+ citations from:

- Anthropic (Claude Code documentation, engineering blog)
- Microsoft (Code metrics, Azure DevOps best practices)
- Google (SRE handbook, style guides)
- ArXiv (Software engineering research papers)
- IEEE/ACM (Academic publications on code quality)

See `src/agentready/data/agent-ready-codebase-attributes.md` for complete research report.

## License

MIT License - see LICENSE file for details.

## Contributing

Contributions welcome! Please ensure:

- All tests pass (`pytest`)
- Code is formatted (`black`, `isort`)
- Linting passes (`flake8`)
- Test coverage >80%

## Support

- Documentation: See `/docs` directory
- Issues: Report at GitHub Issues
- Questions: Open a discussion on GitHub

---

**Quick Start**: `pip install -e ".[dev]" && agentready .` - Ready in <5 minutes!
</file>

<file path="src/agentready.egg-info/requires.txt">
click>=8.1.0
jinja2>=3.1.0
pyyaml>=6.0
gitpython>=3.1.0
radon>=6.0.0
lizard>=1.17.0

[dev]
pytest>=7.4.0
pytest-cov>=4.1.0
black>=23.0.0
isort>=5.12.0
flake8>=6.0.0
</file>

<file path="src/agentready.egg-info/SOURCES.txt">
README.md
pyproject.toml
src/agentready/__init__.py
src/agentready.egg-info/PKG-INFO
src/agentready.egg-info/SOURCES.txt
src/agentready.egg-info/dependency_links.txt
src/agentready.egg-info/entry_points.txt
src/agentready.egg-info/requires.txt
src/agentready.egg-info/top_level.txt
src/agentready/assessors/__init__.py
src/agentready/assessors/base.py
src/agentready/assessors/code_quality.py
src/agentready/assessors/documentation.py
src/agentready/assessors/structure.py
src/agentready/assessors/stub_assessors.py
src/agentready/assessors/testing.py
src/agentready/cli/__init__.py
src/agentready/cli/bootstrap.py
src/agentready/cli/main.py
src/agentready/data/agent-ready-codebase-attributes.md
src/agentready/data/default-weights.yaml
src/agentready/models/__init__.py
src/agentready/models/assessment.py
src/agentready/models/attribute.py
src/agentready/models/citation.py
src/agentready/models/config.py
src/agentready/models/finding.py
src/agentready/models/repository.py
src/agentready/reporters/__init__.py
src/agentready/reporters/base.py
src/agentready/reporters/html.py
src/agentready/reporters/markdown.py
src/agentready/services/__init__.py
src/agentready/services/bootstrap.py
src/agentready/services/language_detector.py
src/agentready/services/research_loader.py
src/agentready/services/scanner.py
src/agentready/services/scorer.py
src/agentready/templates/report.html.j2
</file>

<file path="src/agentready.egg-info/top_level.txt">
agentready
</file>

<file path=".claude/commands/speckit.analyze.md">
---
description: Perform a non-destructive cross-artifact consistency and quality analysis across spec.md, plan.md, and tasks.md after task generation.
---

## User Input

```text
$ARGUMENTS
```

You **MUST** consider the user input before proceeding (if not empty).

## Goal

Identify inconsistencies, duplications, ambiguities, and underspecified items across the three core artifacts (`spec.md`, `plan.md`, `tasks.md`) before implementation. This command MUST run only after `/speckit.tasks` has successfully produced a complete `tasks.md`.

## Operating Constraints

**STRICTLY READ-ONLY**: Do **not** modify any files. Output a structured analysis report. Offer an optional remediation plan (user must explicitly approve before any follow-up editing commands would be invoked manually).

**Constitution Authority**: The project constitution (`.specify/memory/constitution.md`) is **non-negotiable** within this analysis scope. Constitution conflicts are automatically CRITICAL and require adjustment of the spec, plan, or tasks‚Äînot dilution, reinterpretation, or silent ignoring of the principle. If a principle itself needs to change, that must occur in a separate, explicit constitution update outside `/speckit.analyze`.

## Execution Steps

### 1. Initialize Analysis Context

Run `.specify/scripts/bash/check-prerequisites.sh --json --require-tasks --include-tasks` once from repo root and parse JSON for FEATURE_DIR and AVAILABLE_DOCS. Derive absolute paths:

- SPEC = FEATURE_DIR/spec.md
- PLAN = FEATURE_DIR/plan.md
- TASKS = FEATURE_DIR/tasks.md

Abort with an error message if any required file is missing (instruct the user to run missing prerequisite command).
For single quotes in args like "I'm Groot", use escape syntax: e.g 'I'\''m Groot' (or double-quote if possible: "I'm Groot").

### 2. Load Artifacts (Progressive Disclosure)

Load only the minimal necessary context from each artifact:

**From spec.md:**

- Overview/Context
- Functional Requirements
- Non-Functional Requirements
- User Stories
- Edge Cases (if present)

**From plan.md:**

- Architecture/stack choices
- Data Model references
- Phases
- Technical constraints

**From tasks.md:**

- Task IDs
- Descriptions
- Phase grouping
- Parallel markers [P]
- Referenced file paths

**From constitution:**

- Load `.specify/memory/constitution.md` for principle validation

### 3. Build Semantic Models

Create internal representations (do not include raw artifacts in output):

- **Requirements inventory**: Each functional + non-functional requirement with a stable key (derive slug based on imperative phrase; e.g., "User can upload file" ‚Üí `user-can-upload-file`)
- **User story/action inventory**: Discrete user actions with acceptance criteria
- **Task coverage mapping**: Map each task to one or more requirements or stories (inference by keyword / explicit reference patterns like IDs or key phrases)
- **Constitution rule set**: Extract principle names and MUST/SHOULD normative statements

### 4. Detection Passes (Token-Efficient Analysis)

Focus on high-signal findings. Limit to 50 findings total; aggregate remainder in overflow summary.

#### A. Duplication Detection

- Identify near-duplicate requirements
- Mark lower-quality phrasing for consolidation

#### B. Ambiguity Detection

- Flag vague adjectives (fast, scalable, secure, intuitive, robust) lacking measurable criteria
- Flag unresolved placeholders (TODO, TKTK, ???, `<placeholder>`, etc.)

#### C. Underspecification

- Requirements with verbs but missing object or measurable outcome
- User stories missing acceptance criteria alignment
- Tasks referencing files or components not defined in spec/plan

#### D. Constitution Alignment

- Any requirement or plan element conflicting with a MUST principle
- Missing mandated sections or quality gates from constitution

#### E. Coverage Gaps

- Requirements with zero associated tasks
- Tasks with no mapped requirement/story
- Non-functional requirements not reflected in tasks (e.g., performance, security)

#### F. Inconsistency

- Terminology drift (same concept named differently across files)
- Data entities referenced in plan but absent in spec (or vice versa)
- Task ordering contradictions (e.g., integration tasks before foundational setup tasks without dependency note)
- Conflicting requirements (e.g., one requires Next.js while other specifies Vue)

### 5. Severity Assignment

Use this heuristic to prioritize findings:

- **CRITICAL**: Violates constitution MUST, missing core spec artifact, or requirement with zero coverage that blocks baseline functionality
- **HIGH**: Duplicate or conflicting requirement, ambiguous security/performance attribute, untestable acceptance criterion
- **MEDIUM**: Terminology drift, missing non-functional task coverage, underspecified edge case
- **LOW**: Style/wording improvements, minor redundancy not affecting execution order

### 6. Produce Compact Analysis Report

Output a Markdown report (no file writes) with the following structure:

## Specification Analysis Report

| ID | Category | Severity | Location(s) | Summary | Recommendation |
|----|----------|----------|-------------|---------|----------------|
| A1 | Duplication | HIGH | spec.md:L120-134 | Two similar requirements ... | Merge phrasing; keep clearer version |

(Add one row per finding; generate stable IDs prefixed by category initial.)

**Coverage Summary Table:**

| Requirement Key | Has Task? | Task IDs | Notes |
|-----------------|-----------|----------|-------|

**Constitution Alignment Issues:** (if any)

**Unmapped Tasks:** (if any)

**Metrics:**

- Total Requirements
- Total Tasks
- Coverage % (requirements with >=1 task)
- Ambiguity Count
- Duplication Count
- Critical Issues Count

### 7. Provide Next Actions

At end of report, output a concise Next Actions block:

- If CRITICAL issues exist: Recommend resolving before `/speckit.implement`
- If only LOW/MEDIUM: User may proceed, but provide improvement suggestions
- Provide explicit command suggestions: e.g., "Run /speckit.specify with refinement", "Run /speckit.plan to adjust architecture", "Manually edit tasks.md to add coverage for 'performance-metrics'"

### 8. Offer Remediation

Ask the user: "Would you like me to suggest concrete remediation edits for the top N issues?" (Do NOT apply them automatically.)

## Operating Principles

### Context Efficiency

- **Minimal high-signal tokens**: Focus on actionable findings, not exhaustive documentation
- **Progressive disclosure**: Load artifacts incrementally; don't dump all content into analysis
- **Token-efficient output**: Limit findings table to 50 rows; summarize overflow
- **Deterministic results**: Rerunning without changes should produce consistent IDs and counts

### Analysis Guidelines

- **NEVER modify files** (this is read-only analysis)
- **NEVER hallucinate missing sections** (if absent, report them accurately)
- **Prioritize constitution violations** (these are always CRITICAL)
- **Use examples over exhaustive rules** (cite specific instances, not generic patterns)
- **Report zero issues gracefully** (emit success report with coverage statistics)

## Context

$ARGUMENTS
</file>

<file path=".claude/commands/speckit.checklist.md">
---
description: Generate a custom checklist for the current feature based on user requirements.
---

## Checklist Purpose: "Unit Tests for English"

**CRITICAL CONCEPT**: Checklists are **UNIT TESTS FOR REQUIREMENTS WRITING** - they validate the quality, clarity, and completeness of requirements in a given domain.

**NOT for verification/testing**:

- ‚ùå NOT "Verify the button clicks correctly"
- ‚ùå NOT "Test error handling works"
- ‚ùå NOT "Confirm the API returns 200"
- ‚ùå NOT checking if code/implementation matches the spec

**FOR requirements quality validation**:

- ‚úÖ "Are visual hierarchy requirements defined for all card types?" (completeness)
- ‚úÖ "Is 'prominent display' quantified with specific sizing/positioning?" (clarity)
- ‚úÖ "Are hover state requirements consistent across all interactive elements?" (consistency)
- ‚úÖ "Are accessibility requirements defined for keyboard navigation?" (coverage)
- ‚úÖ "Does the spec define what happens when logo image fails to load?" (edge cases)

**Metaphor**: If your spec is code written in English, the checklist is its unit test suite. You're testing whether the requirements are well-written, complete, unambiguous, and ready for implementation - NOT whether the implementation works.

## User Input

```text
$ARGUMENTS
```

You **MUST** consider the user input before proceeding (if not empty).

## Execution Steps

1. **Setup**: Run `.specify/scripts/bash/check-prerequisites.sh --json` from repo root and parse JSON for FEATURE_DIR and AVAILABLE_DOCS list.
   - All file paths must be absolute.
   - For single quotes in args like "I'm Groot", use escape syntax: e.g 'I'\''m Groot' (or double-quote if possible: "I'm Groot").

2. **Clarify intent (dynamic)**: Derive up to THREE initial contextual clarifying questions (no pre-baked catalog). They MUST:
   - Be generated from the user's phrasing + extracted signals from spec/plan/tasks
   - Only ask about information that materially changes checklist content
   - Be skipped individually if already unambiguous in `$ARGUMENTS`
   - Prefer precision over breadth

   Generation algorithm:
   1. Extract signals: feature domain keywords (e.g., auth, latency, UX, API), risk indicators ("critical", "must", "compliance"), stakeholder hints ("QA", "review", "security team"), and explicit deliverables ("a11y", "rollback", "contracts").
   2. Cluster signals into candidate focus areas (max 4) ranked by relevance.
   3. Identify probable audience & timing (author, reviewer, QA, release) if not explicit.
   4. Detect missing dimensions: scope breadth, depth/rigor, risk emphasis, exclusion boundaries, measurable acceptance criteria.
   5. Formulate questions chosen from these archetypes:
      - Scope refinement (e.g., "Should this include integration touchpoints with X and Y or stay limited to local module correctness?")
      - Risk prioritization (e.g., "Which of these potential risk areas should receive mandatory gating checks?")
      - Depth calibration (e.g., "Is this a lightweight pre-commit sanity list or a formal release gate?")
      - Audience framing (e.g., "Will this be used by the author only or peers during PR review?")
      - Boundary exclusion (e.g., "Should we explicitly exclude performance tuning items this round?")
      - Scenario class gap (e.g., "No recovery flows detected‚Äîare rollback / partial failure paths in scope?")

   Question formatting rules:
   - If presenting options, generate a compact table with columns: Option | Candidate | Why It Matters
   - Limit to A‚ÄìE options maximum; omit table if a free-form answer is clearer
   - Never ask the user to restate what they already said
   - Avoid speculative categories (no hallucination). If uncertain, ask explicitly: "Confirm whether X belongs in scope."

   Defaults when interaction impossible:
   - Depth: Standard
   - Audience: Reviewer (PR) if code-related; Author otherwise
   - Focus: Top 2 relevance clusters

   Output the questions (label Q1/Q2/Q3). After answers: if ‚â•2 scenario classes (Alternate / Exception / Recovery / Non-Functional domain) remain unclear, you MAY ask up to TWO more targeted follow‚Äëups (Q4/Q5) with a one-line justification each (e.g., "Unresolved recovery path risk"). Do not exceed five total questions. Skip escalation if user explicitly declines more.

3. **Understand user request**: Combine `$ARGUMENTS` + clarifying answers:
   - Derive checklist theme (e.g., security, review, deploy, ux)
   - Consolidate explicit must-have items mentioned by user
   - Map focus selections to category scaffolding
   - Infer any missing context from spec/plan/tasks (do NOT hallucinate)

4. **Load feature context**: Read from FEATURE_DIR:
   - spec.md: Feature requirements and scope
   - plan.md (if exists): Technical details, dependencies
   - tasks.md (if exists): Implementation tasks

   **Context Loading Strategy**:
   - Load only necessary portions relevant to active focus areas (avoid full-file dumping)
   - Prefer summarizing long sections into concise scenario/requirement bullets
   - Use progressive disclosure: add follow-on retrieval only if gaps detected
   - If source docs are large, generate interim summary items instead of embedding raw text

5. **Generate checklist** - Create "Unit Tests for Requirements":
   - Create `FEATURE_DIR/checklists/` directory if it doesn't exist
   - Generate unique checklist filename:
     - Use short, descriptive name based on domain (e.g., `ux.md`, `api.md`, `security.md`)
     - Format: `[domain].md`
     - If file exists, append to existing file
   - Number items sequentially starting from CHK001
   - Each `/speckit.checklist` run creates a NEW file (never overwrites existing checklists)

   **CORE PRINCIPLE - Test the Requirements, Not the Implementation**:
   Every checklist item MUST evaluate the REQUIREMENTS THEMSELVES for:
   - **Completeness**: Are all necessary requirements present?
   - **Clarity**: Are requirements unambiguous and specific?
   - **Consistency**: Do requirements align with each other?
   - **Measurability**: Can requirements be objectively verified?
   - **Coverage**: Are all scenarios/edge cases addressed?

   **Category Structure** - Group items by requirement quality dimensions:
   - **Requirement Completeness** (Are all necessary requirements documented?)
   - **Requirement Clarity** (Are requirements specific and unambiguous?)
   - **Requirement Consistency** (Do requirements align without conflicts?)
   - **Acceptance Criteria Quality** (Are success criteria measurable?)
   - **Scenario Coverage** (Are all flows/cases addressed?)
   - **Edge Case Coverage** (Are boundary conditions defined?)
   - **Non-Functional Requirements** (Performance, Security, Accessibility, etc. - are they specified?)
   - **Dependencies & Assumptions** (Are they documented and validated?)
   - **Ambiguities & Conflicts** (What needs clarification?)

   **HOW TO WRITE CHECKLIST ITEMS - "Unit Tests for English"**:

   ‚ùå **WRONG** (Testing implementation):
   - "Verify landing page displays 3 episode cards"
   - "Test hover states work on desktop"
   - "Confirm logo click navigates home"

   ‚úÖ **CORRECT** (Testing requirements quality):
   - "Are the exact number and layout of featured episodes specified?" [Completeness]
   - "Is 'prominent display' quantified with specific sizing/positioning?" [Clarity]
   - "Are hover state requirements consistent across all interactive elements?" [Consistency]
   - "Are keyboard navigation requirements defined for all interactive UI?" [Coverage]
   - "Is the fallback behavior specified when logo image fails to load?" [Edge Cases]
   - "Are loading states defined for asynchronous episode data?" [Completeness]
   - "Does the spec define visual hierarchy for competing UI elements?" [Clarity]

   **ITEM STRUCTURE**:
   Each item should follow this pattern:
   - Question format asking about requirement quality
   - Focus on what's WRITTEN (or not written) in the spec/plan
   - Include quality dimension in brackets [Completeness/Clarity/Consistency/etc.]
   - Reference spec section `[Spec ¬ßX.Y]` when checking existing requirements
   - Use `[Gap]` marker when checking for missing requirements

   **EXAMPLES BY QUALITY DIMENSION**:

   Completeness:
   - "Are error handling requirements defined for all API failure modes? [Gap]"
   - "Are accessibility requirements specified for all interactive elements? [Completeness]"
   - "Are mobile breakpoint requirements defined for responsive layouts? [Gap]"

   Clarity:
   - "Is 'fast loading' quantified with specific timing thresholds? [Clarity, Spec ¬ßNFR-2]"
   - "Are 'related episodes' selection criteria explicitly defined? [Clarity, Spec ¬ßFR-5]"
   - "Is 'prominent' defined with measurable visual properties? [Ambiguity, Spec ¬ßFR-4]"

   Consistency:
   - "Do navigation requirements align across all pages? [Consistency, Spec ¬ßFR-10]"
   - "Are card component requirements consistent between landing and detail pages? [Consistency]"

   Coverage:
   - "Are requirements defined for zero-state scenarios (no episodes)? [Coverage, Edge Case]"
   - "Are concurrent user interaction scenarios addressed? [Coverage, Gap]"
   - "Are requirements specified for partial data loading failures? [Coverage, Exception Flow]"

   Measurability:
   - "Are visual hierarchy requirements measurable/testable? [Acceptance Criteria, Spec ¬ßFR-1]"
   - "Can 'balanced visual weight' be objectively verified? [Measurability, Spec ¬ßFR-2]"

   **Scenario Classification & Coverage** (Requirements Quality Focus):
   - Check if requirements exist for: Primary, Alternate, Exception/Error, Recovery, Non-Functional scenarios
   - For each scenario class, ask: "Are [scenario type] requirements complete, clear, and consistent?"
   - If scenario class missing: "Are [scenario type] requirements intentionally excluded or missing? [Gap]"
   - Include resilience/rollback when state mutation occurs: "Are rollback requirements defined for migration failures? [Gap]"

   **Traceability Requirements**:
   - MINIMUM: ‚â•80% of items MUST include at least one traceability reference
   - Each item should reference: spec section `[Spec ¬ßX.Y]`, or use markers: `[Gap]`, `[Ambiguity]`, `[Conflict]`, `[Assumption]`
   - If no ID system exists: "Is a requirement & acceptance criteria ID scheme established? [Traceability]"

   **Surface & Resolve Issues** (Requirements Quality Problems):
   Ask questions about the requirements themselves:
   - Ambiguities: "Is the term 'fast' quantified with specific metrics? [Ambiguity, Spec ¬ßNFR-1]"
   - Conflicts: "Do navigation requirements conflict between ¬ßFR-10 and ¬ßFR-10a? [Conflict]"
   - Assumptions: "Is the assumption of 'always available podcast API' validated? [Assumption]"
   - Dependencies: "Are external podcast API requirements documented? [Dependency, Gap]"
   - Missing definitions: "Is 'visual hierarchy' defined with measurable criteria? [Gap]"

   **Content Consolidation**:
   - Soft cap: If raw candidate items > 40, prioritize by risk/impact
   - Merge near-duplicates checking the same requirement aspect
   - If >5 low-impact edge cases, create one item: "Are edge cases X, Y, Z addressed in requirements? [Coverage]"

   **üö´ ABSOLUTELY PROHIBITED** - These make it an implementation test, not a requirements test:
   - ‚ùå Any item starting with "Verify", "Test", "Confirm", "Check" + implementation behavior
   - ‚ùå References to code execution, user actions, system behavior
   - ‚ùå "Displays correctly", "works properly", "functions as expected"
   - ‚ùå "Click", "navigate", "render", "load", "execute"
   - ‚ùå Test cases, test plans, QA procedures
   - ‚ùå Implementation details (frameworks, APIs, algorithms)

   **‚úÖ REQUIRED PATTERNS** - These test requirements quality:
   - ‚úÖ "Are [requirement type] defined/specified/documented for [scenario]?"
   - ‚úÖ "Is [vague term] quantified/clarified with specific criteria?"
   - ‚úÖ "Are requirements consistent between [section A] and [section B]?"
   - ‚úÖ "Can [requirement] be objectively measured/verified?"
   - ‚úÖ "Are [edge cases/scenarios] addressed in requirements?"
   - ‚úÖ "Does the spec define [missing aspect]?"

6. **Structure Reference**: Generate the checklist following the canonical template in `.specify/templates/checklist-template.md` for title, meta section, category headings, and ID formatting. If template is unavailable, use: H1 title, purpose/created meta lines, `##` category sections containing `- [ ] CHK### <requirement item>` lines with globally incrementing IDs starting at CHK001.

7. **Report**: Output full path to created checklist, item count, and remind user that each run creates a new file. Summarize:
   - Focus areas selected
   - Depth level
   - Actor/timing
   - Any explicit user-specified must-have items incorporated

**Important**: Each `/speckit.checklist` command invocation creates a checklist file using short, descriptive names unless file already exists. This allows:

- Multiple checklists of different types (e.g., `ux.md`, `test.md`, `security.md`)
- Simple, memorable filenames that indicate checklist purpose
- Easy identification and navigation in the `checklists/` folder

To avoid clutter, use descriptive types and clean up obsolete checklists when done.

## Example Checklist Types & Sample Items

**UX Requirements Quality:** `ux.md`

Sample items (testing the requirements, NOT the implementation):

- "Are visual hierarchy requirements defined with measurable criteria? [Clarity, Spec ¬ßFR-1]"
- "Is the number and positioning of UI elements explicitly specified? [Completeness, Spec ¬ßFR-1]"
- "Are interaction state requirements (hover, focus, active) consistently defined? [Consistency]"
- "Are accessibility requirements specified for all interactive elements? [Coverage, Gap]"
- "Is fallback behavior defined when images fail to load? [Edge Case, Gap]"
- "Can 'prominent display' be objectively measured? [Measurability, Spec ¬ßFR-4]"

**API Requirements Quality:** `api.md`

Sample items:

- "Are error response formats specified for all failure scenarios? [Completeness]"
- "Are rate limiting requirements quantified with specific thresholds? [Clarity]"
- "Are authentication requirements consistent across all endpoints? [Consistency]"
- "Are retry/timeout requirements defined for external dependencies? [Coverage, Gap]"
- "Is versioning strategy documented in requirements? [Gap]"

**Performance Requirements Quality:** `performance.md`

Sample items:

- "Are performance requirements quantified with specific metrics? [Clarity]"
- "Are performance targets defined for all critical user journeys? [Coverage]"
- "Are performance requirements under different load conditions specified? [Completeness]"
- "Can performance requirements be objectively measured? [Measurability]"
- "Are degradation requirements defined for high-load scenarios? [Edge Case, Gap]"

**Security Requirements Quality:** `security.md`

Sample items:

- "Are authentication requirements specified for all protected resources? [Coverage]"
- "Are data protection requirements defined for sensitive information? [Completeness]"
- "Is the threat model documented and requirements aligned to it? [Traceability]"
- "Are security requirements consistent with compliance obligations? [Consistency]"
- "Are security failure/breach response requirements defined? [Gap, Exception Flow]"

## Anti-Examples: What NOT To Do

**‚ùå WRONG - These test implementation, not requirements:**

```markdown
- [ ] CHK001 - Verify landing page displays 3 episode cards [Spec ¬ßFR-001]
- [ ] CHK002 - Test hover states work correctly on desktop [Spec ¬ßFR-003]
- [ ] CHK003 - Confirm logo click navigates to home page [Spec ¬ßFR-010]
- [ ] CHK004 - Check that related episodes section shows 3-5 items [Spec ¬ßFR-005]
```

**‚úÖ CORRECT - These test requirements quality:**

```markdown
- [ ] CHK001 - Are the number and layout of featured episodes explicitly specified? [Completeness, Spec ¬ßFR-001]
- [ ] CHK002 - Are hover state requirements consistently defined for all interactive elements? [Consistency, Spec ¬ßFR-003]
- [ ] CHK003 - Are navigation requirements clear for all clickable brand elements? [Clarity, Spec ¬ßFR-010]
- [ ] CHK004 - Is the selection criteria for related episodes documented? [Gap, Spec ¬ßFR-005]
- [ ] CHK005 - Are loading state requirements defined for asynchronous episode data? [Gap]
- [ ] CHK006 - Can "visual hierarchy" requirements be objectively measured? [Measurability, Spec ¬ßFR-001]
```

**Key Differences:**

- Wrong: Tests if the system works correctly
- Correct: Tests if the requirements are written correctly
- Wrong: Verification of behavior
- Correct: Validation of requirement quality
- Wrong: "Does it do X?"
- Correct: "Is X clearly specified?"
</file>

<file path=".claude/commands/speckit.clarify.md">
---
description: Identify underspecified areas in the current feature spec by asking up to 5 highly targeted clarification questions and encoding answers back into the spec.
handoffs: 
  - label: Build Technical Plan
    agent: speckit.plan
    prompt: Create a plan for the spec. I am building with...
---

## User Input

```text
$ARGUMENTS
```

You **MUST** consider the user input before proceeding (if not empty).

## Outline

Goal: Detect and reduce ambiguity or missing decision points in the active feature specification and record the clarifications directly in the spec file.

Note: This clarification workflow is expected to run (and be completed) BEFORE invoking `/speckit.plan`. If the user explicitly states they are skipping clarification (e.g., exploratory spike), you may proceed, but must warn that downstream rework risk increases.

Execution steps:

1. Run `.specify/scripts/bash/check-prerequisites.sh --json --paths-only` from repo root **once** (combined `--json --paths-only` mode / `-Json -PathsOnly`). Parse minimal JSON payload fields:
   - `FEATURE_DIR`
   - `FEATURE_SPEC`
   - (Optionally capture `IMPL_PLAN`, `TASKS` for future chained flows.)
   - If JSON parsing fails, abort and instruct user to re-run `/speckit.specify` or verify feature branch environment.
   - For single quotes in args like "I'm Groot", use escape syntax: e.g 'I'\''m Groot' (or double-quote if possible: "I'm Groot").

2. Load the current spec file. Perform a structured ambiguity & coverage scan using this taxonomy. For each category, mark status: Clear / Partial / Missing. Produce an internal coverage map used for prioritization (do not output raw map unless no questions will be asked).

   Functional Scope & Behavior:
   - Core user goals & success criteria
   - Explicit out-of-scope declarations
   - User roles / personas differentiation

   Domain & Data Model:
   - Entities, attributes, relationships
   - Identity & uniqueness rules
   - Lifecycle/state transitions
   - Data volume / scale assumptions

   Interaction & UX Flow:
   - Critical user journeys / sequences
   - Error/empty/loading states
   - Accessibility or localization notes

   Non-Functional Quality Attributes:
   - Performance (latency, throughput targets)
   - Scalability (horizontal/vertical, limits)
   - Reliability & availability (uptime, recovery expectations)
   - Observability (logging, metrics, tracing signals)
   - Security & privacy (authN/Z, data protection, threat assumptions)
   - Compliance / regulatory constraints (if any)

   Integration & External Dependencies:
   - External services/APIs and failure modes
   - Data import/export formats
   - Protocol/versioning assumptions

   Edge Cases & Failure Handling:
   - Negative scenarios
   - Rate limiting / throttling
   - Conflict resolution (e.g., concurrent edits)

   Constraints & Tradeoffs:
   - Technical constraints (language, storage, hosting)
   - Explicit tradeoffs or rejected alternatives

   Terminology & Consistency:
   - Canonical glossary terms
   - Avoided synonyms / deprecated terms

   Completion Signals:
   - Acceptance criteria testability
   - Measurable Definition of Done style indicators

   Misc / Placeholders:
   - TODO markers / unresolved decisions
   - Ambiguous adjectives ("robust", "intuitive") lacking quantification

   For each category with Partial or Missing status, add a candidate question opportunity unless:
   - Clarification would not materially change implementation or validation strategy
   - Information is better deferred to planning phase (note internally)

3. Generate (internally) a prioritized queue of candidate clarification questions (maximum 5). Do NOT output them all at once. Apply these constraints:
    - Maximum of 10 total questions across the whole session.
    - Each question must be answerable with EITHER:
       - A short multiple‚Äëchoice selection (2‚Äì5 distinct, mutually exclusive options), OR
       - A one-word / short‚Äëphrase answer (explicitly constrain: "Answer in <=5 words").
    - Only include questions whose answers materially impact architecture, data modeling, task decomposition, test design, UX behavior, operational readiness, or compliance validation.
    - Ensure category coverage balance: attempt to cover the highest impact unresolved categories first; avoid asking two low-impact questions when a single high-impact area (e.g., security posture) is unresolved.
    - Exclude questions already answered, trivial stylistic preferences, or plan-level execution details (unless blocking correctness).
    - Favor clarifications that reduce downstream rework risk or prevent misaligned acceptance tests.
    - If more than 5 categories remain unresolved, select the top 5 by (Impact * Uncertainty) heuristic.

4. Sequential questioning loop (interactive):
    - Present EXACTLY ONE question at a time.
    - For multiple‚Äëchoice questions:
       - **Analyze all options** and determine the **most suitable option** based on:
          - Best practices for the project type
          - Common patterns in similar implementations
          - Risk reduction (security, performance, maintainability)
          - Alignment with any explicit project goals or constraints visible in the spec
       - Present your **recommended option prominently** at the top with clear reasoning (1-2 sentences explaining why this is the best choice).
       - Format as: `**Recommended:** Option [X] - <reasoning>`
       - Then render all options as a Markdown table:

       | Option | Description |
       |--------|-------------|
       | A | <Option A description> |
       | B | <Option B description> |
       | C | <Option C description> (add D/E as needed up to 5) |
       | Short | Provide a different short answer (<=5 words) (Include only if free-form alternative is appropriate) |

       - After the table, add: `You can reply with the option letter (e.g., "A"), accept the recommendation by saying "yes" or "recommended", or provide your own short answer.`
    - For short‚Äëanswer style (no meaningful discrete options):
       - Provide your **suggested answer** based on best practices and context.
       - Format as: `**Suggested:** <your proposed answer> - <brief reasoning>`
       - Then output: `Format: Short answer (<=5 words). You can accept the suggestion by saying "yes" or "suggested", or provide your own answer.`
    - After the user answers:
       - If the user replies with "yes", "recommended", or "suggested", use your previously stated recommendation/suggestion as the answer.
       - Otherwise, validate the answer maps to one option or fits the <=5 word constraint.
       - If ambiguous, ask for a quick disambiguation (count still belongs to same question; do not advance).
       - Once satisfactory, record it in working memory (do not yet write to disk) and move to the next queued question.
    - Stop asking further questions when:
       - All critical ambiguities resolved early (remaining queued items become unnecessary), OR
       - User signals completion ("done", "good", "no more"), OR
       - You reach 5 asked questions.
    - Never reveal future queued questions in advance.
    - If no valid questions exist at start, immediately report no critical ambiguities.

5. Integration after EACH accepted answer (incremental update approach):
    - Maintain in-memory representation of the spec (loaded once at start) plus the raw file contents.
    - For the first integrated answer in this session:
       - Ensure a `## Clarifications` section exists (create it just after the highest-level contextual/overview section per the spec template if missing).
       - Under it, create (if not present) a `### Session YYYY-MM-DD` subheading for today.
    - Append a bullet line immediately after acceptance: `- Q: <question> ‚Üí A: <final answer>`.
    - Then immediately apply the clarification to the most appropriate section(s):
       - Functional ambiguity ‚Üí Update or add a bullet in Functional Requirements.
       - User interaction / actor distinction ‚Üí Update User Stories or Actors subsection (if present) with clarified role, constraint, or scenario.
       - Data shape / entities ‚Üí Update Data Model (add fields, types, relationships) preserving ordering; note added constraints succinctly.
       - Non-functional constraint ‚Üí Add/modify measurable criteria in Non-Functional / Quality Attributes section (convert vague adjective to metric or explicit target).
       - Edge case / negative flow ‚Üí Add a new bullet under Edge Cases / Error Handling (or create such subsection if template provides placeholder for it).
       - Terminology conflict ‚Üí Normalize term across spec; retain original only if necessary by adding `(formerly referred to as "X")` once.
    - If the clarification invalidates an earlier ambiguous statement, replace that statement instead of duplicating; leave no obsolete contradictory text.
    - Save the spec file AFTER each integration to minimize risk of context loss (atomic overwrite).
    - Preserve formatting: do not reorder unrelated sections; keep heading hierarchy intact.
    - Keep each inserted clarification minimal and testable (avoid narrative drift).

6. Validation (performed after EACH write plus final pass):
   - Clarifications session contains exactly one bullet per accepted answer (no duplicates).
   - Total asked (accepted) questions ‚â§ 5.
   - Updated sections contain no lingering vague placeholders the new answer was meant to resolve.
   - No contradictory earlier statement remains (scan for now-invalid alternative choices removed).
   - Markdown structure valid; only allowed new headings: `## Clarifications`, `### Session YYYY-MM-DD`.
   - Terminology consistency: same canonical term used across all updated sections.

7. Write the updated spec back to `FEATURE_SPEC`.

8. Report completion (after questioning loop ends or early termination):
   - Number of questions asked & answered.
   - Path to updated spec.
   - Sections touched (list names).
   - Coverage summary table listing each taxonomy category with Status: Resolved (was Partial/Missing and addressed), Deferred (exceeds question quota or better suited for planning), Clear (already sufficient), Outstanding (still Partial/Missing but low impact).
   - If any Outstanding or Deferred remain, recommend whether to proceed to `/speckit.plan` or run `/speckit.clarify` again later post-plan.
   - Suggested next command.

Behavior rules:

- If no meaningful ambiguities found (or all potential questions would be low-impact), respond: "No critical ambiguities detected worth formal clarification." and suggest proceeding.
- If spec file missing, instruct user to run `/speckit.specify` first (do not create a new spec here).
- Never exceed 5 total asked questions (clarification retries for a single question do not count as new questions).
- Avoid speculative tech stack questions unless the absence blocks functional clarity.
- Respect user early termination signals ("stop", "done", "proceed").
- If no questions asked due to full coverage, output a compact coverage summary (all categories Clear) then suggest advancing.
- If quota reached with unresolved high-impact categories remaining, explicitly flag them under Deferred with rationale.

Context for prioritization: $ARGUMENTS
</file>

<file path=".claude/commands/speckit.constitution.md">
---
description: Create or update the project constitution from interactive or provided principle inputs, ensuring all dependent templates stay in sync.
handoffs: 
  - label: Build Specification
    agent: speckit.specify
    prompt: Implement the feature specification based on the updated constitution. I want to build...
---

## User Input

```text
$ARGUMENTS
```

You **MUST** consider the user input before proceeding (if not empty).

## Outline

You are updating the project constitution at `.specify/memory/constitution.md`. This file is a TEMPLATE containing placeholder tokens in square brackets (e.g. `[PROJECT_NAME]`, `[PRINCIPLE_1_NAME]`). Your job is to (a) collect/derive concrete values, (b) fill the template precisely, and (c) propagate any amendments across dependent artifacts.

Follow this execution flow:

1. Load the existing constitution template at `.specify/memory/constitution.md`.
   - Identify every placeholder token of the form `[ALL_CAPS_IDENTIFIER]`.
   **IMPORTANT**: The user might require less or more principles than the ones used in the template. If a number is specified, respect that - follow the general template. You will update the doc accordingly.

2. Collect/derive values for placeholders:
   - If user input (conversation) supplies a value, use it.
   - Otherwise infer from existing repo context (README, docs, prior constitution versions if embedded).
   - For governance dates: `RATIFICATION_DATE` is the original adoption date (if unknown ask or mark TODO), `LAST_AMENDED_DATE` is today if changes are made, otherwise keep previous.
   - `CONSTITUTION_VERSION` must increment according to semantic versioning rules:
     - MAJOR: Backward incompatible governance/principle removals or redefinitions.
     - MINOR: New principle/section added or materially expanded guidance.
     - PATCH: Clarifications, wording, typo fixes, non-semantic refinements.
   - If version bump type ambiguous, propose reasoning before finalizing.

3. Draft the updated constitution content:
   - Replace every placeholder with concrete text (no bracketed tokens left except intentionally retained template slots that the project has chosen not to define yet‚Äîexplicitly justify any left).
   - Preserve heading hierarchy and comments can be removed once replaced unless they still add clarifying guidance.
   - Ensure each Principle section: succinct name line, paragraph (or bullet list) capturing non‚Äënegotiable rules, explicit rationale if not obvious.
   - Ensure Governance section lists amendment procedure, versioning policy, and compliance review expectations.

4. Consistency propagation checklist (convert prior checklist into active validations):
   - Read `.specify/templates/plan-template.md` and ensure any "Constitution Check" or rules align with updated principles.
   - Read `.specify/templates/spec-template.md` for scope/requirements alignment‚Äîupdate if constitution adds/removes mandatory sections or constraints.
   - Read `.specify/templates/tasks-template.md` and ensure task categorization reflects new or removed principle-driven task types (e.g., observability, versioning, testing discipline).
   - Read each command file in `.specify/templates/commands/*.md` (including this one) to verify no outdated references (agent-specific names like CLAUDE only) remain when generic guidance is required.
   - Read any runtime guidance docs (e.g., `README.md`, `docs/quickstart.md`, or agent-specific guidance files if present). Update references to principles changed.

5. Produce a Sync Impact Report (prepend as an HTML comment at top of the constitution file after update):
   - Version change: old ‚Üí new
   - List of modified principles (old title ‚Üí new title if renamed)
   - Added sections
   - Removed sections
   - Templates requiring updates (‚úÖ updated / ‚ö† pending) with file paths
   - Follow-up TODOs if any placeholders intentionally deferred.

6. Validation before final output:
   - No remaining unexplained bracket tokens.
   - Version line matches report.
   - Dates ISO format YYYY-MM-DD.
   - Principles are declarative, testable, and free of vague language ("should" ‚Üí replace with MUST/SHOULD rationale where appropriate).

7. Write the completed constitution back to `.specify/memory/constitution.md` (overwrite).

8. Output a final summary to the user with:
   - New version and bump rationale.
   - Any files flagged for manual follow-up.
   - Suggested commit message (e.g., `docs: amend constitution to vX.Y.Z (principle additions + governance update)`).

Formatting & Style Requirements:

- Use Markdown headings exactly as in the template (do not demote/promote levels).
- Wrap long rationale lines to keep readability (<100 chars ideally) but do not hard enforce with awkward breaks.
- Keep a single blank line between sections.
- Avoid trailing whitespace.

If the user supplies partial updates (e.g., only one principle revision), still perform validation and version decision steps.

If critical info missing (e.g., ratification date truly unknown), insert `TODO(<FIELD_NAME>): explanation` and include in the Sync Impact Report under deferred items.

Do not create a new template; always operate on the existing `.specify/memory/constitution.md` file.
</file>

<file path=".claude/commands/speckit.implement.md">
---
description: Execute the implementation plan by processing and executing all tasks defined in tasks.md
---

## User Input

```text
$ARGUMENTS
```

You **MUST** consider the user input before proceeding (if not empty).

## Outline

1. Run `.specify/scripts/bash/check-prerequisites.sh --json --require-tasks --include-tasks` from repo root and parse FEATURE_DIR and AVAILABLE_DOCS list. All paths must be absolute. For single quotes in args like "I'm Groot", use escape syntax: e.g 'I'\''m Groot' (or double-quote if possible: "I'm Groot").

2. **Check checklists status** (if FEATURE_DIR/checklists/ exists):
   - Scan all checklist files in the checklists/ directory
   - For each checklist, count:
     - Total items: All lines matching `- [ ]` or `- [X]` or `- [x]`
     - Completed items: Lines matching `- [X]` or `- [x]`
     - Incomplete items: Lines matching `- [ ]`
   - Create a status table:

     ```text
     | Checklist | Total | Completed | Incomplete | Status |
     |-----------|-------|-----------|------------|--------|
     | ux.md     | 12    | 12        | 0          | ‚úì PASS |
     | test.md   | 8     | 5         | 3          | ‚úó FAIL |
     | security.md | 6   | 6         | 0          | ‚úì PASS |
     ```

   - Calculate overall status:
     - **PASS**: All checklists have 0 incomplete items
     - **FAIL**: One or more checklists have incomplete items

   - **If any checklist is incomplete**:
     - Display the table with incomplete item counts
     - **STOP** and ask: "Some checklists are incomplete. Do you want to proceed with implementation anyway? (yes/no)"
     - Wait for user response before continuing
     - If user says "no" or "wait" or "stop", halt execution
     - If user says "yes" or "proceed" or "continue", proceed to step 3

   - **If all checklists are complete**:
     - Display the table showing all checklists passed
     - Automatically proceed to step 3

3. Load and analyze the implementation context:
   - **REQUIRED**: Read tasks.md for the complete task list and execution plan
   - **REQUIRED**: Read plan.md for tech stack, architecture, and file structure
   - **IF EXISTS**: Read data-model.md for entities and relationships
   - **IF EXISTS**: Read contracts/ for API specifications and test requirements
   - **IF EXISTS**: Read research.md for technical decisions and constraints
   - **IF EXISTS**: Read quickstart.md for integration scenarios

4. **Project Setup Verification**:
   - **REQUIRED**: Create/verify ignore files based on actual project setup:

   **Detection & Creation Logic**:
   - Check if the following command succeeds to determine if the repository is a git repo (create/verify .gitignore if so):

     ```sh
     git rev-parse --git-dir 2>/dev/null
     ```

   - Check if Dockerfile* exists or Docker in plan.md ‚Üí create/verify .dockerignore
   - Check if .eslintrc* exists ‚Üí create/verify .eslintignore
   - Check if eslint.config.* exists ‚Üí ensure the config's `ignores` entries cover required patterns
   - Check if .prettierrc* exists ‚Üí create/verify .prettierignore
   - Check if .npmrc or package.json exists ‚Üí create/verify .npmignore (if publishing)
   - Check if terraform files (*.tf) exist ‚Üí create/verify .terraformignore
   - Check if .helmignore needed (helm charts present) ‚Üí create/verify .helmignore

   **If ignore file already exists**: Verify it contains essential patterns, append missing critical patterns only
   **If ignore file missing**: Create with full pattern set for detected technology

   **Common Patterns by Technology** (from plan.md tech stack):
   - **Node.js/JavaScript/TypeScript**: `node_modules/`, `dist/`, `build/`, `*.log`, `.env*`
   - **Python**: `__pycache__/`, `*.pyc`, `.venv/`, `venv/`, `dist/`, `*.egg-info/`
   - **Java**: `target/`, `*.class`, `*.jar`, `.gradle/`, `build/`
   - **C#/.NET**: `bin/`, `obj/`, `*.user`, `*.suo`, `packages/`
   - **Go**: `*.exe`, `*.test`, `vendor/`, `*.out`
   - **Ruby**: `.bundle/`, `log/`, `tmp/`, `*.gem`, `vendor/bundle/`
   - **PHP**: `vendor/`, `*.log`, `*.cache`, `*.env`
   - **Rust**: `target/`, `debug/`, `release/`, `*.rs.bk`, `*.rlib`, `*.prof*`, `.idea/`, `*.log`, `.env*`
   - **Kotlin**: `build/`, `out/`, `.gradle/`, `.idea/`, `*.class`, `*.jar`, `*.iml`, `*.log`, `.env*`
   - **C++**: `build/`, `bin/`, `obj/`, `out/`, `*.o`, `*.so`, `*.a`, `*.exe`, `*.dll`, `.idea/`, `*.log`, `.env*`
   - **C**: `build/`, `bin/`, `obj/`, `out/`, `*.o`, `*.a`, `*.so`, `*.exe`, `Makefile`, `config.log`, `.idea/`, `*.log`, `.env*`
   - **Swift**: `.build/`, `DerivedData/`, `*.swiftpm/`, `Packages/`
   - **R**: `.Rproj.user/`, `.Rhistory`, `.RData`, `.Ruserdata`, `*.Rproj`, `packrat/`, `renv/`
   - **Universal**: `.DS_Store`, `Thumbs.db`, `*.tmp`, `*.swp`, `.vscode/`, `.idea/`

   **Tool-Specific Patterns**:
   - **Docker**: `node_modules/`, `.git/`, `Dockerfile*`, `.dockerignore`, `*.log*`, `.env*`, `coverage/`
   - **ESLint**: `node_modules/`, `dist/`, `build/`, `coverage/`, `*.min.js`
   - **Prettier**: `node_modules/`, `dist/`, `build/`, `coverage/`, `package-lock.json`, `yarn.lock`, `pnpm-lock.yaml`
   - **Terraform**: `.terraform/`, `*.tfstate*`, `*.tfvars`, `.terraform.lock.hcl`
   - **Kubernetes/k8s**: `*.secret.yaml`, `secrets/`, `.kube/`, `kubeconfig*`, `*.key`, `*.crt`

5. Parse tasks.md structure and extract:
   - **Task phases**: Setup, Tests, Core, Integration, Polish
   - **Task dependencies**: Sequential vs parallel execution rules
   - **Task details**: ID, description, file paths, parallel markers [P]
   - **Execution flow**: Order and dependency requirements

6. Execute implementation following the task plan:
   - **Phase-by-phase execution**: Complete each phase before moving to the next
   - **Respect dependencies**: Run sequential tasks in order, parallel tasks [P] can run together  
   - **Follow TDD approach**: Execute test tasks before their corresponding implementation tasks
   - **File-based coordination**: Tasks affecting the same files must run sequentially
   - **Validation checkpoints**: Verify each phase completion before proceeding

7. Implementation execution rules:
   - **Setup first**: Initialize project structure, dependencies, configuration
   - **Tests before code**: If you need to write tests for contracts, entities, and integration scenarios
   - **Core development**: Implement models, services, CLI commands, endpoints
   - **Integration work**: Database connections, middleware, logging, external services
   - **Polish and validation**: Unit tests, performance optimization, documentation

8. Progress tracking and error handling:
   - Report progress after each completed task
   - Halt execution if any non-parallel task fails
   - For parallel tasks [P], continue with successful tasks, report failed ones
   - Provide clear error messages with context for debugging
   - Suggest next steps if implementation cannot proceed
   - **IMPORTANT** For completed tasks, make sure to mark the task off as [X] in the tasks file.

9. Completion validation:
   - Verify all required tasks are completed
   - Check that implemented features match the original specification
   - Validate that tests pass and coverage meets requirements
   - Confirm the implementation follows the technical plan
   - Report final status with summary of completed work

Note: This command assumes a complete task breakdown exists in tasks.md. If tasks are incomplete or missing, suggest running `/speckit.tasks` first to regenerate the task list.
</file>

<file path=".claude/commands/speckit.plan.md">
---
description: Execute the implementation planning workflow using the plan template to generate design artifacts.
handoffs: 
  - label: Create Tasks
    agent: speckit.tasks
    prompt: Break the plan into tasks
    send: true
  - label: Create Checklist
    agent: speckit.checklist
    prompt: Create a checklist for the following domain...
---

## User Input

```text
$ARGUMENTS
```

You **MUST** consider the user input before proceeding (if not empty).

## Outline

1. **Setup**: Run `.specify/scripts/bash/setup-plan.sh --json` from repo root and parse JSON for FEATURE_SPEC, IMPL_PLAN, SPECS_DIR, BRANCH. For single quotes in args like "I'm Groot", use escape syntax: e.g 'I'\''m Groot' (or double-quote if possible: "I'm Groot").

2. **Load context**: Read FEATURE_SPEC and `.specify/memory/constitution.md`. Load IMPL_PLAN template (already copied).

3. **Execute plan workflow**: Follow the structure in IMPL_PLAN template to:
   - Fill Technical Context (mark unknowns as "NEEDS CLARIFICATION")
   - Fill Constitution Check section from constitution
   - Evaluate gates (ERROR if violations unjustified)
   - Phase 0: Generate research.md (resolve all NEEDS CLARIFICATION)
   - Phase 1: Generate data-model.md, contracts/, quickstart.md
   - Phase 1: Update agent context by running the agent script
   - Re-evaluate Constitution Check post-design

4. **Stop and report**: Command ends after Phase 2 planning. Report branch, IMPL_PLAN path, and generated artifacts.

## Phases

### Phase 0: Outline & Research

1. **Extract unknowns from Technical Context** above:
   - For each NEEDS CLARIFICATION ‚Üí research task
   - For each dependency ‚Üí best practices task
   - For each integration ‚Üí patterns task

2. **Generate and dispatch research agents**:

   ```text
   For each unknown in Technical Context:
     Task: "Research {unknown} for {feature context}"
   For each technology choice:
     Task: "Find best practices for {tech} in {domain}"
   ```

3. **Consolidate findings** in `research.md` using format:
   - Decision: [what was chosen]
   - Rationale: [why chosen]
   - Alternatives considered: [what else evaluated]

**Output**: research.md with all NEEDS CLARIFICATION resolved

### Phase 1: Design & Contracts

**Prerequisites:** `research.md` complete

1. **Extract entities from feature spec** ‚Üí `data-model.md`:
   - Entity name, fields, relationships
   - Validation rules from requirements
   - State transitions if applicable

2. **Generate API contracts** from functional requirements:
   - For each user action ‚Üí endpoint
   - Use standard REST/GraphQL patterns
   - Output OpenAPI/GraphQL schema to `/contracts/`

3. **Agent context update**:
   - Run `.specify/scripts/bash/update-agent-context.sh claude`
   - These scripts detect which AI agent is in use
   - Update the appropriate agent-specific context file
   - Add only new technology from current plan
   - Preserve manual additions between markers

**Output**: data-model.md, /contracts/*, quickstart.md, agent-specific file

## Key rules

- Use absolute paths
- ERROR on gate failures or unresolved clarifications
</file>

<file path=".claude/commands/speckit.specify.md">
---
description: Create or update the feature specification from a natural language feature description.
handoffs: 
  - label: Build Technical Plan
    agent: speckit.plan
    prompt: Create a plan for the spec. I am building with...
  - label: Clarify Spec Requirements
    agent: speckit.clarify
    prompt: Clarify specification requirements
    send: true
---

## User Input

```text
$ARGUMENTS
```

You **MUST** consider the user input before proceeding (if not empty).

## Outline

The text the user typed after `/speckit.specify` in the triggering message **is** the feature description. Assume you always have it available in this conversation even if `$ARGUMENTS` appears literally below. Do not ask the user to repeat it unless they provided an empty command.

Given that feature description, do this:

1. **Generate a concise short name** (2-4 words) for the branch:
   - Analyze the feature description and extract the most meaningful keywords
   - Create a 2-4 word short name that captures the essence of the feature
   - Use action-noun format when possible (e.g., "add-user-auth", "fix-payment-bug")
   - Preserve technical terms and acronyms (OAuth2, API, JWT, etc.)
   - Keep it concise but descriptive enough to understand the feature at a glance
   - Examples:
     - "I want to add user authentication" ‚Üí "user-auth"
     - "Implement OAuth2 integration for the API" ‚Üí "oauth2-api-integration"
     - "Create a dashboard for analytics" ‚Üí "analytics-dashboard"
     - "Fix payment processing timeout bug" ‚Üí "fix-payment-timeout"

2. **Check for existing branches before creating new one**:
   
   a. First, fetch all remote branches to ensure we have the latest information:
      ```bash
      git fetch --all --prune
      ```
   
   b. Find the highest feature number across all sources for the short-name:
      - Remote branches: `git ls-remote --heads origin | grep -E 'refs/heads/[0-9]+-<short-name>$'`
      - Local branches: `git branch | grep -E '^[* ]*[0-9]+-<short-name>$'`
      - Specs directories: Check for directories matching `specs/[0-9]+-<short-name>`
   
   c. Determine the next available number:
      - Extract all numbers from all three sources
      - Find the highest number N
      - Use N+1 for the new branch number
   
   d. Run the script `.specify/scripts/bash/create-new-feature.sh --json "$ARGUMENTS"` with the calculated number and short-name:
      - Pass `--number N+1` and `--short-name "your-short-name"` along with the feature description
      - Bash example: `.specify/scripts/bash/create-new-feature.sh --json "$ARGUMENTS" --json --number 5 --short-name "user-auth" "Add user authentication"`
      - PowerShell example: `.specify/scripts/bash/create-new-feature.sh --json "$ARGUMENTS" -Json -Number 5 -ShortName "user-auth" "Add user authentication"`
   
   **IMPORTANT**:
   - Check all three sources (remote branches, local branches, specs directories) to find the highest number
   - Only match branches/directories with the exact short-name pattern
   - If no existing branches/directories found with this short-name, start with number 1
   - You must only ever run this script once per feature
   - The JSON is provided in the terminal as output - always refer to it to get the actual content you're looking for
   - The JSON output will contain BRANCH_NAME and SPEC_FILE paths
   - For single quotes in args like "I'm Groot", use escape syntax: e.g 'I'\''m Groot' (or double-quote if possible: "I'm Groot")

3. Load `.specify/templates/spec-template.md` to understand required sections.

4. Follow this execution flow:

    1. Parse user description from Input
       If empty: ERROR "No feature description provided"
    2. Extract key concepts from description
       Identify: actors, actions, data, constraints
    3. For unclear aspects:
       - Make informed guesses based on context and industry standards
       - Only mark with [NEEDS CLARIFICATION: specific question] if:
         - The choice significantly impacts feature scope or user experience
         - Multiple reasonable interpretations exist with different implications
         - No reasonable default exists
       - **LIMIT: Maximum 3 [NEEDS CLARIFICATION] markers total**
       - Prioritize clarifications by impact: scope > security/privacy > user experience > technical details
    4. Fill User Scenarios & Testing section
       If no clear user flow: ERROR "Cannot determine user scenarios"
    5. Generate Functional Requirements
       Each requirement must be testable
       Use reasonable defaults for unspecified details (document assumptions in Assumptions section)
    6. Define Success Criteria
       Create measurable, technology-agnostic outcomes
       Include both quantitative metrics (time, performance, volume) and qualitative measures (user satisfaction, task completion)
       Each criterion must be verifiable without implementation details
    7. Identify Key Entities (if data involved)
    8. Return: SUCCESS (spec ready for planning)

5. Write the specification to SPEC_FILE using the template structure, replacing placeholders with concrete details derived from the feature description (arguments) while preserving section order and headings.

6. **Specification Quality Validation**: After writing the initial spec, validate it against quality criteria:

   a. **Create Spec Quality Checklist**: Generate a checklist file at `FEATURE_DIR/checklists/requirements.md` using the checklist template structure with these validation items:

      ```markdown
      # Specification Quality Checklist: [FEATURE NAME]
      
      **Purpose**: Validate specification completeness and quality before proceeding to planning
      **Created**: [DATE]
      **Feature**: [Link to spec.md]
      
      ## Content Quality
      
      - [ ] No implementation details (languages, frameworks, APIs)
      - [ ] Focused on user value and business needs
      - [ ] Written for non-technical stakeholders
      - [ ] All mandatory sections completed
      
      ## Requirement Completeness
      
      - [ ] No [NEEDS CLARIFICATION] markers remain
      - [ ] Requirements are testable and unambiguous
      - [ ] Success criteria are measurable
      - [ ] Success criteria are technology-agnostic (no implementation details)
      - [ ] All acceptance scenarios are defined
      - [ ] Edge cases are identified
      - [ ] Scope is clearly bounded
      - [ ] Dependencies and assumptions identified
      
      ## Feature Readiness
      
      - [ ] All functional requirements have clear acceptance criteria
      - [ ] User scenarios cover primary flows
      - [ ] Feature meets measurable outcomes defined in Success Criteria
      - [ ] No implementation details leak into specification
      
      ## Notes
      
      - Items marked incomplete require spec updates before `/speckit.clarify` or `/speckit.plan`
      ```

   b. **Run Validation Check**: Review the spec against each checklist item:
      - For each item, determine if it passes or fails
      - Document specific issues found (quote relevant spec sections)

   c. **Handle Validation Results**:

      - **If all items pass**: Mark checklist complete and proceed to step 6

      - **If items fail (excluding [NEEDS CLARIFICATION])**:
        1. List the failing items and specific issues
        2. Update the spec to address each issue
        3. Re-run validation until all items pass (max 3 iterations)
        4. If still failing after 3 iterations, document remaining issues in checklist notes and warn user

      - **If [NEEDS CLARIFICATION] markers remain**:
        1. Extract all [NEEDS CLARIFICATION: ...] markers from the spec
        2. **LIMIT CHECK**: If more than 3 markers exist, keep only the 3 most critical (by scope/security/UX impact) and make informed guesses for the rest
        3. For each clarification needed (max 3), present options to user in this format:

           ```markdown
           ## Question [N]: [Topic]
           
           **Context**: [Quote relevant spec section]
           
           **What we need to know**: [Specific question from NEEDS CLARIFICATION marker]
           
           **Suggested Answers**:
           
           | Option | Answer | Implications |
           |--------|--------|--------------|
           | A      | [First suggested answer] | [What this means for the feature] |
           | B      | [Second suggested answer] | [What this means for the feature] |
           | C      | [Third suggested answer] | [What this means for the feature] |
           | Custom | Provide your own answer | [Explain how to provide custom input] |
           
           **Your choice**: _[Wait for user response]_
           ```

        4. **CRITICAL - Table Formatting**: Ensure markdown tables are properly formatted:
           - Use consistent spacing with pipes aligned
           - Each cell should have spaces around content: `| Content |` not `|Content|`
           - Header separator must have at least 3 dashes: `|--------|`
           - Test that the table renders correctly in markdown preview
        5. Number questions sequentially (Q1, Q2, Q3 - max 3 total)
        6. Present all questions together before waiting for responses
        7. Wait for user to respond with their choices for all questions (e.g., "Q1: A, Q2: Custom - [details], Q3: B")
        8. Update the spec by replacing each [NEEDS CLARIFICATION] marker with the user's selected or provided answer
        9. Re-run validation after all clarifications are resolved

   d. **Update Checklist**: After each validation iteration, update the checklist file with current pass/fail status

7. Report completion with branch name, spec file path, checklist results, and readiness for the next phase (`/speckit.clarify` or `/speckit.plan`).

**NOTE:** The script creates and checks out the new branch and initializes the spec file before writing.

## General Guidelines

## Quick Guidelines

- Focus on **WHAT** users need and **WHY**.
- Avoid HOW to implement (no tech stack, APIs, code structure).
- Written for business stakeholders, not developers.
- DO NOT create any checklists that are embedded in the spec. That will be a separate command.

### Section Requirements

- **Mandatory sections**: Must be completed for every feature
- **Optional sections**: Include only when relevant to the feature
- When a section doesn't apply, remove it entirely (don't leave as "N/A")

### For AI Generation

When creating this spec from a user prompt:

1. **Make informed guesses**: Use context, industry standards, and common patterns to fill gaps
2. **Document assumptions**: Record reasonable defaults in the Assumptions section
3. **Limit clarifications**: Maximum 3 [NEEDS CLARIFICATION] markers - use only for critical decisions that:
   - Significantly impact feature scope or user experience
   - Have multiple reasonable interpretations with different implications
   - Lack any reasonable default
4. **Prioritize clarifications**: scope > security/privacy > user experience > technical details
5. **Think like a tester**: Every vague requirement should fail the "testable and unambiguous" checklist item
6. **Common areas needing clarification** (only if no reasonable default exists):
   - Feature scope and boundaries (include/exclude specific use cases)
   - User types and permissions (if multiple conflicting interpretations possible)
   - Security/compliance requirements (when legally/financially significant)

**Examples of reasonable defaults** (don't ask about these):

- Data retention: Industry-standard practices for the domain
- Performance targets: Standard web/mobile app expectations unless specified
- Error handling: User-friendly messages with appropriate fallbacks
- Authentication method: Standard session-based or OAuth2 for web apps
- Integration patterns: RESTful APIs unless specified otherwise

### Success Criteria Guidelines

Success criteria must be:

1. **Measurable**: Include specific metrics (time, percentage, count, rate)
2. **Technology-agnostic**: No mention of frameworks, languages, databases, or tools
3. **User-focused**: Describe outcomes from user/business perspective, not system internals
4. **Verifiable**: Can be tested/validated without knowing implementation details

**Good examples**:

- "Users can complete checkout in under 3 minutes"
- "System supports 10,000 concurrent users"
- "95% of searches return results in under 1 second"
- "Task completion rate improves by 40%"

**Bad examples** (implementation-focused):

- "API response time is under 200ms" (too technical, use "Users see results instantly")
- "Database can handle 1000 TPS" (implementation detail, use user-facing metric)
- "React components render efficiently" (framework-specific)
- "Redis cache hit rate above 80%" (technology-specific)
</file>

<file path=".claude/commands/speckit.tasks.md">
---
description: Generate an actionable, dependency-ordered tasks.md for the feature based on available design artifacts.
handoffs: 
  - label: Analyze For Consistency
    agent: speckit.analyze
    prompt: Run a project analysis for consistency
    send: true
  - label: Implement Project
    agent: speckit.implement
    prompt: Start the implementation in phases
    send: true
---

## User Input

```text
$ARGUMENTS
```

You **MUST** consider the user input before proceeding (if not empty).

## Outline

1. **Setup**: Run `.specify/scripts/bash/check-prerequisites.sh --json` from repo root and parse FEATURE_DIR and AVAILABLE_DOCS list. All paths must be absolute. For single quotes in args like "I'm Groot", use escape syntax: e.g 'I'\''m Groot' (or double-quote if possible: "I'm Groot").

2. **Load design documents**: Read from FEATURE_DIR:
   - **Required**: plan.md (tech stack, libraries, structure), spec.md (user stories with priorities)
   - **Optional**: data-model.md (entities), contracts/ (API endpoints), research.md (decisions), quickstart.md (test scenarios)
   - Note: Not all projects have all documents. Generate tasks based on what's available.

3. **Execute task generation workflow**:
   - Load plan.md and extract tech stack, libraries, project structure
   - Load spec.md and extract user stories with their priorities (P1, P2, P3, etc.)
   - If data-model.md exists: Extract entities and map to user stories
   - If contracts/ exists: Map endpoints to user stories
   - If research.md exists: Extract decisions for setup tasks
   - Generate tasks organized by user story (see Task Generation Rules below)
   - Generate dependency graph showing user story completion order
   - Create parallel execution examples per user story
   - Validate task completeness (each user story has all needed tasks, independently testable)

4. **Generate tasks.md**: Use `.specify.specify/templates/tasks-template.md` as structure, fill with:
   - Correct feature name from plan.md
   - Phase 1: Setup tasks (project initialization)
   - Phase 2: Foundational tasks (blocking prerequisites for all user stories)
   - Phase 3+: One phase per user story (in priority order from spec.md)
   - Each phase includes: story goal, independent test criteria, tests (if requested), implementation tasks
   - Final Phase: Polish & cross-cutting concerns
   - All tasks must follow the strict checklist format (see Task Generation Rules below)
   - Clear file paths for each task
   - Dependencies section showing story completion order
   - Parallel execution examples per story
   - Implementation strategy section (MVP first, incremental delivery)

5. **Report**: Output path to generated tasks.md and summary:
   - Total task count
   - Task count per user story
   - Parallel opportunities identified
   - Independent test criteria for each story
   - Suggested MVP scope (typically just User Story 1)
   - Format validation: Confirm ALL tasks follow the checklist format (checkbox, ID, labels, file paths)

Context for task generation: $ARGUMENTS

The tasks.md should be immediately executable - each task must be specific enough that an LLM can complete it without additional context.

## Task Generation Rules

**CRITICAL**: Tasks MUST be organized by user story to enable independent implementation and testing.

**Tests are OPTIONAL**: Only generate test tasks if explicitly requested in the feature specification or if user requests TDD approach.

### Checklist Format (REQUIRED)

Every task MUST strictly follow this format:

```text
- [ ] [TaskID] [P?] [Story?] Description with file path
```

**Format Components**:

1. **Checkbox**: ALWAYS start with `- [ ]` (markdown checkbox)
2. **Task ID**: Sequential number (T001, T002, T003...) in execution order
3. **[P] marker**: Include ONLY if task is parallelizable (different files, no dependencies on incomplete tasks)
4. **[Story] label**: REQUIRED for user story phase tasks only
   - Format: [US1], [US2], [US3], etc. (maps to user stories from spec.md)
   - Setup phase: NO story label
   - Foundational phase: NO story label  
   - User Story phases: MUST have story label
   - Polish phase: NO story label
5. **Description**: Clear action with exact file path

**Examples**:

- ‚úÖ CORRECT: `- [ ] T001 Create project structure per implementation plan`
- ‚úÖ CORRECT: `- [ ] T005 [P] Implement authentication middleware in src/middleware/auth.py`
- ‚úÖ CORRECT: `- [ ] T012 [P] [US1] Create User model in src/models/user.py`
- ‚úÖ CORRECT: `- [ ] T014 [US1] Implement UserService in src/services/user_service.py`
- ‚ùå WRONG: `- [ ] Create User model` (missing ID and Story label)
- ‚ùå WRONG: `T001 [US1] Create model` (missing checkbox)
- ‚ùå WRONG: `- [ ] [US1] Create User model` (missing Task ID)
- ‚ùå WRONG: `- [ ] T001 [US1] Create model` (missing file path)

### Task Organization

1. **From User Stories (spec.md)** - PRIMARY ORGANIZATION:
   - Each user story (P1, P2, P3...) gets its own phase
   - Map all related components to their story:
     - Models needed for that story
     - Services needed for that story
     - Endpoints/UI needed for that story
     - If tests requested: Tests specific to that story
   - Mark story dependencies (most stories should be independent)

2. **From Contracts**:
   - Map each contract/endpoint ‚Üí to the user story it serves
   - If tests requested: Each contract ‚Üí contract test task [P] before implementation in that story's phase

3. **From Data Model**:
   - Map each entity to the user story(ies) that need it
   - If entity serves multiple stories: Put in earliest story or Setup phase
   - Relationships ‚Üí service layer tasks in appropriate story phase

4. **From Setup/Infrastructure**:
   - Shared infrastructure ‚Üí Setup phase (Phase 1)
   - Foundational/blocking tasks ‚Üí Foundational phase (Phase 2)
   - Story-specific setup ‚Üí within that story's phase

### Phase Structure

- **Phase 1**: Setup (project initialization)
- **Phase 2**: Foundational (blocking prerequisites - MUST complete before user stories)
- **Phase 3+**: User Stories in priority order (P1, P2, P3...)
  - Within each story: Tests (if requested) ‚Üí Models ‚Üí Services ‚Üí Endpoints ‚Üí Integration
  - Each phase should be a complete, independently testable increment
- **Final Phase**: Polish & Cross-Cutting Concerns
</file>

<file path=".claude/commands/speckit.taskstoissues.md">
---
description: Convert existing tasks into actionable, dependency-ordered GitHub issues for the feature based on available design artifacts.
tools: ['github/github-mcp-server/issue_write']
---

## User Input

```text
$ARGUMENTS
```

You **MUST** consider the user input before proceeding (if not empty).

## Outline

1. Run `.specify/scripts/bash/check-prerequisites.sh --json --require-tasks --include-tasks` from repo root and parse FEATURE_DIR and AVAILABLE_DOCS list. All paths must be absolute. For single quotes in args like "I'm Groot", use escape syntax: e.g 'I'\''m Groot' (or double-quote if possible: "I'm Groot").
1. From the executed script, extract the path to **tasks**.
1. Get the Git remote by running:

```bash
git config --get remote.origin.url
```

**ONLY PROCEED TO NEXT STEPS IF THE REMOTE IS A GITHUB URL**

1. For each task in the list, use the GitHub MCP server to create a new issue in the repository that is representative of the Git remote.

**UNDER NO CIRCUMSTANCES EVER CREATE ISSUES IN REPOSITORIES THAT DO NOT MATCH THE REMOTE URL**
</file>

<file path=".specify/scripts/bash/check-prerequisites.sh">
#!/usr/bin/env bash

# Consolidated prerequisite checking script
#
# This script provides unified prerequisite checking for Spec-Driven Development workflow.
# It replaces the functionality previously spread across multiple scripts.
#
# Usage: ./check-prerequisites.sh [OPTIONS]
#
# OPTIONS:
#   --json              Output in JSON format
#   --require-tasks     Require tasks.md to exist (for implementation phase)
#   --include-tasks     Include tasks.md in AVAILABLE_DOCS list
#   --paths-only        Only output path variables (no validation)
#   --help, -h          Show help message
#
# OUTPUTS:
#   JSON mode: {"FEATURE_DIR":"...", "AVAILABLE_DOCS":["..."]}
#   Text mode: FEATURE_DIR:... \n AVAILABLE_DOCS: \n ‚úì/‚úó file.md
#   Paths only: REPO_ROOT: ... \n BRANCH: ... \n FEATURE_DIR: ... etc.

set -e

# Parse command line arguments
JSON_MODE=false
REQUIRE_TASKS=false
INCLUDE_TASKS=false
PATHS_ONLY=false

for arg in "$@"; do
    case "$arg" in
        --json)
            JSON_MODE=true
            ;;
        --require-tasks)
            REQUIRE_TASKS=true
            ;;
        --include-tasks)
            INCLUDE_TASKS=true
            ;;
        --paths-only)
            PATHS_ONLY=true
            ;;
        --help|-h)
            cat << 'EOF'
Usage: check-prerequisites.sh [OPTIONS]

Consolidated prerequisite checking for Spec-Driven Development workflow.

OPTIONS:
  --json              Output in JSON format
  --require-tasks     Require tasks.md to exist (for implementation phase)
  --include-tasks     Include tasks.md in AVAILABLE_DOCS list
  --paths-only        Only output path variables (no prerequisite validation)
  --help, -h          Show this help message

EXAMPLES:
  # Check task prerequisites (plan.md required)
  ./check-prerequisites.sh --json
  
  # Check implementation prerequisites (plan.md + tasks.md required)
  ./check-prerequisites.sh --json --require-tasks --include-tasks
  
  # Get feature paths only (no validation)
  ./check-prerequisites.sh --paths-only
  
EOF
            exit 0
            ;;
        *)
            echo "ERROR: Unknown option '$arg'. Use --help for usage information." >&2
            exit 1
            ;;
    esac
done

# Source common functions
SCRIPT_DIR="$(CDPATH="" cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
source "$SCRIPT_DIR/common.sh"

# Get feature paths and validate branch
eval $(get_feature_paths)
check_feature_branch "$CURRENT_BRANCH" "$HAS_GIT" || exit 1

# If paths-only mode, output paths and exit (support JSON + paths-only combined)
if $PATHS_ONLY; then
    if $JSON_MODE; then
        # Minimal JSON paths payload (no validation performed)
        printf '{"REPO_ROOT":"%s","BRANCH":"%s","FEATURE_DIR":"%s","FEATURE_SPEC":"%s","IMPL_PLAN":"%s","TASKS":"%s"}\n' \
            "$REPO_ROOT" "$CURRENT_BRANCH" "$FEATURE_DIR" "$FEATURE_SPEC" "$IMPL_PLAN" "$TASKS"
    else
        echo "REPO_ROOT: $REPO_ROOT"
        echo "BRANCH: $CURRENT_BRANCH"
        echo "FEATURE_DIR: $FEATURE_DIR"
        echo "FEATURE_SPEC: $FEATURE_SPEC"
        echo "IMPL_PLAN: $IMPL_PLAN"
        echo "TASKS: $TASKS"
    fi
    exit 0
fi

# Validate required directories and files
if [[ ! -d "$FEATURE_DIR" ]]; then
    echo "ERROR: Feature directory not found: $FEATURE_DIR" >&2
    echo "Run /speckit.specify first to create the feature structure." >&2
    exit 1
fi

if [[ ! -f "$IMPL_PLAN" ]]; then
    echo "ERROR: plan.md not found in $FEATURE_DIR" >&2
    echo "Run /speckit.plan first to create the implementation plan." >&2
    exit 1
fi

# Check for tasks.md if required
if $REQUIRE_TASKS && [[ ! -f "$TASKS" ]]; then
    echo "ERROR: tasks.md not found in $FEATURE_DIR" >&2
    echo "Run /speckit.tasks first to create the task list." >&2
    exit 1
fi

# Build list of available documents
docs=()

# Always check these optional docs
[[ -f "$RESEARCH" ]] && docs+=("research.md")
[[ -f "$DATA_MODEL" ]] && docs+=("data-model.md")

# Check contracts directory (only if it exists and has files)
if [[ -d "$CONTRACTS_DIR" ]] && [[ -n "$(ls -A "$CONTRACTS_DIR" 2>/dev/null)" ]]; then
    docs+=("contracts/")
fi

[[ -f "$QUICKSTART" ]] && docs+=("quickstart.md")

# Include tasks.md if requested and it exists
if $INCLUDE_TASKS && [[ -f "$TASKS" ]]; then
    docs+=("tasks.md")
fi

# Output results
if $JSON_MODE; then
    # Build JSON array of documents
    if [[ ${#docs[@]} -eq 0 ]]; then
        json_docs="[]"
    else
        json_docs=$(printf '"%s",' "${docs[@]}")
        json_docs="[${json_docs%,}]"
    fi
    
    printf '{"FEATURE_DIR":"%s","AVAILABLE_DOCS":%s}\n' "$FEATURE_DIR" "$json_docs"
else
    # Text output
    echo "FEATURE_DIR:$FEATURE_DIR"
    echo "AVAILABLE_DOCS:"
    
    # Show status of each potential document
    check_file "$RESEARCH" "research.md"
    check_file "$DATA_MODEL" "data-model.md"
    check_dir "$CONTRACTS_DIR" "contracts/"
    check_file "$QUICKSTART" "quickstart.md"
    
    if $INCLUDE_TASKS; then
        check_file "$TASKS" "tasks.md"
    fi
fi
</file>

<file path=".specify/scripts/bash/common.sh">
#!/usr/bin/env bash
# Common functions and variables for all scripts

# Get repository root, with fallback for non-git repositories
get_repo_root() {
    if git rev-parse --show-toplevel >/dev/null 2>&1; then
        git rev-parse --show-toplevel
    else
        # Fall back to script location for non-git repos
        local script_dir="$(CDPATH="" cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
        (cd "$script_dir/../../.." && pwd)
    fi
}

# Get current branch, with fallback for non-git repositories
get_current_branch() {
    # First check if SPECIFY_FEATURE environment variable is set
    if [[ -n "${SPECIFY_FEATURE:-}" ]]; then
        echo "$SPECIFY_FEATURE"
        return
    fi

    # Then check git if available
    if git rev-parse --abbrev-ref HEAD >/dev/null 2>&1; then
        git rev-parse --abbrev-ref HEAD
        return
    fi

    # For non-git repos, try to find the latest feature directory
    local repo_root=$(get_repo_root)
    local specs_dir="$repo_root/specs"

    if [[ -d "$specs_dir" ]]; then
        local latest_feature=""
        local highest=0

        for dir in "$specs_dir"/*; do
            if [[ -d "$dir" ]]; then
                local dirname=$(basename "$dir")
                if [[ "$dirname" =~ ^([0-9]{3})- ]]; then
                    local number=${BASH_REMATCH[1]}
                    number=$((10#$number))
                    if [[ "$number" -gt "$highest" ]]; then
                        highest=$number
                        latest_feature=$dirname
                    fi
                fi
            fi
        done

        if [[ -n "$latest_feature" ]]; then
            echo "$latest_feature"
            return
        fi
    fi

    echo "main"  # Final fallback
}

# Check if we have git available
has_git() {
    git rev-parse --show-toplevel >/dev/null 2>&1
}

check_feature_branch() {
    local branch="$1"
    local has_git_repo="$2"

    # For non-git repos, we can't enforce branch naming but still provide output
    if [[ "$has_git_repo" != "true" ]]; then
        echo "[specify] Warning: Git repository not detected; skipped branch validation" >&2
        return 0
    fi

    if [[ ! "$branch" =~ ^[0-9]{3}- ]]; then
        echo "ERROR: Not on a feature branch. Current branch: $branch" >&2
        echo "Feature branches should be named like: 001-feature-name" >&2
        return 1
    fi

    return 0
}

get_feature_dir() { echo "$1/specs/$2"; }

# Find feature directory by numeric prefix instead of exact branch match
# This allows multiple branches to work on the same spec (e.g., 004-fix-bug, 004-add-feature)
find_feature_dir_by_prefix() {
    local repo_root="$1"
    local branch_name="$2"
    local specs_dir="$repo_root/specs"

    # Extract numeric prefix from branch (e.g., "004" from "004-whatever")
    if [[ ! "$branch_name" =~ ^([0-9]{3})- ]]; then
        # If branch doesn't have numeric prefix, fall back to exact match
        echo "$specs_dir/$branch_name"
        return
    fi

    local prefix="${BASH_REMATCH[1]}"

    # Search for directories in specs/ that start with this prefix
    local matches=()
    if [[ -d "$specs_dir" ]]; then
        for dir in "$specs_dir"/"$prefix"-*; do
            if [[ -d "$dir" ]]; then
                matches+=("$(basename "$dir")")
            fi
        done
    fi

    # Handle results
    if [[ ${#matches[@]} -eq 0 ]]; then
        # No match found - return the branch name path (will fail later with clear error)
        echo "$specs_dir/$branch_name"
    elif [[ ${#matches[@]} -eq 1 ]]; then
        # Exactly one match - perfect!
        echo "$specs_dir/${matches[0]}"
    else
        # Multiple matches - this shouldn't happen with proper naming convention
        echo "ERROR: Multiple spec directories found with prefix '$prefix': ${matches[*]}" >&2
        echo "Please ensure only one spec directory exists per numeric prefix." >&2
        echo "$specs_dir/$branch_name"  # Return something to avoid breaking the script
    fi
}

get_feature_paths() {
    local repo_root=$(get_repo_root)
    local current_branch=$(get_current_branch)
    local has_git_repo="false"

    if has_git; then
        has_git_repo="true"
    fi

    # Use prefix-based lookup to support multiple branches per spec
    local feature_dir=$(find_feature_dir_by_prefix "$repo_root" "$current_branch")

    cat <<EOF
REPO_ROOT='$repo_root'
CURRENT_BRANCH='$current_branch'
HAS_GIT='$has_git_repo'
FEATURE_DIR='$feature_dir'
FEATURE_SPEC='$feature_dir/spec.md'
IMPL_PLAN='$feature_dir/plan.md'
TASKS='$feature_dir/tasks.md'
RESEARCH='$feature_dir/research.md'
DATA_MODEL='$feature_dir/data-model.md'
QUICKSTART='$feature_dir/quickstart.md'
CONTRACTS_DIR='$feature_dir/contracts'
EOF
}

check_file() { [[ -f "$1" ]] && echo "  ‚úì $2" || echo "  ‚úó $2"; }
check_dir() { [[ -d "$1" && -n $(ls -A "$1" 2>/dev/null) ]] && echo "  ‚úì $2" || echo "  ‚úó $2"; }
</file>

<file path=".specify/scripts/bash/create-new-feature.sh">
#!/usr/bin/env bash

set -e

JSON_MODE=false
SHORT_NAME=""
BRANCH_NUMBER=""
ARGS=()
i=1
while [ $i -le $# ]; do
    arg="${!i}"
    case "$arg" in
        --json) 
            JSON_MODE=true 
            ;;
        --short-name)
            if [ $((i + 1)) -gt $# ]; then
                echo 'Error: --short-name requires a value' >&2
                exit 1
            fi
            i=$((i + 1))
            next_arg="${!i}"
            # Check if the next argument is another option (starts with --)
            if [[ "$next_arg" == --* ]]; then
                echo 'Error: --short-name requires a value' >&2
                exit 1
            fi
            SHORT_NAME="$next_arg"
            ;;
        --number)
            if [ $((i + 1)) -gt $# ]; then
                echo 'Error: --number requires a value' >&2
                exit 1
            fi
            i=$((i + 1))
            next_arg="${!i}"
            if [[ "$next_arg" == --* ]]; then
                echo 'Error: --number requires a value' >&2
                exit 1
            fi
            BRANCH_NUMBER="$next_arg"
            ;;
        --help|-h) 
            echo "Usage: $0 [--json] [--short-name <name>] [--number N] <feature_description>"
            echo ""
            echo "Options:"
            echo "  --json              Output in JSON format"
            echo "  --short-name <name> Provide a custom short name (2-4 words) for the branch"
            echo "  --number N          Specify branch number manually (overrides auto-detection)"
            echo "  --help, -h          Show this help message"
            echo ""
            echo "Examples:"
            echo "  $0 'Add user authentication system' --short-name 'user-auth'"
            echo "  $0 'Implement OAuth2 integration for API' --number 5"
            exit 0
            ;;
        *) 
            ARGS+=("$arg") 
            ;;
    esac
    i=$((i + 1))
done

FEATURE_DESCRIPTION="${ARGS[*]}"
if [ -z "$FEATURE_DESCRIPTION" ]; then
    echo "Usage: $0 [--json] [--short-name <name>] [--number N] <feature_description>" >&2
    exit 1
fi

# Function to find the repository root by searching for existing project markers
find_repo_root() {
    local dir="$1"
    while [ "$dir" != "/" ]; do
        if [ -d "$dir/.git" ] || [ -d "$dir/.specify" ]; then
            echo "$dir"
            return 0
        fi
        dir="$(dirname "$dir")"
    done
    return 1
}

# Function to get highest number from specs directory
get_highest_from_specs() {
    local specs_dir="$1"
    local highest=0
    
    if [ -d "$specs_dir" ]; then
        for dir in "$specs_dir"/*; do
            [ -d "$dir" ] || continue
            dirname=$(basename "$dir")
            number=$(echo "$dirname" | grep -o '^[0-9]\+' || echo "0")
            number=$((10#$number))
            if [ "$number" -gt "$highest" ]; then
                highest=$number
            fi
        done
    fi
    
    echo "$highest"
}

# Function to get highest number from git branches
get_highest_from_branches() {
    local highest=0
    
    # Get all branches (local and remote)
    branches=$(git branch -a 2>/dev/null || echo "")
    
    if [ -n "$branches" ]; then
        while IFS= read -r branch; do
            # Clean branch name: remove leading markers and remote prefixes
            clean_branch=$(echo "$branch" | sed 's/^[* ]*//; s|^remotes/[^/]*/||')
            
            # Extract feature number if branch matches pattern ###-*
            if echo "$clean_branch" | grep -q '^[0-9]\{3\}-'; then
                number=$(echo "$clean_branch" | grep -o '^[0-9]\{3\}' || echo "0")
                number=$((10#$number))
                if [ "$number" -gt "$highest" ]; then
                    highest=$number
                fi
            fi
        done <<< "$branches"
    fi
    
    echo "$highest"
}

# Function to check existing branches (local and remote) and return next available number
check_existing_branches() {
    local short_name="$1"
    local specs_dir="$2"
    
    # Fetch all remotes to get latest branch info (suppress errors if no remotes)
    git fetch --all --prune 2>/dev/null || true
    
    # Find all branches matching the pattern using git ls-remote (more reliable)
    local remote_branches=$(git ls-remote --heads origin 2>/dev/null | grep -E "refs/heads/[0-9]+-${short_name}$" | sed 's/.*\/\([0-9]*\)-.*/\1/' | sort -n)
    
    # Also check local branches
    local local_branches=$(git branch 2>/dev/null | grep -E "^[* ]*[0-9]+-${short_name}$" | sed 's/^[* ]*//' | sed 's/-.*//' | sort -n)
    
    # Check specs directory as well
    local spec_dirs=""
    if [ -d "$specs_dir" ]; then
        spec_dirs=$(find "$specs_dir" -maxdepth 1 -type d -name "[0-9]*-${short_name}" 2>/dev/null | xargs -n1 basename 2>/dev/null | sed 's/-.*//' | sort -n)
    fi
    
    # Combine all sources and get the highest number
    local max_num=0
    for num in $remote_branches $local_branches $spec_dirs; do
        if [ "$num" -gt "$max_num" ]; then
            max_num=$num
        fi
    done
    
    # Return next number
    echo $((max_num + 1))
}

# Function to clean and format a branch name
clean_branch_name() {
    local name="$1"
    echo "$name" | tr '[:upper:]' '[:lower:]' | sed 's/[^a-z0-9]/-/g' | sed 's/-\+/-/g' | sed 's/^-//' | sed 's/-$//'
}

# Resolve repository root. Prefer git information when available, but fall back
# to searching for repository markers so the workflow still functions in repositories that
# were initialised with --no-git.
SCRIPT_DIR="$(CDPATH="" cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"

if git rev-parse --show-toplevel >/dev/null 2>&1; then
    REPO_ROOT=$(git rev-parse --show-toplevel)
    HAS_GIT=true
else
    REPO_ROOT="$(find_repo_root "$SCRIPT_DIR")"
    if [ -z "$REPO_ROOT" ]; then
        echo "Error: Could not determine repository root. Please run this script from within the repository." >&2
        exit 1
    fi
    HAS_GIT=false
fi

cd "$REPO_ROOT"

SPECS_DIR="$REPO_ROOT/specs"
mkdir -p "$SPECS_DIR"

# Function to generate branch name with stop word filtering and length filtering
generate_branch_name() {
    local description="$1"
    
    # Common stop words to filter out
    local stop_words="^(i|a|an|the|to|for|of|in|on|at|by|with|from|is|are|was|were|be|been|being|have|has|had|do|does|did|will|would|should|could|can|may|might|must|shall|this|that|these|those|my|your|our|their|want|need|add|get|set)$"
    
    # Convert to lowercase and split into words
    local clean_name=$(echo "$description" | tr '[:upper:]' '[:lower:]' | sed 's/[^a-z0-9]/ /g')
    
    # Filter words: remove stop words and words shorter than 3 chars (unless they're uppercase acronyms in original)
    local meaningful_words=()
    for word in $clean_name; do
        # Skip empty words
        [ -z "$word" ] && continue
        
        # Keep words that are NOT stop words AND (length >= 3 OR are potential acronyms)
        if ! echo "$word" | grep -qiE "$stop_words"; then
            if [ ${#word} -ge 3 ]; then
                meaningful_words+=("$word")
            elif echo "$description" | grep -q "\b${word^^}\b"; then
                # Keep short words if they appear as uppercase in original (likely acronyms)
                meaningful_words+=("$word")
            fi
        fi
    done
    
    # If we have meaningful words, use first 3-4 of them
    if [ ${#meaningful_words[@]} -gt 0 ]; then
        local max_words=3
        if [ ${#meaningful_words[@]} -eq 4 ]; then max_words=4; fi
        
        local result=""
        local count=0
        for word in "${meaningful_words[@]}"; do
            if [ $count -ge $max_words ]; then break; fi
            if [ -n "$result" ]; then result="$result-"; fi
            result="$result$word"
            count=$((count + 1))
        done
        echo "$result"
    else
        # Fallback to original logic if no meaningful words found
        local cleaned=$(clean_branch_name "$description")
        echo "$cleaned" | tr '-' '\n' | grep -v '^$' | head -3 | tr '\n' '-' | sed 's/-$//'
    fi
}

# Generate branch name
if [ -n "$SHORT_NAME" ]; then
    # Use provided short name, just clean it up
    BRANCH_SUFFIX=$(clean_branch_name "$SHORT_NAME")
else
    # Generate from description with smart filtering
    BRANCH_SUFFIX=$(generate_branch_name "$FEATURE_DESCRIPTION")
fi

# Determine branch number
if [ -z "$BRANCH_NUMBER" ]; then
    if [ "$HAS_GIT" = true ]; then
        # Check existing branches on remotes
        BRANCH_NUMBER=$(check_existing_branches "$BRANCH_SUFFIX" "$SPECS_DIR")
    else
        # Fall back to local directory check
        HIGHEST=$(get_highest_from_specs "$SPECS_DIR")
        BRANCH_NUMBER=$((HIGHEST + 1))
    fi
fi

FEATURE_NUM=$(printf "%03d" "$BRANCH_NUMBER")
BRANCH_NAME="${FEATURE_NUM}-${BRANCH_SUFFIX}"

# GitHub enforces a 244-byte limit on branch names
# Validate and truncate if necessary
MAX_BRANCH_LENGTH=244
if [ ${#BRANCH_NAME} -gt $MAX_BRANCH_LENGTH ]; then
    # Calculate how much we need to trim from suffix
    # Account for: feature number (3) + hyphen (1) = 4 chars
    MAX_SUFFIX_LENGTH=$((MAX_BRANCH_LENGTH - 4))
    
    # Truncate suffix at word boundary if possible
    TRUNCATED_SUFFIX=$(echo "$BRANCH_SUFFIX" | cut -c1-$MAX_SUFFIX_LENGTH)
    # Remove trailing hyphen if truncation created one
    TRUNCATED_SUFFIX=$(echo "$TRUNCATED_SUFFIX" | sed 's/-$//')
    
    ORIGINAL_BRANCH_NAME="$BRANCH_NAME"
    BRANCH_NAME="${FEATURE_NUM}-${TRUNCATED_SUFFIX}"
    
    >&2 echo "[specify] Warning: Branch name exceeded GitHub's 244-byte limit"
    >&2 echo "[specify] Original: $ORIGINAL_BRANCH_NAME (${#ORIGINAL_BRANCH_NAME} bytes)"
    >&2 echo "[specify] Truncated to: $BRANCH_NAME (${#BRANCH_NAME} bytes)"
fi

if [ "$HAS_GIT" = true ]; then
    git checkout -b "$BRANCH_NAME"
else
    >&2 echo "[specify] Warning: Git repository not detected; skipped branch creation for $BRANCH_NAME"
fi

FEATURE_DIR="$SPECS_DIR/$BRANCH_NAME"
mkdir -p "$FEATURE_DIR"

TEMPLATE="$REPO_ROOT/.specify/templates/spec-template.md"
SPEC_FILE="$FEATURE_DIR/spec.md"
if [ -f "$TEMPLATE" ]; then cp "$TEMPLATE" "$SPEC_FILE"; else touch "$SPEC_FILE"; fi

# Set the SPECIFY_FEATURE environment variable for the current session
export SPECIFY_FEATURE="$BRANCH_NAME"

if $JSON_MODE; then
    printf '{"BRANCH_NAME":"%s","SPEC_FILE":"%s","FEATURE_NUM":"%s"}\n' "$BRANCH_NAME" "$SPEC_FILE" "$FEATURE_NUM"
else
    echo "BRANCH_NAME: $BRANCH_NAME"
    echo "SPEC_FILE: $SPEC_FILE"
    echo "FEATURE_NUM: $FEATURE_NUM"
    echo "SPECIFY_FEATURE environment variable set to: $BRANCH_NAME"
fi
</file>

<file path=".specify/scripts/bash/setup-plan.sh">
#!/usr/bin/env bash

set -e

# Parse command line arguments
JSON_MODE=false
ARGS=()

for arg in "$@"; do
    case "$arg" in
        --json) 
            JSON_MODE=true 
            ;;
        --help|-h) 
            echo "Usage: $0 [--json]"
            echo "  --json    Output results in JSON format"
            echo "  --help    Show this help message"
            exit 0 
            ;;
        *) 
            ARGS+=("$arg") 
            ;;
    esac
done

# Get script directory and load common functions
SCRIPT_DIR="$(CDPATH="" cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
source "$SCRIPT_DIR/common.sh"

# Get all paths and variables from common functions
eval $(get_feature_paths)

# Check if we're on a proper feature branch (only for git repos)
check_feature_branch "$CURRENT_BRANCH" "$HAS_GIT" || exit 1

# Ensure the feature directory exists
mkdir -p "$FEATURE_DIR"

# Copy plan template if it exists
TEMPLATE="$REPO_ROOT/.specify/templates/plan-template.md"
if [[ -f "$TEMPLATE" ]]; then
    cp "$TEMPLATE" "$IMPL_PLAN"
    echo "Copied plan template to $IMPL_PLAN"
else
    echo "Warning: Plan template not found at $TEMPLATE"
    # Create a basic plan file if template doesn't exist
    touch "$IMPL_PLAN"
fi

# Output results
if $JSON_MODE; then
    printf '{"FEATURE_SPEC":"%s","IMPL_PLAN":"%s","SPECS_DIR":"%s","BRANCH":"%s","HAS_GIT":"%s"}\n' \
        "$FEATURE_SPEC" "$IMPL_PLAN" "$FEATURE_DIR" "$CURRENT_BRANCH" "$HAS_GIT"
else
    echo "FEATURE_SPEC: $FEATURE_SPEC"
    echo "IMPL_PLAN: $IMPL_PLAN" 
    echo "SPECS_DIR: $FEATURE_DIR"
    echo "BRANCH: $CURRENT_BRANCH"
    echo "HAS_GIT: $HAS_GIT"
fi
</file>

<file path=".specify/scripts/bash/update-agent-context.sh">
#!/usr/bin/env bash

# Update agent context files with information from plan.md
#
# This script maintains AI agent context files by parsing feature specifications 
# and updating agent-specific configuration files with project information.
#
# MAIN FUNCTIONS:
# 1. Environment Validation
#    - Verifies git repository structure and branch information
#    - Checks for required plan.md files and templates
#    - Validates file permissions and accessibility
#
# 2. Plan Data Extraction
#    - Parses plan.md files to extract project metadata
#    - Identifies language/version, frameworks, databases, and project types
#    - Handles missing or incomplete specification data gracefully
#
# 3. Agent File Management
#    - Creates new agent context files from templates when needed
#    - Updates existing agent files with new project information
#    - Preserves manual additions and custom configurations
#    - Supports multiple AI agent formats and directory structures
#
# 4. Content Generation
#    - Generates language-specific build/test commands
#    - Creates appropriate project directory structures
#    - Updates technology stacks and recent changes sections
#    - Maintains consistent formatting and timestamps
#
# 5. Multi-Agent Support
#    - Handles agent-specific file paths and naming conventions
#    - Supports: Claude, Gemini, Copilot, Cursor, Qwen, opencode, Codex, Windsurf, Kilo Code, Auggie CLI, Roo Code, CodeBuddy CLI, Amp, SHAI, or Amazon Q Developer CLI
#    - Can update single agents or all existing agent files
#    - Creates default Claude file if no agent files exist
#
# Usage: ./update-agent-context.sh [agent_type]
# Agent types: claude|gemini|copilot|cursor-agent|qwen|opencode|codex|windsurf|kilocode|auggie|shai|q
# Leave empty to update all existing agent files

set -e

# Enable strict error handling
set -u
set -o pipefail

#==============================================================================
# Configuration and Global Variables
#==============================================================================

# Get script directory and load common functions
SCRIPT_DIR="$(CDPATH="" cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
source "$SCRIPT_DIR/common.sh"

# Get all paths and variables from common functions
eval $(get_feature_paths)

NEW_PLAN="$IMPL_PLAN"  # Alias for compatibility with existing code
AGENT_TYPE="${1:-}"

# Agent-specific file paths  
CLAUDE_FILE="$REPO_ROOT/CLAUDE.md"
GEMINI_FILE="$REPO_ROOT/GEMINI.md"
COPILOT_FILE="$REPO_ROOT/.github/agents/copilot-instructions.md"
CURSOR_FILE="$REPO_ROOT/.cursor/rules/specify-rules.mdc"
QWEN_FILE="$REPO_ROOT/QWEN.md"
AGENTS_FILE="$REPO_ROOT/AGENTS.md"
WINDSURF_FILE="$REPO_ROOT/.windsurf/rules/specify-rules.md"
KILOCODE_FILE="$REPO_ROOT/.kilocode/rules/specify-rules.md"
AUGGIE_FILE="$REPO_ROOT/.augment/rules/specify-rules.md"
ROO_FILE="$REPO_ROOT/.roo/rules/specify-rules.md"
CODEBUDDY_FILE="$REPO_ROOT/CODEBUDDY.md"
AMP_FILE="$REPO_ROOT/AGENTS.md"
SHAI_FILE="$REPO_ROOT/SHAI.md"
Q_FILE="$REPO_ROOT/AGENTS.md"

# Template file
TEMPLATE_FILE="$REPO_ROOT/.specify/templates/agent-file-template.md"

# Global variables for parsed plan data
NEW_LANG=""
NEW_FRAMEWORK=""
NEW_DB=""
NEW_PROJECT_TYPE=""

#==============================================================================
# Utility Functions
#==============================================================================

log_info() {
    echo "INFO: $1"
}

log_success() {
    echo "‚úì $1"
}

log_error() {
    echo "ERROR: $1" >&2
}

log_warning() {
    echo "WARNING: $1" >&2
}

# Cleanup function for temporary files
cleanup() {
    local exit_code=$?
    rm -f /tmp/agent_update_*_$$
    rm -f /tmp/manual_additions_$$
    exit $exit_code
}

# Set up cleanup trap
trap cleanup EXIT INT TERM

#==============================================================================
# Validation Functions
#==============================================================================

validate_environment() {
    # Check if we have a current branch/feature (git or non-git)
    if [[ -z "$CURRENT_BRANCH" ]]; then
        log_error "Unable to determine current feature"
        if [[ "$HAS_GIT" == "true" ]]; then
            log_info "Make sure you're on a feature branch"
        else
            log_info "Set SPECIFY_FEATURE environment variable or create a feature first"
        fi
        exit 1
    fi
    
    # Check if plan.md exists
    if [[ ! -f "$NEW_PLAN" ]]; then
        log_error "No plan.md found at $NEW_PLAN"
        log_info "Make sure you're working on a feature with a corresponding spec directory"
        if [[ "$HAS_GIT" != "true" ]]; then
            log_info "Use: export SPECIFY_FEATURE=your-feature-name or create a new feature first"
        fi
        exit 1
    fi
    
    # Check if template exists (needed for new files)
    if [[ ! -f "$TEMPLATE_FILE" ]]; then
        log_warning "Template file not found at $TEMPLATE_FILE"
        log_warning "Creating new agent files will fail"
    fi
}

#==============================================================================
# Plan Parsing Functions
#==============================================================================

extract_plan_field() {
    local field_pattern="$1"
    local plan_file="$2"
    
    grep "^\*\*${field_pattern}\*\*: " "$plan_file" 2>/dev/null | \
        head -1 | \
        sed "s|^\*\*${field_pattern}\*\*: ||" | \
        sed 's/^[ \t]*//;s/[ \t]*$//' | \
        grep -v "NEEDS CLARIFICATION" | \
        grep -v "^N/A$" || echo ""
}

parse_plan_data() {
    local plan_file="$1"
    
    if [[ ! -f "$plan_file" ]]; then
        log_error "Plan file not found: $plan_file"
        return 1
    fi
    
    if [[ ! -r "$plan_file" ]]; then
        log_error "Plan file is not readable: $plan_file"
        return 1
    fi
    
    log_info "Parsing plan data from $plan_file"
    
    NEW_LANG=$(extract_plan_field "Language/Version" "$plan_file")
    NEW_FRAMEWORK=$(extract_plan_field "Primary Dependencies" "$plan_file")
    NEW_DB=$(extract_plan_field "Storage" "$plan_file")
    NEW_PROJECT_TYPE=$(extract_plan_field "Project Type" "$plan_file")
    
    # Log what we found
    if [[ -n "$NEW_LANG" ]]; then
        log_info "Found language: $NEW_LANG"
    else
        log_warning "No language information found in plan"
    fi
    
    if [[ -n "$NEW_FRAMEWORK" ]]; then
        log_info "Found framework: $NEW_FRAMEWORK"
    fi
    
    if [[ -n "$NEW_DB" ]] && [[ "$NEW_DB" != "N/A" ]]; then
        log_info "Found database: $NEW_DB"
    fi
    
    if [[ -n "$NEW_PROJECT_TYPE" ]]; then
        log_info "Found project type: $NEW_PROJECT_TYPE"
    fi
}

format_technology_stack() {
    local lang="$1"
    local framework="$2"
    local parts=()
    
    # Add non-empty parts
    [[ -n "$lang" && "$lang" != "NEEDS CLARIFICATION" ]] && parts+=("$lang")
    [[ -n "$framework" && "$framework" != "NEEDS CLARIFICATION" && "$framework" != "N/A" ]] && parts+=("$framework")
    
    # Join with proper formatting
    if [[ ${#parts[@]} -eq 0 ]]; then
        echo ""
    elif [[ ${#parts[@]} -eq 1 ]]; then
        echo "${parts[0]}"
    else
        # Join multiple parts with " + "
        local result="${parts[0]}"
        for ((i=1; i<${#parts[@]}; i++)); do
            result="$result + ${parts[i]}"
        done
        echo "$result"
    fi
}

#==============================================================================
# Template and Content Generation Functions
#==============================================================================

get_project_structure() {
    local project_type="$1"
    
    if [[ "$project_type" == *"web"* ]]; then
        echo "backend/\\nfrontend/\\ntests/"
    else
        echo "src/\\ntests/"
    fi
}

get_commands_for_language() {
    local lang="$1"
    
    case "$lang" in
        *"Python"*)
            echo "cd src && pytest && ruff check ."
            ;;
        *"Rust"*)
            echo "cargo test && cargo clippy"
            ;;
        *"JavaScript"*|*"TypeScript"*)
            echo "npm test \\&\\& npm run lint"
            ;;
        *)
            echo "# Add commands for $lang"
            ;;
    esac
}

get_language_conventions() {
    local lang="$1"
    echo "$lang: Follow standard conventions"
}

create_new_agent_file() {
    local target_file="$1"
    local temp_file="$2"
    local project_name="$3"
    local current_date="$4"
    
    if [[ ! -f "$TEMPLATE_FILE" ]]; then
        log_error "Template not found at $TEMPLATE_FILE"
        return 1
    fi
    
    if [[ ! -r "$TEMPLATE_FILE" ]]; then
        log_error "Template file is not readable: $TEMPLATE_FILE"
        return 1
    fi
    
    log_info "Creating new agent context file from template..."
    
    if ! cp "$TEMPLATE_FILE" "$temp_file"; then
        log_error "Failed to copy template file"
        return 1
    fi
    
    # Replace template placeholders
    local project_structure
    project_structure=$(get_project_structure "$NEW_PROJECT_TYPE")
    
    local commands
    commands=$(get_commands_for_language "$NEW_LANG")
    
    local language_conventions
    language_conventions=$(get_language_conventions "$NEW_LANG")
    
    # Perform substitutions with error checking using safer approach
    # Escape special characters for sed by using a different delimiter or escaping
    local escaped_lang=$(printf '%s\n' "$NEW_LANG" | sed 's/[\[\.*^$()+{}|]/\\&/g')
    local escaped_framework=$(printf '%s\n' "$NEW_FRAMEWORK" | sed 's/[\[\.*^$()+{}|]/\\&/g')
    local escaped_branch=$(printf '%s\n' "$CURRENT_BRANCH" | sed 's/[\[\.*^$()+{}|]/\\&/g')
    
    # Build technology stack and recent change strings conditionally
    local tech_stack
    if [[ -n "$escaped_lang" && -n "$escaped_framework" ]]; then
        tech_stack="- $escaped_lang + $escaped_framework ($escaped_branch)"
    elif [[ -n "$escaped_lang" ]]; then
        tech_stack="- $escaped_lang ($escaped_branch)"
    elif [[ -n "$escaped_framework" ]]; then
        tech_stack="- $escaped_framework ($escaped_branch)"
    else
        tech_stack="- ($escaped_branch)"
    fi

    local recent_change
    if [[ -n "$escaped_lang" && -n "$escaped_framework" ]]; then
        recent_change="- $escaped_branch: Added $escaped_lang + $escaped_framework"
    elif [[ -n "$escaped_lang" ]]; then
        recent_change="- $escaped_branch: Added $escaped_lang"
    elif [[ -n "$escaped_framework" ]]; then
        recent_change="- $escaped_branch: Added $escaped_framework"
    else
        recent_change="- $escaped_branch: Added"
    fi

    local substitutions=(
        "s|\[PROJECT NAME\]|$project_name|"
        "s|\[DATE\]|$current_date|"
        "s|\[EXTRACTED FROM ALL PLAN.MD FILES\]|$tech_stack|"
        "s|\[ACTUAL STRUCTURE FROM PLANS\]|$project_structure|g"
        "s|\[ONLY COMMANDS FOR ACTIVE TECHNOLOGIES\]|$commands|"
        "s|\[LANGUAGE-SPECIFIC, ONLY FOR LANGUAGES IN USE\]|$language_conventions|"
        "s|\[LAST 3 FEATURES AND WHAT THEY ADDED\]|$recent_change|"
    )
    
    for substitution in "${substitutions[@]}"; do
        if ! sed -i.bak -e "$substitution" "$temp_file"; then
            log_error "Failed to perform substitution: $substitution"
            rm -f "$temp_file" "$temp_file.bak"
            return 1
        fi
    done
    
    # Convert \n sequences to actual newlines
    newline=$(printf '\n')
    sed -i.bak2 "s/\\\\n/${newline}/g" "$temp_file"
    
    # Clean up backup files
    rm -f "$temp_file.bak" "$temp_file.bak2"
    
    return 0
}




update_existing_agent_file() {
    local target_file="$1"
    local current_date="$2"
    
    log_info "Updating existing agent context file..."
    
    # Use a single temporary file for atomic update
    local temp_file
    temp_file=$(mktemp) || {
        log_error "Failed to create temporary file"
        return 1
    }
    
    # Process the file in one pass
    local tech_stack=$(format_technology_stack "$NEW_LANG" "$NEW_FRAMEWORK")
    local new_tech_entries=()
    local new_change_entry=""
    
    # Prepare new technology entries
    if [[ -n "$tech_stack" ]] && ! grep -q "$tech_stack" "$target_file"; then
        new_tech_entries+=("- $tech_stack ($CURRENT_BRANCH)")
    fi
    
    if [[ -n "$NEW_DB" ]] && [[ "$NEW_DB" != "N/A" ]] && [[ "$NEW_DB" != "NEEDS CLARIFICATION" ]] && ! grep -q "$NEW_DB" "$target_file"; then
        new_tech_entries+=("- $NEW_DB ($CURRENT_BRANCH)")
    fi
    
    # Prepare new change entry
    if [[ -n "$tech_stack" ]]; then
        new_change_entry="- $CURRENT_BRANCH: Added $tech_stack"
    elif [[ -n "$NEW_DB" ]] && [[ "$NEW_DB" != "N/A" ]] && [[ "$NEW_DB" != "NEEDS CLARIFICATION" ]]; then
        new_change_entry="- $CURRENT_BRANCH: Added $NEW_DB"
    fi
    
    # Check if sections exist in the file
    local has_active_technologies=0
    local has_recent_changes=0
    
    if grep -q "^## Active Technologies" "$target_file" 2>/dev/null; then
        has_active_technologies=1
    fi
    
    if grep -q "^## Recent Changes" "$target_file" 2>/dev/null; then
        has_recent_changes=1
    fi
    
    # Process file line by line
    local in_tech_section=false
    local in_changes_section=false
    local tech_entries_added=false
    local changes_entries_added=false
    local existing_changes_count=0
    local file_ended=false
    
    while IFS= read -r line || [[ -n "$line" ]]; do
        # Handle Active Technologies section
        if [[ "$line" == "## Active Technologies" ]]; then
            echo "$line" >> "$temp_file"
            in_tech_section=true
            continue
        elif [[ $in_tech_section == true ]] && [[ "$line" =~ ^##[[:space:]] ]]; then
            # Add new tech entries before closing the section
            if [[ $tech_entries_added == false ]] && [[ ${#new_tech_entries[@]} -gt 0 ]]; then
                printf '%s\n' "${new_tech_entries[@]}" >> "$temp_file"
                tech_entries_added=true
            fi
            echo "$line" >> "$temp_file"
            in_tech_section=false
            continue
        elif [[ $in_tech_section == true ]] && [[ -z "$line" ]]; then
            # Add new tech entries before empty line in tech section
            if [[ $tech_entries_added == false ]] && [[ ${#new_tech_entries[@]} -gt 0 ]]; then
                printf '%s\n' "${new_tech_entries[@]}" >> "$temp_file"
                tech_entries_added=true
            fi
            echo "$line" >> "$temp_file"
            continue
        fi
        
        # Handle Recent Changes section
        if [[ "$line" == "## Recent Changes" ]]; then
            echo "$line" >> "$temp_file"
            # Add new change entry right after the heading
            if [[ -n "$new_change_entry" ]]; then
                echo "$new_change_entry" >> "$temp_file"
            fi
            in_changes_section=true
            changes_entries_added=true
            continue
        elif [[ $in_changes_section == true ]] && [[ "$line" =~ ^##[[:space:]] ]]; then
            echo "$line" >> "$temp_file"
            in_changes_section=false
            continue
        elif [[ $in_changes_section == true ]] && [[ "$line" == "- "* ]]; then
            # Keep only first 2 existing changes
            if [[ $existing_changes_count -lt 2 ]]; then
                echo "$line" >> "$temp_file"
                ((existing_changes_count++))
            fi
            continue
        fi
        
        # Update timestamp
        if [[ "$line" =~ \*\*Last\ updated\*\*:.*[0-9][0-9][0-9][0-9]-[0-9][0-9]-[0-9][0-9] ]]; then
            echo "$line" | sed "s/[0-9][0-9][0-9][0-9]-[0-9][0-9]-[0-9][0-9]/$current_date/" >> "$temp_file"
        else
            echo "$line" >> "$temp_file"
        fi
    done < "$target_file"
    
    # Post-loop check: if we're still in the Active Technologies section and haven't added new entries
    if [[ $in_tech_section == true ]] && [[ $tech_entries_added == false ]] && [[ ${#new_tech_entries[@]} -gt 0 ]]; then
        printf '%s\n' "${new_tech_entries[@]}" >> "$temp_file"
        tech_entries_added=true
    fi
    
    # If sections don't exist, add them at the end of the file
    if [[ $has_active_technologies -eq 0 ]] && [[ ${#new_tech_entries[@]} -gt 0 ]]; then
        echo "" >> "$temp_file"
        echo "## Active Technologies" >> "$temp_file"
        printf '%s\n' "${new_tech_entries[@]}" >> "$temp_file"
        tech_entries_added=true
    fi
    
    if [[ $has_recent_changes -eq 0 ]] && [[ -n "$new_change_entry" ]]; then
        echo "" >> "$temp_file"
        echo "## Recent Changes" >> "$temp_file"
        echo "$new_change_entry" >> "$temp_file"
        changes_entries_added=true
    fi
    
    # Move temp file to target atomically
    if ! mv "$temp_file" "$target_file"; then
        log_error "Failed to update target file"
        rm -f "$temp_file"
        return 1
    fi
    
    return 0
}
#==============================================================================
# Main Agent File Update Function
#==============================================================================

update_agent_file() {
    local target_file="$1"
    local agent_name="$2"
    
    if [[ -z "$target_file" ]] || [[ -z "$agent_name" ]]; then
        log_error "update_agent_file requires target_file and agent_name parameters"
        return 1
    fi
    
    log_info "Updating $agent_name context file: $target_file"
    
    local project_name
    project_name=$(basename "$REPO_ROOT")
    local current_date
    current_date=$(date +%Y-%m-%d)
    
    # Create directory if it doesn't exist
    local target_dir
    target_dir=$(dirname "$target_file")
    if [[ ! -d "$target_dir" ]]; then
        if ! mkdir -p "$target_dir"; then
            log_error "Failed to create directory: $target_dir"
            return 1
        fi
    fi
    
    if [[ ! -f "$target_file" ]]; then
        # Create new file from template
        local temp_file
        temp_file=$(mktemp) || {
            log_error "Failed to create temporary file"
            return 1
        }
        
        if create_new_agent_file "$target_file" "$temp_file" "$project_name" "$current_date"; then
            if mv "$temp_file" "$target_file"; then
                log_success "Created new $agent_name context file"
            else
                log_error "Failed to move temporary file to $target_file"
                rm -f "$temp_file"
                return 1
            fi
        else
            log_error "Failed to create new agent file"
            rm -f "$temp_file"
            return 1
        fi
    else
        # Update existing file
        if [[ ! -r "$target_file" ]]; then
            log_error "Cannot read existing file: $target_file"
            return 1
        fi
        
        if [[ ! -w "$target_file" ]]; then
            log_error "Cannot write to existing file: $target_file"
            return 1
        fi
        
        if update_existing_agent_file "$target_file" "$current_date"; then
            log_success "Updated existing $agent_name context file"
        else
            log_error "Failed to update existing agent file"
            return 1
        fi
    fi
    
    return 0
}

#==============================================================================
# Agent Selection and Processing
#==============================================================================

update_specific_agent() {
    local agent_type="$1"
    
    case "$agent_type" in
        claude)
            update_agent_file "$CLAUDE_FILE" "Claude Code"
            ;;
        gemini)
            update_agent_file "$GEMINI_FILE" "Gemini CLI"
            ;;
        copilot)
            update_agent_file "$COPILOT_FILE" "GitHub Copilot"
            ;;
        cursor-agent)
            update_agent_file "$CURSOR_FILE" "Cursor IDE"
            ;;
        qwen)
            update_agent_file "$QWEN_FILE" "Qwen Code"
            ;;
        opencode)
            update_agent_file "$AGENTS_FILE" "opencode"
            ;;
        codex)
            update_agent_file "$AGENTS_FILE" "Codex CLI"
            ;;
        windsurf)
            update_agent_file "$WINDSURF_FILE" "Windsurf"
            ;;
        kilocode)
            update_agent_file "$KILOCODE_FILE" "Kilo Code"
            ;;
        auggie)
            update_agent_file "$AUGGIE_FILE" "Auggie CLI"
            ;;
        roo)
            update_agent_file "$ROO_FILE" "Roo Code"
            ;;
        codebuddy)
            update_agent_file "$CODEBUDDY_FILE" "CodeBuddy CLI"
            ;;
        amp)
            update_agent_file "$AMP_FILE" "Amp"
            ;;
        shai)
            update_agent_file "$SHAI_FILE" "SHAI"
            ;;
        q)
            update_agent_file "$Q_FILE" "Amazon Q Developer CLI"
            ;;
        *)
            log_error "Unknown agent type '$agent_type'"
            log_error "Expected: claude|gemini|copilot|cursor-agent|qwen|opencode|codex|windsurf|kilocode|auggie|roo|amp|shai|q"
            exit 1
            ;;
    esac
}

update_all_existing_agents() {
    local found_agent=false
    
    # Check each possible agent file and update if it exists
    if [[ -f "$CLAUDE_FILE" ]]; then
        update_agent_file "$CLAUDE_FILE" "Claude Code"
        found_agent=true
    fi
    
    if [[ -f "$GEMINI_FILE" ]]; then
        update_agent_file "$GEMINI_FILE" "Gemini CLI"
        found_agent=true
    fi
    
    if [[ -f "$COPILOT_FILE" ]]; then
        update_agent_file "$COPILOT_FILE" "GitHub Copilot"
        found_agent=true
    fi
    
    if [[ -f "$CURSOR_FILE" ]]; then
        update_agent_file "$CURSOR_FILE" "Cursor IDE"
        found_agent=true
    fi
    
    if [[ -f "$QWEN_FILE" ]]; then
        update_agent_file "$QWEN_FILE" "Qwen Code"
        found_agent=true
    fi
    
    if [[ -f "$AGENTS_FILE" ]]; then
        update_agent_file "$AGENTS_FILE" "Codex/opencode"
        found_agent=true
    fi
    
    if [[ -f "$WINDSURF_FILE" ]]; then
        update_agent_file "$WINDSURF_FILE" "Windsurf"
        found_agent=true
    fi
    
    if [[ -f "$KILOCODE_FILE" ]]; then
        update_agent_file "$KILOCODE_FILE" "Kilo Code"
        found_agent=true
    fi

    if [[ -f "$AUGGIE_FILE" ]]; then
        update_agent_file "$AUGGIE_FILE" "Auggie CLI"
        found_agent=true
    fi
    
    if [[ -f "$ROO_FILE" ]]; then
        update_agent_file "$ROO_FILE" "Roo Code"
        found_agent=true
    fi

    if [[ -f "$CODEBUDDY_FILE" ]]; then
        update_agent_file "$CODEBUDDY_FILE" "CodeBuddy CLI"
        found_agent=true
    fi

    if [[ -f "$SHAI_FILE" ]]; then
        update_agent_file "$SHAI_FILE" "SHAI"
        found_agent=true
    fi

    if [[ -f "$Q_FILE" ]]; then
        update_agent_file "$Q_FILE" "Amazon Q Developer CLI"
        found_agent=true
    fi
    
    # If no agent files exist, create a default Claude file
    if [[ "$found_agent" == false ]]; then
        log_info "No existing agent files found, creating default Claude file..."
        update_agent_file "$CLAUDE_FILE" "Claude Code"
    fi
}
print_summary() {
    echo
    log_info "Summary of changes:"
    
    if [[ -n "$NEW_LANG" ]]; then
        echo "  - Added language: $NEW_LANG"
    fi
    
    if [[ -n "$NEW_FRAMEWORK" ]]; then
        echo "  - Added framework: $NEW_FRAMEWORK"
    fi
    
    if [[ -n "$NEW_DB" ]] && [[ "$NEW_DB" != "N/A" ]]; then
        echo "  - Added database: $NEW_DB"
    fi
    
    echo

    log_info "Usage: $0 [claude|gemini|copilot|cursor-agent|qwen|opencode|codex|windsurf|kilocode|auggie|codebuddy|shai|q]"
}

#==============================================================================
# Main Execution
#==============================================================================

main() {
    # Validate environment before proceeding
    validate_environment
    
    log_info "=== Updating agent context files for feature $CURRENT_BRANCH ==="
    
    # Parse the plan file to extract project information
    if ! parse_plan_data "$NEW_PLAN"; then
        log_error "Failed to parse plan data"
        exit 1
    fi
    
    # Process based on agent type argument
    local success=true
    
    if [[ -z "$AGENT_TYPE" ]]; then
        # No specific agent provided - update all existing agent files
        log_info "No agent specified, updating all existing agent files..."
        if ! update_all_existing_agents; then
            success=false
        fi
    else
        # Specific agent provided - update only that agent
        log_info "Updating specific agent: $AGENT_TYPE"
        if ! update_specific_agent "$AGENT_TYPE"; then
            success=false
        fi
    fi
    
    # Print summary
    print_summary
    
    if [[ "$success" == true ]]; then
        log_success "Agent context update completed successfully"
        exit 0
    else
        log_error "Agent context update completed with errors"
        exit 1
    fi
}

# Execute main function if script is run directly
if [[ "${BASH_SOURCE[0]}" == "${0}" ]]; then
    main "$@"
fi
</file>

<file path=".specify/templates/agent-file-template.md">
# [PROJECT NAME] Development Guidelines

Auto-generated from all feature plans. Last updated: [DATE]

## Active Technologies

[EXTRACTED FROM ALL PLAN.MD FILES]

## Project Structure

```text
[ACTUAL STRUCTURE FROM PLANS]
```

## Commands

[ONLY COMMANDS FOR ACTIVE TECHNOLOGIES]

## Code Style

[LANGUAGE-SPECIFIC, ONLY FOR LANGUAGES IN USE]

## Recent Changes

[LAST 3 FEATURES AND WHAT THEY ADDED]

<!-- MANUAL ADDITIONS START -->
<!-- MANUAL ADDITIONS END -->
</file>

<file path=".specify/templates/checklist-template.md">
# [CHECKLIST TYPE] Checklist: [FEATURE NAME]

**Purpose**: [Brief description of what this checklist covers]
**Created**: [DATE]
**Feature**: [Link to spec.md or relevant documentation]

**Note**: This checklist is generated by the `/speckit.checklist` command based on feature context and requirements.

<!-- 
  ============================================================================
  IMPORTANT: The checklist items below are SAMPLE ITEMS for illustration only.
  
  The /speckit.checklist command MUST replace these with actual items based on:
  - User's specific checklist request
  - Feature requirements from spec.md
  - Technical context from plan.md
  - Implementation details from tasks.md
  
  DO NOT keep these sample items in the generated checklist file.
  ============================================================================
-->

## [Category 1]

- [ ] CHK001 First checklist item with clear action
- [ ] CHK002 Second checklist item
- [ ] CHK003 Third checklist item

## [Category 2]

- [ ] CHK004 Another category item
- [ ] CHK005 Item with specific criteria
- [ ] CHK006 Final item in this category

## Notes

- Check items off as completed: `[x]`
- Add comments or findings inline
- Link to relevant resources or documentation
- Items are numbered sequentially for easy reference
</file>

<file path=".specify/templates/plan-template.md">
# Implementation Plan: [FEATURE]

**Branch**: `[###-feature-name]` | **Date**: [DATE] | **Spec**: [link]
**Input**: Feature specification from `/specs/[###-feature-name]/spec.md`

**Note**: This template is filled in by the `/speckit.plan` command. See `.specify/templates/commands/plan.md` for the execution workflow.

## Summary

[Extract from feature spec: primary requirement + technical approach from research]

## Technical Context

<!--
  ACTION REQUIRED: Replace the content in this section with the technical details
  for the project. The structure here is presented in advisory capacity to guide
  the iteration process.
-->

**Language/Version**: [e.g., Python 3.11, Swift 5.9, Rust 1.75 or NEEDS CLARIFICATION]  
**Primary Dependencies**: [e.g., FastAPI, UIKit, LLVM or NEEDS CLARIFICATION]  
**Storage**: [if applicable, e.g., PostgreSQL, CoreData, files or N/A]  
**Testing**: [e.g., pytest, XCTest, cargo test or NEEDS CLARIFICATION]  
**Target Platform**: [e.g., Linux server, iOS 15+, WASM or NEEDS CLARIFICATION]
**Project Type**: [single/web/mobile - determines source structure]  
**Performance Goals**: [domain-specific, e.g., 1000 req/s, 10k lines/sec, 60 fps or NEEDS CLARIFICATION]  
**Constraints**: [domain-specific, e.g., <200ms p95, <100MB memory, offline-capable or NEEDS CLARIFICATION]  
**Scale/Scope**: [domain-specific, e.g., 10k users, 1M LOC, 50 screens or NEEDS CLARIFICATION]

## Constitution Check

*GATE: Must pass before Phase 0 research. Re-check after Phase 1 design.*

[Gates determined based on constitution file]

## Project Structure

### Documentation (this feature)

```text
specs/[###-feature]/
‚îú‚îÄ‚îÄ plan.md              # This file (/speckit.plan command output)
‚îú‚îÄ‚îÄ research.md          # Phase 0 output (/speckit.plan command)
‚îú‚îÄ‚îÄ data-model.md        # Phase 1 output (/speckit.plan command)
‚îú‚îÄ‚îÄ quickstart.md        # Phase 1 output (/speckit.plan command)
‚îú‚îÄ‚îÄ contracts/           # Phase 1 output (/speckit.plan command)
‚îî‚îÄ‚îÄ tasks.md             # Phase 2 output (/speckit.tasks command - NOT created by /speckit.plan)
```

### Source Code (repository root)
<!--
  ACTION REQUIRED: Replace the placeholder tree below with the concrete layout
  for this feature. Delete unused options and expand the chosen structure with
  real paths (e.g., apps/admin, packages/something). The delivered plan must
  not include Option labels.
-->

```text
# [REMOVE IF UNUSED] Option 1: Single project (DEFAULT)
src/
‚îú‚îÄ‚îÄ models/
‚îú‚îÄ‚îÄ services/
‚îú‚îÄ‚îÄ cli/
‚îî‚îÄ‚îÄ lib/

tests/
‚îú‚îÄ‚îÄ contract/
‚îú‚îÄ‚îÄ integration/
‚îî‚îÄ‚îÄ unit/

# [REMOVE IF UNUSED] Option 2: Web application (when "frontend" + "backend" detected)
backend/
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ models/
‚îÇ   ‚îú‚îÄ‚îÄ services/
‚îÇ   ‚îî‚îÄ‚îÄ api/
‚îî‚îÄ‚îÄ tests/

frontend/
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ components/
‚îÇ   ‚îú‚îÄ‚îÄ pages/
‚îÇ   ‚îî‚îÄ‚îÄ services/
‚îî‚îÄ‚îÄ tests/

# [REMOVE IF UNUSED] Option 3: Mobile + API (when "iOS/Android" detected)
api/
‚îî‚îÄ‚îÄ [same as backend above]

ios/ or android/
‚îî‚îÄ‚îÄ [platform-specific structure: feature modules, UI flows, platform tests]
```

**Structure Decision**: [Document the selected structure and reference the real
directories captured above]

## Complexity Tracking

> **Fill ONLY if Constitution Check has violations that must be justified**

| Violation | Why Needed | Simpler Alternative Rejected Because |
|-----------|------------|-------------------------------------|
| [e.g., 4th project] | [current need] | [why 3 projects insufficient] |
| [e.g., Repository pattern] | [specific problem] | [why direct DB access insufficient] |
</file>

<file path=".specify/templates/spec-template.md">
# Feature Specification: [FEATURE NAME]

**Feature Branch**: `[###-feature-name]`  
**Created**: [DATE]  
**Status**: Draft  
**Input**: User description: "$ARGUMENTS"

## User Scenarios & Testing *(mandatory)*

<!--
  IMPORTANT: User stories should be PRIORITIZED as user journeys ordered by importance.
  Each user story/journey must be INDEPENDENTLY TESTABLE - meaning if you implement just ONE of them,
  you should still have a viable MVP (Minimum Viable Product) that delivers value.
  
  Assign priorities (P1, P2, P3, etc.) to each story, where P1 is the most critical.
  Think of each story as a standalone slice of functionality that can be:
  - Developed independently
  - Tested independently
  - Deployed independently
  - Demonstrated to users independently
-->

### User Story 1 - [Brief Title] (Priority: P1)

[Describe this user journey in plain language]

**Why this priority**: [Explain the value and why it has this priority level]

**Independent Test**: [Describe how this can be tested independently - e.g., "Can be fully tested by [specific action] and delivers [specific value]"]

**Acceptance Scenarios**:

1. **Given** [initial state], **When** [action], **Then** [expected outcome]
2. **Given** [initial state], **When** [action], **Then** [expected outcome]

---

### User Story 2 - [Brief Title] (Priority: P2)

[Describe this user journey in plain language]

**Why this priority**: [Explain the value and why it has this priority level]

**Independent Test**: [Describe how this can be tested independently]

**Acceptance Scenarios**:

1. **Given** [initial state], **When** [action], **Then** [expected outcome]

---

### User Story 3 - [Brief Title] (Priority: P3)

[Describe this user journey in plain language]

**Why this priority**: [Explain the value and why it has this priority level]

**Independent Test**: [Describe how this can be tested independently]

**Acceptance Scenarios**:

1. **Given** [initial state], **When** [action], **Then** [expected outcome]

---

[Add more user stories as needed, each with an assigned priority]

### Edge Cases

<!--
  ACTION REQUIRED: The content in this section represents placeholders.
  Fill them out with the right edge cases.
-->

- What happens when [boundary condition]?
- How does system handle [error scenario]?

## Requirements *(mandatory)*

<!--
  ACTION REQUIRED: The content in this section represents placeholders.
  Fill them out with the right functional requirements.
-->

### Functional Requirements

- **FR-001**: System MUST [specific capability, e.g., "allow users to create accounts"]
- **FR-002**: System MUST [specific capability, e.g., "validate email addresses"]  
- **FR-003**: Users MUST be able to [key interaction, e.g., "reset their password"]
- **FR-004**: System MUST [data requirement, e.g., "persist user preferences"]
- **FR-005**: System MUST [behavior, e.g., "log all security events"]

*Example of marking unclear requirements:*

- **FR-006**: System MUST authenticate users via [NEEDS CLARIFICATION: auth method not specified - email/password, SSO, OAuth?]
- **FR-007**: System MUST retain user data for [NEEDS CLARIFICATION: retention period not specified]

### Key Entities *(include if feature involves data)*

- **[Entity 1]**: [What it represents, key attributes without implementation]
- **[Entity 2]**: [What it represents, relationships to other entities]

## Success Criteria *(mandatory)*

<!--
  ACTION REQUIRED: Define measurable success criteria.
  These must be technology-agnostic and measurable.
-->

### Measurable Outcomes

- **SC-001**: [Measurable metric, e.g., "Users can complete account creation in under 2 minutes"]
- **SC-002**: [Measurable metric, e.g., "System handles 1000 concurrent users without degradation"]
- **SC-003**: [User satisfaction metric, e.g., "90% of users successfully complete primary task on first attempt"]
- **SC-004**: [Business metric, e.g., "Reduce support tickets related to [X] by 50%"]
</file>

<file path=".specify/templates/tasks-template.md">
---

description: "Task list template for feature implementation"
---

# Tasks: [FEATURE NAME]

**Input**: Design documents from `/specs/[###-feature-name]/`
**Prerequisites**: plan.md (required), spec.md (required for user stories), research.md, data-model.md, contracts/

**Tests**: The examples below include test tasks. Tests are OPTIONAL - only include them if explicitly requested in the feature specification.

**Organization**: Tasks are grouped by user story to enable independent implementation and testing of each story.

## Format: `[ID] [P?] [Story] Description`

- **[P]**: Can run in parallel (different files, no dependencies)
- **[Story]**: Which user story this task belongs to (e.g., US1, US2, US3)
- Include exact file paths in descriptions

## Path Conventions

- **Single project**: `src/`, `tests/` at repository root
- **Web app**: `backend/src/`, `frontend/src/`
- **Mobile**: `api/src/`, `ios/src/` or `android/src/`
- Paths shown below assume single project - adjust based on plan.md structure

<!-- 
  ============================================================================
  IMPORTANT: The tasks below are SAMPLE TASKS for illustration purposes only.
  
  The /speckit.tasks command MUST replace these with actual tasks based on:
  - User stories from spec.md (with their priorities P1, P2, P3...)
  - Feature requirements from plan.md
  - Entities from data-model.md
  - Endpoints from contracts/
  
  Tasks MUST be organized by user story so each story can be:
  - Implemented independently
  - Tested independently
  - Delivered as an MVP increment
  
  DO NOT keep these sample tasks in the generated tasks.md file.
  ============================================================================
-->

## Phase 1: Setup (Shared Infrastructure)

**Purpose**: Project initialization and basic structure

- [ ] T001 Create project structure per implementation plan
- [ ] T002 Initialize [language] project with [framework] dependencies
- [ ] T003 [P] Configure linting and formatting tools

---

## Phase 2: Foundational (Blocking Prerequisites)

**Purpose**: Core infrastructure that MUST be complete before ANY user story can be implemented

**‚ö†Ô∏è CRITICAL**: No user story work can begin until this phase is complete

Examples of foundational tasks (adjust based on your project):

- [ ] T004 Setup database schema and migrations framework
- [ ] T005 [P] Implement authentication/authorization framework
- [ ] T006 [P] Setup API routing and middleware structure
- [ ] T007 Create base models/entities that all stories depend on
- [ ] T008 Configure error handling and logging infrastructure
- [ ] T009 Setup environment configuration management

**Checkpoint**: Foundation ready - user story implementation can now begin in parallel

---

## Phase 3: User Story 1 - [Title] (Priority: P1) üéØ MVP

**Goal**: [Brief description of what this story delivers]

**Independent Test**: [How to verify this story works on its own]

### Tests for User Story 1 (OPTIONAL - only if tests requested) ‚ö†Ô∏è

> **NOTE: Write these tests FIRST, ensure they FAIL before implementation**

- [ ] T010 [P] [US1] Contract test for [endpoint] in tests/contract/test_[name].py
- [ ] T011 [P] [US1] Integration test for [user journey] in tests/integration/test_[name].py

### Implementation for User Story 1

- [ ] T012 [P] [US1] Create [Entity1] model in src/models/[entity1].py
- [ ] T013 [P] [US1] Create [Entity2] model in src/models/[entity2].py
- [ ] T014 [US1] Implement [Service] in src/services/[service].py (depends on T012, T013)
- [ ] T015 [US1] Implement [endpoint/feature] in src/[location]/[file].py
- [ ] T016 [US1] Add validation and error handling
- [ ] T017 [US1] Add logging for user story 1 operations

**Checkpoint**: At this point, User Story 1 should be fully functional and testable independently

---

## Phase 4: User Story 2 - [Title] (Priority: P2)

**Goal**: [Brief description of what this story delivers]

**Independent Test**: [How to verify this story works on its own]

### Tests for User Story 2 (OPTIONAL - only if tests requested) ‚ö†Ô∏è

- [ ] T018 [P] [US2] Contract test for [endpoint] in tests/contract/test_[name].py
- [ ] T019 [P] [US2] Integration test for [user journey] in tests/integration/test_[name].py

### Implementation for User Story 2

- [ ] T020 [P] [US2] Create [Entity] model in src/models/[entity].py
- [ ] T021 [US2] Implement [Service] in src/services/[service].py
- [ ] T022 [US2] Implement [endpoint/feature] in src/[location]/[file].py
- [ ] T023 [US2] Integrate with User Story 1 components (if needed)

**Checkpoint**: At this point, User Stories 1 AND 2 should both work independently

---

## Phase 5: User Story 3 - [Title] (Priority: P3)

**Goal**: [Brief description of what this story delivers]

**Independent Test**: [How to verify this story works on its own]

### Tests for User Story 3 (OPTIONAL - only if tests requested) ‚ö†Ô∏è

- [ ] T024 [P] [US3] Contract test for [endpoint] in tests/contract/test_[name].py
- [ ] T025 [P] [US3] Integration test for [user journey] in tests/integration/test_[name].py

### Implementation for User Story 3

- [ ] T026 [P] [US3] Create [Entity] model in src/models/[entity].py
- [ ] T027 [US3] Implement [Service] in src/services/[service].py
- [ ] T028 [US3] Implement [endpoint/feature] in src/[location]/[file].py

**Checkpoint**: All user stories should now be independently functional

---

[Add more user story phases as needed, following the same pattern]

---

## Phase N: Polish & Cross-Cutting Concerns

**Purpose**: Improvements that affect multiple user stories

- [ ] TXXX [P] Documentation updates in docs/
- [ ] TXXX Code cleanup and refactoring
- [ ] TXXX Performance optimization across all stories
- [ ] TXXX [P] Additional unit tests (if requested) in tests/unit/
- [ ] TXXX Security hardening
- [ ] TXXX Run quickstart.md validation

---

## Dependencies & Execution Order

### Phase Dependencies

- **Setup (Phase 1)**: No dependencies - can start immediately
- **Foundational (Phase 2)**: Depends on Setup completion - BLOCKS all user stories
- **User Stories (Phase 3+)**: All depend on Foundational phase completion
  - User stories can then proceed in parallel (if staffed)
  - Or sequentially in priority order (P1 ‚Üí P2 ‚Üí P3)
- **Polish (Final Phase)**: Depends on all desired user stories being complete

### User Story Dependencies

- **User Story 1 (P1)**: Can start after Foundational (Phase 2) - No dependencies on other stories
- **User Story 2 (P2)**: Can start after Foundational (Phase 2) - May integrate with US1 but should be independently testable
- **User Story 3 (P3)**: Can start after Foundational (Phase 2) - May integrate with US1/US2 but should be independently testable

### Within Each User Story

- Tests (if included) MUST be written and FAIL before implementation
- Models before services
- Services before endpoints
- Core implementation before integration
- Story complete before moving to next priority

### Parallel Opportunities

- All Setup tasks marked [P] can run in parallel
- All Foundational tasks marked [P] can run in parallel (within Phase 2)
- Once Foundational phase completes, all user stories can start in parallel (if team capacity allows)
- All tests for a user story marked [P] can run in parallel
- Models within a story marked [P] can run in parallel
- Different user stories can be worked on in parallel by different team members

---

## Parallel Example: User Story 1

```bash
# Launch all tests for User Story 1 together (if tests requested):
Task: "Contract test for [endpoint] in tests/contract/test_[name].py"
Task: "Integration test for [user journey] in tests/integration/test_[name].py"

# Launch all models for User Story 1 together:
Task: "Create [Entity1] model in src/models/[entity1].py"
Task: "Create [Entity2] model in src/models/[entity2].py"
```

---

## Implementation Strategy

### MVP First (User Story 1 Only)

1. Complete Phase 1: Setup
2. Complete Phase 2: Foundational (CRITICAL - blocks all stories)
3. Complete Phase 3: User Story 1
4. **STOP and VALIDATE**: Test User Story 1 independently
5. Deploy/demo if ready

### Incremental Delivery

1. Complete Setup + Foundational ‚Üí Foundation ready
2. Add User Story 1 ‚Üí Test independently ‚Üí Deploy/Demo (MVP!)
3. Add User Story 2 ‚Üí Test independently ‚Üí Deploy/Demo
4. Add User Story 3 ‚Üí Test independently ‚Üí Deploy/Demo
5. Each story adds value without breaking previous stories

### Parallel Team Strategy

With multiple developers:

1. Team completes Setup + Foundational together
2. Once Foundational is done:
   - Developer A: User Story 1
   - Developer B: User Story 2
   - Developer C: User Story 3
3. Stories complete and integrate independently

---

## Notes

- [P] tasks = different files, no dependencies
- [Story] label maps task to specific user story for traceability
- Each user story should be independently completable and testable
- Verify tests fail before implementing
- Commit after each task or logical group
- Stop at any checkpoint to validate story independently
- Avoid: vague tasks, same file conflicts, cross-story dependencies that break independence
</file>

<file path=".specify/memory/constitution.md">
# [PROJECT_NAME] Constitution
<!-- Example: Spec Constitution, TaskFlow Constitution, etc. -->

## Core Principles

### [PRINCIPLE_1_NAME]
<!-- Example: I. Library-First -->
[PRINCIPLE_1_DESCRIPTION]
<!-- Example: Every feature starts as a standalone library; Libraries must be self-contained, independently testable, documented; Clear purpose required - no organizational-only libraries -->

### [PRINCIPLE_2_NAME]
<!-- Example: II. CLI Interface -->
[PRINCIPLE_2_DESCRIPTION]
<!-- Example: Every library exposes functionality via CLI; Text in/out protocol: stdin/args ‚Üí stdout, errors ‚Üí stderr; Support JSON + human-readable formats -->

### [PRINCIPLE_3_NAME]
<!-- Example: III. Test-First (NON-NEGOTIABLE) -->
[PRINCIPLE_3_DESCRIPTION]
<!-- Example: TDD mandatory: Tests written ‚Üí User approved ‚Üí Tests fail ‚Üí Then implement; Red-Green-Refactor cycle strictly enforced -->

### [PRINCIPLE_4_NAME]
<!-- Example: IV. Integration Testing -->
[PRINCIPLE_4_DESCRIPTION]
<!-- Example: Focus areas requiring integration tests: New library contract tests, Contract changes, Inter-service communication, Shared schemas -->

### [PRINCIPLE_5_NAME]
<!-- Example: V. Observability, VI. Versioning & Breaking Changes, VII. Simplicity -->
[PRINCIPLE_5_DESCRIPTION]
<!-- Example: Text I/O ensures debuggability; Structured logging required; Or: MAJOR.MINOR.BUILD format; Or: Start simple, YAGNI principles -->

## [SECTION_2_NAME]
<!-- Example: Additional Constraints, Security Requirements, Performance Standards, etc. -->

[SECTION_2_CONTENT]
<!-- Example: Technology stack requirements, compliance standards, deployment policies, etc. -->

## [SECTION_3_NAME]
<!-- Example: Development Workflow, Review Process, Quality Gates, etc. -->

[SECTION_3_CONTENT]
<!-- Example: Code review requirements, testing gates, deployment approval process, etc. -->

## Governance
<!-- Example: Constitution supersedes all other practices; Amendments require documentation, approval, migration plan -->

[GOVERNANCE_RULES]
<!-- Example: All PRs/reviews must verify compliance; Complexity must be justified; Use [GUIDANCE_FILE] for runtime development guidance -->

**Version**: [CONSTITUTION_VERSION] | **Ratified**: [RATIFICATION_DATE] | **Last Amended**: [LAST_AMENDED_DATE]
<!-- Example: Version: 2.1.1 | Ratified: 2025-06-13 | Last Amended: 2025-07-16 -->
</file>

</files>
````

## File: repomix.config.json
````json
{
  "$schema": "https://repomix.com/schemas/latest/schema.json",
  "input": {
    "maxFileSize": 52428800
  },
  "output": {
    "filePath": "repomix-output.md",
    "style": "markdown",
    "parsableStyle": false,
    "fileSummary": true,
    "directoryStructure": true,
    "files": true,
    "removeComments": false,
    "removeEmptyLines": false,
    "compress": false,
    "topFilesLength": 5,
    "showLineNumbers": false,
    "truncateBase64": false,
    "copyToClipboard": false,
    "includeFullDirectoryStructure": false,
    "tokenCountTree": false,
    "git": {
      "sortByChanges": true,
      "sortByChangesMaxCommits": 100,
      "includeDiffs": false,
      "includeLogs": false,
      "includeLogsCount": 50
    }
  },
  "include": [],
  "ignore": {
    "useGitignore": true,
    "useDotIgnore": true,
    "useDefaultPatterns": true,
    "customPatterns": []
  },
  "security": {
    "enableSecurityCheck": true
  },
  "tokenCount": {
    "encoding": "o200k_base"
  }
}
````

## File: repos.txt
````
http://github.com/ambient-code/agentready
````

## File: .github/workflows/claude-code-action.yml
````yaml
name: Claude Code Action

on:
  issue_comment:
    types: [created]
  pull_request_review_comment:
    types: [created]
  issues:
    types: [opened, assigned]
  pull_request:
    types: [opened, ready_for_review]

permissions:
  contents: write
  pull-requests: write
  issues: write

jobs:
  claude-code:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Claude Code Action
        uses: anthropics/claude-code-action@v1
        with:
          anthropic_api_key: ${{ secrets.ANTHROPIC_API_KEY }}
          github_token: ${{ secrets.GITHUB_TOKEN }}
````

## File: .specify/memory/constitution.md
````markdown
# AgentReady Constitution

<!--
SYNC IMPACT REPORT:
Version: 0.0.0 ‚Üí 1.0.0 (Initial constitution creation)
Rationale: MINOR version - New constitution created from template

CHANGES:
- Initial constitution ratification
- 7 core principles established: Evidence-Based Design, Measurable Quality, Tool-First Mindset,
  Test-Driven Development, Structured Output, Incremental Delivery, Documentation as Code
- Development workflow and quality gates defined
- Governance procedures established

TEMPLATE CONSISTENCY STATUS:
- ‚úÖ plan-template.md - Constitution check placeholder aligns with new principles
- ‚úÖ spec-template.md - Requirements structure supports measurable criteria
- ‚úÖ tasks-template.md - Task categorization supports TDD and incremental delivery
- ‚úÖ All command files - Generic guidance compatible with principles

DEFERRED ITEMS: None
-->

## Core Principles

### I. Evidence-Based Design

Every attribute, metric, and recommendation MUST be grounded in authoritative research. No "best
practices" without citations. Claims require supporting evidence from academic papers (ArXiv,
IEEE/ACM), industry leaders (Google, Microsoft, GitHub, Anthropic), or empirical data.

**Rationale**: Building a tool that evaluates code quality requires our own methodology to be
beyond reproach. Evidence-based design ensures credibility and prevents cargo-culting.

### II. Measurable Quality

Every quality attribute MUST have:
- Clear, objective criteria (not subjective opinions)
- Automated tooling for measurement where possible
- Quantifiable thresholds (e.g., ">80% coverage", "<10 cyclomatic complexity")
- Good vs. bad examples demonstrating the attribute

**Rationale**: You cannot improve what you cannot measure. Subjective quality assessments lead to
inconsistent results and lost user trust.

### III. Tool-First Mindset

Features MUST be implemented as libraries first, with CLI interfaces second. Each library must be:
- Self-contained and independently testable
- Documented with clear purpose and API
- Usable without the broader agentready toolchain
- Text-based I/O (stdin/args ‚Üí stdout, errors ‚Üí stderr)

**Rationale**: Libraries enable composition and reuse. CLI tools provide accessibility. Together,
they create a toolkit that serves both developers and automation pipelines.

### IV. Test-Driven Development (NON-NEGOTIABLE)

TDD is MANDATORY for all implementation work:
1. Write tests FIRST (red phase)
2. Get user approval of test scenarios
3. Verify tests FAIL as expected
4. Implement to make tests pass (green phase)
5. Refactor for quality (refactor phase)

No code may be written before tests exist and fail.

**Rationale**: Testing first ensures we build what users need, not what we assume they need. It
prevents scope creep and provides regression safety for AI-assisted development.

### V. Structured Output

All output MUST support both machine-readable and human-readable formats:
- JSON for automation and parsing
- Markdown/text for human consumption
- Structured logging with consistent field names
- Error messages include context, guidance, and request IDs

**Rationale**: AI agents and automation tools need structured data. Humans need readable formats.
Supporting both maximizes utility and debuggability.

### VI. Incremental Delivery

Features MUST be broken into independently deliverable user stories:
- Each story delivers measurable value on its own
- Stories prioritized P1 (MVP), P2, P3+ (enhancements)
- P1 story alone should constitute viable minimum product
- Stories can be developed, tested, deployed independently

**Rationale**: Incremental delivery enables faster feedback, reduces risk, and allows pivoting
based on user needs. MVP-first prevents over-engineering.

### VII. Documentation as Code

Documentation lives alongside code and is versioned, reviewed, and tested:
- README with quick start (<5 minute setup)
- CLAUDE.md with project context and conventions
- ADRs for architectural decisions
- Inline docstrings with "why" not "what"
- Examples and quickstarts in all guides

**Rationale**: Stale docs are worse than no docs. Documentation-as-code ensures accuracy through
automated validation and review processes.

## Development Workflow

### Quality Gates

All changes MUST pass:
1. **Constitution Check**: Verify compliance with core principles
2. **Linting**: black, isort, flake8, mypy (Python), markdownlint (docs)
3. **Tests**: All tests pass, coverage >80% for new code
4. **Security**: No critical/high vulnerabilities (bandit, safety, Dependabot)
5. **Documentation**: Updated for behavior changes, examples tested

### Implementation Phases

1. **Phase 0 - Research**: Gather evidence, citations, and requirements
2. **Phase 1 - Design**: Data models, contracts, quickstart, architecture
3. **Phase 2 - Tasks**: Generate dependency-ordered task list from designs
4. **Phase 3 - Implementation**: Execute tasks following TDD red-green-refactor
5. **Phase N - Polish**: Cross-cutting concerns, optimization, hardening

### Code Review Requirements

- All PRs require review approval
- Reviewers verify Constitution Check compliance
- Breaking changes require ADR documentation
- New features require tests and documentation
- Performance regressions blocked unless justified

## Constraints

### Technology Standards

- **Language**: Python 3.11+ (only N and N-1 versions supported)
- **Dependency Management**: uv preferred over pip
- **Virtual Environments**: MANDATORY - never modify system Python
- **Version Control**: Git with conventional commits (type(scope): description)
- **CI/CD**: GitHub Actions with required status checks
- **Container Runtime**: Podman preferred over Docker

### Complexity Limits

- **File Size**: <300 lines per file (exceptions: generated code, data)
- **Function Length**: <50 lines (warning), <100 lines (hard limit)
- **Cyclomatic Complexity**: <10 (target), <25 (hard limit)
- **Projects**: Prefer single project; justify if multiple needed
- **Dependencies**: Minimal necessary; audit for vulnerabilities quarterly

### Security & Compliance

- No hardcoded secrets (use .env files, secret managers)
- Pre-commit hooks include secret scanning (detect-secrets)
- Dependabot enabled for automated dependency updates
- Security scan in CI/CD fails on high/critical issues
- Vulnerability SLA: High severity fixed within 7 days

## Governance

This Constitution supersedes all other development practices. Amendments require:
- Documented rationale (why change is needed)
- Version increment (semantic versioning: MAJOR.MINOR.PATCH)
- Migration plan for projects affected by changes
- Approval from project maintainers
- Update to all dependent templates and documentation

All code reviews, PRs, and design decisions MUST verify Constitution compliance. Complexity
that violates principles MUST be explicitly justified with trade-off analysis.

Use `.specify/templates/agent-file-template.md` for agent-specific runtime guidance. This
Constitution applies to all agents and developers working on agentready.

**Version**: 1.0.0 | **Ratified**: 2025-11-20 | **Last Amended**: 2025-11-20
````

## File: docs/api-reference.md
````markdown
---
layout: page
title: API Reference
---

# API Reference

Complete reference for AgentReady's Python API. Use these APIs to integrate AgentReady into your own tools, CI/CD pipelines, or custom workflows.

## Table of Contents

- [Installation](#installation)
- [Core Models](#core-models)
- [Services](#services)
- [Assessors](#assessors)
- [Reporters](#reporters)
- [Usage Examples](#usage-examples)

---

## Installation

```bash
pip install agentready
```

Import the library:

```python
from agentready.models import Repository, Assessment, Finding, Attribute
from agentready.services import Scanner, Scorer, LanguageDetector
from agentready.reporters import HTMLReporter, MarkdownReporter, JSONReporter
```

---

## Core Models

### Repository

Represents a git repository being assessed.

```python
from agentready.models import Repository

class Repository:
    """Immutable representation of a repository."""

    path: str                    # Absolute path to repository
    name: str                    # Repository name
    languages: Dict[str, int]    # Language ‚Üí file count mapping
```

#### Constructor

```python
Repository(
    path: str,
    name: str,
    languages: Dict[str, int]
)
```

#### Example

```python
repo = Repository(
    path="/home/user/myproject",
    name="myproject",
    languages={"Python": 42, "JavaScript": 18}
)
```

---

### Attribute

Defines a single agent-ready attribute to assess.

```python
from agentready.models import Attribute

class Attribute:
    """Immutable attribute definition."""

    id: str              # Unique identifier (e.g., "claude_md_file")
    name: str            # Display name
    tier: int            # Tier 1-4 (1 = most important)
    weight: float        # Weight in scoring (0.0-1.0, sum to 1.0 across all)
    category: str        # Category (e.g., "Documentation", "Testing")
    description: str     # What this attribute measures
    rationale: str       # Why it matters for AI agents
```

#### Constructor

```python
Attribute(
    id: str,
    name: str,
    tier: int,
    weight: float,
    category: str = "",
    description: str = "",
    rationale: str = ""
)
```

#### Example

```python
attribute = Attribute(
    id="claude_md_file",
    name="CLAUDE.md File",
    tier=1,
    weight=0.10,
    category="Context Window Optimization",
    description="CLAUDE.md file at repository root",
    rationale="Provides immediate project context to AI agents"
)
```

---

### Finding

Result of assessing a single attribute.

```python
from agentready.models import Finding, Remediation

class Finding:
    """Assessment finding for a single attribute."""

    attribute: Attribute      # Attribute being assessed
    status: str              # "pass", "fail", or "skipped"
    score: float             # 0.0-100.0
    evidence: str            # What was found (specific details)
    remediation: Optional[Remediation]  # How to fix (if failed)
    reason: Optional[str]    # Why skipped (if status="skipped")
```

#### Factory Methods

```python
# Create passing finding
Finding.create_pass(
    attribute: Attribute,
    evidence: str,
    remediation: Optional[Remediation] = None
) -> Finding

# Create failing finding
Finding.create_fail(
    attribute: Attribute,
    evidence: str,
    remediation: Remediation
) -> Finding

# Create skipped finding
Finding.create_skip(
    attribute: Attribute,
    reason: str
) -> Finding
```

#### Example

```python
# Pass
finding = Finding.create_pass(
    attribute=claude_md_attr,
    evidence="Found CLAUDE.md at repository root (245 lines)"
)

# Fail
finding = Finding.create_fail(
    attribute=readme_attr,
    evidence="README.md missing quick start section",
    remediation=Remediation(
        steps=["Add Quick Start section to README.md"],
        tools=["text editor"],
        examples=["# Quick Start\n\nInstall: `pip install myproject`"]
    )
)

# Skip
finding = Finding.create_skip(
    attribute=container_attr,
    reason="Assessor not yet implemented"
)
```

---

### Remediation

Actionable guidance for fixing a failed attribute.

```python
from agentready.models import Remediation

class Remediation:
    """Remediation guidance for failed findings."""

    steps: List[str]           # Ordered steps to fix
    tools: List[str]           # Required tools
    commands: List[str]        # Shell commands (optional)
    examples: List[str]        # Code/config examples (optional)
    citations: List[str]       # Reference documentation (optional)
```

#### Example

```python
remediation = Remediation(
    steps=[
        "Install pre-commit framework",
        "Create .pre-commit-config.yaml",
        "Add black and isort hooks",
        "Install git hooks: pre-commit install"
    ],
    tools=["pre-commit", "black", "isort"],
    commands=[
        "pip install pre-commit",
        "pre-commit install",
        "pre-commit run --all-files"
    ],
    examples=[
        '''repos:
  - repo: https://github.com/psf/black
    rev: 23.12.0
    hooks:
      - id: black'''
    ],
    citations=[
        "https://pre-commit.com/",
        "Memfault: Automatically format and lint code with pre-commit"
    ]
)
```

---

### Assessment

Complete assessment result for a repository.

```python
from agentready.models import Assessment

class Assessment:
    """Complete assessment result."""

    repository: Repository           # Repository assessed
    overall_score: float            # 0.0-100.0
    certification_level: str        # "Platinum", "Gold", "Silver", "Bronze", "Needs Improvement"
    findings: List[Finding]         # Individual attribute findings
    tier_scores: Dict[str, float]   # Tier 1-4 scores
    metadata: Dict[str, Any]        # Timestamp, version, duration, etc.
```

#### Properties

```python
assessment.passing_count -> int      # Number of passing attributes
assessment.failing_count -> int      # Number of failing attributes
assessment.skipped_count -> int      # Number of skipped attributes
```

---

## Services

### Scanner

Orchestrates repository assessment by running all assessors.

```python
from agentready.services import Scanner

class Scanner:
    """Assessment orchestration."""

    def __init__(self):
        """Initialize scanner with all assessors."""

    def scan(self, repository: Repository) -> Assessment:
        """
        Run all assessors and generate assessment.

        Args:
            repository: Repository to assess

        Returns:
            Complete Assessment object
        """
```

#### Example

```python
from agentready.services import Scanner, LanguageDetector
from agentready.models import Repository

# Detect languages
detector = LanguageDetector()
languages = detector.detect("/path/to/repo")

# Create repository object
repo = Repository(
    path="/path/to/repo",
    name="myproject",
    languages=languages
)

# Run assessment
scanner = Scanner()
assessment = scanner.scan(repo)

print(f"Score: {assessment.overall_score}/100")
print(f"Certification: {assessment.certification_level}")
print(f"Passing: {assessment.passing_count}/25")
```

---

### Scorer

Calculates weighted scores and certification levels.

```python
from agentready.services import Scorer

class Scorer:
    """Score calculation and certification determination."""

    @staticmethod
    def calculate_overall_score(findings: List[Finding]) -> float:
        """
        Calculate weighted average score.

        Args:
            findings: List of assessment findings

        Returns:
            Overall score (0.0-100.0)
        """

    @staticmethod
    def determine_certification(score: float) -> str:
        """
        Determine certification level from score.

        Args:
            score: Overall score (0.0-100.0)

        Returns:
            Certification level string
        """

    @staticmethod
    def calculate_tier_scores(findings: List[Finding]) -> Dict[str, float]:
        """
        Calculate scores by tier.

        Args:
            findings: List of assessment findings

        Returns:
            Dict mapping tier (e.g., "tier_1") to score
        """
```

#### Example

```python
from agentready.services import Scorer

# Calculate overall score
score = Scorer.calculate_overall_score(findings)
print(f"Score: {score}/100")

# Determine certification
cert = Scorer.determine_certification(score)
print(f"Certification: {cert}")

# Calculate tier scores
tier_scores = Scorer.calculate_tier_scores(findings)
for tier, score in tier_scores.items():
    print(f"{tier}: {score:.1f}/100")
```

---

### LanguageDetector

Detects programming languages in repository via `git ls-files`.

```python
from agentready.services import LanguageDetector

class LanguageDetector:
    """Detect repository languages via git."""

    def detect(self, repo_path: str) -> Dict[str, int]:
        """
        Detect languages by file extensions.

        Args:
            repo_path: Path to git repository

        Returns:
            Dict mapping language name to file count

        Raises:
            ValueError: If not a git repository
        """
```

#### Example

```python
from agentready.services import LanguageDetector

detector = LanguageDetector()
languages = detector.detect("/path/to/repo")

for lang, count in languages.items():
    print(f"{lang}: {count} files")

# Output:
# Python: 42 files
# JavaScript: 18 files
# TypeScript: 12 files
```

---

## Assessors

### BaseAssessor

Abstract base class for all assessors. Implement this to create custom assessors.

```python
from abc import ABC, abstractmethod
from agentready.assessors.base import BaseAssessor
from agentready.models import Repository, Finding

class BaseAssessor(ABC):
    """Abstract base class for assessors."""

    @property
    @abstractmethod
    def attribute_id(self) -> str:
        """Unique attribute identifier."""

    @abstractmethod
    def assess(self, repository: Repository) -> Finding:
        """
        Assess repository for this attribute.

        Args:
            repository: Repository to assess

        Returns:
            Finding with pass/fail/skip status
        """

    def is_applicable(self, repository: Repository) -> bool:
        """
        Check if this assessor applies to repository.

        Override to skip assessment for irrelevant repositories
        (e.g., JavaScript-only repo for Python-specific assessor).

        Args:
            repository: Repository being assessed

        Returns:
            True if assessor should run, False to skip
        """
        return True

    def calculate_proportional_score(
        self,
        actual: float,
        target: float
    ) -> float:
        """
        Calculate proportional score for partial compliance.

        Args:
            actual: Actual value (e.g., 0.65 for 65% coverage)
            target: Target value (e.g., 0.80 for 80% target)

        Returns:
            Score (0-100)

        Example:
            >>> calculate_proportional_score(0.70, 0.80)
            87.5  # 70/80 = 87.5%
        """
```

#### Example: Custom Assessor

```python
from agentready.assessors.base import BaseAssessor
from agentready.models import Repository, Finding, Remediation

class MyCustomAssessor(BaseAssessor):
    """Assess my custom attribute."""

    @property
    def attribute_id(self) -> str:
        return "my_custom_attribute"

    def assess(self, repository: Repository) -> Finding:
        # Check if attribute is satisfied
        if self._check_condition(repository):
            return Finding.create_pass(
                self.attribute,
                evidence="Condition met",
                remediation=None
            )
        else:
            return Finding.create_fail(
                self.attribute,
                evidence="Condition not met",
                remediation=self._create_remediation()
            )

    def is_applicable(self, repository: Repository) -> bool:
        # Only apply to Python repositories
        return "Python" in repository.languages

    def _check_condition(self, repository: Repository) -> bool:
        # Custom assessment logic
        pass

    def _create_remediation(self) -> Remediation:
        return Remediation(
            steps=["Step 1", "Step 2"],
            tools=["tool1", "tool2"]
        )
```

---

## Reporters

### HTMLReporter

Generate interactive HTML reports.

```python
from agentready.reporters import HTMLReporter

class HTMLReporter:
    """Generate interactive HTML reports."""

    def generate(self, assessment: Assessment) -> str:
        """
        Generate HTML report.

        Args:
            assessment: Complete assessment result

        Returns:
            HTML string (self-contained, no external dependencies)
        """
```

#### Example

```python
from agentready.reporters import HTMLReporter

reporter = HTMLReporter()
html = reporter.generate(assessment)

# Save to file
with open("report.html", "w") as f:
    f.write(html)

print(f"HTML report saved: {len(html)} bytes")
```

---

### MarkdownReporter

Generate GitHub-Flavored Markdown reports.

```python
from agentready.reporters import MarkdownReporter

class MarkdownReporter:
    """Generate Markdown reports."""

    def generate(self, assessment: Assessment) -> str:
        """
        Generate Markdown report.

        Args:
            assessment: Complete assessment result

        Returns:
            Markdown string (GitHub-Flavored Markdown)
        """
```

#### Example

```python
from agentready.reporters import MarkdownReporter

reporter = MarkdownReporter()
markdown = reporter.generate(assessment)

# Save to file
with open("report.md", "w") as f:
    f.write(markdown)
```

---

### JSONReporter

Generate machine-readable JSON reports.

```python
from agentready.reporters import JSONReporter

class JSONReporter:
    """Generate JSON reports."""

    def generate(self, assessment: Assessment) -> str:
        """
        Generate JSON report.

        Args:
            assessment: Complete assessment result

        Returns:
            JSON string (formatted with indentation)
        """
```

#### Example

```python
from agentready.reporters import JSONReporter
import json

reporter = JSONReporter()
json_str = reporter.generate(assessment)

# Save to file
with open("assessment.json", "w") as f:
    f.write(json_str)

# Parse for programmatic access
data = json.loads(json_str)
print(f"Score: {data['overall_score']}")
print(f"Certification: {data['certification_level']}")
```

---

## Usage Examples

### Complete Assessment Workflow

```python
from agentready.services import Scanner, LanguageDetector
from agentready.models import Repository
from agentready.reporters import HTMLReporter, MarkdownReporter, JSONReporter

# 1. Detect languages
detector = LanguageDetector()
languages = detector.detect("/path/to/repo")

# 2. Create repository object
repo = Repository(
    path="/path/to/repo",
    name="myproject",
    languages=languages
)

# 3. Run assessment
scanner = Scanner()
assessment = scanner.scan(repo)

# 4. Generate reports
html_reporter = HTMLReporter()
md_reporter = MarkdownReporter()
json_reporter = JSONReporter()

html = html_reporter.generate(assessment)
markdown = md_reporter.generate(assessment)
json_str = json_reporter.generate(assessment)

# 5. Save reports
with open("report.html", "w") as f:
    f.write(html)

with open("report.md", "w") as f:
    f.write(markdown)

with open("assessment.json", "w") as f:
    f.write(json_str)

# 6. Print summary
print(f"Assessment complete!")
print(f"Score: {assessment.overall_score}/100")
print(f"Certification: {assessment.certification_level}")
print(f"Passing: {assessment.passing_count}/{len(assessment.findings)}")
```

---

### CI/CD Integration

```python
import sys
from agentready.services import Scanner, LanguageDetector
from agentready.models import Repository

# Assess repository
detector = LanguageDetector()
languages = detector.detect(".")

repo = Repository(path=".", name="myproject", languages=languages)
scanner = Scanner()
assessment = scanner.scan(repo)

# Fail build if score < 70
if assessment.overall_score < 70:
    print(f"‚ùå AgentReady score too low: {assessment.overall_score}/100")
    print(f"Minimum required: 70/100")
    sys.exit(1)
else:
    print(f"‚úÖ AgentReady score: {assessment.overall_score}/100")
    sys.exit(0)
```

---

### Custom Filtering

```python
# Filter findings by status
passing = [f for f in assessment.findings if f.status == "pass"]
failing = [f for f in assessment.findings if f.status == "fail"]
skipped = [f for f in assessment.findings if f.status == "skipped"]

print(f"Passing: {len(passing)}")
print(f"Failing: {len(failing)}")
print(f"Skipped: {len(skipped)}")

# Find Tier 1 failures (highest priority)
tier1_failures = [
    f for f in failing
    if f.attribute.tier == 1
]

for finding in tier1_failures:
    print(f"‚ùå {finding.attribute.name}")
    print(f"   {finding.evidence}")
    if finding.remediation:
        print(f"   Fix: {finding.remediation.steps[0]}")
```

---

### Historical Tracking

```python
import json
import glob
from datetime import datetime

# Load all historical assessments
assessments = []
for file in sorted(glob.glob(".agentready/assessment-*.json")):
    with open(file) as f:
        data = json.load(f)
        assessments.append(data)

# Track score progression
print("Score history:")
for a in assessments:
    timestamp = a["metadata"]["timestamp"]
    score = a["overall_score"]
    cert = a["certification_level"]
    print(f"{timestamp}: {score:.1f}/100 ({cert})")

# Calculate improvement
if len(assessments) >= 2:
    initial = assessments[0]["overall_score"]
    latest = assessments[-1]["overall_score"]
    improvement = latest - initial
    print(f"\nTotal improvement: +{improvement:.1f} points")
```

---

### Custom Weight Configuration

```python
from agentready.services import Scanner
from agentready.models import Repository

# Override default weights programmatically
custom_weights = {
    "claude_md_file": 0.15,      # Increase from 0.10
    "readme_structure": 0.12,    # Increase from 0.10
    "type_annotations": 0.08,    # Decrease from 0.10
    # ... other attributes
}

# Note: Full weight customization requires modifying
# attribute definitions before scanner initialization.
# Typically done via .agentready-config.yaml file.

# For programmatic use, you can filter/reweight findings
# after assessment:
assessment = scanner.scan(repo)

# Custom scoring logic
def custom_score(findings, weights):
    total = 0.0
    for finding in findings:
        attr_id = finding.attribute.id
        weight = weights.get(attr_id, finding.attribute.weight)
        total += finding.score * weight
    return total

score = custom_score(assessment.findings, custom_weights)
print(f"Custom weighted score: {score}/100")
```

---

## Type Annotations

All AgentReady APIs include full type annotations for excellent IDE support:

```python
from agentready.models import Repository, Assessment, Finding
from agentready.services import Scanner

def assess_repository(repo_path: str) -> Assessment:
    """Assess repository and return results."""
    # Type hints enable autocomplete and type checking
    detector: LanguageDetector = LanguageDetector()
    languages: Dict[str, int] = detector.detect(repo_path)

    repo: Repository = Repository(
        path=repo_path,
        name=Path(repo_path).name,
        languages=languages
    )

    scanner: Scanner = Scanner()
    assessment: Assessment = scanner.scan(repo)

    return assessment
```

Use with mypy for static type checking:

```bash
mypy your_script.py
```

---

## Error Handling

AgentReady follows defensive programming principles:

```python
from agentready.services import LanguageDetector

try:
    detector = LanguageDetector()
    languages = detector.detect("/path/to/repo")
except ValueError as e:
    print(f"Error: {e}")
    # Typically: "Not a git repository"

except FileNotFoundError as e:
    print(f"Error: {e}")
    # Path does not exist

except Exception as e:
    print(f"Unexpected error: {e}")
```

**Best practices**:
- Assessors fail gracefully (return "skipped" if tools missing)
- Scanner continues on individual assessor errors
- Reports always generated (even with partial results)

---

## Next Steps

- **[User Guide](user-guide.html)** ‚Äî CLI usage and configuration
- **[Developer Guide](developer-guide.html)** ‚Äî Implementing custom assessors
- **[Attributes](attributes.html)** ‚Äî Complete attribute reference
- **[Examples](examples.html)** ‚Äî Real-world integration examples

---

**Questions?** Open an issue on [GitHub](https://github.com/ambient-code/agentready/issues).
````

## File: docs/attributes.md
````markdown
---
layout: page
title: Attributes Reference
---

# Attributes Reference

Complete reference for all 25 agent-ready attributes assessed by AgentReady.

<div class="feature" style="background-color: #dbeafe; border-left: 4px solid #2563eb; padding: 1rem; margin: 1rem 0;">
  <h3 style="margin-top: 0;">ü§ñ Bootstrap Automation</h3>
  <p><strong>AgentReady Bootstrap automatically implements many of these attributes.</strong> Look for the <strong>‚úÖ Bootstrap Addresses This</strong> marker to see which infrastructure Bootstrap generates for you.</p>
  <p>Instead of manually implementing each attribute, run <code>agentready bootstrap .</code> to generate complete GitHub setup in seconds.</p>
  <p><a href="user-guide.html#bootstrap-your-repository">Learn about Bootstrap ‚Üí</a></p>
</div>

## Table of Contents

- [Overview](#overview)
- [Tier System](#tier-system)
- [Tier 1: Essential Attributes](#tier-1-essential-attributes)
- [Tier 2: Critical Attributes](#tier-2-critical-attributes)
- [Tier 3: Important Attributes](#tier-3-important-attributes)
- [Tier 4: Advanced Attributes](#tier-4-advanced-attributes)
- [Implementation Status](#implementation-status)

---

## Overview

AgentReady evaluates repositories against **25 evidence-based attributes** that improve AI agent effectiveness. Each attribute is:

- **Research-backed**: Derived from 50+ authoritative sources (Anthropic, Microsoft, Google, academic research)
- **Measurable**: Specific criteria with clear pass/fail thresholds
- **Actionable**: Concrete tools, commands, and examples for remediation
- **Weighted**: Importance reflected in tier-based scoring (50/30/15/5 distribution)

**Every attribute includes**:
- Definition and importance for AI agents
- Impact on agent behavior
- Measurable criteria
- Authoritative citations
- Good vs. bad examples
- Remediation guidance

---

## Tier System

Attributes are organized into four weighted tiers:

| Tier | Weight | Focus | Attribute Count |
|------|--------|-------|-----------------|
| **Tier 1: Essential** | 50% | Fundamentals enabling basic AI functionality | 5 attributes |
| **Tier 2: Critical** | 30% | Major quality improvements and safety nets | 6 attributes |
| **Tier 3: Important** | 15% | Significant improvements in specific areas | 9 attributes |
| **Tier 4: Advanced** | 5% | Refinement and optimization | 5 attributes |

**Impact**: Missing a Tier 1 attribute (10% weight) has **10x the impact** of missing a Tier 4 attribute (1% weight).

---

## Tier 1: Essential Attributes

*Fundamentals that enable basic AI agent functionality ‚Äî 50% of total score*

### 1. CLAUDE.md Configuration File

**ID**: `claude_md_file`
**Weight**: 10%
**Category**: Context Window Optimization
**Status**: ‚úÖ Implemented

#### Definition

Markdown file at repository root (`CLAUDE.md` or `.claude/CLAUDE.md`) automatically ingested by Claude Code at conversation start.

#### Why It Matters

CLAUDE.md files provide **immediate project context** without repeated explanations. Research shows they reduce prompt engineering time by ~40% and frame entire sessions with project-specific guidance.

#### Impact on AI Agents

- Immediate understanding of tech stack, repository structure, standard commands
- Consistent adherence to project conventions
- Reduced need for repeated context-setting
- Proper framing for all AI suggestions

#### Measurable Criteria

**Passes if**:
- File exists at `CLAUDE.md` or `.claude/CLAUDE.md`
- File size: <1000 lines (concise, focused)
- Contains essential sections:
  - Tech stack with versions
  - Repository map/structure
  - Standard commands (build, test, lint, format)
  - Testing strategy
  - Style/lint rules
  - Branch/PR workflow

**Bonus points** (not required for pass):
- "Do not touch" zones documented
- Security/compliance notes included
- Common gotchas and edge cases

#### Example: Good CLAUDE.md

```markdown
# Tech Stack
- Python 3.11+, pytest, black + isort
- FastAPI, PostgreSQL, Redis

# Standard Commands
- Setup: `make setup` (installs deps, runs migrations)
- Test: `pytest tests/` (requires Redis running)
- Format: `black . && isort .`
- Lint: `ruff check .`
- Build: `docker build -t myapp .`

# Repository Structure
- src/myapp/ - Main application code
- tests/ - Test files mirror src/
- docs/ - Sphinx documentation
- migrations/ - Database migrations

# Boundaries
- Never modify files in legacy/ (deprecated, scheduled for removal)
- Require approval before changing config.yaml
- All database changes must have reversible migrations

# Testing Strategy
- Unit tests: Fast, isolated, no external dependencies
- Integration tests: Require PostgreSQL and Redis
- Run integration tests: `make test-integration`
```

#### Remediation

**If missing**:

1. **Create CLAUDE.md** in repository root
2. **Add tech stack** section with language/framework versions
3. **Document standard commands** (essential: setup, test, build)
4. **Map repository structure** (key directories and their purpose)
5. **Define boundaries** (files/areas not to modify)

**Tools**: Any text editor

**Time**: 15-30 minutes for initial creation

**Citations**:
- Anthropic Engineering Blog: "Claude Code Best Practices" (2025)
- AgentReady Research: "Context Window Optimization"

---

### 2. README Structure

**ID**: `readme_structure`
**Weight**: 10%
**Category**: Documentation Standards
**Status**: ‚úÖ Implemented

#### Definition

Standardized README.md with essential sections in predictable order, serving as primary entry point for understanding the project.

#### Why It Matters

Repositories with well-structured READMEs receive more engagement (GitHub data). README serves as AI agent's entry point for understanding project purpose, setup, and usage.

#### Impact on AI Agents

- Faster project comprehension
- Accurate answers to onboarding questions
- Better architectural understanding without exploring entire codebase
- Consistent expectations across projects

#### Measurable Criteria

**Passes if README.md contains (in order)**:
1. Project title and description
2. Installation/setup instructions
3. Quick start/usage examples
4. Core features
5. Dependencies and requirements
6. Testing instructions
7. Contributing guidelines
8. License

**Bonus sections**:
- Table of contents (for longer READMEs)
- Badges (build status, coverage, version)
- Screenshots or demos
- FAQ section
- Changelog link

#### Example: Well-Structured README

```markdown
# MyProject

Brief description of what this project does and why it exists.

## Installation

\```bash
pip install myproject
\```

## Quick Start

\```python
from myproject import Client

client = Client(api_key="your-key")
result = client.do_something()
print(result)
\```

## Features

- Feature 1: Does X efficiently
- Feature 2: Supports Y protocol
- Feature 3: Integrates with Z

## Requirements

- Python 3.11+
- PostgreSQL 14+
- Redis 7+ (optional, for caching)

## Testing

\```bash
# Run all tests
pytest

# Run with coverage
pytest --cov
\```

## Contributing

See [CONTRIBUTING.md](CONTRIBUTING.md) for development setup and guidelines.

## License

MIT License - see [LICENSE](LICENSE) for details.
```

#### Remediation

**If missing sections**:

1. **Audit current README**: Check which required sections are present
2. **Add missing sections**: Use template above as guide
3. **Reorder if needed**: Follow standard section order
4. **Add examples**: Include code snippets for quick start
5. **Keep concise**: Aim for <500 lines, link to detailed docs

**Tools**: Any text editor, Markdown linters

**Commands**:
```bash
# Validate Markdown syntax
markdownlint README.md

# Check for common issues
npx markdown-link-check README.md
```

**Citations**:
- GitHub Blog: "How to write a great README"
- Make a README project documentation

---

### 3. Type Annotations (Static Typing)

**ID**: `type_annotations`
**Weight**: 10%
**Category**: Code Quality
**Status**: ‚úÖ Implemented

#### Definition

Explicit type declarations for variables, function parameters, and return values in statically-typed or optionally-typed languages.

#### Why It Matters

Type hints **significantly improve LLM code understanding**. Research shows higher-quality codebases have type annotations, directing LLMs toward higher-quality latent space regions‚Äîsimilar to how LaTeX-formatted math gets better results.

#### Impact on AI Agents

- Better input validation suggestions
- Type error detection before execution
- Structured output generation
- Improved autocomplete accuracy
- Enhanced refactoring safety
- More confident code modifications

#### Measurable Criteria

**Python**:
- All public functions have parameter and return type hints
- Generic types from `typing` module used appropriately
- Coverage: >80% of functions typed
- Tools: mypy, pyright

**TypeScript**:
- `strict` mode enabled in tsconfig.json
- No `any` types (use `unknown` if needed)
- Interfaces for complex objects

**Go**:
- Inherently typed (always passes)

**JavaScript**:
- JSDoc type annotations OR migrate to TypeScript

#### Example: Good Type Annotations (Python)

```python
from typing import List, Optional, Dict

def find_users(
    role: str,
    active: bool = True,
    limit: Optional[int] = None
) -> List[Dict[str, str]]:
    """
    Find users matching criteria.

    Args:
        role: User role to filter by
        active: Include only active users
        limit: Maximum number of results

    Returns:
        List of user dictionaries
    """
    # Implementation
    pass

# Complex types
from dataclasses import dataclass

@dataclass
class User:
    id: str
    email: str
    role: str
    active: bool = True

def create_user(email: str, role: str) -> User:
    """Create new user with validation."""
    return User(id=generate_id(), email=email, role=role)
```

#### Example: Bad (No Type Hints)

```python
def find_users(role, active=True, limit=None):
    # What types? AI must guess
    pass

def create_user(email, role):
    # Return type unclear
    pass
```

#### Remediation

**Python**:

1. **Install type checker**:
   ```bash
   pip install mypy
   ```

2. **Add type hints** to public functions:
   ```bash
   # Use tool to auto-generate hints
   pip install monkeytype
   monkeytype run pytest tests/
   monkeytype apply module_name
   ```

3. **Run type checker**:
   ```bash
   mypy src/
   ```

4. **Fix errors iteratively**

**TypeScript**:

1. **Enable strict mode** in `tsconfig.json`:
   ```json
   {
     "compilerOptions": {
       "strict": true,
       "noImplicitAny": true
     }
   }
   ```

2. **Fix type errors**:
   ```bash
   tsc --noEmit
   ```

**Tools**: mypy, pyright, pytype (Python); tsc (TypeScript)

**Citations**:
- Medium: "LLM Coding Concepts: Static Typing"
- ArXiv: "Automated Type Annotation in Python Using LLMs"
- Dropbox: "Our journey to type checking 4 million lines of Python"

---

### 4. Standard Project Layout

**ID**: `standard_layout`
**Weight**: 10%
**Category**: Repository Structure
**Status**: ‚úÖ Implemented

#### Definition

Using community-recognized directory structures for each language/framework (e.g., Python's `src/` layout, Go's `cmd/` and `internal/`, Maven structure for Java).

#### Why It Matters

Standard layouts reduce cognitive overhead. AI models trained on open-source code recognize patterns and navigate predictably.

#### Impact on AI Agents

- Faster file location
- Accurate placement suggestions for new files
- Automatic adherence to established conventions
- Reduced confusion about file organization

#### Measurable Criteria

**Python (src/ layout)**:
```
project/
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îî‚îÄ‚îÄ package/
‚îÇ       ‚îú‚îÄ‚îÄ __init__.py
‚îÇ       ‚îî‚îÄ‚îÄ module.py
‚îú‚îÄ‚îÄ tests/
‚îú‚îÄ‚îÄ docs/
‚îú‚îÄ‚îÄ README.md
‚îú‚îÄ‚îÄ pyproject.toml
‚îî‚îÄ‚îÄ requirements.txt
```

**Go**:
```
project/
‚îú‚îÄ‚îÄ cmd/           # Main applications
‚îÇ   ‚îî‚îÄ‚îÄ app/
‚îÇ       ‚îî‚îÄ‚îÄ main.go
‚îú‚îÄ‚îÄ internal/      # Private code
‚îú‚îÄ‚îÄ pkg/           # Public libraries
‚îú‚îÄ‚îÄ go.mod
‚îî‚îÄ‚îÄ go.sum
```

**JavaScript/TypeScript**:
```
project/
‚îú‚îÄ‚îÄ src/
‚îú‚îÄ‚îÄ test/
‚îú‚îÄ‚îÄ dist/
‚îú‚îÄ‚îÄ package.json
‚îú‚îÄ‚îÄ package-lock.json
‚îî‚îÄ‚îÄ tsconfig.json (if TypeScript)
```

**Java (Maven)**:
```
project/
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ main/java/
‚îÇ   ‚îî‚îÄ‚îÄ test/java/
‚îú‚îÄ‚îÄ pom.xml
‚îî‚îÄ‚îÄ target/
```

#### Remediation

**If non-standard layout**:

1. **Identify target layout** for your language
2. **Create migration plan** (avoid breaking changes)
3. **Move files incrementally**:
   ```bash
   # Python: Migrate to src/ layout
   mkdir -p src/mypackage
   git mv mypackage/* src/mypackage/
   ```

4. **Update imports/references**
5. **Update build configuration** (setup.py, pyproject.toml, etc.)
6. **Test thoroughly**

**Tools**: IDE refactoring tools, git mv

**Citations**:
- Real Python: "Python Application Layouts"
- GitHub: golang-standards/project-layout
- Maven standard directory layout

---

### 5. Dependency Lock Files

**ID**: `lock_files`
**Weight**: 10%
**Category**: Dependency Management
**Status**: ‚úÖ Implemented

#### Definition

Pinning exact dependency versions including transitive dependencies (e.g., `package-lock.json`, `poetry.lock`, `go.sum`).

#### Why It Matters

Lock files ensure **reproducible builds** across environments. Without them, "works on my machine" problems plague AI-generated code. Different dependency versions can break builds, fail tests, or introduce bugs.

#### Impact on AI Agents

- Confident dependency-related suggestions
- Accurate compatibility issue diagnosis
- Reproducible environment recommendations
- Version-specific API usage

#### Measurable Criteria

**Passes if lock file exists and committed**:
- **npm**: `package-lock.json` or `yarn.lock`
- **Python**: `poetry.lock`, `Pipfile.lock`, or `requirements.txt` from `pip freeze` (or `uv.lock`)
- **Go**: `go.sum` (automatically managed)
- **Ruby**: `Gemfile.lock`
- **Rust**: `Cargo.lock`

**Additional requirements**:
- Lock file updated with every dependency change
- CI/CD uses lock file for installation
- Lock file not in `.gitignore`

**Note**: Library projects may intentionally exclude lock files. AgentReady recognizes this pattern and adjusts scoring.

#### Remediation

**Python (poetry)**:
```bash
# Install poetry
pip install poetry

# Create lock file
poetry lock

# Install from lock file
poetry install
```

**Python (pip)**:
```bash
# Create requirements with exact versions
pip freeze > requirements.txt

# Install from requirements
pip install -r requirements.txt
```

**npm**:
```bash
# Generate lock file
npm install

# Commit package-lock.json
git add package-lock.json
```

**Go**:
```bash
# Lock file auto-generated
go mod download
go mod tidy
```

**Citations**:
- npm Blog: "Why Keep package-lock.json?"
- Python Packaging User Guide
- Go Modules documentation

---

## Tier 2: Critical Attributes

*Major quality improvements and safety nets ‚Äî 30% of total score*

### 6. Test Coverage

**ID**: `test_coverage`
**Weight**: 5%
**Category**: Testing & CI/CD
**Status**: ‚úÖ Implemented

#### Definition

Percentage of code executed by automated tests, measured by line coverage, branch coverage, or function coverage.

#### Why It Matters

High test coverage enables **confident AI modifications**. Research shows AI tools can cut test coverage time by 85% while maintaining quality‚Äîbut only when good tests exist as foundation.

#### Measurable Criteria

**Minimum thresholds**:
- 70% line coverage (Bronze)
- 80% line coverage (Silver/Gold)
- 90% line coverage (Platinum)

**Critical paths**: 100% coverage for core business logic

**Measured via**:
- pytest-cov (Python)
- Jest/Istanbul (JavaScript/TypeScript)
- go test -cover (Go)
- JaCoCo (Java)

#### Remediation

```bash
# Python
pip install pytest pytest-cov
pytest --cov=src --cov-report=html

# JavaScript
npm install --save-dev jest
jest --coverage

# Go
go test -cover ./...
go test -coverprofile=coverage.out
go tool cover -html=coverage.out
```

**Citations**:
- Salesforce Engineering: "How Cursor AI Cut Legacy Code Coverage Time by 85%"

---

### 7. Pre-commit Hooks & CI/CD Linting

**ID**: `precommit_hooks`
**Weight**: 5%
**Category**: Testing & CI/CD
**Status**: ‚úÖ Implemented

**‚úÖ Bootstrap Addresses This**: `agentready bootstrap` automatically creates `.pre-commit-config.yaml` with language-specific hooks and corresponding GitHub Actions workflow.

#### Definition

Automated code quality checks before commits (pre-commit hooks) and in CI/CD pipeline, ensuring consistent standards.

#### Why It Matters

Pre-commit hooks provide immediate feedback. Running same checks in CI/CD ensures enforcement (hooks can be bypassed). Prevents low-quality code from entering repository.

#### Measurable Criteria

**Passes if**:
- `.pre-commit-config.yaml` exists
- Hooks include formatters (black, prettier) and linters (flake8, eslint)
- Same checks run in CI/CD (GitHub Actions, GitLab CI, etc.)
- CI fails on linting errors

#### Remediation

**Automated** (recommended):
```bash
agentready bootstrap .  # Generates .pre-commit-config.yaml + GitHub Actions
pre-commit install      # Install git hooks locally
```

**Manual**:
```bash
# Install pre-commit
pip install pre-commit

# Create .pre-commit-config.yaml
cat > .pre-commit-config.yaml << 'EOF'
repos:
  - repo: https://github.com/psf/black
    rev: 23.12.0
    hooks:
      - id: black

  - repo: https://github.com/pycqa/isort
    rev: 5.13.0
    hooks:
      - id: isort

  - repo: https://github.com/pycqa/flake8
    rev: 7.0.0
    hooks:
      - id: flake8
EOF

# Install hooks
pre-commit install

# Run manually
pre-commit run --all-files
```

**Citations**:
- Memfault: "Automatically format and lint code with pre-commit"
- GitHub: pre-commit/pre-commit

---

### 8. Conventional Commit Messages

**ID**: `conventional_commits`
**Weight**: 5%
**Category**: Git & Version Control
**Status**: üî∂ Partially Implemented

#### Definition

Structured commit messages following format: `<type>(<scope>): <description>`.

#### Why It Matters

Conventional commits enable **automated semantic versioning**, changelog generation, and commit intent understanding. AI can parse history to understand feature evolution.

#### Measurable Criteria

**Format**: `type(scope): description`

**Types**: feat, fix, docs, style, refactor, perf, test, chore, build, ci

**Enforcement**: commitlint in pre-commit hooks or CI

**Examples**:
- ‚úÖ `feat(auth): add OAuth2 login support`
- ‚úÖ `fix(api): handle null values in user response`
- ‚úÖ `docs(readme): update installation instructions`
- ‚ùå `update stuff`
- ‚ùå `fixed bug`

#### Remediation

```bash
# Install commitlint
npm install -g @commitlint/cli @commitlint/config-conventional

# Create commitlint.config.js
echo "module.exports = {extends: ['@commitlint/config-conventional']}" > commitlint.config.js

# Add to pre-commit hooks
cat >> .pre-commit-config.yaml << 'EOF'
  - repo: https://github.com/alessandrojcm/commitlint-pre-commit-hook
    rev: v9.5.0
    hooks:
      - id: commitlint
        stages: [commit-msg]
EOF
```

**Citations**:
- Conventional Commits specification v1.0.0
- Medium: "GIT ‚Äî Semantic versioning and conventional commits"

---

### 9. .gitignore Completeness

**ID**: `gitignore_completeness`
**Weight**: 5%
**Category**: Git & Version Control
**Status**: ‚úÖ Implemented

#### Definition

Comprehensive `.gitignore` preventing build artifacts, dependencies, IDE files, OS files, secrets, and logs from version control.

#### Why It Matters

Incomplete `.gitignore` pollutes repository with irrelevant files, consuming context window space and creating security risks (accidentally committing `.env`, credentials).

#### Measurable Criteria

**Must exclude**:
- Build artifacts (`dist/`, `build/`, `*.pyc`, `*.class`)
- Dependencies (`node_modules/`, `venv/`, `vendor/`)
- IDE files (`.vscode/`, `.idea/`, `*.swp`)
- OS files (`.DS_Store`, `Thumbs.db`)
- Environment variables (`.env`, `.env.local`)
- Credentials (`*.pem`, `*.key`, `credentials.json`)
- Logs (`*.log`, `logs/`)

**Best practice**: Use templates from [github/gitignore](https://github.com/github/gitignore)

#### Remediation

```bash
# Download language-specific template
curl https://raw.githubusercontent.com/github/gitignore/main/Python.gitignore > .gitignore

# Or generate with gitignore.io
curl -sL https://www.toptal.com/developers/gitignore/api/python,node,visualstudiocode > .gitignore

# Add custom patterns
echo ".env" >> .gitignore
echo "*.log" >> .gitignore
```

**Citations**:
- GitHub: github/gitignore
- Medium: "Mastering .gitignore"

---

### 10. One-Command Build/Setup

**ID**: `one_command_setup`
**Weight**: 5%
**Category**: Build & Development
**Status**: üî∂ Partially Implemented

#### Definition

Single command to set up development environment from fresh clone (`make setup`, `npm install`, `./bootstrap.sh`).

#### Why It Matters

One-command setup enables AI to quickly reproduce environments and test changes. Reduces "works on my machine" problems.

#### Measurable Criteria

**Passes if**:
- Single command documented in README
- Command handles:
  - Dependency installation
  - Virtual environment creation
  - Database setup/migrations
  - Configuration file creation
  - Pre-commit hooks installation
- Success in <5 minutes on fresh clone
- Idempotent (safe to run multiple times)

#### Example: Makefile

```makefile
.PHONY: setup
setup:
	python -m venv venv
	. venv/bin/activate && pip install -r requirements.txt
	pre-commit install
	cp .env.example .env
	python manage.py migrate
	@echo "‚úì Setup complete! Run 'make test' to verify."
```

#### Remediation

1. **Create setup script** (Makefile, package.json script, or shell script)
2. **Document in README** quick start section
3. **Test on fresh clone**
4. **Automate common setup steps**

**Citations**:
- freeCodeCamp: "Using Make as a Build Tool"

---

### 11. Development Environment Documentation

**ID**: `dev_environment_docs`
**Weight**: 5%
**Category**: Build & Development
**Status**: üî∂ Partially Implemented

#### Definition

Clear documentation of prerequisites, environment variables, and configuration requirements.

#### Measurable Criteria

**Must document**:
- Language/runtime version (Python 3.11+, Node.js 18+)
- System dependencies (PostgreSQL, Redis, etc.)
- Environment variables (`.env.example` with all variables)
- Optional: IDE setup, debugging config

#### Example: .env.example

```bash
# Database
DATABASE_URL=postgresql://user:pass@localhost:5432/myapp

# Redis (optional, for caching)
REDIS_URL=redis://localhost:6379

# API Keys (get from https://example.com/api)
API_KEY=your-key-here
API_SECRET=your-secret-here

# Feature Flags
ENABLE_FEATURE_X=false
```

**Citations**:
- Medium: "Creating Reproducible Development Environments"

---

## Tier 3: Important Attributes

*Significant improvements in specific areas ‚Äî 15% of total score*

### 12. Cyclomatic Complexity Limits

**ID**: `cyclomatic_complexity`
**Weight**: 3%
**Category**: Code Quality
**Status**: ‚úÖ Implemented

#### Definition

Measurement of linearly independent paths through code (decision point density). Target: <10 per function.

#### Why It Matters

High complexity confuses both humans and AI. Functions with complexity >25 are error-prone and hard to test.

#### Measurable Criteria

- Target: <10 per function
- Warning: 15
- Error: 25

**Tools**:
- radon (Python)
- complexity-report (JavaScript)
- gocyclo (Go)
- clang-tidy (C++)

#### Remediation

```bash
# Python
pip install radon
radon cc src/ -a -nb

# JavaScript
npm install -g complexity-report
cr src/**/*.js

# Refactor complex functions
# Break into smaller helper functions
# Extract conditional logic
# Use polymorphism instead of switch statements
```

**Citations**:
- Microsoft Learn: "Code metrics - Cyclomatic complexity"

---

### 13-20. Additional Tier 3 Attributes

**13. Function/Method Length Limits** (`function_length`) ‚Äî Target: <50 lines per function
**14. Code Smell Elimination** (`code_smells`) ‚Äî DRY violations, long methods, magic numbers
**15. Separation of Concerns** (`separation_of_concerns`) ‚Äî SOLID principles adherence
**16. Inline Documentation** (`inline_documentation`) ‚Äî Docstrings >80% coverage
**17. Architecture Decision Records** (`adrs`) ‚Äî Document major decisions in `docs/adr/`
**18. Structured Logging** (`structured_logging`) ‚Äî JSON logs with consistent fields
**19. OpenAPI/Swagger Specs** (`api_documentation`) ‚Äî Machine-readable API docs
**20. DRY Principle** (`dry_principle`) ‚Äî <5% duplicate code

*Full details for each attribute available in the [research document](https://github.com/ambient-code/agentready/blob/main/agent-ready-codebase-attributes.md).*

---

## Tier 4: Advanced Attributes

*Refinement and optimization ‚Äî 5% of total score*

### 21-25. Tier 4 Attributes

**21. Issue & PR Templates** (`pr_issue_templates`) ‚Äî `.github/` templates
**22. Container/Virtualization Setup** (`container_setup`) ‚Äî Dockerfile, docker-compose.yml
**23. Dependency Security Scanning** (`dependency_security`) ‚Äî Snyk, Dependabot, npm audit
**24. Secrets Management** (`secrets_management`) ‚Äî No hardcoded secrets, use env vars
**25. Performance Benchmarks** (`performance_benchmarks`) ‚Äî Automated perf tests in CI

*Full details for each attribute available in the [research document](https://github.com/ambient-code/agentready/blob/main/agent-ready-codebase-attributes.md).*

---

## Implementation Status

| Tier | Implemented | Stub | Total |
|------|-------------|------|-------|
| **Tier 1** | 5 | 0 | 5 |
| **Tier 2** | 3 | 3 | 6 |
| **Tier 3** | 1 | 8 | 9 |
| **Tier 4** | 1 | 4 | 5 |
| **Total** | **10** | **15** | **25** |

**Current version (1.0.0)**: 10 assessors fully implemented, 15 return "not_applicable" (stub implementations).

**Roadmap**: Future versions will expand stub assessors to provide complete 25-attribute coverage.

---

## Next Steps

- **[User Guide](user-guide.html)** ‚Äî Learn how to run assessments
- **[Developer Guide](developer-guide.html)** ‚Äî Implement new assessors
- **[API Reference](api-reference.html)** ‚Äî Integrate AgentReady
- **[Examples](examples.html)** ‚Äî View real assessment reports

---

**Complete attribute research**: See [agent-ready-codebase-attributes.md](https://github.com/ambient-code/agentready/blob/main/agent-ready-codebase-attributes.md) for full citations, examples, and detailed criteria.
````

## File: docs/examples.md
````markdown
---
layout: page
title: Examples
---

# Examples & Showcase

Real-world AgentReady assessments demonstrating report formats, interpretation guidance, and remediation patterns.

## Table of Contents

- [AgentReady Self-Assessment](#agentready-self-assessment)
- [Report Interpretation Guide](#report-interpretation-guide)
- [Common Remediation Patterns](#common-remediation-patterns)
- [Integration Examples](#integration-examples)

---

## AgentReady Self-Assessment

AgentReady assesses itself to validate the scoring algorithm and demonstrate expected output.

### Assessment Summary

**Date**: 2025-11-21
**Score**: 75.4/100
**Certification**: ü•á Gold

**Breakdown**:
- Attributes Assessed: 10/25 (15 stub implementations)
- Passing: 7/10
- Failing: 3/10
- Skipped: 15/25

### Tier Scores

| Tier | Score | Weighted Contribution |
|------|-------|----------------------|
| Tier 1 (Essential) | 80.0/100 | 40.0/50 points |
| Tier 2 (Critical) | 66.7/100 | 20.0/30 points |
| Tier 3 (Important) | 100.0/100 | 15.0/15 points |
| Tier 4 (Advanced) | 0.0/100 | 0.0/5 points |

**Analysis**: Strong essential attributes (Tier 1), good documentation and code quality. Missing some critical CI/CD automation (Tier 2). Tier 4 attributes not yet implemented.

### Passing Attributes (7)

#### 1. ‚úÖ CLAUDE.md File (Tier 1, 10%)

**Evidence**: Found CLAUDE.md at repository root (482 lines)

**Why it passes**: Comprehensive project documentation covering:
- Tech stack (Python 3.11+, pytest, black, isort, ruff)
- Repository structure (src/, tests/, docs/, examples/)
- Standard commands (setup, test, lint, format)
- Development workflow (GitHub Flow, feature branches)
- Testing strategy (unit, integration, contract tests)

**Impact**: Immediate project context for AI agents, ~40% reduction in prompt engineering.

---

#### 2. ‚úÖ README Structure (Tier 1, 10%)

**Evidence**: Well-structured README.md with all essential sections

**Sections present**:
- ‚úÖ Project title and description
- ‚úÖ Installation instructions (pip install)
- ‚úÖ Quick start with code examples
- ‚úÖ Feature overview (25 attributes, tier-based scoring)
- ‚úÖ CLI reference
- ‚úÖ Architecture overview
- ‚úÖ Development setup
- ‚úÖ License (MIT)

**Impact**: Fast project comprehension for both users and AI agents.

---

#### 3. ‚úÖ Type Annotations (Tier 1, 10%)

**Evidence**: Python type hints present in 95% of functions

**Examples from codebase**:
```python
def assess(self, repository: Repository) -> Finding:
    """Assess repository for this attribute."""

def calculate_overall_score(findings: List[Finding]) -> float:
    """Calculate weighted average score."""

class Repository:
    path: str
    name: str
    languages: Dict[str, int]
```

**Impact**: Better AI comprehension, type-safe refactoring, improved autocomplete.

---

#### 4. ‚úÖ Standard Layout (Tier 2, 3%)

**Evidence**: Follows Python src/ layout convention

**Structure**:
```
agentready/
‚îú‚îÄ‚îÄ src/agentready/    # Source code
‚îú‚îÄ‚îÄ tests/             # Tests mirror src/
‚îú‚îÄ‚îÄ docs/              # Documentation
‚îú‚îÄ‚îÄ examples/          # Example reports
‚îú‚îÄ‚îÄ pyproject.toml     # Package config
‚îî‚îÄ‚îÄ README.md          # Entry point
```

**Impact**: Predictable file locations, AI navigates efficiently.

---

#### 5. ‚úÖ Test Coverage (Tier 2, 3%)

**Evidence**: 37% coverage with focused unit tests

**Coverage details**:
- Unit tests for models: 95% coverage
- Assessor tests: 60% coverage
- Integration tests: End-to-end workflow
- Total lines covered: 890/2400

**Note**: While below 80% target, core logic (models, scoring) has excellent coverage. Future work: expand assessor coverage.

**Impact**: Safety net for AI-assisted refactoring of critical paths.

---

#### 6. ‚úÖ Gitignore Completeness (Tier 2, 3%)

**Evidence**: Comprehensive .gitignore covering all necessary patterns

**Excluded**:
- ‚úÖ Python artifacts (\_\_pycache\_\_, \*.pyc, \*.pyo, .pytest\_cache)
- ‚úÖ Virtual environments (.venv, venv, env)
- ‚úÖ IDE files (.vscode/, .idea/, \*.swp)
- ‚úÖ OS files (.DS\_Store, Thumbs.db)
- ‚úÖ Build artifacts (dist/, build/, \*.egg-info)
- ‚úÖ Reports (.agentready/)

**Impact**: Clean repository, no context pollution for AI.

---

#### 7. ‚úÖ Cyclomatic Complexity (Tier 3, 1.5%)

**Evidence**: Low complexity across codebase (average: 4.2, max: 12)

**Analysis** (via radon):
- Functions with complexity >10: 2/180 (1%)
- Average complexity: 4.2 (excellent)
- Most complex function: `Scanner.scan()` (12)

**Impact**: Easy comprehension for AI, low cognitive load.

---

### Failing Attributes (3)

#### 1. ‚ùå Lock Files (Tier 2, 3%)

**Evidence**: No requirements.txt, poetry.lock, or uv.lock present

**Why it fails**: Intentional decision for library projects (libraries specify version ranges, not exact pins). Applications should have lock files.

**Remediation** (if this were an application):
```bash
# Using poetry
poetry lock

# Using pip
pip freeze > requirements.txt

# Using uv
uv pip compile pyproject.toml -o requirements.txt
```

**Note**: This is acceptable for libraries. AgentReady recognizes this pattern and adjusts scoring accordingly in future versions.

---

#### 2. ‚ùå Pre-commit Hooks (Tier 2, 3%)

**Evidence**: No .pre-commit-config.yaml found

**Why it fails**: Missing automation for code quality enforcement. Currently relying on manual `black`, `isort`, `ruff` runs.

**Remediation**:

1. **Install pre-commit**:
   ```bash
   pip install pre-commit
   ```

2. **Create .pre-commit-config.yaml**:
   ```yaml
   repos:
     - repo: https://github.com/psf/black
       rev: 23.12.0
       hooks:
         - id: black

     - repo: https://github.com/pycqa/isort
       rev: 5.13.0
       hooks:
         - id: isort

     - repo: https://github.com/astral-sh/ruff-pre-commit
       rev: v0.1.9
       hooks:
         - id: ruff
   ```

3. **Install hooks**:
   ```bash
   pre-commit install
   ```

4. **Test**:
   ```bash
   pre-commit run --all-files
   ```

**Impact**: +3 points (78.4/100 total, still Gold)

**Priority**: P0 fix (identified in BACKLOG.md)

---

#### 3. ‚ùå Conventional Commits (Tier 3, 1.5%)

**Evidence**: Git history uses conventional commits, but not enforced via tooling

**Sample commits**:
- ‚úÖ `feat(assessors): add inline documentation assessor`
- ‚úÖ `fix: correct type annotation detection in Python 3.12`
- ‚úÖ `docs: update CLAUDE.md with architecture details`

**Why it fails**: No commitlint or automated enforcement (could be bypassed).

**Remediation**:

1. **Install commitlint**:
   ```bash
   npm install -g @commitlint/cli @commitlint/config-conventional
   ```

2. **Create commitlint.config.js**:
   ```javascript
   module.exports = {extends: ['@commitlint/config-conventional']};
   ```

3. **Add to pre-commit hooks**:
   ```yaml
   - repo: https://github.com/alessandrojcm/commitlint-pre-commit-hook
     rev: v9.5.0
     hooks:
       - id: commitlint
         stages: [commit-msg]
   ```

**Impact**: +1.5 points (76.9/100 total)

**Priority**: P1 enhancement

---

### Next Steps for AgentReady

**Immediate improvements** (would reach 79.9/100):
1. Add pre-commit hooks (+3 points)
2. Enforce conventional commits (+1.5 points)

**Path to Platinum (90+)**:
1. Expand 15 stub assessors to full implementations
2. Increase test coverage to 80%+
3. Add GitHub Actions CI/CD workflow
4. Implement remaining Tier 4 attributes

---

## Report Interpretation Guide

### Understanding Your Score

#### Certification Levels

| Level | Range | Meaning |
|-------|-------|---------|
| üèÜ Platinum | 90-100 | Exemplary agent-ready codebase |
| ü•á Gold | 75-89 | Highly optimized for AI agents |
| ü•à Silver | 60-74 | Well-suited for AI development |
| ü•â Bronze | 40-59 | Basic agent compatibility |
| üìà Needs Improvement | 0-39 | Significant friction for AI agents |

**What the ranges mean**:
- **90+**: World-class. Few improvements possible.
- **75-89**: Excellent foundation. Some gaps in advanced areas.
- **60-74**: Good baseline. Missing some critical attributes.
- **40-59**: Functional but friction-heavy. Major improvements needed.
- **<40**: Difficult for AI agents. Focus on essential attributes first.

---

### Reading the HTML Report

#### 1. Score Card (Top Section)

**Overall Score**: Weighted average across all attributes
**Certification Level**: Your badge (Platinum/Gold/Silver/Bronze)
**Visual Gauge**: Color-coded progress bar

**Tier Breakdown Table**:
- Shows score for each tier
- Weighted contribution to overall score
- Quickly identifies weak areas

**Example interpretation**:
- Tier 1: 80/100 ‚Üí Contributing 40/50 points (good)
- Tier 2: 50/100 ‚Üí Contributing 15/30 points (needs work)
- Tier 3: 100/100 ‚Üí Contributing 15/15 points (perfect)
- Tier 4: 0/100 ‚Üí Contributing 0/5 points (not critical)

**Analysis**: Focus on Tier 2 for highest impact (+15 points possible).

---

#### 2. Attribute Table (Middle Section)

**Columns**:
- **Status**: ‚úÖ Pass, ‚ùå Fail, ‚äò Skipped
- **Attribute**: Name and ID
- **Tier**: 1-4 (importance)
- **Weight**: Percentage contribution to score
- **Score**: 0-100 for this attribute
- **Evidence**: What was found

**Sorting**:
- By score (ascending): See worst attributes first
- By tier: Focus on high-tier failures
- By weight: Maximize point gains

**Filtering**:
- "Failed only": Focus on remediation opportunities
- "Tier 1 only": Essential attributes
- Search: Find specific attribute by name

---

#### 3. Detailed Findings (Expandable Sections)

Click any attribute to expand:

**For passing attributes**:
- Evidence of compliance
- Examples from your codebase
- Why this matters for AI agents

**For failing attributes**:
- Specific evidence of what's missing
- **Remediation section**:
  - Ordered steps to fix
  - Required tools
  - Copy-paste ready commands
  - Code/config examples
  - Reference citations

**For skipped attributes**:
- Reason (not applicable, not implemented, or tool missing)

---

### Prioritizing Improvements

#### Strategy 1: Maximize Points (Tier √ó Weight)

Focus on high-tier, high-weight failures:

1. **Calculate potential gain**: `weight √ó (100 - current_score)`
2. **Sort by potential gain** (descending)
3. **Fix top 3-5 attributes**

**Example**:
- ‚ùå CLAUDE.md (Tier 1, 10%, score 0) ‚Üí +10 points
- ‚ùå Pre-commit hooks (Tier 2, 3%, score 0) ‚Üí +3 points
- ‚ùå Type annotations (Tier 1, 10%, score 50) ‚Üí +5 points

**Best ROI**: Fix CLAUDE.md first (+10), then type annotations (+5).

---

#### Strategy 2: Quick Wins (<1 hour)

Some attributes are fast to fix:

**<15 minutes**:
- Create CLAUDE.md (outline version)
- Add .gitignore from template
- Create .env.example

**<30 minutes**:
- Add README sections
- Configure pre-commit hooks
- Add PR/issue templates

**<1 hour**:
- Write initial tests
- Add type hints to 10 key functions
- Create ADR template

---

#### Strategy 3: Foundational First (Tier 1)

Ensure all Tier 1 attributes pass before moving to Tier 2:

**Tier 1 checklist**:
- [ ] CLAUDE.md exists and comprehensive
- [ ] README has all essential sections
- [ ] Type annotations >80% coverage
- [ ] Standard project layout
- [ ] Lock file committed

**Why**: Tier 1 = 50% of score. Missing one Tier 1 attribute (-10 points) hurts more than missing five Tier 4 attributes (-5 points total).

---

## Common Remediation Patterns

### Pattern 1: Documentation Gaps

**Symptoms**:
- Missing CLAUDE.md
- Incomplete README
- No inline documentation

**Solution Template**:

1. **Create CLAUDE.md** (15 min):
   ```markdown
   # Tech Stack
   - [Language] [Version]
   - [Framework] [Version]

   # Standard Commands
   - Setup: [command]
   - Test: [command]
   - Build: [command]

   # Repository Structure
   - src/ - [description]
   - tests/ - [description]
   ```

2. **Enhance README** (30 min):
   - Add Quick Start section
   - Include code examples
   - Document installation steps

3. **Add docstrings** (ongoing):
   ```python
   def function_name(param: Type) -> ReturnType:
       """
       Brief description.

       Args:
           param: Description

       Returns:
           Description
       """
   ```

---

### Pattern 2: Missing Automation

**Symptoms**:
- No pre-commit hooks
- No CI/CD
- Manual testing only

**Solution Template**:

1. **Pre-commit hooks** (15 min):
   ```bash
   pip install pre-commit
   pre-commit sample-config > .pre-commit-config.yaml
   # Edit to add language-specific hooks
   pre-commit install
   ```

2. **GitHub Actions** (30 min):
   ```yaml
   # .github/workflows/ci.yml
   name: CI
   on: [push, pull_request]
   jobs:
     test:
       runs-on: ubuntu-latest
       steps:
         - uses: actions/checkout@v4
         - uses: actions/setup-python@v4
         - run: pip install -e ".[dev]"
         - run: pytest --cov
         - run: black --check .
   ```

3. **Automated dependency updates** (10 min):
   ```yaml
   # .github/dependabot.yml
   version: 2
   updates:
     - package-ecosystem: "pip"
       directory: "/"
       schedule:
         interval: "weekly"
   ```

---

### Pattern 3: Code Quality Deficits

**Symptoms**:
- No type annotations
- High cyclomatic complexity
- Code smells

**Solution Template**:

1. **Add type hints incrementally**:
   ```bash
   # Install mypy
   pip install mypy

   # Check current state
   mypy src/

   # Add hints to 5 functions per day
   # Focus on public APIs first
   ```

2. **Reduce complexity**:
   ```bash
   # Measure complexity
   pip install radon
   radon cc src/ -a -nb

   # Refactor functions with CC >10
   # Extract helper functions
   # Replace nested ifs with early returns
   ```

3. **Eliminate code smells**:
   ```bash
   # Install SonarQube or use pylint
   pip install pylint
   pylint src/

   # Fix critical/high issues first
   # DRY violations: extract shared code
   # Long functions: split into smaller functions
   ```

---

## Integration Examples

### Example 1: GitHub Actions CI

Fail builds if AgentReady score drops below threshold:

```yaml
# .github/workflows/agentready.yml
name: AgentReady Assessment

on:
  pull_request:
  push:
    branches: [main]

jobs:
  assess:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - uses: actions/setup-python@v4
        with:
          python-version: '3.11'

      - name: Install AgentReady
        run: pip install agentready

      - name: Run Assessment
        run: |
          agentready assess . --output-dir ./reports

      - name: Check Score Threshold
        run: |
          score=$(jq '.overall_score' .agentready/assessment-latest.json)
          echo "AgentReady Score: $score/100"

          if (( $(echo "$score < 70" | bc -l) )); then
            echo "‚ùå Score below threshold (70)"
            exit 1
          fi

          echo "‚úÖ Score meets threshold"

      - name: Upload Report
        uses: actions/upload-artifact@v3
        with:
          name: agentready-report
          path: .agentready/report-latest.html
```

---

### Example 2: Pre-commit Hook

Run AgentReady assessment before commits:

```yaml
# .pre-commit-config.yaml
repos:
  - repo: local
    hooks:
      - id: agentready
        name: AgentReady Assessment
        entry: agentready assess .
        language: system
        pass_filenames: false
        always_run: true
```

**Note**: This runs on every commit (slow). Better to run in CI/CD and use pre-commit for formatting/linting only.

---

### Example 3: Badge in README

Display AgentReady score badge:

```markdown
# MyProject

![AgentReady Score](https://img.shields.io/badge/AgentReady-75.4%2F100-gold)

<!-- Update badge after each assessment -->
```

**Automation** (via GitHub Actions):
```yaml
- name: Update Badge
  run: |
    score=$(jq '.overall_score' .agentready/assessment-latest.json)
    cert=$(jq -r '.certification_level' .agentready/assessment-latest.json)

    # Update README badge via script
    ./scripts/update-badge.sh $score $cert
```

---

### Example 4: Historical Tracking

Track score improvements over time:

```python
# scripts/track-improvements.py
import json
import glob
import matplotlib.pyplot as plt
from datetime import datetime

# Load all assessments
assessments = []
for file in sorted(glob.glob('.agentready/assessment-*.json')):
    with open(file) as f:
        data = json.load(f)
        timestamp = datetime.fromisoformat(data['metadata']['timestamp'])
        score = data['overall_score']
        assessments.append((timestamp, score))

# Plot trend
timestamps, scores = zip(*assessments)
plt.plot(timestamps, scores, marker='o')
plt.xlabel('Date')
plt.ylabel('AgentReady Score')
plt.title('AgentReady Score Progression')
plt.ylim(0, 100)
plt.grid(True)
plt.savefig('agentready-trend.png')
print("Trend chart saved: agentready-trend.png")
```

---

## Next Steps

- **[User Guide](user-guide.html)** ‚Äî Learn how to run assessments
- **[Developer Guide](developer-guide.html)** ‚Äî Implement custom assessors
- **[Attributes](attributes.html)** ‚Äî Complete attribute reference
- **[API Reference](api-reference.html)** ‚Äî Integrate AgentReady programmatically

---

**View full reports**: Check out [`examples/self-assessment/`](https://github.com/ambient-code/agentready/tree/main/examples/self-assessment) in the repository for complete HTML, Markdown, and JSON reports.
````

## File: docs/roadmaps.md
````markdown
---
layout: page
title: Strategic Roadmaps
---

# Strategic Roadmaps

Three paths to transform AgentReady from quality assessment tool to essential infrastructure for Red Hat's AI-assisted development initiative.

**Current Status**: v1.0.0 with bootstrap command ([learn more](user-guide.html#bootstrap-your-repository))

**Target Audience**: Engineering leadership, product managers, and teams evaluating AgentReady adoption

---

## Table of Contents

- [Executive Summary](#executive-summary)
- [Roadmap 1: The Compliance Engine](#roadmap-1-the-compliance-engine)
- [Roadmap 2: The Agent Coach](#roadmap-2-the-agent-coach)
- [Roadmap 3: The Intelligence Layer](#roadmap-3-the-intelligence-layer)
- [Roadmap Comparison](#roadmap-comparison)
- [Recommended Approach](#recommended-approach)
- [Getting Started Today](#getting-started-today)

---

## Executive Summary

AgentReady can evolve along three strategic paths, each building on our core strength: **systematically making codebases more effective for AI-assisted development**.

### The Three Roadmaps

| Roadmap | Vision | Timeline | Strategic Impact |
|---------|--------|----------|------------------|
| **üèÖ Compliance Engine** | Quality gate for AI tools | 6-8 weeks | Adoption velocity via enforcement |
| **ü§ñ Agent Coach** | AI-powered remediation | 8-10 weeks | Retention via assistance |
| **üß† Intelligence Layer** | Codebase understanding platform | 10-12 weeks | Platform moat |

### Recommended Path

**Start with Roadmap 1, then evolve:**

- **Months 1-2**: Implement compliance features ‚Üí fastest path to adoption
- **Months 3-4**: Layer in AI-powered fixes ‚Üí convert enforcement to assistance
- **Months 5-6**: Build out API/platform ‚Üí leverage data from mature deployment

This progression maximizes **adoption velocity** (enforcement), **retention** (AI assistance), and **strategic positioning** (platform moat).

---

## Roadmap 1: The Compliance Engine

**Agent-Ready Certification as Quality Gate**

### Vision

Make AgentReady a required quality gate for Red Hat's AI-assisted development. Repositories must hit **Silver (60+)** to use AI tools, **Gold (75+)** for production deployments.

**Strategic Value**: Immediate adoption through mandate, establishes AgentReady as standard across Red Hat engineering.

### Timeline

**6-8 weeks** from v1.0 to production-ready compliance system

### Core Features

#### 1. GitHub Actions Integration
- **PR status checks** with pass/fail based on score threshold
- **Automated PR comments** with assessment summary and trend analysis
- **Custom certification levels** per team/product (override default thresholds)
- **Workflow templates** for easy adoption

#### 2. Organization Dashboard
- **Leaderboard** showing all repositories with scores and certification levels
- **Trend tracking** over time (score improvements, regression detection)
- **Team rollups** aggregating scores by team/product
- **Executive reporting** with high-level metrics and health indicators

#### 3. Automated Remediation (`agentready align`)
- **Template-based fixes** for common issues (missing files, standard configs)
- **One-click remediation** from HTML reports
- **Batch operations** to fix multiple issues at once
- **Preview mode** showing what will change before applying

#### 4. Interactive Reports
- **"Fix This" buttons** in HTML reports triggering automated remediation
- **Issue creation** directly from report findings
- **Copy-paste commands** for manual remediation
- **Progress tracking** showing improvement over time

#### 5. Customizable Certification
- **Team-specific thresholds** (e.g., RHOAI requires Gold, others Silver)
- **Product-specific attributes** (enable/disable based on project type)
- **Custom scoring weights** via configuration files
- **Exemption workflows** for special cases with approval process

### Adoption Strategy

#### Phase 1: Dogfooding (Week 1-2)
- **Apply to AgentReady itself** and achieve Platinum certification
- **Document process** and create adoption playbook
- **Identify pain points** and refine UX

#### Phase 2: Friendly Pilot (Week 3-4)
- **Recruit 3 teams**: RHOAI, RHEL AI, OpenShift AI
- **Target**: Each team reaches Silver (60+) within 2 weeks
- **Collect feedback** on automation, report quality, remediation guidance
- **Iterate** based on real-world usage

#### Phase 3: Executive Mandate (Week 5-6)
- **Steven Huels backing**: Announce requirement for AI tool usage
- **Policy**: New AI-assisted projects must hit Silver before tool access
- **Enforcement**: GitHub App integration blocks AI tool PRs if score < threshold
- **Communication**: Engineering-wide announcement with training resources

#### Phase 4: Scale Deployment (Week 7-8)
- **GitHub App integration** for automatic repository onboarding
- **100+ repos** with AgentReady checks enabled
- **Self-service** adoption via bootstrap command
- **Success stories** showcasing teams that improved scores

### Success Metrics

#### Adoption Metrics
- **100+ repositories** with AgentReady checks in 8 weeks
- **80% of active repos** hit Silver (60+) in 12 weeks
- **20+ teams** actively using dashboard and reports

#### Impact Metrics
- **70% reduction** in "agent can't understand my repo" issues
- **50% faster** AI tool onboarding (better codebase context)
- **90% positive feedback** from pilot teams

#### Business Metrics
- **Reduced support burden** for AI tools (better-prepared codebases)
- **Improved AI tool effectiveness** (higher quality context)
- **Cultural shift** toward agent-ready practices

---

## Roadmap 2: The Agent Coach

**Real-Time Remediation & Learning**

### Vision

Transform AgentReady from static scanner to **interactive AI coach** that not only identifies issues but fixes them automatically with Claude-powered suggestions.

**Strategic Value**: Converts enforcement (Roadmap 1) into assistance, dramatically reducing friction and improving developer experience.

### Timeline

**8-10 weeks** from Roadmap 1 completion to AI-powered coach

### Core Features

#### 1. Claude-Powered Fix Generation
- **Type annotations**: Auto-add type hints to Python functions
- **Docstrings**: Generate Google-style docstrings from function signatures
- **Test generation**: Create pytest tests for uncovered functions
- **Refactoring**: Simplify complex functions flagged by assessors
- **Context-aware**: Uses repository context (CLAUDE.md, existing patterns)

#### 2. Fix Preview & Approval Workflow
- **Show diff** before applying changes
- **Interactive approval** (approve all, approve individually, reject)
- **Undo capability** to revert AI changes
- **Learn from feedback** (track which fixes get accepted/rejected)

#### 3. VS Code Extension (Optional)
- **Real-time assessment** as you code
- **Inline suggestions** for agent-readiness improvements
- **Quick fixes** via VS Code actions
- **Dashboard view** showing repository score and trends

#### 4. Claude Code Agent Integration
- **Agent-native interface** for remediation
- **Conversational fixes**: "Make this repository Gold-certified"
- **Contextual suggestions** based on project patterns
- **Automated PR creation** with fixes

#### 5. Automated PR Campaigns
- **Scheduled remediation**: Weekly PRs addressing low-hanging fruit
- **Batch improvements**: Fix similar issues across multiple files
- **Team review**: Auto-assign reviewers via CODEOWNERS
- **Continuous improvement**: Gradually increase score over time

#### 6. Telemetry & Learning
- **Track fix acceptance rate** (which AI fixes get merged)
- **Identify patterns** in successful vs rejected fixes
- **Improve suggestions** based on repository-specific preferences
- **Personalized coaching**: Adapt to team coding style

### Adoption Strategy

#### Phase 1: Build AI Fix Engine (Week 1-3)
- **Integrate Claude API** for fix generation
- **Implement core fixers**: Type annotations, docstrings, tests
- **Test on AgentReady codebase** (dogfooding)
- **Achieve >80%** AI fix acceptance rate internally

#### Phase 2: Pilot with RHOAI (Week 4-6)
- **Deploy to RHOAI team** as early adopters
- **Target**: 50+ AI-generated PRs merged
- **Collect feedback** on fix quality, UX, workflow integration
- **Iterate** based on real-world usage

#### Phase 3: VS Code Extension Launch (Week 7-8)
- **Publish to Red Hat extension registry**
- **Marketing**: Demo at engineering all-hands
- **Tutorial**: Step-by-step guide for installation and usage
- **Support**: Office hours for questions and feedback

#### Phase 4: Enable Auto-PR Campaigns (Week 9-10)
- **Opt-in system**: Teams enable automated weekly PRs
- **Guardrails**: Require approval, limit batch size
- **Metrics dashboard**: Track PRs created, merged, rejected
- **Success stories**: Highlight teams with highest improvement rates

### Success Metrics

#### AI Fix Quality
- **>75% of AI-generated fixes** merged without changes
- **<5% of AI fixes** cause regressions or test failures
- **90% developer satisfaction** with fix quality

#### Efficiency Gains
- **90% reduction** in time to fix agent-readiness (2 hours ‚Üí 10 mins)
- **5+ PRs merged** per repository per quarter
- **50% reduction** in manual remediation effort

#### Adoption Metrics
- **500+ developers** using AI fix generation monthly
- **100+ repositories** with auto-PR campaigns enabled
- **1,000+ AI fixes** merged across Red Hat

---

## Roadmap 3: The Intelligence Layer

**Codebase Understanding Platform**

### Vision

Evolve AgentReady into a **foundational intelligence layer** for ALL Red Hat AI/agent tools. Become the source of truth for codebase context, structure, and agent-readiness.

**Strategic Value**: Platform moat‚ÄîAgentReady data powers multiple AI products, creating lock-in and strategic positioning.

### Timeline

**10-12 weeks** from Roadmap 2 completion to platform launch

### Core Features

#### 1. REST API for Repository Insights
- **Assessment endpoint**: Get current score, findings, certification level
- **Structure endpoint**: Codebase layout, file organization, dependencies
- **Context endpoint**: Auto-generated summaries, key patterns, tech stack
- **Dependency endpoint**: Library versions, security vulnerabilities, freshness
- **Agent capability matching**: Which agents work best with this repo

#### 2. Auto-Generated Context Files
- **Dynamic CLAUDE.md**: Keep agent context files up-to-date automatically
- **Repomix integration**: Generate compressed context for token-limited tools
- **Custom templates**: Per-team context file formats
- **Version control**: Track context file changes over time

#### 3. Agent Capability Matching
- **Agent profiles**: Define capabilities of different AI agents/tools
- **Compatibility scoring**: How well does repo match agent requirements
- **Recommendations**: "This repo works best with Claude Code, not GitHub Copilot"
- **Gap analysis**: What's missing for optimal agent usage

#### 4. Cross-Repository Intelligence
- **Pattern detection**: Identify common practices across successful repos
- **Best practice propagation**: "Top-rated repos use X pattern, suggest for yours"
- **Anomaly detection**: Flag unusual patterns (security risks, anti-patterns)
- **Benchmarking**: Compare your repo to similar projects

#### 5. Integration with Red Hat AI Products
- **RHOAI integration**: Assess training data repositories for quality
- **RHEL AI integration**: Optimize model deployment repositories
- **Instructlab integration**: Improve knowledge base repository structure
- **CI/CD platform**: Gate deployments on agent-readiness scores

#### 6. Plugin Architecture
- **Custom assessors**: Teams can add product-specific checks
- **Community plugins**: Marketplace for third-party assessors
- **Language-specific packs**: Deep analysis for specific languages
- **Industry standards**: Compliance checks (HIPAA, SOC2, etc.)

#### 7. Historical Analysis & Predictive Insights
- **Trend prediction**: Forecast score trajectory based on commit patterns
- **Risk analysis**: Predict likelihood of agent failures based on recent changes
- **Churn correlation**: Identify teams with low scores and high support burden
- **ROI tracking**: Measure impact of agent-readiness on development velocity

### Adoption Strategy

#### Phase 1: Build API & Deploy (Week 1-4)
- **Design REST API** following OpenAPI spec
- **Implement core endpoints** (assessment, structure, context)
- **Deploy to Red Hat OpenShift** with HA setup
- **Documentation**: API reference, integration guides, SDKs

#### Phase 2: Partner with AI Initiatives (Week 5-7)
- **Recruit 2 partners**: RHOAI and CI/CD platform teams
- **Build integrations**: Connect their tools to AgentReady API
- **Demonstrate value**: Show how context data improves their products
- **Collect feedback**: Refine API based on real integration needs

#### Phase 3: Engineering Summit Demo (Week 8-9)
- **Keynote demo**: Show cross-product integration at Red Hat summit
- **Technical sessions**: Deep-dive workshops on API usage
- **Office hours**: Help teams integrate with their products
- **Success stories**: Case studies from pilot partners

#### Phase 4: Expand to External Partners (Week 10-12)
- **GitHub partnership**: Explore native integration with GitHub
- **JetBrains partnership**: Integrate with IntelliJ, PyCharm
- **Claude Code**: Become default codebase context provider
- **Open source**: Release core API as open source for community adoption

### Success Metrics

#### Platform Adoption
- **5+ Red Hat AI products** integrate with AgentReady API
- **3+ external partners** using AgentReady data
- **10,000+ API calls per day**

#### Data Quality
- **90% of auto-generated CLAUDE.md files** used without modification
- **95% uptime** for API service
- **<100ms latency** for assessment endpoint

#### Strategic Impact
- **AgentReady as standard**: Referenced in Red Hat AI strategy docs
- **Competitive advantage**: Unique codebase intelligence layer
- **Revenue opportunity**: Potential SaaS offering for external customers

---

## Roadmap Comparison

### Feature Matrix

| Feature | Compliance Engine | Agent Coach | Intelligence Layer |
|---------|-------------------|-------------|-------------------|
| **GitHub Actions Integration** | ‚úÖ Core | ‚úÖ Enhanced | ‚úÖ API-powered |
| **Organization Dashboard** | ‚úÖ Core | ‚úÖ Enhanced | ‚úÖ Enterprise |
| **Automated Remediation** | ‚úÖ Templates | ‚úÖ AI-powered | ‚úÖ Cross-repo |
| **Interactive Reports** | ‚úÖ Basic | ‚úÖ AI suggestions | ‚úÖ Predictive |
| **Custom Certification** | ‚úÖ Core | ‚úÖ Core | ‚úÖ Pluggable |
| **AI Fix Generation** | ‚ùå | ‚úÖ Core | ‚úÖ Advanced |
| **VS Code Extension** | ‚ùå | ‚úÖ Optional | ‚úÖ Full IDE suite |
| **Claude Code Integration** | ‚ùå | ‚úÖ Core | ‚úÖ Native API |
| **Auto-PR Campaigns** | ‚ùå | ‚úÖ Core | ‚úÖ Cross-repo |
| **REST API** | ‚ùå | ‚ùå | ‚úÖ Core |
| **Auto-generated Context** | ‚ùå | ‚ùå | ‚úÖ Core |
| **Agent Capability Matching** | ‚ùå | ‚ùå | ‚úÖ Core |
| **Cross-repo Intelligence** | ‚ùå | ‚ùå | ‚úÖ Core |
| **Plugin Architecture** | ‚ùå | ‚ùå | ‚úÖ Core |
| **Historical Analysis** | ‚ùå | ‚ùå | ‚úÖ Core |

### Timeline & Dependencies

```
Roadmap 1: Compliance Engine (Weeks 1-8)
‚îú‚îÄ‚îÄ GitHub Actions integration (Week 1-2)
‚îú‚îÄ‚îÄ Organization dashboard (Week 3-4)
‚îú‚îÄ‚îÄ Automated remediation (Week 5-6)
‚îî‚îÄ‚îÄ Scale deployment (Week 7-8)

Roadmap 2: Agent Coach (Weeks 9-18) [requires Roadmap 1]
‚îú‚îÄ‚îÄ AI fix engine (Week 9-11)
‚îú‚îÄ‚îÄ RHOAI pilot (Week 12-14)
‚îú‚îÄ‚îÄ VS Code extension (Week 15-16)
‚îî‚îÄ‚îÄ Auto-PR campaigns (Week 17-18)

Roadmap 3: Intelligence Layer (Weeks 19-30) [requires Roadmap 2]
‚îú‚îÄ‚îÄ REST API build (Week 19-22)
‚îú‚îÄ‚îÄ Partner integrations (Week 23-25)
‚îú‚îÄ‚îÄ Summit demo (Week 26-27)
‚îî‚îÄ‚îÄ External partnerships (Week 28-30)
```

### Strategic Positioning

| Dimension | Compliance Engine | Agent Coach | Intelligence Layer |
|-----------|-------------------|-------------|-------------------|
| **Adoption Driver** | Enforcement (mandate) | Assistance (value) | Integration (ecosystem) |
| **Market Position** | Internal tool | Developer tool | Platform |
| **Revenue Model** | Cost center | Productivity gain | SaaS potential |
| **Competitive Moat** | Low (policy-based) | Medium (AI quality) | High (data network effects) |
| **Strategic Value** | Enabler | Differentiator | Foundation |

---

## Recommended Approach

### Why Sequential Execution?

**Start with Roadmap 1, then layer in 2 and 3:**

#### Months 1-2: Compliance Engine
- **Fastest path to adoption** via executive mandate
- **Establishes baseline** for all repositories
- **Generates data** for AI training and pattern detection
- **Proves value** with concrete metrics (adoption, score improvements)

#### Months 3-4: Agent Coach
- **Converts enforcement to assistance** (carrot after stick)
- **Improves developer experience** dramatically
- **Increases engagement** (from compliance to eager usage)
- **Builds trust** in AI-generated fixes

#### Months 5-6: Intelligence Layer
- **Leverages mature deployment** (100+ repos with rich data)
- **Enables cross-product synergies** (RHOAI, RHEL AI, etc.)
- **Creates platform moat** (hard to replicate data advantage)
- **Opens revenue opportunities** (external partnerships, SaaS)

### Why NOT Parallel Development?

**Parallel development risks:**
- **Resource constraints**: Stretching team too thin reduces quality
- **Integration complexity**: Features designed independently may not mesh well
- **Data dependency**: Roadmap 3 needs data from Roadmap 1 deployment
- **Market feedback**: Each phase informs next (pilot learnings, usage patterns)

### Success Checkpoints

**Proceed to next roadmap only if:**

| Checkpoint | Criteria |
|------------|----------|
| **Roadmap 1 ‚Üí 2** | 50+ repos at Silver, 80% pilot satisfaction, <5% regression rate |
| **Roadmap 2 ‚Üí 3** | 75% AI fix acceptance, 200+ repos using coach, 10+ teams with auto-PRs |
| **Roadmap 3 ‚Üí External** | 5+ internal integrations, 10K+ API calls/day, 95% uptime |

---

## Getting Started Today

### For Individual Developers

```bash
# Bootstrap your repository now
cd /path/to/your/repo
agentready bootstrap .

# Review generated files
git status

# Commit and see automated assessment on next PR
git add . && git commit -m "build: Bootstrap agent-ready infrastructure"
git push
```

**Learn more**: [Bootstrap tutorial ‚Üí](user-guide.html#bootstrap-your-repository)

### For Team Leads

1. **Assess current state**: Run `agentready assess .` on your team's repos
2. **Set team target**: Decide on certification level (Silver, Gold, Platinum)
3. **Bootstrap infrastructure**: Enable GitHub Actions via bootstrap command
4. **Track progress**: Use reports to monitor score improvements
5. **Share results**: Include assessment scores in team metrics

**Learn more**: [User guide ‚Üí](user-guide.html)

### For Engineering Leadership

1. **Pilot program**: Recruit 3 friendly teams for initial rollout
2. **Success metrics**: Define KPIs (adoption rate, score targets, velocity impact)
3. **Executive sponsorship**: Align with AI-assisted development strategy
4. **Policy development**: Draft agent-readiness requirements for AI tool access
5. **Communication plan**: Announce mandate with clear timelines and support

**Contact**: Reach out to [Jeremy Eder](mailto:jeder@redhat.com) to discuss strategic rollout

---

## Next Steps

- **[User Guide](user-guide.html)** ‚Äî Install and run your first assessment
- **[Developer Guide](developer-guide.html)** ‚Äî Contribute to AgentReady development
- **[Attributes Reference](attributes.html)** ‚Äî Understand the 25 agent-ready attributes
- **[API Reference](api-reference.html)** ‚Äî Integrate AgentReady into your tools
- **[Examples](examples.html)** ‚Äî See real-world assessment reports

---

**Questions?** Join the discussion on [GitHub](https://github.com/ambient-code/agentready/discussions) or contact the AgentReady team.

**Last Updated**: 2025-11-21
````

## File: docs/user-guide.md
````markdown
---
layout: page
title: User Guide
---

# User Guide

Complete guide to installing, configuring, and using AgentReady to assess your repositories.

## Table of Contents

- [Installation](#installation)
- [Quick Start](#quick-start)
- [Bootstrap Your Repository](#bootstrap-your-repository) ‚≠ê **NEW**
  - [What is Bootstrap?](#what-is-bootstrap)
  - [When to Use Bootstrap vs Assess](#when-to-use-bootstrap-vs-assess)
  - [Step-by-Step Tutorial](#step-by-step-tutorial)
  - [Generated Files Explained](#generated-files-explained)
  - [Post-Bootstrap Checklist](#post-bootstrap-checklist)
- [Running Assessments](#running-assessments)
- [Understanding Reports](#understanding-reports)
- [Configuration](#configuration)
- [CLI Reference](#cli-reference)
- [Troubleshooting](#troubleshooting)

---

## Installation

### Prerequisites

- **Python 3.11 or 3.12** (AgentReady supports versions N and N-1)
- **Git** (for repository analysis)
- **pip** or **uv** (Python package manager)

### Install from PyPI

```bash
# Using pip
pip install agentready

# Using uv (recommended)
uv pip install agentready

# Verify installation
agentready --version
```

### Install from Source

```bash
# Clone the repository
git clone https://github.com/ambient-code/agentready.git
cd agentready

# Create virtual environment
python3 -m venv .venv
source .venv/bin/activate  # On Windows: .venv\Scripts\activate

# Install in development mode
pip install -e .

# Or using uv
uv pip install -e .
```

### Development Installation

If you plan to contribute or modify AgentReady:

```bash
# Install with development dependencies
pip install -e ".[dev]"

# Or using uv
uv pip install -e ".[dev]"

# Verify installation
pytest --version
black --version
```

---

## Quick Start

### Bootstrap-First Approach (Recommended)

Transform your repository with one command:

```bash
# Navigate to your repository
cd /path/to/your/repo

# Bootstrap infrastructure
agentready bootstrap .

# Review generated files
git status

# Commit and push
git add .
git commit -m "build: Bootstrap agent-ready infrastructure"
git push
```

**What happens:**
- ‚úÖ GitHub Actions workflows created (tests, security, assessment)
- ‚úÖ Pre-commit hooks configured
- ‚úÖ Issue/PR templates added
- ‚úÖ Dependabot enabled
- ‚úÖ Assessment runs automatically on next PR

**Duration**: <60 seconds including review time.

[See detailed Bootstrap tutorial ‚Üí](#bootstrap-your-repository)

### Manual Assessment Approach

For one-time analysis without infrastructure changes:

```bash
# Navigate to your repository
cd /path/to/your/repo

# Run assessment
agentready assess .

# View the HTML report
open .agentready/report-latest.html  # macOS
xdg-open .agentready/report-latest.html  # Linux
start .agentready/report-latest.html  # Windows
```

**Output location**: `.agentready/` directory in your repository root.

**Duration**: Most assessments complete in under 5 seconds.

---

## Bootstrap Your Repository

### What is Bootstrap?

**Bootstrap is AgentReady's automated infrastructure generator.** Instead of manually implementing recommendations from assessment reports, Bootstrap creates complete GitHub setup in one command:

**Generated Infrastructure:**
- **GitHub Actions workflows** ‚Äî Tests, security scanning, AgentReady assessment
- **Pre-commit hooks** ‚Äî Language-specific formatters and linters
- **Issue/PR templates** ‚Äî Structured bug reports, feature requests, PR checklist
- **CODEOWNERS** ‚Äî Automated review assignments
- **Dependabot** ‚Äî Weekly dependency updates
- **Contributing guidelines** ‚Äî If not present
- **Code of Conduct** ‚Äî Red Hat standard (if not present)

**Language Detection:**
Bootstrap automatically detects your primary language (Python, JavaScript, Go) via `git ls-files` and generates appropriate configurations.

**Safe to Use:**
- Use `--dry-run` to preview changes without creating files
- All files are created, never overwritten
- Review with `git status` before committing

---

### When to Use Bootstrap vs Assess

<table>
<thead>
<tr>
<th>Scenario</th>
<th>Use Bootstrap</th>
<th>Use Assess</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>New project</strong></td>
<td>‚úÖ Start with best practices</td>
<td>Later, to track progress</td>
</tr>
<tr>
<td><strong>Missing GitHub Actions</strong></td>
<td>‚úÖ Generate workflows instantly</td>
<td>Shows it's missing</td>
</tr>
<tr>
<td><strong>No pre-commit hooks</strong></td>
<td>‚úÖ Configure automatically</td>
<td>Shows it's missing</td>
</tr>
<tr>
<td><strong>Understanding current state</strong></td>
<td>Use after bootstrapping</td>
<td>‚úÖ Detailed analysis</td>
</tr>
<tr>
<td><strong>Existing infrastructure</strong></td>
<td>Safe (won't overwrite)</td>
<td>‚úÖ Validate setup</td>
</tr>
<tr>
<td><strong>Tracking improvements</strong></td>
<td>One-time setup</td>
<td>‚úÖ Run repeatedly</td>
</tr>
<tr>
<td><strong>CI/CD integration</strong></td>
<td>Generates the workflows</td>
<td>‚úÖ Runs in CI (via Bootstrap)</td>
</tr>
</tbody>
</table>

**Recommended workflow:**
1. **Bootstrap first** ‚Äî Generate infrastructure
2. **Review and commit** ‚Äî Inspect generated files
3. **Assess automatically** ‚Äî Every PR via GitHub Actions
4. **Manual assess** ‚Äî When validating improvements

---

### Step-by-Step Tutorial

#### Step 1: Preview Changes (Dry Run)

Always start with `--dry-run` to see what will be created:

```bash
cd /path/to/your/repo
agentready bootstrap . --dry-run
```

**Example output:**
```
Detecting primary language...
‚úì Detected: Python (42 files)

Files that will be created:
  .github/workflows/agentready-assessment.yml
  .github/workflows/tests.yml
  .github/workflows/security.yml
  .github/ISSUE_TEMPLATE/bug_report.md
  .github/ISSUE_TEMPLATE/feature_request.md
  .github/PULL_REQUEST_TEMPLATE.md
  .github/CODEOWNERS
  .github/dependabot.yml
  .pre-commit-config.yaml
  CONTRIBUTING.md (not present, will create)
  CODE_OF_CONDUCT.md (not present, will create)

Run without --dry-run to create these files.
```

**Review the list carefully:**
- Files marked "(not present, will create)" are new
- Existing files are never overwritten
- Check for conflicts with existing workflows

---

#### Step 2: Run Bootstrap

If dry-run output looks good, run without flag:

```bash
agentready bootstrap .
```

**Example output:**
```
Detecting primary language...
‚úì Detected: Python (42 files)

Creating infrastructure...
  ‚úì .github/workflows/agentready-assessment.yml
  ‚úì .github/workflows/tests.yml
  ‚úì .github/workflows/security.yml
  ‚úì .github/ISSUE_TEMPLATE/bug_report.md
  ‚úì .github/ISSUE_TEMPLATE/feature_request.md
  ‚úì .github/PULL_REQUEST_TEMPLATE.md
  ‚úì .github/CODEOWNERS
  ‚úì .github/dependabot.yml
  ‚úì .pre-commit-config.yaml
  ‚úì CONTRIBUTING.md
  ‚úì CODE_OF_CONDUCT.md

Bootstrap complete! 11 files created.

Next steps:
1. Review generated files: git status
2. Customize as needed (CODEOWNERS, workflow triggers, etc.)
3. Commit: git add . && git commit -m "build: Bootstrap infrastructure"
4. Enable GitHub Actions in repository settings
5. Push and create PR to see assessment in action!
```

---

#### Step 3: Review Generated Files

Inspect what was created:

```bash
# View all new files
git status

# Inspect key files
cat .github/workflows/agentready-assessment.yml
cat .pre-commit-config.yaml
cat .github/CODEOWNERS
```

**What to check:**
- **CODEOWNERS** ‚Äî Add actual team member GitHub usernames
- **Workflows** ‚Äî Adjust triggers (e.g., only main branch, specific paths)
- **Pre-commit hooks** ‚Äî Add/remove tools based on your stack
- **Issue templates** ‚Äî Customize labels and assignees

---

#### Step 4: Install Pre-commit Hooks (Local)

Bootstrap creates `.pre-commit-config.yaml`, but you must install locally:

```bash
# Install pre-commit (if not already)
pip install pre-commit

# Install git hooks
pre-commit install

# Test hooks on all files
pre-commit run --all-files
```

**Expected output:**
```
black....................................................................Passed
isort....................................................................Passed
ruff.....................................................................Passed
```

**If failures occur:**
- Review suggested fixes
- Run formatters: `black .` and `isort .`
- Fix linting errors: `ruff check . --fix`
- Re-run: `pre-commit run --all-files`

---

#### Step 5: Commit and Push

```bash
# Stage all generated files
git add .

# Commit with conventional commit message
git commit -m "build: Bootstrap agent-ready infrastructure

- Add GitHub Actions workflows (tests, security, assessment)
- Configure pre-commit hooks (black, isort, ruff)
- Add issue and PR templates
- Enable Dependabot for weekly updates
- Add CONTRIBUTING.md and CODE_OF_CONDUCT.md"

# Push to repository
git push origin main
```

---

#### Step 6: Enable GitHub Actions

If this is the first time using Actions:

1. **Navigate to repository on GitHub**
2. **Go to Settings ‚Üí Actions ‚Üí General**
3. **Enable Actions** (select "Allow all actions")
4. **Set workflow permissions** to "Read and write permissions"
5. **Save**

---

#### Step 7: Test with a PR

Create a test PR to see Bootstrap in action:

```bash
# Create feature branch
git checkout -b test-agentready-bootstrap

# Make trivial change
echo "# Test" >> README.md

# Commit and push
git add README.md
git commit -m "test: Verify AgentReady assessment workflow"
git push origin test-agentready-bootstrap

# Create PR on GitHub
gh pr create --title "Test: AgentReady Bootstrap" --body "Testing automated assessment"
```

**What happens automatically:**
1. **Tests workflow** runs pytest (Python) or appropriate tests
2. **Security workflow** runs CodeQL analysis
3. **AgentReady assessment workflow** runs assessment and posts results as PR comment

**PR comment example:**
```
## AgentReady Assessment

**Score:** 75.4/100 (ü•á Gold)

**Tier Breakdown:**
- Tier 1 (Essential): 80/100
- Tier 2 (Critical): 70/100
- Tier 3 (Important): 65/100
- Tier 4 (Advanced): 50/100

**Passing:** 15/25 | **Failing:** 8/25 | **Skipped:** 2/25

[View full HTML report](link-to-artifact)
```

---

### Generated Files Explained

#### GitHub Actions Workflows

**`.github/workflows/agentready-assessment.yml`**
```yaml
# Runs on every PR and push to main
# Posts assessment results as PR comment
# Fails if score drops below configured threshold (default: 60)

Triggers: pull_request, push (main branch)
Duration: ~30 seconds
Artifacts: HTML report, JSON data
```

**`.github/workflows/tests.yml`**
```yaml
# Language-specific test workflow

Python:
  - Runs pytest with coverage
  - Coverage report posted as PR comment
  - Requires test/ directory

JavaScript:
  - Runs jest with coverage
  - Generates lcov report

Go:
  - Runs go test with race detection
  - Coverage profiling enabled
```

**`.github/workflows/security.yml`**
```yaml
# Comprehensive security scanning

CodeQL:
  - Analyzes code for vulnerabilities
  - Runs on push to main and PR
  - Supports 10+ languages

Dependency Scanning:
  - GitHub Advisory Database
  - Fails on high/critical vulnerabilities
```

---

#### Pre-commit Configuration

**`.pre-commit-config.yaml`**

Language-specific hooks configured:

**Python:**
- `black` ‚Äî Code formatter (88 char line length)
- `isort` ‚Äî Import sorter
- `ruff` ‚Äî Fast linter
- `trailing-whitespace` ‚Äî Remove trailing spaces
- `end-of-file-fixer` ‚Äî Ensure newline at EOF

**JavaScript/TypeScript:**
- `prettier` ‚Äî Code formatter
- `eslint` ‚Äî Linter
- `trailing-whitespace`
- `end-of-file-fixer`

**Go:**
- `gofmt` ‚Äî Code formatter
- `golint` ‚Äî Linter
- `go-vet` ‚Äî Static analysis

**To customize:**
Edit `.pre-commit-config.yaml` and adjust hook versions or add new repos.

---

#### GitHub Templates

**`.github/ISSUE_TEMPLATE/bug_report.md`**
- Structured bug report with reproduction steps
- Environment details (OS, version)
- Expected vs actual behavior
- Auto-labels as `bug`

**`.github/ISSUE_TEMPLATE/feature_request.md`**
- Structured feature proposal
- Use case and motivation
- Proposed solution
- Auto-labels as `enhancement`

**`.github/PULL_REQUEST_TEMPLATE.md`**
- Checklist for PR authors:
  - [ ] Tests added/updated
  - [ ] Documentation updated
  - [ ] Passes all checks
  - [ ] Breaking changes documented
- Links to related issues
- Change description

**`.github/CODEOWNERS`**
```
# Auto-assign reviewers based on file paths
# CUSTOMIZE: Replace with actual GitHub usernames

* @yourteam/maintainers
/docs/ @yourteam/docs
/.github/ @yourteam/devops
```

**`.github/dependabot.yml`**
```yaml
# Weekly dependency update checks
# Creates PRs for outdated dependencies
# Supports Python, npm, Go modules

Updates:
  - package-ecosystem: pip (or npm, gomod)
    schedule: weekly
    labels: [dependencies]
```

---

#### Development Guidelines

**`CONTRIBUTING.md`** (created if missing)
- Setup instructions
- Development workflow
- Code style guidelines
- PR process
- Testing requirements

**`CODE_OF_CONDUCT.md`** (created if missing)
- Red Hat standard Code of Conduct
- Community guidelines
- Reporting process
- Enforcement policy

---

### Post-Bootstrap Checklist

After running `agentready bootstrap`, complete these steps:

#### 1. Customize CODEOWNERS

```bash
# Edit .github/CODEOWNERS
vim .github/CODEOWNERS

# Replace placeholder usernames with actual team members
# * @yourteam/maintainers  ‚Üí  * @alice @bob
# /docs/ @yourteam/docs    ‚Üí  /docs/ @carol
```

#### 2. Review Workflow Triggers

```bash
# Check if workflow triggers match your branching strategy
cat .github/workflows/*.yml | grep "on:"

# Common adjustments:
# - Change 'main' to 'master' or 'develop'
# - Add path filters (e.g., only run tests when src/ changes)
# - Adjust schedule (e.g., nightly instead of push)
```

#### 3. Install Pre-commit Hooks

```bash
pip install pre-commit
pre-commit install
pre-commit run --all-files  # Test on existing code
```

#### 4. Enable GitHub Actions

- Repository Settings ‚Üí Actions ‚Üí General
- Enable "Allow all actions"
- Set "Read and write permissions" for workflows

#### 5. Configure Branch Protection (Recommended)

- Settings ‚Üí Branches ‚Üí Add rule for `main`
- Require status checks: `tests`, `security`, `agentready-assessment`
- Require PR reviews (at least 1 approval)
- Require branches to be up to date

#### 6. Test the Workflows

Create a test PR to verify:
```bash
git checkout -b test-workflows
echo "# Test" >> README.md
git add README.md
git commit -m "test: Verify automated workflows"
git push origin test-workflows
gh pr create --title "Test: Verify workflows" --body "Testing Bootstrap"
```

**Verify:**
- ‚úÖ All workflows run successfully
- ‚úÖ AgentReady posts PR comment with assessment results
- ‚úÖ Test coverage report appears
- ‚úÖ Security scan completes without errors

#### 7. Update Documentation

Add Badge to README.md:

```markdown
# MyProject

![AgentReady](https://img.shields.io/badge/AgentReady-Bootstrap-blue)
![Tests](https://github.com/yourusername/repo/workflows/tests/badge.svg)
![Security](https://github.com/yourusername/repo/workflows/security/badge.svg)
```

Mention Bootstrap in README:

```markdown
## Development Setup

This repository uses AgentReady Bootstrap for automated quality assurance.

All PRs are automatically assessed for agent-readiness. See the PR comment
for detailed findings and remediation guidance.
```

---

### Language-Specific Notes

#### Python Projects

Bootstrap generates:
- `pytest` workflow with coverage (`pytest-cov`)
- Pre-commit hooks: `black`, `isort`, `ruff`, `mypy`
- Dependabot for pip dependencies

**Customizations:**
- Adjust `pytest` command in `tests.yml` if using different test directory
- Add `mypy` configuration in `pyproject.toml` if type checking required
- Modify `black` line length in `.pre-commit-config.yaml` if needed

#### JavaScript/TypeScript Projects

Bootstrap generates:
- `jest` or `npm test` workflow
- Pre-commit hooks: `prettier`, `eslint`
- Dependabot for npm dependencies

**Customizations:**
- Update test command in `tests.yml` to match `package.json` scripts
- Adjust `prettier` config (`.prettierrc`) if different style
- Add TypeScript type checking (`tsc --noEmit`) to workflow

#### Go Projects

Bootstrap generates:
- `go test` workflow with race detection
- Pre-commit hooks: `gofmt`, `golint`, `go-vet`
- Dependabot for Go modules

**Customizations:**
- Add build step to workflow if needed (`go build ./...`)
- Configure `golangci-lint` for advanced linting
- Add coverage reporting (`go test -coverprofile=coverage.out`)

---

### Bootstrap Command Reference

```bash
agentready bootstrap [REPOSITORY] [OPTIONS]
```

**Arguments:**
- `REPOSITORY` ‚Äî Path to repository (default: current directory)

**Options:**
- `--dry-run` ‚Äî Preview files without creating
- `--language TEXT` ‚Äî Override auto-detection: `python|javascript|go|auto` (default: auto)

**Examples:**

```bash
# Bootstrap current directory (auto-detect language)
agentready bootstrap .

# Preview without creating files
agentready bootstrap . --dry-run

# Force Python configuration
agentready bootstrap . --language python

# Bootstrap different directory
agentready bootstrap /path/to/repo

# Combine dry-run and language override
agentready bootstrap /path/to/repo --dry-run --language go
```

**Exit codes:**
- `0` ‚Äî Success
- `1` ‚Äî Error (not a git repository, permission denied, etc.)

---

## Running Assessments

### Basic Usage

```bash
# Assess current directory
agentready assess .

# Assess specific repository
agentready assess /path/to/repo

# Assess with verbose output
agentready assess . --verbose

# Custom output directory
agentready assess . --output-dir ./custom-reports
```

### Assessment Output

AgentReady creates a `.agentready/` directory containing:

```
.agentready/
‚îú‚îÄ‚îÄ assessment-YYYYMMDD-HHMMSS.json    # Machine-readable data
‚îú‚îÄ‚îÄ report-YYYYMMDD-HHMMSS.html        # Interactive web report
‚îú‚îÄ‚îÄ report-YYYYMMDD-HHMMSS.md          # Markdown report
‚îú‚îÄ‚îÄ assessment-latest.json             # Symlink to latest
‚îú‚îÄ‚îÄ report-latest.html                 # Symlink to latest
‚îî‚îÄ‚îÄ report-latest.md                   # Symlink to latest
```

**Timestamps**: All files are timestamped for historical tracking.

**Latest links**: Symlinks always point to the most recent assessment.

### Verbose Mode

Get detailed progress information during assessment:

```bash
agentready assess . --verbose
```

**Output includes**:
- Repository path and detected languages
- Each assessor's execution status
- Finding summaries (pass/fail/skip)
- Final score calculation breakdown
- Report generation progress

---

## Understanding Reports

AgentReady generates three complementary report formats.

### HTML Report (Interactive)

**File**: `report-YYYYMMDD-HHMMSS.html`

The HTML report provides an interactive, visual interface:

#### Features

- **Overall Score Card**: Certification level, score, and visual gauge
- **Tier Summary**: Breakdown by attribute tier (Essential/Critical/Important/Advanced)
- **Attribute Table**: Sortable, filterable list of all attributes
- **Detailed Findings**: Expandable sections for each attribute
- **Search**: Find specific attributes by name or ID
- **Filters**: Show only passed, failed, or skipped attributes
- **Copy Buttons**: One-click code example copying
- **Offline**: No CDN dependencies, works anywhere

#### How to Use

1. **Open in browser**: Double-click the HTML file
2. **Review overall score**: Check certification level and tier breakdown
3. **Explore findings**:
   - Green ‚úÖ = Passed
   - Red ‚ùå = Failed (needs remediation)
   - Gray ‚äò = Skipped (not applicable or not yet implemented)
4. **Click to expand**: View detailed evidence and remediation steps
5. **Filter results**: Focus on specific attribute statuses
6. **Copy remediation commands**: Use one-click copy for code examples

#### Sharing

The HTML report is self-contained and can be:
- Emailed to stakeholders
- Uploaded to internal wikis
- Viewed on any device with a browser
- Archived for compliance/audit purposes

### Markdown Report (Version Control Friendly)

**File**: `report-YYYYMMDD-HHMMSS.md`

The Markdown report is optimized for git tracking:

#### Features

- **GitHub-Flavored Markdown**: Renders beautifully on GitHub
- **Git-Diffable**: Track score improvements over time
- **ASCII Tables**: Attribute summaries without HTML
- **Emoji Indicators**: ‚úÖ‚ùå‚äò for visual status
- **Certification Ladder**: Visual progress chart
- **Prioritized Next Steps**: Highest-impact improvements first

#### How to Use

1. **Commit to repository**:
   ```bash
   git add .agentready/report-latest.md
   git commit -m "docs: Add AgentReady assessment report"
   ```

2. **Track progress**:
   ```bash
   # Run new assessment
   agentready assess .

   # Compare to previous
   git diff .agentready/report-latest.md
   ```

3. **Review on GitHub**: Push and view formatted Markdown

4. **Share in PRs**: Reference in pull request descriptions

#### Recommended Workflow

```bash
# Initial baseline
agentready assess .
git add .agentready/report-latest.md
git commit -m "docs: AgentReady baseline (Score: 65.2)"

# Make improvements
# ... implement recommendations ...

# Re-assess
agentready assess .
git add .agentready/report-latest.md
git commit -m "docs: AgentReady improvements (Score: 72.8, +7.6)"
```

### JSON Report (Machine-Readable)

**File**: `assessment-YYYYMMDD-HHMMSS.json`

The JSON report contains complete assessment data:

#### Structure

```json
{
  "metadata": {
    "timestamp": "2025-11-21T10:30:00Z",
    "repository_path": "/path/to/repo",
    "agentready_version": "1.0.0",
    "duration_seconds": 2.35
  },
  "repository": {
    "path": "/path/to/repo",
    "name": "myproject",
    "languages": {"Python": 42, "JavaScript": 18}
  },
  "overall_score": 75.4,
  "certification_level": "Gold",
  "tier_scores": {
    "tier_1": 85.0,
    "tier_2": 70.0,
    "tier_3": 65.0,
    "tier_4": 50.0
  },
  "findings": [
    {
      "attribute_id": "claude_md_file",
      "attribute_name": "CLAUDE.md File",
      "tier": 1,
      "weight": 0.10,
      "status": "pass",
      "score": 100,
      "evidence": "Found CLAUDE.md at repository root",
      "remediation": null
    }
  ]
}
```

#### Use Cases

**CI/CD Integration**:
```bash
# Fail build if score < 70
score=$(jq '.overall_score' .agentready/assessment-latest.json)
if (( $(echo "$score < 70" | bc -l) )); then
  echo "AgentReady score too low: $score"
  exit 1
fi
```

**Trend Analysis**:
```python
import json
import glob

# Load all historical assessments
assessments = []
for file in sorted(glob.glob('.agentready/assessment-*.json')):
    with open(file) as f:
        assessments.append(json.load(f))

# Track score over time
for a in assessments:
    print(f"{a['metadata']['timestamp']}: {a['overall_score']}")
```

**Custom Reporting**:
```python
import json

with open('.agentready/assessment-latest.json') as f:
    assessment = json.load(f)

# Extract failed attributes
failed = [
    f for f in assessment['findings']
    if f['status'] == 'fail'
]

# Create custom report
for finding in failed:
    print(f"‚ùå {finding['attribute_name']}")
    print(f"   {finding['evidence']}")
    print()
```

---

## Configuration

### Default Behavior

AgentReady works out-of-the-box with sensible defaults. No configuration required for basic usage.

### Custom Configuration File

Create `.agentready-config.yaml` to customize:

```yaml
# Custom attribute weights (must sum to 1.0)
weights:
  claude_md_file: 0.15      # Increase from default 0.10
  readme_structure: 0.12    # Increase from default 0.10
  type_annotations: 0.08    # Decrease from default 0.10
  # ... other 22 attributes

# Exclude specific attributes
excluded_attributes:
  - performance_benchmarks  # Skip this assessment
  - container_setup         # Not applicable to our project

# Custom output directory
output_dir: ./reports

# Verbosity (true/false)
verbose: false
```

### Weight Customization Rules

1. **Must sum to 1.0**: Total weight across all attributes (excluding excluded ones)
2. **Minimum weight**: 0.01 (1%)
3. **Maximum weight**: 0.20 (20%)
4. **Automatic rebalancing**: Excluded attributes' weights redistribute proportionally

### Example: Security-Focused Configuration

```yaml
# Emphasize security attributes
weights:
  dependency_security: 0.15    # Default: 0.05
  secrets_management: 0.12     # Default: 0.05
  security_scanning: 0.10      # Default: 0.03
  # Other weights adjusted to sum to 1.0

excluded_attributes:
  - performance_benchmarks
```

### Example: Documentation-Focused Configuration

```yaml
# Emphasize documentation quality
weights:
  claude_md_file: 0.20         # Default: 0.10
  readme_structure: 0.15       # Default: 0.10
  inline_documentation: 0.12   # Default: 0.08
  api_documentation: 0.10      # Default: 0.05
  # Other weights adjusted to sum to 1.0
```

### Validate Configuration

```bash
# Validate configuration file
agentready --validate-config .agentready-config.yaml

# Generate example configuration
agentready --generate-config > .agentready-config.yaml
```

---

## CLI Reference

### Main Commands

#### `agentready assess PATH`

Assess a repository at the specified path.

**Arguments**:
- `PATH` ‚Äî Repository path to assess (required)

**Options**:
- `--verbose, -v` ‚Äî Show detailed progress information
- `--config FILE, -c FILE` ‚Äî Use custom configuration file
- `--output-dir DIR, -o DIR` ‚Äî Custom report output directory

**Examples**:
```bash
agentready assess .
agentready assess /path/to/repo
agentready assess . --verbose
agentready assess . --config custom.yaml
agentready assess . --output-dir ./reports
```

### Configuration Commands

#### `agentready --generate-config`

Generate example configuration file.

**Output**: Prints YAML configuration to stdout.

**Example**:
```bash
agentready --generate-config > .agentready-config.yaml
```

#### `agentready --validate-config FILE`

Validate configuration file syntax and weights.

**Example**:
```bash
agentready --validate-config .agentready-config.yaml
```

### Research Commands

#### `agentready --research-version`

Show bundled research document version.

**Example**:
```bash
agentready --research-version
# Output: Research version: 1.0.0 (2025-11-20)
```

### Utility Commands

#### `agentready --version`

Show AgentReady version.

#### `agentready --help`

Show help message with all commands.

---

## Troubleshooting

### Common Issues

#### "No module named 'agentready'"

**Cause**: AgentReady not installed or wrong Python environment.

**Solution**:
```bash
# Verify Python version
python --version  # Should be 3.11 or 3.12

# Check installation
pip list | grep agentready

# Reinstall if missing
pip install agentready
```

#### "Permission denied: .agentready/"

**Cause**: No write permissions in repository directory.

**Solution**:
```bash
# Use custom output directory
agentready assess . --output-dir ~/agentready-reports

# Or fix permissions
chmod u+w .
```

#### "Repository not found"

**Cause**: Path does not point to a git repository.

**Solution**:
```bash
# Verify git repository
git status

# If not a git repo, initialize one
git init
```

#### "Assessment taking too long"

**Cause**: Large repository with many files.

**Solution**:
AgentReady should complete in <10 seconds for most repositories. If it hangs:

1. **Check verbose output**:
   ```bash
   agentready assess . --verbose
   ```

2. **Verify git performance**:
   ```bash
   time git ls-files
   ```

3. **Report issue** with repository size and language breakdown.

#### "Invalid configuration file"

**Cause**: Malformed YAML or incorrect weight values.

**Solution**:
```bash
# Validate configuration
agentready --validate-config .agentready-config.yaml

# Check YAML syntax
python -c "import yaml; yaml.safe_load(open('.agentready-config.yaml'))"

# Regenerate from template
agentready --generate-config > .agentready-config.yaml
```

---

### Bootstrap-Specific Issues

#### "File already exists" error

**Cause**: Bootstrap refuses to overwrite existing files.

**Solution**:
Bootstrap is safe by design‚Äîit never overwrites existing files. This is expected behavior:
```bash
# Review what files already exist
ls -la .github/workflows/
ls -la .pre-commit-config.yaml

# If you want to regenerate, manually remove first
rm .github/workflows/agentready-assessment.yml
agentready bootstrap .

# Or keep existing and only add missing files
agentready bootstrap .  # Safely skips existing
```

---

#### "Language detection failed"

**Cause**: No recognizable language files in repository.

**Solution**:
```bash
# Check what files git tracks
git ls-files

# If empty, add some files first
git add *.py  # or *.js, *.go

# Force specific language
agentready bootstrap . --language python

# Or if mixed language project
agentready bootstrap . --language auto  # Uses majority language
```

---

#### "GitHub Actions not running"

**Cause**: Actions not enabled or insufficient permissions.

**Solution**:

1. **Enable Actions**:
   - Repository Settings ‚Üí Actions ‚Üí General
   - Select "Allow all actions"
   - Save

2. **Check workflow permissions**:
   - Settings ‚Üí Actions ‚Üí General ‚Üí Workflow permissions
   - Select "Read and write permissions"
   - Save

3. **Verify workflow files**:
   ```bash
   # Check files were created
   ls -la .github/workflows/

   # Validate YAML syntax
   cat .github/workflows/agentready-assessment.yml
   ```

4. **Trigger manually**:
   - Actions tab ‚Üí Select workflow ‚Üí "Run workflow"

---

#### "Pre-commit hooks not running"

**Cause**: Hooks not installed locally.

**Solution**:
```bash
# Install pre-commit framework
pip install pre-commit

# Install git hooks
pre-commit install

# Verify installation
ls -la .git/hooks/
# Should see pre-commit file

# Test hooks
pre-commit run --all-files
```

**If hooks fail:**
```bash
# Update hook versions
pre-commit autoupdate

# Clear cache
pre-commit clean

# Reinstall
pre-commit uninstall
pre-commit install
```

---

#### "Dependabot PRs not appearing"

**Cause**: Dependabot not enabled for repository or incorrect config.

**Solution**:

1. **Check Dependabot is enabled**:
   - Repository Settings ‚Üí Security & analysis
   - Enable "Dependabot alerts" and "Dependabot security updates"

2. **Verify config**:
   ```bash
   cat .github/dependabot.yml

   # Should have correct package-ecosystem:
   # - pip (for Python)
   # - npm (for JavaScript)
   # - gomod (for Go)
   ```

3. **Check for existing dependency issues**:
   - Security tab ‚Üí Dependabot
   - View pending updates

4. **Manual trigger**:
   - Wait up to 1 week for first scheduled run
   - Or manually trigger via GitHub API

---

#### "CODEOWNERS not assigning reviewers"

**Cause**: Invalid usernames or team names in CODEOWNERS.

**Solution**:
```bash
# Edit CODEOWNERS
vim .github/CODEOWNERS

# Use valid GitHub usernames (check they exist)
* @alice @bob

# Or use teams (requires org ownership)
* @myorg/team-name

# Verify syntax
# Each line: <file pattern> <owner1> <owner2>
*.py @python-experts
/docs/ @documentation-team
```

**Common mistakes:**
- Using email instead of GitHub username
- Typo in username
- Team name without org prefix (@myorg/team)
- Missing @ symbol

---

#### "Assessment workflow failing"

**Cause**: Various potential issues with workflow execution.

**Solution**:

1. **Check workflow logs**:
   - Actions tab ‚Üí Select failed run ‚Üí View logs

2. **Common failures**:

   **Python not found:**
   ```yaml
   # In .github/workflows/agentready-assessment.yml
   # Ensure correct Python version
   - uses: actions/setup-python@v4
     with:
       python-version: '3.11'  # Or '3.12'
   ```

   **AgentReady not installing:**
   ```yaml
   # Check pip install step
   - run: pip install agentready

   # Or use specific version
   - run: pip install agentready==1.1.0
   ```

   **Permission denied:**
   ```yaml
   # Add permissions to workflow
   permissions:
     contents: read
     pull-requests: write  # For PR comments
   ```

3. **Test locally**:
   ```bash
   # Run same commands as workflow
   pip install agentready
   agentready assess .
   ```

---

### Report Issues

If you encounter issues not covered here:

1. **Check GitHub Issues**: [github.com/ambient-code/agentready/issues](https://github.com/ambient-code/agentready/issues)
2. **Search Discussions**: Someone may have encountered similar problems
3. **Create New Issue**: Use the bug report template with:
   - AgentReady version (`agentready --version`)
   - Python version (`python --version`)
   - Operating system
   - Complete error message
   - Steps to reproduce

---

## Next Steps

- **[Developer Guide](developer-guide.html)** ‚Äî Learn how to contribute and extend AgentReady
- **[Attributes Reference](attributes.html)** ‚Äî Understand each of the 25 attributes
- **[API Reference](api-reference.html)** ‚Äî Integrate AgentReady into your tools
- **[Examples](examples.html)** ‚Äî See real-world assessment reports

---

**Questions?** Join the discussion on [GitHub](https://github.com/ambient-code/agentready/discussions).
````

## File: examples/self-assessment/README.md
````markdown
# AgentReady Self-Assessment

This directory contains the AgentReady tool's own assessment reports, demonstrating the output formats and serving as a reference implementation.

## Assessment Results

**Date**: 2025-11-21
**Score**: 80.0/100 (+4.6 from previous 75.4)
**Certification Level**: ü•á Gold

### Improvement Story

After implementing the `agentready bootstrap` command, we ran it on the AgentReady repository itself:

```bash
agentready bootstrap .
```

This created GitHub infrastructure files including `.pre-commit-config.yaml`, which improved our score from **75.4 ‚Üí 80.0** (+4.6 points). This demonstrates dogfooding: AgentReady successfully improved its own repository quality using its own tooling.

**Note**: Lock files assessment still fails because `uv.lock` is not yet recognized by the stub assessor (needs to add `uv.lock` to supported lock file list).

### Summary

- **Attributes Assessed**: 10/25
- **Attributes Passed**: 8/10 (80%)
- **Attributes Failed**: 2/10 (20%)
- **Attributes Skipped**: 15 (not yet implemented)
- **Duration**: 0.2 seconds

### Passing Attributes (8)

1. ‚úÖ **CLAUDE.md File** (Tier 1, 10%) - Present and comprehensive
2. ‚úÖ **README Structure** (Tier 1, 10%) - Well-structured documentation
3. ‚úÖ **Type Annotations** (Tier 1, 10%) - Python type hints coverage
4. ‚úÖ **Standard Layout** (Tier 2, 3%) - Follows src/ layout convention
5. ‚úÖ **Test Coverage** (Tier 2, 3%) - Pytest suite with unit and integration tests
6. ‚úÖ **Pre-commit Hooks** (Tier 2, 3%) - **.pre-commit-config.yaml created by bootstrap** ‚¨ÜÔ∏è **NEW**
7. ‚úÖ **Gitignore Completeness** (Tier 2, 3%) - Comprehensive .gitignore
8. ‚úÖ **Cyclomatic Complexity** (Tier 3, 1.5%) - Low complexity in codebase

### Failing Attributes (2)

1. ‚ùå **Lock Files** (Tier 1, 10%) - `uv.lock` exists but not recognized by assessor (needs implementation fix)
2. ‚ùå **Conventional Commits** (Tier 2, 3%) - Pre-commit hook exists but commitlint config not detected

### Not Applicable (15)

These attributes are stub implementations returning "not_applicable" - they will be implemented in future iterations.

## Report Formats

This directory contains three report formats generated by AgentReady:

### 1. JSON Report (`assessment-20251121-035845.json`)

Machine-readable format for:
- CI/CD integration
- Programmatic processing
- Historical tracking
- Trend analysis

**Use Case**: Parse assessment data in automation scripts or dashboards.

### 2. HTML Report (`report-20251121-035845.html`)

Interactive web report with:
- üé® Color-coded certification levels
- üîç Search and filter functionality
- üìä Visual score indicators
- üìã Collapsible finding details
- üåê Works offline (no CDN dependencies)

**Use Case**: Share with stakeholders or view in browser for detailed analysis.

**To view**: Open `report-20251121-035845.html` in any web browser.

### 3. Markdown Report (`report-20251121-035845.md`)

Version-control friendly format with:
- üìù GitHub-Flavored Markdown
- ‚úÖ Status indicators (‚úÖ‚ùå‚äò)
- üìä ASCII tables
- üéñÔ∏è Certification ladder
- üîß Prioritized next steps

**Use Case**: Commit to git for tracking improvements over time.

**To view**: View on GitHub or any Markdown renderer.

## How to Run Your Own Assessment

```bash
# Install AgentReady
pip install agentready

# Run assessment on current directory
agentready assess .

# Run with verbose output
agentready assess . --verbose

# Specify output directory
agentready assess /path/to/repo --output-dir ./reports
```

## Next Steps for AgentReady Repository

Based on this assessment, the highest-priority improvements are:

1. **Fix Lock Files Assessor** (+10 points) - Add `uv.lock` to recognized lock file list
2. **Add Commitlint Config** (+3 points) - Create `.commitlintrc.json` for conventional commit enforcement

These two changes would bring the score from **80.0** to **93.0** (üèÜ Platinum certification at 90+ threshold).

## About This Assessment

This self-assessment demonstrates:

- ‚úÖ The tool successfully assesses itself
- ‚úÖ All three report formats are generated
- ‚úÖ Assessment completes in under 1 second
- ‚úÖ Provides actionable remediation guidance
- ‚úÖ Accurately reflects repository state
- ‚úÖ **Bootstrap command improves repository quality** (+4.6 points proven)

The fact that AgentReady achieves **Gold certification** when assessing itself validates that the scoring algorithm and attribute definitions are reasonable and achievable for real-world repositories.

## Assessment History

- **2025-11-21 02:11**: Initial assessment - **75.4/100** (7 passing, 3 failing)
- **2025-11-21 03:58**: After bootstrap - **80.0/100** (8 passing, 2 failing) - **+4.6 improvement**
````

## File: examples/self-assessment/report-20251121.md
````markdown
# ü§ñ AgentReady Assessment Report

| Repository | Branch | Commit | Score | Level | Date |
|------------|--------|--------|-------|-------|------|
| **git@github.com:ambient-code/agentready.git** | main | `292ffa8d` | **80.0/100** | **Gold** | 2025-11-21 04:10 |

---

## üìä Summary

| Metric | Value |
|--------|-------|
| **Overall Score** | **80.0/100** |
| **Certification Level** | **Gold** |
| **Attributes Assessed** | 10/25 |
| **Attributes Skipped** | 15 |
| **Assessment Duration** | 0.3s |

### Languages Detected

- **Markdown**: 75 files
- **Python**: 36 files
- **YAML**: 9 files
- **Shell**: 5 files
- **JSON**: 5 files

### Repository Stats

- **Total Files**: 157
- **Total Lines**: 69,295

## üéñÔ∏è Certification Ladder

- üíé **Platinum** (90-100) 
- ü•á **Gold** (75-89) **‚Üí YOUR LEVEL ‚Üê**
- ü•à **Silver** (60-74) 
- ü•â **Bronze** (40-59) 
- ‚ö†Ô∏è **Needs Improvement** (0-39) 

## üìã Detailed Findings

### API Documentation

| Attribute | Tier | Status | Score |
|-----------|------|--------|-------|
| OpenAPI/Swagger Specifications | T3 | ‚äò not_applicable | ‚Äî |

### Build & Development

| Attribute | Tier | Status | Score |
|-----------|------|--------|-------|
| One-Command Build/Setup | T2 | ‚äò not_applicable | ‚Äî |
| Container/Virtualization Setup | T4 | ‚äò not_applicable | ‚Äî |

### Code Quality

| Attribute | Tier | Status | Score |
|-----------|------|--------|-------|
| Type Annotations | T1 | ‚úÖ pass | 100 |
| Cyclomatic Complexity Thresholds | T3 | ‚úÖ pass | 100 |
| Code Smell Elimination | T4 | ‚äò not_applicable | ‚Äî |

### Context Window Optimization

| Attribute | Tier | Status | Score |
|-----------|------|--------|-------|
| CLAUDE.md Configuration Files | T1 | ‚úÖ pass | 100 |
| Concise Structured Documentation | T2 | ‚äò not_applicable | ‚Äî |
| File Size Limits | T2 | ‚äò not_applicable | ‚Äî |

### Dependency Management

| Attribute | Tier | Status | Score |
|-----------|------|--------|-------|
| Lock Files for Reproducibility | T1 | ‚ùå fail | 0 |
| Dependency Freshness & Security | T2 | ‚äò not_applicable | ‚Äî |

#### ‚ùå Lock Files for Reproducibility

**Measured**: none (Threshold: at least one lock file)

**Evidence**:
- No lock files found

<details><summary><strong>üìù Remediation Steps</strong></summary>


Add lock file for dependency reproducibility

1. Use npm install, poetry lock, or equivalent to generate lock file

**Commands**:

```bash
npm install  # generates package-lock.json
```

</details>

### Documentation Standards

| Attribute | Tier | Status | Score |
|-----------|------|--------|-------|
| README Structure | T1 | ‚úÖ pass | 100 |
| Inline Documentation | T2 | ‚äò not_applicable | ‚Äî |
| Architecture Decision Records | T3 | ‚äò not_applicable | ‚Äî |

### Error Handling

| Attribute | Tier | Status | Score |
|-----------|------|--------|-------|
| Structured Logging | T3 | ‚äò not_applicable | ‚Äî |

### Git & Version Control

| Attribute | Tier | Status | Score |
|-----------|------|--------|-------|
| Conventional Commit Messages | T2 | ‚ùå fail | 0 |
| .gitignore Completeness | T2 | ‚úÖ pass | 100 |
| Issue & Pull Request Templates | T4 | ‚äò not_applicable | ‚Äî |

#### ‚ùå Conventional Commit Messages

**Measured**: not configured (Threshold: configured)

**Evidence**:
- No commitlint or husky configuration

<details><summary><strong>üìù Remediation Steps</strong></summary>


Configure conventional commits with commitlint

1. Install commitlint
2. Configure husky for commit-msg hook

**Commands**:

```bash
npm install --save-dev @commitlint/cli @commitlint/config-conventional husky
```

</details>

### Modularity

| Attribute | Tier | Status | Score |
|-----------|------|--------|-------|
| Semantic File & Directory Naming | T3 | ‚äò not_applicable | ‚Äî |

### Performance

| Attribute | Tier | Status | Score |
|-----------|------|--------|-------|
| Performance Benchmarks | T4 | ‚äò not_applicable | ‚Äî |

### Repository Structure

| Attribute | Tier | Status | Score |
|-----------|------|--------|-------|
| Standard Project Layouts | T1 | ‚úÖ pass | 100 |
| Separation of Concerns | T2 | ‚äò not_applicable | ‚Äî |

### Security

| Attribute | Tier | Status | Score |
|-----------|------|--------|-------|
| Security Scanning Automation | T4 | ‚äò not_applicable | ‚Äî |

### Testing & CI/CD

| Attribute | Tier | Status | Score |
|-----------|------|--------|-------|
| Test Coverage Requirements | T2 | ‚úÖ pass | 100 |
| Pre-commit Hooks & CI/CD Linting | T2 | ‚úÖ pass | 100 |

## üéØ Next Steps

**Priority Improvements** (highest impact first):

1. **Lock Files for Reproducibility** (Tier 1) - +10.0 points potential
   - Add lock file for dependency reproducibility
2. **Conventional Commit Messages** (Tier 2) - +3.0 points potential
   - Configure conventional commits with commitlint

---

## üìù Assessment Metadata

- **Tool Version**: AgentReady v1.0.0
- **Research Report**: Bundled version
- **Repository Snapshot**: 292ffa8d5541b2edf545fc492c9367d15675f932
- **Assessment Duration**: 0.3s

ü§ñ Generated with [Claude Code](https://claude.com/claude-code)
````

## File: specs/001-agentready-scorer/tasks.md
````markdown
# Tasks: AgentReady Repository Scorer

**Input**: Design documents from `/specs/001-agentready-scorer/`
**Prerequisites**: plan.md, spec.md, research.md, data-model.md, contracts/

**Tests**: NOT REQUESTED - Implementation follows library-first + CLI wrapper pattern per constitution Principle IV

**Organization**: Tasks are grouped by user story to enable independent implementation and testing of each story.

## Format: `[ID] [P?] [Story] Description`

- **[P]**: Can run in parallel (different files, no dependencies)
- **[Story]**: Which user story this task belongs to (e.g., US1, US2, US3)
- Include exact file paths in descriptions

## Path Conventions

- **Single project**: `src/agentready/`, `tests/` at repository root per plan.md

---

## Phase 1: Setup (Shared Infrastructure)

**Purpose**: Project initialization and basic structure

- [X] T001 Create project directory structure matching plan.md (src/agentready/{cli,assessors,models,services,reporters,templates,data})
- [X] T002 Initialize Python project with pyproject.toml including Click, Jinja2, PyYAML, gitpython, radon, lizard, pytest dependencies
- [X] T003 [P] Configure black, isort, flake8, and pytest-cov in pyproject.toml per constitution
- [X] T004 [P] Create README.md with project overview and installation instructions
- [X] T005 [P] Copy agent-ready-codebase-attributes.md to src/agentready/data/
- [X] T006 [P] Create .agentready-config.example.yaml with documented weight customization examples
- [X] T007 [P] Create src/agentready/data/default-weights.yaml with tier-based weight distribution per research.md

---

## Phase 2: Foundational (Blocking Prerequisites)

**Purpose**: Core infrastructure that MUST be complete before ANY user story can be implemented

**‚ö†Ô∏è CRITICAL**: No user story work can begin until this phase is complete

- [X] T008 [P] Create Repository model in src/agentready/models/repository.py (path, name, url, branch, commit_hash, languages, total_files, total_lines)
- [X] T009 [P] Create Attribute model in src/agentready/models/attribute.py (id, name, category, tier, description, criteria, default_weight)
- [X] T010 [P] Create Finding model in src/agentready/models/finding.py (attribute, status, score, measured_value, threshold, evidence, remediation, error_message)
- [X] T011 [P] Create Citation model in src/agentready/models/citation.py (source, title, url, relevance)
- [X] T012 [P] Create Remediation model (inline in finding.py) (summary, steps, tools, commands, examples, citations)
- [X] T013 [P] Create Config model in src/agentready/models/config.py (weights, excluded_attributes, language_overrides, output_dir)
- [X] T014 Create Assessment model in src/agentready/models/assessment.py (repository, timestamp, overall_score, certification_level, attributes_assessed, attributes_skipped, attributes_total, findings, config, duration_seconds)
- [X] T015 [P] Create BaseAssessor abstract interface in src/agentready/assessors/base.py with attribute_id, tier, assess(), is_applicable() methods
- [X] T016 [P] Implement LanguageDetector service in src/agentready/services/language_detector.py (file extension mapping, gitignore awareness)
- [X] T017 [P] Implement ResearchLoader service in src/agentready/services/research_loader.py (load bundled report, validate structure, support updates)
- [X] T018 Create Scorer service in src/agentready/services/scorer.py (tier-based weight calculation, certification level determination, score normalization)
- [X] T019 Create Scanner service in src/agentready/services/scanner.py (orchestrate assessment workflow, try-assess-skip error handling, progress tracking)

**Checkpoint**: Foundation ready - user story implementation can now begin in parallel

---

## Phase 3: User Story 1 - Score Repository Against Agent-Ready Criteria (Priority: P1) üéØ MVP

**Goal**: Assess repository against all 25 attributes and generate overall agent-readiness score with findings

**Independent Test**: Run agentready on any git repository and verify it produces valid JSON assessment data with score, certification level, and findings for all 25 attributes

### Implementation for User Story 1

#### Assessor Implementations (12 category assessors for 25 attributes)

- [ ] T020 [P] [US1] Implement DocumentationAssessor in src/agentready/assessors/documentation.py (attributes 1.1-2.3: CLAUDE.md, README size, docstrings, ADRs)
- [ ] T021 [P] [US1] Implement CodeQualityAssessor in src/agentready/assessors/code_quality.py (attributes 3.1-3.4: cyclomatic complexity, file length, type annotations, code smells)
- [ ] T022 [P] [US1] Implement StructureAssessor in src/agentready/assessors/structure.py (attributes 4.1-4.2: layout patterns, separation of concerns)
- [ ] T023 [P] [US1] Implement TestingAssessor in src/agentready/assessors/testing.py (attributes 5.1-5.3: coverage, naming, pre-commit hooks)
- [ ] T024 [P] [US1] Implement DependenciesAssessor in src/agentready/assessors/dependencies.py (attributes 6.1-6.2: lock files, freshness/security)
- [ ] T025 [P] [US1] Implement VCSAssessor in src/agentready/assessors/vcs.py (attributes 7.1-7.3: conventional commits, gitignore, PR/issue templates)
- [ ] T026 [P] [US1] Implement BuildAssessor in src/agentready/assessors/build.py (attributes 8.1-8.3: one-command setup, build docs, containerization)
- [ ] T027 [P] [US1] Implement ErrorsAssessor in src/agentready/assessors/errors.py (attributes 9.1-9.2: error clarity, structured logging)
- [ ] T028 [P] [US1] Implement APIDocsAssessor in src/agentready/assessors/api_docs.py (attributes 10.1-10.2: OpenAPI/Swagger, GraphQL schemas)
- [ ] T029 [P] [US1] Implement ModularityAssessor in src/agentready/assessors/modularity.py (attributes 11.1-11.3: DRY violations, naming consistency, semantic files)
- [ ] T030 [P] [US1] Implement CICDAssessor in src/agentready/assessors/cicd.py (attributes 12.1-12.2: pipeline visibility, branch protection)
- [ ] T031 [P] [US1] Implement SecurityAssessor in src/agentready/assessors/security.py (attributes 13.1-13.2: security scanning, secrets management)
- [ ] T032 [P] [US1] Implement PerformanceAssessor in src/agentready/assessors/performance.py (attribute 15.1: performance benchmarks)

#### Core Workflow

- [ ] T033 [US1] Integrate all 13 assessors into Scanner service with parallel/sequential execution strategy
- [ ] T034 [US1] Implement Repository validation in Scanner (check for .git directory per FR-017)
- [ ] T035 [US1] Implement graceful degradation with MissingToolError, PermissionError handling per research.md
- [ ] T036 [US1] Implement progress tracking with assessed/skipped/error counts (FR-018, FR-028)
- [ ] T037 [US1] Implement weighted score calculation with tier-based defaults (FR-004, FR-031)
- [ ] T038 [US1] Implement certification level mapping (Platinum/Gold/Silver/Bronze/Needs Improvement per FR-016)
- [ ] T039 [US1] Add JSON serialization for Assessment model using dataclasses.asdict() per data-model.md
- [ ] T040 [US1] Implement Config loading from .agentready-config.yaml with weight validation (FR-032, FR-033)

#### CLI Interface

- [ ] T041 [US1] Create Click CLI entry point in src/agentready/cli/main.py with repository path argument
- [ ] T042 [US1] Add --verbose flag for detailed logging (FR-029)
- [ ] T043 [US1] Add --output-dir flag for custom report location (FR-021)
- [ ] T044 [US1] Add --config flag for custom configuration file path
- [ ] T045 [US1] Implement stdout progress summary and stderr error output (FR-028, FR-030)
- [ ] T046 [US1] Add --version flag showing agentready and research report versions

**Checkpoint**: At this point, User Story 1 should produce complete JSON assessment with score and findings

---

## Phase 4: User Story 2 - View Interactive HTML Report (Priority: P2)

**Goal**: Generate self-contained HTML report with interactive filtering, sorting, and collapsible sections

**Independent Test**: Generate HTML report and verify it opens in browser with working filters, sort controls, and expandable attribute details (no internet required)

### Implementation for User Story 2

- [X] T047 [P] [US2] Create Jinja2 template in src/agentready/templates/report.html.j2 with structure from contracts/report-html-schema.md
- [X] T048 [P] [US2] Add embedded CSS in template (header, controls, summary cards, findings, certification ladder, footer)
- [X] T049 [P] [US2] Add embedded JavaScript in template (filtering by status, sorting by score/tier/category, search, smooth scroll)
- [X] T050 [P] [US2] Implement BaseReporter interface in src/agentready/reporters/base.py with generate() method
- [X] T051 [US2] Implement HTMLReporter in src/agentready/reporters/html.py extending BaseReporter
- [X] T052 [US2] Add Jinja2 template rendering with Assessment data embedded as JSON constant
- [X] T053 [US2] Implement collapsible sections using HTML5 details/summary elements per contracts/report-html-schema.md
- [X] T054 [US2] Add color-coded score indicators (red <40, yellow 40-74, green 75+) per contracts/report-html-schema.md
- [X] T055 [US2] Add tier badges with color coding (Tier 1 red, Tier 2 orange, Tier 3 yellow, Tier 4 green)
- [X] T056 [US2] Implement category summary cards with progress bars and click-to-scroll behavior
- [X] T057 [US2] Add filter controls (All, Pass, Fail, Skipped) with dynamic count badges
- [X] T058 [US2] Add sort dropdown (Category, Score Asc/Desc, Tier) with JavaScript implementation
- [X] T059 [US2] Add search box with real-time attribute filtering
- [X] T060 [US2] Implement certification ladder with active level highlighting
- [X] T061 [US2] Add responsive breakpoints for mobile/tablet/desktop per contracts/report-html-schema.md
- [X] T062 [US2] Integrate HTMLReporter into Scanner workflow (dual-format generation per FR-005, FR-019)
- [X] T063 [US2] Add timestamp-based filename with latest symlink per research.md (report-2025-11-20T14-30-00.html)
- [X] T064 [US2] Verify HTML report works offline with no CDN dependencies (FR-003, SC-003)

**Checkpoint**: At this point, User Stories 1 AND 2 should produce both JSON and interactive HTML

---

## Phase 5: User Story 3 - Receive Remediation Guidance (Priority: P3)

**Goal**: For each failing attribute, provide actionable remediation with tools, commands, examples, and research citations

**Independent Test**: Review remediation sections in reports for failing attributes and verify they include specific tools, executable commands, and links to documentation

### Implementation for User Story 3

- [ ] T065 [P] [US3] Create remediation templates in src/agentready/assessors/base.py (summary, steps, tools, commands, examples, citations)
- [ ] T066 [P] [US3] Extract remediation guidance from agent-ready-codebase-attributes.md per attribute
- [ ] T067 [P] [US3] Add language-specific remediation commands (Python: black/pytest-cov, JS: eslint/jest, etc.)
- [ ] T068 [P] [US3] Add tool installation instructions in remediation.tools field (pip install, npm install, etc.)
- [ ] T069 [US3] Implement remediation generation in each assessor's assess() method for failing attributes
- [ ] T070 [US3] Add priority ranking in "Next Steps" section based on tier and point potential
- [ ] T071 [US3] Include research report citations in remediation.citations with source, title, url, relevance
- [ ] T072 [US3] Add code examples in remediation.examples showing before/after transformations
- [ ] T073 [US3] Update HTML template to display remediation in collapsible details blocks per contracts/report-html-schema.md
- [ ] T074 [US3] Add syntax highlighting for code blocks in remediation commands/examples using CSS
- [ ] T075 [US3] Verify all citations from research report preserved in outputs (SC-009)

**Checkpoint**: All user stories should now provide complete scoring, interactive viewing, and actionable remediation

---

## Phase 6: User Story 4 - Read Markdown Report for Version Control (Priority: P4)

**Goal**: Generate GitHub-Flavored Markdown report that renders properly in version control platforms and enables diff-based progress tracking

**Independent Test**: Generate Markdown report, commit to git, and verify it renders with tables, formatting, and links intact on GitHub

### Implementation for User Story 4

- [ ] T076 [P] [US4] Implement MarkdownReporter in src/agentready/reporters/markdown.py extending BaseReporter
- [ ] T077 [US4] Generate header section with repository metadata, timestamp, overall score per contracts/report-markdown-schema.md
- [ ] T078 [US4] Generate summary table with categories, scores, status emoji (‚úÖ‚ö†Ô∏è‚ùå)
- [ ] T079 [US4] Generate detailed findings with attribute subsections using proper heading hierarchy
- [ ] T080 [US4] Add collapsible remediation using details/summary HTML tags (valid in GFM)
- [ ] T081 [US4] Format evidence as bulleted lists with emoji indicators
- [ ] T082 [US4] Format remediation steps as ordered lists with code blocks (bash, python, yaml syntax highlighting)
- [ ] T083 [US4] Add certification level section with score ladder and "YOUR LEVEL" marker
- [ ] T084 [US4] Add "Next Steps" section with top 3-5 improvements ranked by tier and point potential
- [ ] T085 [US4] Generate footer with assessment metadata (duration, research version, repository snapshot)
- [ ] T086 [US4] Preserve all external citation links with absolute URLs per contracts/report-markdown-schema.md
- [ ] T087 [US4] Integrate MarkdownReporter into Scanner workflow (dual-format atomic generation per FR-019)
- [ ] T088 [US4] Add timestamp-based filename with latest symlink (report-2025-11-20T14-30-00.md)
- [ ] T089 [US4] Verify Markdown renders correctly on GitHub/GitLab/Bitbucket (SC-004)

**Checkpoint**: All four user stories complete - MVP delivers scoring, HTML viewing, remediation, and version control

---

## Phase 7: Polish & Cross-Cutting Concerns

**Purpose**: Improvements that affect multiple user stories

- [ ] T090 [P] Create comprehensive unit tests in tests/unit/ for models, services, assessors (>80% coverage per constitution)
- [ ] T091 [P] Create integration tests in tests/integration/test_scan_workflow.py using fixture repositories per research.md
- [ ] T092 [P] Create contract tests in tests/contract/ validating report schemas against contracts/
- [ ] T093 [P] Create test fixtures in tests/fixtures/repositories/ (minimal-python, gold-standard-python, polyglot, edge-cases)
- [ ] T094 Add --update-research CLI command to download latest research report (FR-023, FR-024)
- [ ] T095 Add --generate-config CLI command to create example .agentready-config.yaml
- [ ] T096 Add --validate-config CLI command to check configuration file validity (FR-033)
- [ ] T097 Implement report cleanup logic (keep last N runs) per research.md
- [ ] T098 Add JSON sidecar files for automation integration (assessment-{timestamp}.json)
- [ ] T099 Optimize file system scanning with caching per research.md (don't traverse multiple times)
- [ ] T100 Add performance benchmarks ensuring <5 minutes for <10k files (SC-001)
- [ ] T101 Verify deterministic scoring (same input ‚Üí same output) (SC-010)
- [ ] T102 Add language-specific analyzer lazy loading per research.md
- [ ] T103 [P] Update README.md with full usage documentation and examples
- [ ] T104 [P] Validate quickstart.md instructions work end-to-end (<5 minutes install to report)
- [ ] T105 Run black, isort, flake8 on entire codebase per constitution
- [ ] T106 Run pytest with coverage report ensuring >80% test coverage
- [ ] T107 Create package distribution configuration for PyPI (setup.py or pyproject.toml build config)
- [ ] T108 Add entry point configuration for `agentready` command in pyproject.toml

---

## Dependencies & Execution Order

### Phase Dependencies

- **Setup (Phase 1)**: No dependencies - can start immediately
- **Foundational (Phase 2)**: Depends on Setup completion - BLOCKS all user stories
- **User Stories (Phase 3-6)**: All depend on Foundational phase completion
  - User Story 1 (P1): Scoring engine - Can start after Phase 2
  - User Story 2 (P2): HTML reports - Depends on US1 JSON output
  - User Story 3 (P3): Remediation - Depends on US1 findings, enhances US2 HTML
  - User Story 4 (P4): Markdown reports - Depends on US1 JSON output
- **Polish (Phase 7)**: Depends on all desired user stories being complete

### User Story Dependencies

- **User Story 1 (P1)**: FOUNDATIONAL - Scoring engine required by all other stories
- **User Story 2 (P2)**: Consumes US1 JSON output, independently adds HTML generation
- **User Story 3 (P3)**: Enhances US1 findings and US2/US4 reports with remediation
- **User Story 4 (P4)**: Consumes US1 JSON output, independently adds Markdown generation

### Within Each User Story

- **US1**: Models ‚Üí Services ‚Üí Assessors ‚Üí Scanner ‚Üí CLI (sequential dependencies)
- **US2**: Template ‚Üí Reporter ‚Üí Integration (sequential)
- **US3**: Remediation templates ‚Üí Assessor integration ‚Üí Report integration (sequential)
- **US4**: Markdown reporter ‚Üí Integration (sequential)

### Parallel Opportunities

- **Phase 1 (Setup)**: All tasks [P] can run in parallel (T003-T007)
- **Phase 2 (Foundational)**: All model creation tasks (T008-T014) can run in parallel, all service tasks (T016-T019) can run in parallel
- **Phase 3 (US1)**: All 13 assessor implementations (T020-T032) can run fully in parallel (different files)
- **Phase 4 (US2)**: Template design (T047-T049) can run in parallel, reporter features (T051-T064) sequential
- **Phase 5 (US3)**: Remediation extraction (T065-T068) can run in parallel
- **Phase 6 (US4)**: Markdown sections (T076-T086) largely parallelizable
- **Phase 7 (Polish)**: Test creation (T090-T093) can run in parallel, documentation (T103-T104) can run in parallel

**Note**: User Story 2 and User Story 4 could potentially be worked on in parallel after US1 completes, as both independently transform US1's JSON output to different formats.

---

## Parallel Example: User Story 1 Assessors

```bash
# Launch all 13 assessor implementations together:
T020: "Implement DocumentationAssessor in src/agentready/assessors/documentation.py"
T021: "Implement CodeQualityAssessor in src/agentready/assessors/code_quality.py"
T022: "Implement StructureAssessor in src/agentready/assessors/structure.py"
T023: "Implement TestingAssessor in src/agentready/assessors/testing.py"
T024: "Implement DependenciesAssessor in src/agentready/assessors/dependencies.py"
T025: "Implement VCSAssessor in src/agentready/assessors/vcs.py"
T026: "Implement BuildAssessor in src/agentready/assessors/build.py"
T027: "Implement ErrorsAssessor in src/agentready/assessors/errors.py"
T028: "Implement APIDocsAssessor in src/agentready/assessors/api_docs.py"
T029: "Implement ModularityAssessor in src/agentready/assessors/modularity.py"
T030: "Implement CICDAssessor in src/agentready/assessors/cicd.py"
T031: "Implement SecurityAssessor in src/agentready/assessors/security.py"
T032: "Implement PerformanceAssessor in src/agentready/assessors/performance.py"
```

---

## Implementation Strategy

### MVP First (User Story 1 Only)

1. Complete Phase 1: Setup (T001-T007)
2. Complete Phase 2: Foundational (T008-T019) - CRITICAL
3. Complete Phase 3: User Story 1 (T020-T046)
4. **STOP and VALIDATE**: Run agentready on test repository, verify JSON output
5. Optionally proceed to US2 (HTML) or US4 (Markdown) for human-readable reports

**Rationale**: US1 alone provides immediate value - repository owners get a comprehensive score with findings. HTML/Markdown reports enhance UX but core assessment functionality works without them.

### Incremental Delivery

1. **Sprint 1**: Setup + Foundational ‚Üí Can run assessments programmatically
2. **Sprint 2**: User Story 1 ‚Üí MVP with JSON output + CLI
3. **Sprint 3**: User Story 2 ‚Üí Add interactive HTML reports
4. **Sprint 4**: User Story 3 ‚Üí Add actionable remediation guidance
5. **Sprint 5**: User Story 4 ‚Üí Add version-control-friendly Markdown reports
6. **Sprint 6**: Polish ‚Üí Testing, optimization, documentation

### Parallel Team Strategy

With 3-4 developers after Foundational phase:

1. **Developer A**: User Story 1 (scoring engine)
2. **Developer B**: User Story 2 (HTML reporter) - starts after US1 JSON structure defined
3. **Developer C**: User Story 4 (Markdown reporter) - starts after US1 JSON structure defined
4. **Developer D**: User Story 3 (remediation) - coordinates with A/B/C to enhance outputs

**Critical Path**: US1 is on critical path (blocks US2/US3/US4). Parallelizing assessor implementations (T020-T032) is key to reducing US1 duration.

---

## Notes

- [P] tasks = different files, no dependencies, safe to parallelize
- [Story] label maps task to specific user story (US1, US2, US3, US4)
- Tests NOT included per constitution (TDD requested but not in spec)
- Each assessor is independently implementable (strategy pattern per research.md)
- Constitution compliance: library-first (assessors/services/models standalone), tool-second (CLI wrapper)
- File paths follow plan.md structure exactly
- Checkpoint after each user story enables incremental validation
- US1 assessors (T020-T032) are largest parallelization opportunity (13 independent files)
````

## File: src/agentready/assessors/structure.py
````python
"""Structure assessors for project layout and separation of concerns."""

from pathlib import Path

from ..models.attribute import Attribute
from ..models.finding import Citation, Finding, Remediation
from ..models.repository import Repository
from .base import BaseAssessor


class StandardLayoutAssessor(BaseAssessor):
    """Assesses standard project layout patterns.

    Tier 1 Essential (10% weight) - Standard layouts help AI navigate code.
    """

    @property
    def attribute_id(self) -> str:
        return "standard_layout"

    @property
    def tier(self) -> int:
        return 1  # Essential

    @property
    def attribute(self) -> Attribute:
        return Attribute(
            id=self.attribute_id,
            name="Standard Project Layouts",
            category="Repository Structure",
            tier=self.tier,
            description="Follows standard project structure for language",
            criteria="Standard directories (src/, tests/, docs/) present",
            default_weight=0.10,
        )

    def assess(self, repository: Repository) -> Finding:
        """Check for standard project layout directories.

        Expected patterns:
        - Python: src/, tests/, docs/
        - JavaScript: src/, test/, docs/
        - Java: src/main/java, src/test/java
        """
        # Check for common standard directories
        standard_dirs = {
            "src": repository.path / "src",
        }

        # Check for tests directory (either tests/ or test/)
        tests_path = repository.path / "tests"
        if not tests_path.exists():
            tests_path = repository.path / "test"
        standard_dirs["tests"] = tests_path

        found_dirs = sum(1 for d in standard_dirs.values() if d.exists())
        required_dirs = len(standard_dirs)

        score = self.calculate_proportional_score(
            measured_value=found_dirs,
            threshold=required_dirs,
            higher_is_better=True,
        )

        status = "pass" if score >= 75 else "fail"

        evidence = [
            f"Found {found_dirs}/{required_dirs} standard directories",
            f"src/: {'‚úì' if (repository.path / 'src').exists() else '‚úó'}",
            f"tests/: {'‚úì' if (repository.path / 'tests').exists() or (repository.path / 'test').exists() else '‚úó'}",
        ]

        return Finding(
            attribute=self.attribute,
            status=status,
            score=score,
            measured_value=f"{found_dirs}/{required_dirs} directories",
            threshold=f"{required_dirs}/{required_dirs} directories",
            evidence=evidence,
            remediation=self._create_remediation() if status == "fail" else None,
            error_message=None,
        )

    def _create_remediation(self) -> Remediation:
        """Create remediation guidance for standard layout."""
        return Remediation(
            summary="Organize code into standard directories (src/, tests/, docs/)",
            steps=[
                "Create src/ directory for source code",
                "Create tests/ directory for test files",
                "Create docs/ directory for documentation",
                "Move source code into src/",
                "Move tests into tests/",
            ],
            tools=[],
            commands=[
                "mkdir -p src tests docs",
                "# Move source files to src/",
                "# Move test files to tests/",
            ],
            examples=[],
            citations=[
                Citation(
                    source="Python Packaging Authority",
                    title="Python Project Structure",
                    url="https://packaging.python.org/en/latest/tutorials/packaging-projects/",
                    relevance="Standard Python project layout",
                )
            ],
        )
````

## File: src/agentready/reporters/html.py
````python
"""HTML reporter for generating interactive assessment reports."""

import json
from pathlib import Path

from jinja2 import Environment, PackageLoader, select_autoescape

from ..models.assessment import Assessment
from .base import BaseReporter


class HTMLReporter(BaseReporter):
    """Generates self-contained interactive HTML reports.

    Features:
    - Filter by status (pass/fail/skipped)
    - Sort by category, score, or tier
    - Search attributes by name
    - Collapsible finding details
    - Color-coded scores and tiers
    - Certification ladder visualization
    - Works offline (no CDN dependencies)
    """

    def __init__(self):
        """Initialize HTML reporter with Jinja2 environment."""
        self.env = Environment(
            loader=PackageLoader("agentready", "templates"),
            autoescape=select_autoescape(["html", "xml", "j2"]),
        )

    def generate(self, assessment: Assessment, output_path: Path) -> Path:
        """Generate HTML report from assessment data.

        Args:
            assessment: Complete assessment with findings
            output_path: Path where HTML file should be saved

        Returns:
            Path to generated HTML file

        Raises:
            IOError: If HTML cannot be written
        """
        # Load template
        template = self.env.get_template("report.html.j2")

        # Prepare data for template
        template_data = {
            "repository": assessment.repository,
            "timestamp": assessment.timestamp,
            "overall_score": assessment.overall_score,
            "certification_level": assessment.certification_level,
            "attributes_assessed": assessment.attributes_assessed,
            "attributes_skipped": assessment.attributes_skipped,
            "attributes_total": assessment.attributes_total,
            "findings": assessment.findings,
            "duration_seconds": assessment.duration_seconds,
            "config": assessment.config,
            # Embed assessment JSON for JavaScript
            "assessment_json": json.dumps(assessment.to_dict()),
        }

        # Render template
        html_content = template.render(**template_data)

        # Write to file
        output_path.parent.mkdir(parents=True, exist_ok=True)
        with open(output_path, "w", encoding="utf-8") as f:
            f.write(html_content)

        return output_path
````

## File: src/agentready/reporters/markdown.py
````python
"""Markdown reporter for generating version-control-friendly assessment reports."""

from pathlib import Path

from ..models.assessment import Assessment
from .base import BaseReporter


class MarkdownReporter(BaseReporter):
    """Generates GitHub-Flavored Markdown reports.

    Features:
    - Version-control friendly (git diff shows progress)
    - Renders properly on GitHub/GitLab/Bitbucket
    - Tables for summary data
    - Collapsible details using HTML details/summary
    - Code blocks with syntax highlighting
    - Emoji indicators for status
    """

    def generate(self, assessment: Assessment, output_path: Path) -> Path:
        """Generate Markdown report from assessment data.

        Args:
            assessment: Complete assessment with findings
            output_path: Path where Markdown file should be saved

        Returns:
            Path to generated Markdown file

        Raises:
            IOError: If Markdown cannot be written
        """
        sections = []

        # Header
        sections.append(self._generate_header(assessment))

        # Summary
        sections.append(self._generate_summary(assessment))

        # Certification Ladder
        sections.append(self._generate_certification_ladder(assessment))

        # Findings by Category
        sections.append(self._generate_findings(assessment))

        # Next Steps
        sections.append(self._generate_next_steps(assessment))

        # Footer
        sections.append(self._generate_footer(assessment))

        # Combine all sections
        markdown_content = "\n\n".join(sections)

        # Write to file
        output_path.parent.mkdir(parents=True, exist_ok=True)
        with open(output_path, "w", encoding="utf-8") as f:
            f.write(markdown_content)

        return output_path

    def _generate_header(self, assessment: Assessment) -> str:
        """Generate report header with repository info."""
        # Get git remote URL if available, otherwise use repo name
        repo_display = (
            assessment.repository.url
            if assessment.repository.url
            else assessment.repository.name
        )

        return f"""# ü§ñ AgentReady Assessment Report

| Repository | Branch | Commit | Score | Level | Date |
|------------|--------|--------|-------|-------|------|
| **{repo_display}** | {assessment.repository.branch} | `{assessment.repository.commit_hash[:8]}` | **{assessment.overall_score:.1f}/100** | **{assessment.certification_level}** | {assessment.timestamp.strftime('%Y-%m-%d %H:%M')} |

---"""

    def _generate_summary(self, assessment: Assessment) -> str:
        """Generate summary section with key metrics."""
        return f"""## üìä Summary

| Metric | Value |
|--------|-------|
| **Overall Score** | **{assessment.overall_score:.1f}/100** |
| **Certification Level** | **{assessment.certification_level}** |
| **Attributes Assessed** | {assessment.attributes_assessed}/{assessment.attributes_total} |
| **Attributes Skipped** | {assessment.attributes_skipped} |
| **Assessment Duration** | {assessment.duration_seconds:.1f}s |

### Languages Detected

{self._format_languages(assessment.repository.languages)}

### Repository Stats

- **Total Files**: {assessment.repository.total_files:,}
- **Total Lines**: {assessment.repository.total_lines:,}"""

    def _format_languages(self, languages: dict[str, int]) -> str:
        """Format language detection results."""
        if not languages:
            return "No languages detected"

        lines = []
        for lang, count in sorted(languages.items(), key=lambda x: x[1], reverse=True):
            lines.append(f"- **{lang}**: {count} files")
        return "\n".join(lines)

    def _generate_certification_ladder(self, assessment: Assessment) -> str:
        """Generate certification ladder visualization."""
        levels = [
            ("Platinum", "üíé", "90-100", assessment.certification_level == "Platinum"),
            ("Gold", "ü•á", "75-89", assessment.certification_level == "Gold"),
            ("Silver", "ü•à", "60-74", assessment.certification_level == "Silver"),
            ("Bronze", "ü•â", "40-59", assessment.certification_level == "Bronze"),
            (
                "Needs Improvement",
                "‚ö†Ô∏è",
                "0-39",
                assessment.certification_level == "Needs Improvement",
            ),
        ]

        lines = ["## üéñÔ∏è Certification Ladder", ""]
        for name, emoji, range_str, is_active in levels:
            marker = "**‚Üí YOUR LEVEL ‚Üê**" if is_active else ""
            lines.append(f"- {emoji} **{name}** ({range_str}) {marker}")

        return "\n".join(lines)

    def _generate_findings(self, assessment: Assessment) -> str:
        """Generate detailed findings by category."""
        sections = ["## üìã Detailed Findings"]

        # Group findings by category
        by_category = {}
        for finding in assessment.findings:
            category = finding.attribute.category
            if category not in by_category:
                by_category[category] = []
            by_category[category].append(finding)

        # Generate section for each category
        for category in sorted(by_category.keys()):
            findings = by_category[category]
            sections.append(f"\n### {category}")

            # Summary table for this category
            sections.append("\n| Attribute | Tier | Status | Score |")
            sections.append("|-----------|------|--------|-------|")

            for finding in findings:
                status_emoji = self._get_status_emoji(finding.status)
                score_display = (
                    f"{finding.score:.0f}" if finding.score is not None else "‚Äî"
                )
                tier_badge = f"T{finding.attribute.tier}"

                sections.append(
                    f"| {finding.attribute.name} | {tier_badge} | {status_emoji} {finding.status} | {score_display} |"
                )

            # Detailed findings (only for fail/error)
            for finding in findings:
                if finding.status in ("fail", "error"):
                    sections.append(self._generate_finding_detail(finding))

        return "\n".join(sections)

    def _get_status_emoji(self, status: str) -> str:
        """Get emoji for finding status."""
        emoji_map = {
            "pass": "‚úÖ",
            "fail": "‚ùå",
            "skipped": "‚äò",
            "not_applicable": "‚äò",
            "error": "‚ö†Ô∏è",
        }
        return emoji_map.get(status, "‚ùì")

    def _generate_finding_detail(self, finding) -> str:
        """Generate detailed finding section."""
        lines = [
            f"\n#### {self._get_status_emoji(finding.status)} {finding.attribute.name}"
        ]

        # Basic info
        if finding.measured_value:
            lines.append(
                f"\n**Measured**: {finding.measured_value} (Threshold: {finding.threshold})"
            )

        # Evidence
        if finding.evidence:
            lines.append("\n**Evidence**:")
            for item in finding.evidence:
                lines.append(f"- {item}")

        # Remediation
        if finding.remediation:
            lines.append(
                "\n<details><summary><strong>üìù Remediation Steps</strong></summary>\n"
            )
            lines.append(f"\n{finding.remediation.summary}\n")

            if finding.remediation.steps:
                for i, step in enumerate(finding.remediation.steps, 1):
                    lines.append(f"{i}. {step}")

            if finding.remediation.commands:
                lines.append("\n**Commands**:\n")
                lines.append("```bash")
                lines.append("\n".join(finding.remediation.commands))
                lines.append("```")

            if finding.remediation.examples:
                lines.append("\n**Examples**:\n")
                for example in finding.remediation.examples:
                    lines.append("```")
                    lines.append(example)
                    lines.append("```")

            lines.append("\n</details>")

        # Error message
        if finding.error_message:
            lines.append(f"\n**Error**: {finding.error_message}")

        return "\n".join(lines)

    def _generate_next_steps(self, assessment: Assessment) -> str:
        """Generate prioritized next steps based on failures."""
        # Find all failing attributes
        failures = [
            f for f in assessment.findings if f.status == "fail" and f.score is not None
        ]

        if not failures:
            return """## ‚ú® Next Steps

**Congratulations!** All assessed attributes are passing. Consider:
- Implementing currently skipped attributes
- Maintaining these standards as the codebase evolves"""

        # Sort by tier (lower tier = higher priority) and score (lower score = more important)
        failures.sort(key=lambda f: (f.attribute.tier, f.score or 0))

        lines = [
            "## üéØ Next Steps",
            "",
            "**Priority Improvements** (highest impact first):",
            "",
        ]

        for i, finding in enumerate(failures[:5], 1):  # Top 5 only
            potential_points = finding.attribute.default_weight * 100
            lines.append(
                f"{i}. **{finding.attribute.name}** (Tier {finding.attribute.tier}) - "
                f"+{potential_points:.1f} points potential"
            )
            if finding.remediation:
                lines.append(f"   - {finding.remediation.summary}")

        return "\n".join(lines)

    def _generate_footer(self, assessment: Assessment) -> str:
        """Generate report footer."""
        return f"""---

## üìù Assessment Metadata

- **Tool Version**: AgentReady v1.0.0
- **Research Report**: Bundled version
- **Repository Snapshot**: {assessment.repository.commit_hash}
- **Assessment Duration**: {assessment.duration_seconds:.1f}s

ü§ñ Generated with [Claude Code](https://claude.com/claude-code)"""
````

## File: src/agentready/services/bootstrap.py
````python
"""Bootstrap generator for repository infrastructure."""

from pathlib import Path
from typing import List

from jinja2 import Environment, PackageLoader, select_autoescape

from .language_detector import LanguageDetector


class BootstrapGenerator:
    """Generates GitHub infrastructure files for repository."""

    def __init__(self, repo_path: Path, language: str = "auto"):
        """Initialize bootstrap generator.

        Args:
            repo_path: Path to repository
            language: Primary language or "auto" to detect
        """
        self.repo_path = repo_path
        self.language = self._detect_language(language)
        self.env = Environment(
            loader=PackageLoader("agentready", "templates/bootstrap"),
            autoescape=select_autoescape(["html", "xml", "j2", "yaml", "yml"]),
            trim_blocks=True,
            lstrip_blocks=True,
        )

    def _detect_language(self, language: str) -> str:
        """Detect primary language if auto."""
        if language != "auto":
            return language.lower()

        # Use language detector
        detector = LanguageDetector(self.repo_path)
        languages = detector.detect_languages()

        if not languages:
            return "python"  # Default fallback

        # Return most used language
        return max(languages, key=languages.get).lower()

    def generate_all(self, dry_run: bool = False) -> List[Path]:
        """Generate all bootstrap files.

        Args:
            dry_run: If True, don't create files, just return paths

        Returns:
            List of created file paths
        """
        created_files = []

        # GitHub Actions workflows
        created_files.extend(self._generate_workflows(dry_run))

        # GitHub templates
        created_files.extend(self._generate_github_templates(dry_run))

        # Pre-commit hooks
        created_files.extend(self._generate_precommit_config(dry_run))

        # Dependabot
        created_files.extend(self._generate_dependabot(dry_run))

        # Contributing guidelines
        created_files.extend(self._generate_docs(dry_run))

        return created_files

    def _generate_workflows(self, dry_run: bool) -> List[Path]:
        """Generate GitHub Actions workflows."""
        workflows_dir = self.repo_path / ".github" / "workflows"
        created = []

        # Determine test workflow language (fallback to python if template doesn't exist)
        test_language = self.language
        try:
            self.env.get_template(f"workflows/tests-{test_language}.yml.j2")
        except Exception:
            # Template doesn't exist, fallback to python
            test_language = "python"

        # AgentReady assessment workflow
        agentready_workflow = workflows_dir / "agentready-assessment.yml"
        template = self.env.get_template("workflows/agentready-assessment.yml.j2")
        content = template.render(language=test_language)
        created.append(self._write_file(agentready_workflow, content, dry_run))

        # Tests workflow
        tests_workflow = workflows_dir / "tests.yml"
        template = self.env.get_template(f"workflows/tests-{test_language}.yml.j2")
        content = template.render()
        created.append(self._write_file(tests_workflow, content, dry_run))

        # Security workflow
        security_workflow = workflows_dir / "security.yml"
        template = self.env.get_template("workflows/security.yml.j2")
        content = template.render(language=test_language)
        created.append(self._write_file(security_workflow, content, dry_run))

        return created

    def _generate_github_templates(self, dry_run: bool) -> List[Path]:
        """Generate GitHub issue and PR templates."""
        created = []

        # Issue templates
        issue_template_dir = self.repo_path / ".github" / "ISSUE_TEMPLATE"

        bug_template = issue_template_dir / "bug_report.md"
        template = self.env.get_template("issue_templates/bug_report.md.j2")
        content = template.render()
        created.append(self._write_file(bug_template, content, dry_run))

        feature_template = issue_template_dir / "feature_request.md"
        template = self.env.get_template("issue_templates/feature_request.md.j2")
        content = template.render()
        created.append(self._write_file(feature_template, content, dry_run))

        # PR template
        pr_template = self.repo_path / ".github" / "PULL_REQUEST_TEMPLATE.md"
        template = self.env.get_template("PULL_REQUEST_TEMPLATE.md.j2")
        content = template.render()
        created.append(self._write_file(pr_template, content, dry_run))

        # CODEOWNERS
        codeowners = self.repo_path / ".github" / "CODEOWNERS"
        template = self.env.get_template("CODEOWNERS.j2")
        content = template.render()
        created.append(self._write_file(codeowners, content, dry_run))

        return created

    def _generate_precommit_config(self, dry_run: bool) -> List[Path]:
        """Generate pre-commit hooks configuration."""
        precommit_file = self.repo_path / ".pre-commit-config.yaml"

        # Determine language for template (fallback to python)
        template_language = self.language
        try:
            template = self.env.get_template(f"precommit-{template_language}.yaml.j2")
        except Exception:
            # Template doesn't exist, fallback to python
            template_language = "python"
            template = self.env.get_template(f"precommit-{template_language}.yaml.j2")

        content = template.render()
        return [self._write_file(precommit_file, content, dry_run)]

    def _generate_dependabot(self, dry_run: bool) -> List[Path]:
        """Generate Dependabot configuration."""
        dependabot_file = self.repo_path / ".github" / "dependabot.yml"
        template = self.env.get_template("dependabot.yml.j2")
        content = template.render(language=self.language)
        return [self._write_file(dependabot_file, content, dry_run)]

    def _generate_docs(self, dry_run: bool) -> List[Path]:
        """Generate contributing guidelines and code of conduct."""
        created = []

        # CONTRIBUTING.md
        contributing = self.repo_path / "CONTRIBUTING.md"
        if not contributing.exists():  # Don't overwrite existing
            template = self.env.get_template("CONTRIBUTING.md.j2")
            content = template.render(language=self.language)
            created.append(self._write_file(contributing, content, dry_run))

        # CODE_OF_CONDUCT.md (Red Hat standard)
        code_of_conduct = self.repo_path / "CODE_OF_CONDUCT.md"
        if not code_of_conduct.exists():
            template = self.env.get_template("CODE_OF_CONDUCT.md.j2")
            content = template.render()
            created.append(self._write_file(code_of_conduct, content, dry_run))

        return created

    def _write_file(self, path: Path, content: str, dry_run: bool) -> Path:
        """Write file to disk or simulate for dry run."""
        if dry_run:
            return path

        path.parent.mkdir(parents=True, exist_ok=True)
        with open(path, "w", encoding="utf-8") as f:
            f.write(content)

        return path
````

## File: src/agentready/templates/report.html.j2
````
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>AgentReady Assessment - {{ repository.name }}</title>
    <style>
        /* Reset and Base Styles */
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif;
            line-height: 1.6;
            color: #333;
            background: #f5f5f5;
            padding: 20px;
        }

        .container {
            max-width: 1200px;
            margin: 0 auto;
            background: white;
            padding: 40px;
            border-radius: 8px;
            box-shadow: 0 2px 8px rgba(0,0,0,0.1);
        }

        /* Header */
        header {
            border-bottom: 3px solid #2563eb;
            padding-bottom: 20px;
            margin-bottom: 30px;
        }

        h1 {
            font-size: 2rem;
            color: #1e293b;
            margin-bottom: 10px;
        }

        .subtitle {
            color: #64748b;
            font-size: 1rem;
        }

        /* Summary Cards */
        .summary-grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(200px, 1fr));
            gap: 20px;
            margin: 30px 0;
        }

        .summary-card {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 20px;
            border-radius: 8px;
            text-align: center;
        }

        .summary-card.score {
            background: linear-gradient(135deg, #f093fb 0%, #f5576c 100%);
        }

        .summary-card.certification {
            background: linear-gradient(135deg, #4facfe 0%, #00f2fe 100%);
        }

        .summary-card.assessed {
            background: linear-gradient(135deg, #43e97b 0%, #38f9d7 100%);
        }

        .summary-card.duration {
            background: linear-gradient(135deg, #fa709a 0%, #fee140 100%);
        }

        .summary-card h3 {
            font-size: 0.9rem;
            font-weight: 500;
            opacity: 0.9;
            margin-bottom: 10px;
        }

        .summary-card .value {
            font-size: 2rem;
            font-weight: 700;
        }

        /* Controls */
        .controls {
            display: flex;
            flex-wrap: wrap;
            gap: 15px;
            margin: 30px 0;
            padding: 20px;
            background: #f8fafc;
            border-radius: 8px;
        }

        .control-group {
            display: flex;
            align-items: center;
            gap: 10px;
        }

        .control-group label {
            font-weight: 600;
            color: #475569;
            font-size: 0.9rem;
        }

        select, input[type="search"] {
            padding: 8px 12px;
            border: 1px solid #cbd5e1;
            border-radius: 6px;
            font-size: 0.9rem;
            background: white;
        }

        input[type="search"] {
            min-width: 250px;
        }

        .filter-btn {
            padding: 8px 16px;
            border: 2px solid #e2e8f0;
            background: white;
            border-radius: 6px;
            cursor: pointer;
            font-size: 0.9rem;
            transition: all 0.2s;
        }

        .filter-btn:hover {
            border-color: #2563eb;
            background: #eff6ff;
        }

        .filter-btn.active {
            background: #2563eb;
            color: white;
            border-color: #2563eb;
        }

        .badge {
            display: inline-block;
            padding: 2px 8px;
            border-radius: 12px;
            font-size: 0.75rem;
            font-weight: 600;
            margin-left: 5px;
        }

        /* Certification Ladder */
        .cert-ladder {
            display: flex;
            justify-content: space-between;
            margin: 30px 0;
            padding: 20px;
            background: #f8fafc;
            border-radius: 8px;
        }

        .cert-level {
            flex: 1;
            text-align: center;
            padding: 15px 10px;
            border-radius: 6px;
            margin: 0 5px;
            transition: all 0.3s;
        }

        .cert-level.platinum { background: linear-gradient(135deg, #e0e7ff, #c7d2fe); }
        .cert-level.gold { background: linear-gradient(135deg, #fef3c7, #fde68a); }
        .cert-level.silver { background: linear-gradient(135deg, #e5e7eb, #d1d5db); }
        .cert-level.bronze { background: linear-gradient(135deg, #fed7aa, #fdba74); }
        .cert-level.needs-improvement { background: linear-gradient(135deg, #fecaca, #fca5a5); }

        .cert-level.active {
            transform: scale(1.1);
            box-shadow: 0 4px 12px rgba(0,0,0,0.15);
            border: 3px solid #2563eb;
        }

        .cert-level h4 {
            font-size: 0.9rem;
            margin-bottom: 5px;
        }

        .cert-level .range {
            font-size: 0.8rem;
            opacity: 0.7;
        }

        /* Findings */
        .findings {
            margin: 30px 0;
        }

        .finding {
            border: 1px solid #e2e8f0;
            border-radius: 8px;
            margin-bottom: 15px;
            overflow: hidden;
            transition: all 0.2s;
        }

        .finding:hover {
            box-shadow: 0 4px 12px rgba(0,0,0,0.08);
        }

        .finding-header {
            padding: 15px 20px;
            background: #f8fafc;
            cursor: pointer;
            display: flex;
            justify-content: space-between;
            align-items: center;
        }

        .finding-header:hover {
            background: #f1f5f9;
        }

        .finding-title {
            display: flex;
            align-items: center;
            gap: 15px;
            flex: 1;
        }

        .status-icon {
            font-size: 1.5rem;
        }

        .finding-info h3 {
            font-size: 1rem;
            color: #1e293b;
            margin-bottom: 3px;
        }

        .finding-meta {
            font-size: 0.85rem;
            color: #64748b;
        }

        .tier-badge {
            padding: 4px 12px;
            border-radius: 12px;
            font-size: 0.75rem;
            font-weight: 600;
        }

        .tier-1 { background: #fee2e2; color: #991b1b; }
        .tier-2 { background: #fed7aa; color: #9a3412; }
        .tier-3 { background: #fef3c7; color: #92400e; }
        .tier-4 { background: #d1fae5; color: #065f46; }

        .score-display {
            font-size: 1.5rem;
            font-weight: 700;
            min-width: 80px;
            text-align: right;
        }

        .score-pass { color: #059669; }
        .score-fail { color: #dc2626; }
        .score-skip { color: #6b7280; }

        .finding-body {
            padding: 20px;
            display: none;
        }

        .finding.expanded .finding-body {
            display: block;
        }

        .finding-section {
            margin-bottom: 20px;
        }

        .finding-section h4 {
            font-size: 0.9rem;
            color: #475569;
            margin-bottom: 10px;
            text-transform: uppercase;
            letter-spacing: 0.05em;
        }

        .evidence-list, .remediation-steps {
            list-style: none;
            padding-left: 0;
        }

        .evidence-list li {
            padding: 8px 0;
            border-bottom: 1px solid #f1f5f9;
        }

        .evidence-list li:before {
            content: "‚ñ∏ ";
            color: #2563eb;
            font-weight: 700;
            margin-right: 8px;
        }

        .remediation-steps li {
            padding: 10px 0 10px 30px;
            position: relative;
        }

        .remediation-steps li:before {
            content: counter(step);
            counter-increment: step;
            position: absolute;
            left: 0;
            top: 10px;
            background: #2563eb;
            color: white;
            width: 20px;
            height: 20px;
            border-radius: 50%;
            display: flex;
            align-items: center;
            justify-content: center;
            font-size: 0.75rem;
            font-weight: 700;
        }

        .remediation-steps {
            counter-reset: step;
        }

        code {
            background: #f1f5f9;
            padding: 2px 6px;
            border-radius: 3px;
            font-family: 'Courier New', monospace;
            font-size: 0.9em;
        }

        pre {
            background: #1e293b;
            color: #e2e8f0;
            padding: 15px;
            border-radius: 6px;
            overflow-x: auto;
            margin: 10px 0;
        }

        pre code {
            background: none;
            color: inherit;
            padding: 0;
        }

        /* Footer */
        footer {
            margin-top: 50px;
            padding-top: 20px;
            border-top: 1px solid #e2e8f0;
            text-align: center;
            color: #64748b;
            font-size: 0.9rem;
        }

        /* Responsive */
        @media (max-width: 768px) {
            .container {
                padding: 20px;
            }

            .summary-grid {
                grid-template-columns: 1fr;
            }

            .controls {
                flex-direction: column;
            }

            input[type="search"] {
                min-width: 100%;
            }

            .cert-ladder {
                flex-direction: column;
            }

            .cert-level {
                margin: 5px 0;
            }
        }

        /* Hidden class */
        .hidden {
            display: none !important;
        }
    </style>
</head>
<body>
    <div class="container">
        <header>
            <h1>ü§ñ AgentReady Assessment Report</h1>
            <p class="subtitle">{{ repository.name }} ‚Ä¢ {{ timestamp.strftime('%B %d, %Y at %H:%M') }}</p>
        </header>

        <!-- Summary Cards -->
        <div class="summary-grid">
            <div class="summary-card score">
                <h3>Overall Score</h3>
                <div class="value">{{ "%.1f"|format(overall_score) }}</div>
            </div>
            <div class="summary-card certification">
                <h3>Certification</h3>
                <div class="value">{{ certification_level }}</div>
            </div>
            <div class="summary-card assessed">
                <h3>Assessed</h3>
                <div class="value">{{ attributes_assessed }}/{{ attributes_total }}</div>
            </div>
            <div class="summary-card duration">
                <h3>Duration</h3>
                <div class="value">{{ "%.1f"|format(duration_seconds) }}s</div>
            </div>
        </div>

        <!-- Certification Ladder -->
        <div class="cert-ladder">
            <div class="cert-level platinum {% if certification_level == 'Platinum' %}active{% endif %}">
                <h4>üíé Platinum</h4>
                <p class="range">90-100</p>
            </div>
            <div class="cert-level gold {% if certification_level == 'Gold' %}active{% endif %}">
                <h4>ü•á Gold</h4>
                <p class="range">75-89</p>
            </div>
            <div class="cert-level silver {% if certification_level == 'Silver' %}active{% endif %}">
                <h4>ü•à Silver</h4>
                <p class="range">60-74</p>
            </div>
            <div class="cert-level bronze {% if certification_level == 'Bronze' %}active{% endif %}">
                <h4>ü•â Bronze</h4>
                <p class="range">40-59</p>
            </div>
            <div class="cert-level needs-improvement {% if certification_level == 'Needs Improvement' %}active{% endif %}">
                <h4>‚ö†Ô∏è Needs Work</h4>
                <p class="range">0-39</p>
            </div>
        </div>

        <!-- Controls -->
        <div class="controls">
            <div class="control-group">
                <label>Filter:</label>
                <button class="filter-btn active" data-filter="all">All <span class="badge">{{ findings|length }}</span></button>
                <button class="filter-btn" data-filter="pass">Pass <span class="badge">{{ findings|selectattr('status', 'equalto', 'pass')|list|length }}</span></button>
                <button class="filter-btn" data-filter="fail">Fail <span class="badge">{{ findings|selectattr('status', 'equalto', 'fail')|list|length }}</span></button>
                <button class="filter-btn" data-filter="skipped">Skipped <span class="badge">{{ (findings|selectattr('status', 'equalto', 'skipped')|list|length + findings|selectattr('status', 'equalto', 'not_applicable')|list|length) }}</span></button>
            </div>

            <div class="control-group">
                <label>Sort:</label>
                <select id="sort-select">
                    <option value="category">Category</option>
                    <option value="score-desc">Score (High to Low)</option>
                    <option value="score-asc">Score (Low to High)</option>
                    <option value="tier">Tier (Essential First)</option>
                </select>
            </div>

            <div class="control-group">
                <input type="search" id="search-box" placeholder="Search attributes...">
            </div>
        </div>

        <!-- Findings -->
        <div class="findings" id="findings-container">
            {% for finding in findings %}
            <div class="finding"
                 data-status="{{ finding.status }}"
                 data-tier="{{ finding.attribute.tier }}"
                 data-category="{{ finding.attribute.category }}"
                 data-score="{{ finding.score if finding.score else 0 }}"
                 data-name="{{ finding.attribute.name|lower }}">

                <div class="finding-header" onclick="toggleFinding(this)">
                    <div class="finding-title">
                        <span class="status-icon">
                            {% if finding.status == 'pass' %}‚úÖ
                            {% elif finding.status == 'fail' %}‚ùå
                            {% elif finding.status in ['skipped', 'not_applicable'] %}‚äò
                            {% else %}‚ö†Ô∏è{% endif %}
                        </span>
                        <div class="finding-info">
                            <h3>{{ finding.attribute.name }}</h3>
                            <div class="finding-meta">
                                {{ finding.attribute.category }} ‚Ä¢
                                <span class="tier-badge tier-{{ finding.attribute.tier }}">Tier {{ finding.attribute.tier }}</span>
                                {% if finding.measured_value %} ‚Ä¢ {{ finding.measured_value }}{% endif %}
                            </div>
                        </div>
                    </div>
                    {% if finding.score is not none %}
                    <div class="score-display score-{{ finding.status }}">
                        {{ "%.0f"|format(finding.score) }}
                    </div>
                    {% else %}
                    <div class="score-display score-skip">‚Äî</div>
                    {% endif %}
                </div>

                <div class="finding-body">
                    {% if finding.evidence %}
                    <div class="finding-section">
                        <h4>Evidence</h4>
                        <ul class="evidence-list">
                            {% for item in finding.evidence %}
                            <li>{{ item }}</li>
                            {% endfor %}
                        </ul>
                    </div>
                    {% endif %}

                    {% if finding.remediation %}
                    <div class="finding-section">
                        <h4>Remediation</h4>
                        <p><strong>{{ finding.remediation.summary }}</strong></p>

                        {% if finding.remediation.steps %}
                        <ol class="remediation-steps">
                            {% for step in finding.remediation.steps %}
                            <li>{{ step }}</li>
                            {% endfor %}
                        </ol>
                        {% endif %}

                        {% if finding.remediation.commands %}
                        <h4 style="margin-top: 15px;">Commands</h4>
                        <pre><code>{{ finding.remediation.commands|join('\n') }}</code></pre>
                        {% endif %}

                        {% if finding.remediation.examples %}
                        <h4 style="margin-top: 15px;">Examples</h4>
                        {% for example in finding.remediation.examples %}
                        <pre><code>{{ example }}</code></pre>
                        {% endfor %}
                        {% endif %}
                    </div>
                    {% endif %}

                    {% if finding.error_message %}
                    <div class="finding-section">
                        <h4>Error</h4>
                        <p style="color: #dc2626;">{{ finding.error_message }}</p>
                    </div>
                    {% endif %}
                </div>
            </div>
            {% endfor %}
        </div>

        <footer>
            <p>Generated by <strong>AgentReady v1.0.0</strong></p>
            <p>Repository: {{ repository.path }} ‚Ä¢ Branch: {{ repository.branch }} ‚Ä¢ Commit: {{ repository.commit_hash[:8] }}</p>
            <p style="margin-top: 10px; font-size: 0.85rem;">
                ü§ñ Generated with <a href="https://claude.com/claude-code" style="color: #2563eb;">Claude Code</a>
            </p>
        </footer>
    </div>

    <script>
        // Embedded assessment data (properly escaped to prevent XSS)
        const ASSESSMENT = JSON.parse({{ assessment_json|tojson }});

        // Toggle finding expansion
        function toggleFinding(header) {
            const finding = header.parentElement;
            finding.classList.toggle('expanded');
        }

        // Filter functionality
        document.querySelectorAll('.filter-btn').forEach(btn => {
            btn.addEventListener('click', () => {
                // Update active button
                document.querySelectorAll('.filter-btn').forEach(b => b.classList.remove('active'));
                btn.classList.add('active');

                const filter = btn.dataset.filter;
                const findings = document.querySelectorAll('.finding');

                findings.forEach(finding => {
                    const status = finding.dataset.status;

                    if (filter === 'all') {
                        finding.classList.remove('hidden');
                    } else if (filter === 'skipped') {
                        if (status === 'skipped' || status === 'not_applicable' || status === 'error') {
                            finding.classList.remove('hidden');
                        } else {
                            finding.classList.add('hidden');
                        }
                    } else {
                        if (status === filter) {
                            finding.classList.remove('hidden');
                        } else {
                            finding.classList.add('hidden');
                        }
                    }
                });
            });
        });

        // Sort functionality
        document.getElementById('sort-select').addEventListener('change', (e) => {
            const sortBy = e.target.value;
            const container = document.getElementById('findings-container');
            const findings = Array.from(container.querySelectorAll('.finding'));

            findings.sort((a, b) => {
                if (sortBy === 'category') {
                    return a.dataset.category.localeCompare(b.dataset.category);
                } else if (sortBy === 'score-desc') {
                    return parseFloat(b.dataset.score) - parseFloat(a.dataset.score);
                } else if (sortBy === 'score-asc') {
                    return parseFloat(a.dataset.score) - parseFloat(b.dataset.score);
                } else if (sortBy === 'tier') {
                    return parseInt(a.dataset.tier) - parseInt(b.dataset.tier);
                }
            });

            findings.forEach(finding => container.appendChild(finding));
        });

        // Search functionality
        document.getElementById('search-box').addEventListener('input', (e) => {
            const query = e.target.value.toLowerCase();
            const findings = document.querySelectorAll('.finding');

            findings.forEach(finding => {
                const name = finding.dataset.name;
                const category = finding.dataset.category.toLowerCase();

                if (name.includes(query) || category.includes(query)) {
                    finding.classList.remove('hidden');
                } else {
                    finding.classList.add('hidden');
                }
            });
        });
    </script>
</body>
</html>
````

## File: .repomixignore
````
# Add patterns to ignore here, one per line
# Example:
# *.log
# tmp/

# Virtual environment
.venv/
venv/
env/

# Python bytecode and cache
__pycache__/
*.pyc
*.pyo
*.pyd
.Python

# Test coverage
.coverage
.coverage.*
htmlcov/
.pytest_cache/
.tox/
.hypothesis/

# IDE and editor files
.vscode/
.idea/
*.swp
*.swo
*~
.DS_Store

# Generated reports (keep examples/ for reference)
.agentready/*.html
.agentready/*.json
.agentready/*.md

# Build artifacts
build/
dist/
*.egg-info/
*.egg

# Git internal files
.git/

# Temporary files
*.tmp
*.temp
.cache/

# Large generated documentation
repomix-output.md
````

## File: agent-ready-codebase-attributes.md
````markdown
# Agent-Ready Codebase Attributes: Comprehensive Research
*Optimizing Codebases for Claude Code and AI-Assisted Development*

**Version:** 1.0.0
**Date:** 2025-11-20
**Focus:** Claude Code/Claude-specific optimization
**Sources:** 50+ authoritative sources including Anthropic, Microsoft, Google, ArXiv, IEEE/ACM

---

## Executive Summary

This document catalogs 25 high-impact attributes that make codebases optimal for AI-assisted development, specifically Claude Code. Each attribute includes:
- Definition and importance for AI agents
- Impact on agent behavior (context window, comprehension, task success)
- Measurable criteria and tooling
- Authoritative citations
- Good vs. bad examples

**Top 10 Critical Attributes** (highest ROI):
1. CLAUDE.md/AGENTS.md configuration files
2. Conventional commit messages
3. Type annotations (static typing)
4. Test coverage >80%
5. Standard project layouts
6. Comprehensive README
7. Dependency lock files
8. Pre-commit hooks + CI/CD enforcement
9. Structured logging
10. API specifications (OpenAPI/GraphQL)

---

## 1. CONTEXT WINDOW OPTIMIZATION

### 1.1 CLAUDE.md Configuration Files

**Definition:** Markdown file at repository root automatically ingested by Claude at conversation start.

**Why It Matters:** CLAUDE.md files are "naively dropped into context up front," providing immediate project context without repeated explanations. Reduces prompt engineering time by ~40%.

**Impact on Agent Behavior:**
- Immediate understanding of tech stack, repository structure, standard commands
- Consistent adherence to project conventions
- Reduced need for repeated context-setting
- Frames entire session with project-specific guidance

**Measurable Criteria:**
- File size: <1000 lines (concise, focused)
- Essential sections:
  - Tech stack with versions
  - Repository map/structure
  - Standard commands (build, test, lint, format)
  - Testing strategy
  - Style/lint rules
  - Branch/PR workflow
  - "Do not touch" zones
  - Security/compliance notes

**Citation:** Anthropic Engineering Blog - "Claude Code Best Practices" (2025)

**Example:**
```markdown
# Good CLAUDE.md
# Tech Stack
- Python 3.11+, pytest, black + isort

# Standard Commands
- Run tests: `pytest tests/`
- Format: `black . && isort .`
- Build: `make build`

# Repository Structure
- src/ - Main application code
- tests/ - Test files mirror src/
- docs/ - Documentation

# Boundaries
- Never modify files in legacy/
- Require approval before changing config.yaml
```

---

### 1.2 Concise, Structured Documentation

**Definition:** Documentation maximizing information density while minimizing token consumption.

**Why It Matters:** Despite expanding context windows (1M+ tokens), attention mechanisms have quadratic complexity growth. Performance drops significantly on long-context tasks: 29%‚Üí3% (Claude 3.5 Sonnet) or 70.2%‚Üí40% (Qwen2.5).

**Impact on Agent Behavior:**
- Faster information retrieval through clear headings
- Reduced context pollution
- Improved response accuracy
- Better navigation across documentation

**Measurable Criteria:**
- Use standard Markdown headings (#, ##, ###)
- README <500 lines; use wiki/docs for extensive content
- Table of contents for documents >100 lines
- Bullet points over prose paragraphs
- One concept per section

**Citations:**
- ArXiv: "LongCodeBench: Evaluating Coding LLMs at 1M Context Windows" (2025)
- IBM Research: "Why larger LLM context windows are all the rage"

---

### 1.3 File Size Limits

**Definition:** Individual source files <200-300 lines.

**Why It Matters:** Working memory handles ~4 objects simultaneously. Large files exceed cognitive capacity for both humans and AI.

**Impact on Agent Behavior:**
- More precise file selection
- Reduced irrelevant context in responses
- Safer targeted modifications
- Better understanding of module boundaries

**Measurable Criteria:**
- Target: <200-300 lines per file
- Warning threshold: 500 lines
- Exception: Generated code, data files
- Enforce via linters (e.g., pylint max-module-lines)

**Citations:**
- Stack Overflow: "At what point/range is a code file too big?"
- Medium: "Psychology of Code Readability" by Egon Elbre

---

## 2. DOCUMENTATION STANDARDS

### 2.1 README Structure

**Definition:** Standardized README with essential sections in predictable order.

**Why It Matters:** Repositories with well-structured READMEs receive more engagement (GitHub data). README serves as agent's entry point for understanding project purpose, setup, and usage.

**Impact on Agent Behavior:**
- Faster project comprehension
- Accurate answers to onboarding questions
- Better architectural understanding without exploring entire codebase
- Consistent expectations across projects

**Measurable Criteria:**
Essential sections (in order):
1. Project title and description
2. Installation/setup instructions
3. Quick start/usage examples
4. Core features
5. Dependencies and requirements
6. Testing instructions
7. Contributing guidelines
8. License

**Citations:**
- GitHub Blog: "How to write a great agents.md"
- Make a README project documentation
- Welcome to the Jungle: "Essential Sections for Better Documentation"

---

### 2.2 Inline Documentation (Docstrings/Comments)

**Definition:** Function, class, and module-level documentation using language-specific conventions (Python docstrings, JSDoc/TSDoc).

**Why It Matters:** Type hints significantly improve LLM experience. Well-typed code directs LLMs into latent space regions corresponding to higher code quality‚Äîsimilar to how LaTeX-formatted math problems get better results.

**Impact on Agent Behavior:**
- Understanding function purpose without reading implementation
- Better parameter validation suggestions
- More accurate return type predictions
- Improved test generation
- Enhanced refactoring confidence

**Measurable Criteria:**
- All public functions/methods have docstrings
- Docstrings include: description, parameters, return values, exceptions, examples
- Python: PEP 257 compliant
- JavaScript/TypeScript: JSDoc or TSDoc
- Coverage: >80% of public API documented
- Tools: pydocstyle, documentation-js

**Citations:**
- Medium: "LLM Coding Concepts: Static Typing, Structured Output, and AsyncIO"
- ArXiv: "TypyBench: Evaluating LLM Type Inference for Untyped Python Repositories"
- TypeScript Documentation: JSDoc Reference

**Example:**
```python
# Good: Comprehensive docstring
def calculate_discount(price: float, discount_percent: float) -> float:
    """
    Calculate discounted price.

    Args:
        price: Original price in USD
        discount_percent: Discount percentage (0-100)

    Returns:
        Discounted price

    Raises:
        ValueError: If discount_percent not in 0-100 range

    Example:
        >>> calculate_discount(100.0, 20.0)
        80.0
    """
    if not 0 <= discount_percent <= 100:
        raise ValueError("Discount must be 0-100")
    return price * (1 - discount_percent / 100)

# Bad: No documentation
def calc_disc(p, d):
    return p * (1 - d / 100)
```

---

### 2.3 Architecture Decision Records (ADRs)

**Definition:** Lightweight documents capturing architectural decisions with context, decision, and consequences.

**Why It Matters:** ADRs provide historical context for "why" decisions were made. When AI encounters patterns or constraints, ADRs explain rationale, preventing counter-productive suggestions.

**Impact on Agent Behavior:**
- Understanding project evolution and design philosophy
- Avoiding proposing previously rejected alternatives
- Aligning suggestions with established architectural principles
- Better context for refactoring recommendations

**Measurable Criteria:**
- Store in `docs/adr/` or `.adr/` directory
- Use consistent template (Michael Nygard or MADR)
- Each ADR includes: Title, Status, Context, Decision, Consequences
- Status values: Proposed, Accepted, Deprecated, Superseded
- One decision per ADR
- Sequential numbering (ADR-001, ADR-002...)

**Citations:**
- AWS Prescriptive Guidance: "ADR process"
- GitHub: joelparkerhenderson/architecture-decision-record
- Microsoft Azure Well-Architected Framework

**Template:**
```markdown
# ADR-001: Use PostgreSQL for Primary Database

Status: Accepted

## Context
Need persistent storage supporting ACID transactions, complex queries, and JSON data.

## Decision
Use PostgreSQL 14+ as primary database.

## Consequences
Positive:
- Strong ACID guarantees
- Rich query capabilities (joins, window functions)
- JSON support via jsonb

Negative:
- More operational complexity than managed NoSQL
- Requires schema migration planning
- Horizontal scaling more complex
```

---

## 3. CODE QUALITY METRICS

### 3.1 Cyclomatic Complexity Thresholds

**Definition:** Measurement of linearly independent paths through code, indicating decision point density.

**Why It Matters:** High cyclomatic complexity confuses both humans and AI. While not perfect (doesn't capture cognitive complexity), it correlates strongly with testing difficulty and error potential.

**Impact on Agent Behavior:**
- Functions with complexity >25 are harder to understand
- Reduced confidence in safe modifications
- More difficult to generate comprehensive tests
- Increased likelihood of introducing bugs during refactoring

**Measurable Criteria:**
- Target: Cyclomatic complexity <10 per function
- Warning threshold: 15
- Error threshold: 25
- Tools: clang-tidy (C++), radon (Python), complexity-report (JavaScript), gocyclo (Go)

**Citations:**
- Microsoft Learn: "Code metrics - Cyclomatic complexity"
- Checkstyle Documentation
- LinearB Blog: "Cyclomatic Complexity explained"

---

### 3.2 Function/Method Length Limits

**Definition:** Keeping functions/methods small (typically <50 lines, ideally <20).

**Why It Matters:** Working memory handles ~4 objects simultaneously. Long functions exceed cognitive capacity. Research on reading comprehension shows lines >50-75 characters reduce comprehension; code has higher cognitive load per line.

**Impact on Agent Behavior:**
- Easier holistic function understanding
- Better isolation for testing
- Safer modifications without unintended side effects
- Clearer single responsibility principle adherence

**Measurable Criteria:**
- Target: <20 lines per function
- Warning: 50 lines
- Hard limit: 100 lines
- Exception: Complex algorithms with extensive explanatory comments
- Tools: pylint (max-function-lines), eslint (max-lines-per-function)

**Citations:**
- Medium: "Psychology of Code Readability" by Egon Elbre
- UX Stack Exchange: Line length readability research
- Clang-Tidy: readability-function-cognitive-complexity

---

### 3.3 Type Annotations (Static Typing)

**Definition:** Explicit type declarations for variables, parameters, and return values.

**Why It Matters:** Type hints significantly improve LLM code understanding. Higher-quality codebases have type annotations, directing LLMs toward higher-quality latent space regions. Creates synergistic improvement: LLMs generate better typed code, which helps future LLM interactions.

**Impact on Agent Behavior:**
- Better input validation
- Type error detection before execution
- Structured output generation
- Improved autocomplete suggestions
- Enhanced refactoring safety

**Measurable Criteria:**
- Python: All public functions have parameter and return type hints
- TypeScript: `strict` mode enabled in tsconfig.json
- Go: Inherently typed
- Coverage: >80% of functions typed
- Tools: mypy (Python), pyright (Python), tsc --strict (TypeScript)

**Citations:**
- Medium: "LLM Coding Concepts: Static Typing, Structured Output"
- ArXiv: "Automated Type Annotation in Python Using LLMs"
- Dropbox Tech Blog: "Our journey to type checking 4 million lines of Python"

**Example:**
```python
# Good: Full type annotations
from typing import List, Optional

def find_users(
    role: str,
    active: bool = True,
    limit: Optional[int] = None
) -> List[User]:
    """Find users matching criteria."""
    query = User.query.filter_by(role=role, active=active)
    if limit:
        query = query.limit(limit)
    return query.all()

# Bad: No type hints
def find_users(role, active=True, limit=None):
    query = User.query.filter_by(role=role, active=active)
    if limit:
        query = query.limit(limit)
    return query.all()
```

---

### 3.4 Code Smell Elimination

**Definition:** Removing indicators of deeper problems: long methods, large classes, duplicate code, dead code, magic numbers.

**Why It Matters:** Research shows AI-generated code increases "code churn" (copy/paste vs. refactoring) and DRY principle violations. Clean baseline prevents AI from perpetuating anti-patterns.

**Impact on Agent Behavior:**
- Better intent understanding
- More accurate refactoring suggestions
- Avoidance of anti-pattern propagation
- Improved code quality over time

**Measurable Criteria:**
- Tools: SonarQube, PMD, Checkstyle, pylint, eslint
- Zero critical smells
- <5 major smells per 1000 lines of code
- Common smells monitored:
  - Duplicate code (DRY violations)
  - Long methods (>50 lines)
  - Large classes (>500 lines)
  - Long parameter lists (>5 params)
  - Divergent change (one class changing for multiple reasons)

**Citations:**
- GitClear: "Coding on Copilot" whitepaper
- Codacy Blog: "Code Smells and Anti-Patterns"
- ScienceDirect: "Code smells and refactoring: A tertiary systematic review"

---

## 4. REPOSITORY STRUCTURE

### 4.1 Standard Project Layouts

**Definition:** Using community-recognized directory structures for each language/framework.

**Why It Matters:** Standard layouts reduce cognitive overhead. AI models trained on open-source code recognize patterns (Python's src/, Go's cmd/ and internal/, Java's Maven structure).

**Impact on Agent Behavior:**
- Faster navigation
- Accurate location assumptions for new files
- Automatic adherence to established conventions
- Reduced confusion about file placement

**Measurable Criteria:**

**Python (src layout):**
```
project/
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îî‚îÄ‚îÄ package/
‚îÇ       ‚îú‚îÄ‚îÄ __init__.py
‚îÇ       ‚îî‚îÄ‚îÄ module.py
‚îú‚îÄ‚îÄ tests/
‚îú‚îÄ‚îÄ docs/
‚îú‚îÄ‚îÄ README.md
‚îú‚îÄ‚îÄ pyproject.toml
‚îî‚îÄ‚îÄ requirements.txt
```

**Go:**
```
project/
‚îú‚îÄ‚îÄ cmd/           # Main applications
‚îÇ   ‚îî‚îÄ‚îÄ app/
‚îÇ       ‚îî‚îÄ‚îÄ main.go
‚îú‚îÄ‚îÄ internal/      # Private code
‚îú‚îÄ‚îÄ pkg/           # Public libraries
‚îú‚îÄ‚îÄ go.mod
‚îî‚îÄ‚îÄ go.sum
```

**JavaScript/TypeScript (Node.js):**
```
project/
‚îú‚îÄ‚îÄ src/
‚îú‚îÄ‚îÄ test/
‚îú‚îÄ‚îÄ dist/
‚îú‚îÄ‚îÄ package.json
‚îú‚îÄ‚îÄ package-lock.json
‚îî‚îÄ‚îÄ tsconfig.json
```

**Citations:**
- Real Python: "Python Application Layouts"
- GitHub: golang-standards/project-layout
- Stack Overflow: "Best project structure for Python application"

---

### 4.2 Separation of Concerns

**Definition:** Organizing code so each module/file/function has single, well-defined responsibility (SOLID principles).

**Why It Matters:** 2 of 5 SOLID principles derive directly from separation of concerns. Clear boundaries improve testability, maintainability, and reduce cognitive load.

**Impact on Agent Behavior:**
- Targeted modifications without affecting unrelated code
- Better refactoring suggestions
- Clearer module purpose understanding
- Reduced side effect risk

**Measurable Criteria:**
- Each module/class has one reason to change
- High cohesion within modules (related functions together)
- Low coupling between modules (minimal dependencies)
- Organize by feature/domain, not technical layer (avoid separate "controllers", "services", "models" directories)

**Citations:**
- Wikipedia: "Separation of concerns"
- DevIQ: "Separation of Concerns"
- Medium: "Single responsibility and Separation of concerns principles"

---

## 5. TESTING & CI/CD

### 5.1 Test Coverage Requirements

**Definition:** Percentage of code executed by automated tests.

**Why It Matters:** High test coverage enables confident AI modifications. Research shows AI tools (Cursor AI) can cut test coverage time by 85% while maintaining quality‚Äîbut only when good tests exist as foundation.

**Impact on Agent Behavior:**
- Safety net enabling aggressive refactoring
- Tests document expected behavior
- Immediate feedback on breaking changes
- Higher confidence in suggested modifications

**Measurable Criteria:**
- Minimum: 70% line coverage
- Target: 80-90% line coverage
- Critical paths: 100% coverage
- Track: Statement coverage, branch coverage, function coverage
- Tools: pytest-cov (Python), Jest/Istanbul (JavaScript), go test -cover (Go)
- Coverage reports in CI/CD with failure threshold

**Citations:**
- Salesforce Engineering: "How Cursor AI Cut Legacy Code Coverage Time by 85%"
- Qodo AI Blog: "Harnessing AI to Revolutionize Test Coverage Analysis"
- Medium: "How to Improve Code Coverage using Generative AI tools"

---

### 5.2 Test Naming Conventions

**Definition:** Descriptive test names following patterns like `test_should_<expected>_when_<condition>`.

**Why It Matters:** Clear test names help AI understand intent without reading implementation. When tests fail, AI diagnoses issues faster with self-documenting names.

**Impact on Agent Behavior:**
- Generation of similar test patterns
- Faster edge case understanding
- More accurate fix proposals aligned with intent
- Better test coverage gap identification

**Measurable Criteria:**
- Pattern: `test_<method>_<scenario>_<expected_outcome>`
- Example: `test_create_user_with_invalid_email_raises_value_error`
- Avoid: `test1`, `test_edge_case`, `test_bug_fix`, `test_method_name`
- Test names should be readable as sentences

**Citations:**
- pytest documentation: Test naming best practices
- JUnit best practices
- Go testing conventions

**Example:**
```python
# Good: Self-documenting test names
def test_create_user_with_valid_data_returns_user_instance():
    user = create_user(email="test@example.com", name="Test")
    assert isinstance(user, User)

def test_create_user_with_invalid_email_raises_value_error():
    with pytest.raises(ValueError, match="Invalid email"):
        create_user(email="not-an-email", name="Test")

def test_create_user_with_duplicate_email_raises_integrity_error():
    create_user(email="test@example.com", name="Test 1")
    with pytest.raises(IntegrityError):
        create_user(email="test@example.com", name="Test 2")

# Bad: Unclear test names
def test_user1():
    user = create_user(email="test@example.com", name="Test")
    assert user

def test_user2():
    with pytest.raises(ValueError):
        create_user(email="invalid", name="Test")
```

---

### 5.3 Pre-commit Hooks & CI/CD Linting

**Definition:** Automated code quality checks before commits (pre-commit hooks) and in CI/CD pipeline.

**Why It Matters:** Pre-commit hooks provide immediate feedback but can be bypassed. Running same checks in CI/CD ensures enforcement. Linting errors prevent successful CI runs, wasting time and compute.

**Impact on Agent Behavior:**
- Ensures AI-generated code meets quality standards
- Immediate feedback loop for improvements
- Consistent code style across all contributions
- Prevents low-quality code from entering repository

**Measurable Criteria:**
- Pre-commit framework installed and configured
- Hooks include:
  - Formatters: black/autopep8 (Python), prettier (JS/TS), gofmt (Go)
  - Linters: flake8/pylint (Python), eslint (JS/TS), golint (Go)
  - Type checkers: mypy/pyright (Python), tsc (TypeScript)
- **Critical:** Same checks run in CI/CD (non-skippable)
- CI fails on any linting error
- Fast execution: <30 seconds total

**Citations:**
- Memfault Blog: "Automatically format and lint code with pre-commit"
- Medium: "Elevate Your CI: Mastering Pre-commit Hooks and GitHub Actions"
- GitHub: pre-commit/pre-commit

---

## 6. DEPENDENCY MANAGEMENT

### 6.1 Lock Files for Reproducibility

**Definition:** Pinning exact dependency versions including transitive dependencies.

**Why It Matters:** Lock files ensure reproducible builds across environments. Without them, "works on my machine" problems plague AI-generated code. Different dependency versions can break builds, fail tests, or introduce bugs.

**Impact on Agent Behavior:**
- Confident dependency-related suggestions
- Accurate compatibility issue diagnosis
- Reproducible environment recommendations
- Version-specific API usage

**Measurable Criteria:**
- Lock file committed to repository
- **npm:** package-lock.json or yarn.lock
- **Python:** requirements.txt (from pip freeze), poetry.lock, or uv.lock
- **Go:** go.sum (automatically managed)
- **Ruby:** Gemfile.lock
- Lock file updated with every dependency change
- CI/CD uses lock file for installation

**Citations:**
- npm Blog: "Why Keep package-lock.json?"
- DEV Community: "Dependency management: package.json and package-lock.json explained"
- Python Packaging User Guide

---

### 6.2 Dependency Freshness & Security Scanning

**Definition:** Regularly updating dependencies and scanning for known vulnerabilities.

**Why It Matters:** Outdated dependencies introduce security risks and compatibility issues. AI-generated code may use deprecated APIs if dependencies are stale. Security vulnerabilities in dependencies can compromise entire application.

**Impact on Agent Behavior:**
- Suggestions use modern, non-deprecated APIs
- Awareness of security considerations
- Better library feature recommendations
- Avoidance of known vulnerability patterns

**Measurable Criteria:**
- Automated dependency updates: Dependabot, Renovate, or equivalent
- Security scanning in CI/CD: Snyk, npm audit, safety (Python), govulncheck (Go)
- Update cadence:
  - Patch versions: Weekly/automated
  - Minor versions: Monthly
  - Major versions: Quarterly with testing
- Zero known high/critical vulnerabilities in production
- Vulnerability response SLA: High severity within 7 days

**Citations:**
- GitHub Dependabot documentation
- OWASP Dependency-Check
- Snyk best practices
- npm audit documentation

---

## 7. GIT & VERSION CONTROL

### 7.1 Conventional Commit Messages

**Definition:** Structured commit messages following format: `<type>(<scope>): <description>`.

**Why It Matters:** Conventional commits enable automated semantic versioning, changelog generation, and commit intent understanding. AI can parse history to understand feature evolution and impact.

**Impact on Agent Behavior:**
- Generates properly formatted commit messages
- Understands which changes are breaking
- Appropriate version bump suggestions
- Better git history comprehension
- Automated changelog contribution

**Measurable Criteria:**
- Format: `type(scope): description`
- **Types:** feat, fix, docs, style, refactor, perf, test, chore, build, ci
- **Breaking changes:** `BREAKING CHANGE:` footer or `!` after type
- **Tools:** commitlint, commitizen, semantic-release
- **Enforcement:** Pre-commit hook or CI check
- All commits follow convention (enforce in CI)

**Citations:**
- Conventional Commits specification v1.0.0
- Medium: "GIT ‚Äî Semantic versioning and conventional commits"
- CMU SEI Blog: "Versioning with Git Tags and Conventional Commits"

**Example:**
```
# Good commits
feat(auth): add OAuth2 login support
fix(api): handle null values in user response
docs(readme): update installation instructions
perf(database): add index on user_email column

# Breaking change
feat(api)!: change user endpoint from /user to /users

BREAKING CHANGE: User endpoint URL has changed from /user to /users.
Update all API clients accordingly.

# Bad commits
update stuff
fixed bug
changes
wip
asdf
```

---

### 7.2 .gitignore Completeness

**Definition:** Comprehensive .gitignore preventing sensitive files, build artifacts, and environment-specific files from version control.

**Why It Matters:** Incomplete .gitignore pollutes repository with irrelevant files, consuming context window space and creating security risks (accidentally committing .env files, credentials).

**Impact on Agent Behavior:**
- Focus on source code, not build artifacts
- Security files excluded prevent accidental exposure
- Cleaner repository navigation
- Reduced context pollution

**Measurable Criteria:**
- Use language-specific templates from github/gitignore
- **Exclude:**
  - Build artifacts (dist/, build/, *.pyc, *.class)
  - Dependencies (node_modules/, venv/, vendor/)
  - IDE files (.vscode/, .idea/, *.swp)
  - OS files (.DS_Store, Thumbs.db)
  - Environment variables (.env, .env.local)
  - Credentials (*.pem, *.key, credentials.json)
  - Logs (*.log, logs/)
- One .gitignore at repository root (avoid multiple nested)
- Review when adding new tools/frameworks

**Citations:**
- GitHub: github/gitignore template collection
- Medium: "Mastering .gitignore: A Comprehensive Guide"
- Git documentation

---

### 7.3 Issue & Pull Request Templates

**Definition:** Standardized templates for issues and PRs in .github/ directory.

**Why It Matters:** Templates provide structure for AI when creating issues or PRs. Ensures all necessary context is provided consistently.

**Impact on Agent Behavior:**
- Automatically fills templates when creating PRs
- Ensures checklist completion
- Consistent issue reporting format
- Better context for understanding existing issues/PRs

**Measurable Criteria:**
- `PULL_REQUEST_TEMPLATE.md` in .github/ or root
- Issue templates in `.github/ISSUE_TEMPLATE/`
- **PR template includes:**
  - Summary of changes
  - Related issues (Fixes #123)
  - Testing performed
  - Checklist (tests added, docs updated, etc.)
- **Issue templates for:**
  - Bug reports (with reproduction steps)
  - Feature requests (with use case)
  - Questions/discussions

**Citations:**
- GitHub Docs: "About issue and pull request templates"
- GitHub Blog: "Multiple issue and pull request templates"
- Embedded Artistry: "A GitHub Pull Request Template for Your Projects"

---

## 8. BUILD & DEVELOPMENT SETUP

### 8.1 One-Command Build/Setup

**Definition:** Single command to set up development environment from fresh clone.

**Why It Matters:** Lengthy setup documentation increases friction and errors. One-command setup enables AI to quickly reproduce environments and test changes. Reduces "works on my machine" problems.

**Impact on Agent Behavior:**
- Confident environment setup suggestions
- Quick validation of proposed changes
- Easy onboarding recommendations
- Reduced setup-related debugging

**Measurable Criteria:**
- Single command documented prominently in README
- **Examples:** `make setup`, `npm install`, `poetry install`, `./bootstrap.sh`
- **Command handles:**
  - Dependency installation
  - Virtual environment creation
  - Database setup/migrations
  - Configuration file creation (.env from .env.example)
  - Pre-commit hooks installation
- **Success criteria:** Working development environment in <5 minutes
- Idempotent (safe to run multiple times)

**Citations:**
- npm Blog: "Using Npm Scripts as a Build Tool"
- freeCodeCamp: "Want to know the easiest way to save time? Use make!"
- Medium: "Creating Reproducible Development Environments"

**Example:**
```makefile
# Good: Comprehensive Makefile
.PHONY: setup
setup:
	python -m venv venv
	. venv/bin/activate && pip install -r requirements.txt
	pre-commit install
	cp .env.example .env
	python manage.py migrate
	@echo "‚úì Setup complete! Run 'make test' to verify."

.PHONY: test
test:
	pytest tests/ -v --cov

.PHONY: lint
lint:
	black --check .
	isort --check .
	flake8 .
	mypy .

.PHONY: format
format:
	black .
	isort .
```

---

### 8.2 Development Environment Documentation

**Definition:** Clear documentation of prerequisites, environment variables, and configuration.

**Why It Matters:** Environment differences cause "works on my machine" problems. Comprehensive docs enable reproducibility and faster debugging.

**Impact on Agent Behavior:**
- Accurate environment troubleshooting
- Better setup assistance for new contributors
- Environment-specific bug diagnosis
- Configuration recommendation accuracy

**Measurable Criteria:**
- **Prerequisites documented:**
  - Language/runtime version (Python 3.11+, Node.js 18+)
  - System dependencies (PostgreSQL, Redis, etc.)
  - Operating system requirements
- **Environment variables documented:**
  - .env.example file with all variables
  - Description of each variable
  - Required vs. optional clearly marked
  - Safe default values where applicable
- **Optional but helpful:**
  - IDE/editor setup (VS Code extensions, etc.)
  - Debugging configuration
  - Performance optimization tips

**Citations:**
- Medium: "Creating Reproducible Development Environments"
- InfoQ: "Reproducible Development with Containers"
- The Turing Way: "Reproducible Environments"

---

### 8.3 Container/Virtualization Setup

**Definition:** Docker/Podman configurations for consistent development environments.

**Why It Matters:** Containers provide portable, reproducible environments across operating systems. Development containers (devcontainers) are fully functional, batteries-included environments that are shared, versioned, and self-documenting.

**Impact on Agent Behavior:**
- Dockerfile improvement suggestions
- Container debugging assistance
- Consistent build recommendations
- Cross-platform development support

**Measurable Criteria:**
- Dockerfile or Containerfile in repository root
- docker-compose.yml for multi-service setups
- .devcontainer/devcontainer.json for VS Code/GitHub Codespaces
- **Dockerfile best practices:**
  - Multi-stage builds for smaller images
  - Non-root user
  - .dockerignore file
  - Explicit version tags (not :latest)
- Documentation on running containers
- Health checks defined

**Citations:**
- InfoQ: "Reproducible Development with Containers"
- Developer.com: "Creating a Reproducible and Portable Development Environment"
- Docker best practices documentation

---

## 9. ERROR HANDLING & DEBUGGING

### 9.1 Error Message Clarity

**Definition:** Descriptive error messages with context, remediation guidance, and relevant data.

**Why It Matters:** Clear errors enable AI to diagnose issues and suggest fixes. Vague errors ("Error 500", "Something went wrong") provide no actionable information.

**Impact on Agent Behavior:**
- Accurate root cause analysis
- Targeted solution proposals
- Faster debugging cycles
- Better user error handling suggestions

**Measurable Criteria:**
- **Include in error messages:**
  - What failed (operation/function)
  - Why it failed (validation, network, etc.)
  - How to fix it (actionable guidance)
  - Context: Request IDs, user IDs, timestamps, relevant parameters
- **Avoid:**
  - Generic messages ("Invalid input", "Error occurred")
  - Exposing internal stack traces to end users
  - Sensitive information in error messages
- **Provide:** Error codes for categorization
- Consistent error format across application

**Citations:**
- Honeycomb: "Engineers Checklist: Logging Best Practices"
- Paul Serban: "Error Logging Standards: A Practical Guide"
- Stack Overflow Blog: "Best practices for writing code comments"

**Example:**
```python
# Good: Descriptive error with context and guidance
raise ValueError(
    f"Invalid discount percentage: {discount_percent}. "
    f"Expected value between 0 and 100. "
    f"Received: {discount_percent} (type: {type(discount_percent).__name__}). "
    f"Fix: Ensure discount_percent is a number in range [0, 100]."
)

# Bad: Vague error
raise ValueError("Invalid input")

# Good: API error with context
{
    "error": {
        "code": "INVALID_DISCOUNT",
        "message": "Discount percentage must be between 0 and 100",
        "details": {
            "field": "discount_percent",
            "value": 150,
            "constraint": "0 <= value <= 100"
        },
        "request_id": "req_abc123"
    }
}
```

---

### 9.2 Structured Logging

**Definition:** Logging in structured format (JSON) with consistent field names and types.

**Why It Matters:** Structured logs are machine-parseable. AI can analyze logs to diagnose issues, identify patterns, suggest optimizations, and correlate events across distributed systems.

**Impact on Agent Behavior:**
- Log query and analysis capabilities
- Event correlation across services
- Pattern identification for debugging
- Data-driven optimization suggestions
- Anomaly detection

**Measurable Criteria:**
- Use structured logging library: structlog (Python), winston (Node.js), zap (Go)
- **Standard fields across all logs:**
  - timestamp (ISO 8601 format)
  - level (DEBUG, INFO, WARNING, ERROR, CRITICAL)
  - message (human-readable)
  - context: request_id, user_id, session_id, trace_id
- Consistent naming convention (snake_case or camelCase, not both)
- Log levels used appropriately
- **Never log sensitive data:** passwords, tokens, credit cards, PII (without anonymization)
- JSON format for production

**Citations:**
- Daily.dev: "12 Logging Best Practices: Do's & Don'ts"
- Dataset Blog: "Logging Best Practices: The 13 You Should Know"
- Technogise Medium: "Logging Practices: Guidelines for Developers"

**Example:**
```python
# Good: Structured logging
import structlog

logger = structlog.get_logger()

logger.info(
    "user_login_success",
    user_id="user_123",
    request_id="req_abc",
    duration_ms=45,
    ip_address="192.168.1.1"
)

# Output:
# {"timestamp": "2025-01-20T10:30:00Z", "level": "info", "event": "user_login_success",
#  "user_id": "user_123", "request_id": "req_abc", "duration_ms": 45, "ip_address": "192.168.1.1"}

# Bad: Unstructured logging
print("User user_123 logged in from 192.168.1.1 in 45ms")
```

---

## 10. API & INTERFACE DOCUMENTATION

### 10.1 OpenAPI/Swagger Specifications

**Definition:** Machine-readable API documentation in OpenAPI format (formerly Swagger).

**Why It Matters:** OpenAPI specs define everything needed to integrate with an API: authentication, endpoints, HTTP methods, request/response schemas, error codes. AI can read specs to generate client code, tests, and integration code automatically.

**Impact on Agent Behavior:**
- Auto-generation of SDKs and client libraries
- Request/response validation
- API mocking for testing
- Contract compliance verification
- Interactive API exploration

**Measurable Criteria:**
- OpenAPI 3.0+ specification file (openapi.yaml or openapi.json)
- **All endpoints documented with:**
  - Description and purpose
  - HTTP method (GET, POST, PUT, DELETE, PATCH)
  - Parameters (path, query, header)
  - Request body schema
  - Response schemas (success and error cases)
  - Authentication requirements
  - Example requests/responses
- Validation: Use Swagger Editor or Spectral
- Auto-generate from code annotations OR keep manually in sync
- Hosted documentation (Swagger UI, ReDoc)

**Citations:**
- Swagger Blog: "API Documentation Best Practices"
- APItoolkit: "OpenAPI Specification for API Development"
- APImatic: "14 Best Practices to Write OpenAPI for Better API Consumption"

**Example:**
```yaml
# Good: Comprehensive OpenAPI spec
openapi: 3.0.0
info:
  title: User API
  version: 1.0.0
paths:
  /users/{userId}:
    get:
      summary: Get user by ID
      parameters:
        - name: userId
          in: path
          required: true
          schema:
            type: string
      responses:
        '200':
          description: User found
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/User'
        '404':
          description: User not found
components:
  schemas:
    User:
      type: object
      required:
        - id
        - email
      properties:
        id:
          type: string
        email:
          type: string
          format: email
        name:
          type: string
```

---

### 10.2 GraphQL Schemas

**Definition:** Type definitions for GraphQL APIs using Schema Definition Language (SDL).

**Why It Matters:** GraphQL schemas are self-documenting and introspectable. AI can understand available queries, mutations, types, and relationships without exploring implementation code.

**Impact on Agent Behavior:**
- Generate type-safe queries
- Schema validation
- Performance optimization suggestions (N+1 query detection)
- Type-safe client generation
- API evolution guidance

**Measurable Criteria:**
- schema.graphql file in repository
- All types, queries, mutations include descriptions
- Use directives for:
  - Deprecation (@deprecated)
  - Authorization (@auth)
  - Field resolution hints
- Schema validation in CI/CD
- SDL-first approach (schema-first, not code-first)

**Citations:**
- GraphQL documentation: "Schema Definition Language"
- Apollo GraphQL: "Schema design best practices"
- Hasura GraphQL best practices

**Example:**
```graphql
# Good: Well-documented GraphQL schema
"""
Represents a user in the system
"""
type User {
  """
  Unique identifier for the user
  """
  id: ID!

  """
  User's email address (unique)
  """
  email: String!

  """
  User's display name
  """
  name: String

  """
  Posts created by this user
  """
  posts: [Post!]!
}

type Query {
  """
  Find a user by their unique ID
  """
  user(id: ID!): User

  """
  List all users with optional filtering
  """
  users(role: String, active: Boolean): [User!]!
}
```

---

## 11. MODULARITY & CODE ORGANIZATION

### 11.1 DRY Principle (Don't Repeat Yourself)

**Definition:** Every piece of knowledge has single, authoritative representation in the system.

**Why It Matters:** Research shows AI-generated code increases code churn and DRY violations (copy-paste instead of refactoring). Enforcing DRY in codebase teaches AI to refactor rather than duplicate.

**Impact on Agent Behavior:**
- Learns to extract shared logic
- Suggests refactorings instead of duplication
- Avoids creating duplicate implementations
- Better abstraction identification

**Measurable Criteria:**
- "Three Strikes" rule: Third duplicate occurrence triggers refactoring
- Tools detect duplication: SonarQube, PMD (Java), jscpd (JavaScript), pylint (Python)
- Shared logic extracted to:
  - Utility functions/modules
  - Base classes
  - Mixins/traits
  - Libraries
- **Balance:** Avoid premature abstraction ("prefer duplication over wrong abstraction")
- Target: <5% duplicate code

**Citations:**
- Wikipedia: "Don't repeat yourself"
- The Pragmatic Programmer by Hunt & Thomas
- Medium: "The DRY Principle and Incidental Duplication"
- Sandi Metz: "The Wrong Abstraction"

---

### 11.2 Consistent Naming Conventions

**Definition:** Systematic naming patterns for variables, functions, classes, files following language/framework conventions.

**Why It Matters:** Research shows identifier style affects recall and precision. Consistency reduces cognitive load. AI models recognize naming patterns from training on open-source code.

**Impact on Agent Behavior:**
- Accurate intent inference
- Appropriate name suggestions
- Code structure understanding
- Pattern recognition

**Measurable Criteria:**
- Follow language conventions:
  - **Python:** PEP 8 (snake_case functions, PascalCase classes, UPPER_CASE constants)
  - **JavaScript/TypeScript:** camelCase functions/variables, PascalCase classes
  - **Go:** mixedCaps (exported: UpperCase, unexported: lowerCase)
  - **Java:** camelCase methods, PascalCase classes, UPPER_CASE constants
- Use paired opposites consistently: add/remove, start/stop, begin/end, open/close
- Avoid abbreviations unless widely understood (HTTP, API, URL, ID)
- Enforce via linters: pylint, eslint, golint

**Citations:**
- Wikipedia: "Naming convention (programming)"
- Microsoft Learn: "General Naming Conventions"
- PEP 8 - Style Guide for Python Code
- Google Style Guides (Java, Python, JavaScript, Go)

**Example:**
```python
# Good: Consistent naming
class UserService:
    MAX_LOGIN_ATTEMPTS = 5

    def create_user(self, email: str) -> User:
        """Create new user."""
        pass

    def delete_user(self, user_id: str) -> None:
        """Delete existing user."""
        pass

# Bad: Inconsistent naming
class userservice:
    maxLoginAttempts = 5

    def CreateUser(self, e: str) -> User:
        pass

    def removeUser(self, uid: str) -> None:
        pass
```

---

### 11.3 Semantic File & Directory Naming

**Definition:** File names and directory structures that convey purpose and content clearly.

**Why It Matters:** Semantic organization helps AI locate relevant code quickly. Clear names reduce cognitive overhead and enable predictable file location.

**Impact on Agent Behavior:**
- Faster relevant file location
- Accurate placement suggestions for new code
- Better repository organization understanding
- Reduced search time

**Measurable Criteria:**
- **Feature-based organization:** Group related files by feature/domain, not technical layer
- **Clear, descriptive names:** `user_service.py` not `us.py`
- **Avoid abbreviations** unless standard in domain
- **Mirror test structure** to source structure:
  - `src/services/user_service.py` ‚Üí `tests/services/test_user_service.py`
- **Consistent file extensions:** .py, .js, .ts, .go
- **Module files:** `__init__.py`, index.js for package entry points

**Citations:**
- GitHub: kriasoft/Folder-Structure-Conventions
- Iterators: "Comprehensive Guide on Project Codebase Organization"
- Medium: "A Front-End Application Folder Structure that Makes Sense"

**Example:**
```
# Good: Feature-based, semantic organization
src/
‚îú‚îÄ‚îÄ auth/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ login_service.py
‚îÇ   ‚îú‚îÄ‚îÄ oauth_provider.py
‚îÇ   ‚îî‚îÄ‚îÄ session_manager.py
‚îú‚îÄ‚îÄ users/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ user_model.py
‚îÇ   ‚îú‚îÄ‚îÄ user_service.py
‚îÇ   ‚îî‚îÄ‚îÄ user_repository.py
‚îî‚îÄ‚îÄ billing/
    ‚îú‚îÄ‚îÄ __init__.py
    ‚îú‚îÄ‚îÄ payment_processor.py
    ‚îî‚îÄ‚îÄ invoice_generator.py

# Bad: Technical layer organization, unclear names
src/
‚îú‚îÄ‚îÄ models/
‚îÇ   ‚îú‚îÄ‚îÄ u.py
‚îÇ   ‚îî‚îÄ‚îÄ o.py
‚îú‚îÄ‚îÄ services/
‚îÇ   ‚îú‚îÄ‚îÄ svc1.py
‚îÇ   ‚îî‚îÄ‚îÄ svc2.py
‚îî‚îÄ‚îÄ utils/
    ‚îî‚îÄ‚îÄ helpers.py
```

---

## 12. CI/CD INTEGRATION

### 12.1 CI/CD Pipeline Visibility

**Definition:** Clear, well-documented CI/CD configuration files committed to repository.

**Why It Matters:** AI can understand build/test/deploy processes by reading CI configs. When builds fail, AI can suggest targeted fixes. Visible pipelines enable collaboration and debugging.

**Impact on Agent Behavior:**
- CI improvement proposals
- Pipeline failure debugging
- Workflow optimization suggestions
- Better understanding of deployment process

**Measurable Criteria:**
- CI config file in repository:
  - GitHub Actions: `.github/workflows/`
  - GitLab CI: `.gitlab-ci.yml`
  - CircleCI: `.circleci/config.yml`
- **Clear job/step names** (not "step1", "step2")
- **Comments explaining complex logic**
- **Fast feedback:** Tests complete <10 minutes
- **Fail fast:** Stop on first failure to save compute
- **Parallelization:** Run independent jobs concurrently
- **Caching:** Dependencies, build artifacts
- **Artifacts:** Test results, coverage reports, logs

**Citations:**
- CircleCI: "Monorepo dev practices"
- GitHub Actions documentation
- GitLab CI best practices
- Martin Fowler: "Continuous Integration"

---

### 12.2 Branch Protection & Status Checks

**Definition:** Required status checks and review approvals before merging to main/production branches.

**Why It Matters:** Prevents broken code from reaching production. Provides safety net for AI-generated code. Ensures quality gates are enforced.

**Impact on Agent Behavior:**
- Understanding of merge requirements
- Awareness of quality gates
- Suggestions aligned with branch policies
- Better PR creation (ensuring checks pass)

**Measurable Criteria:**
- Branch protection enabled for main/master/production
- **Required status checks:**
  - All tests passing
  - Linting/formatting passing
  - Code coverage threshold met
  - Security scanning passing
- **Required reviews:** At least 1 approval
- **No force pushes** to protected branches
- **No direct commits** to protected branches
- **Up-to-date branch** requirement (rebase/merge before merging)

**Citations:**
- GitHub Docs: "About protected branches"
- GitLab: "Protected branches"
- Industry best practices

---

## 13. SECURITY & COMPLIANCE

### 13.1 Security Scanning Automation

**Definition:** Automated security scans for vulnerabilities, secrets, and compliance issues in CI/CD.

**Why It Matters:** AI can accidentally introduce vulnerabilities (SQL injection, XSS, etc.). Research shows LLM-generated code has security weaknesses, particularly around outdated practices. Automated scanning provides safety net.

**Impact on Agent Behavior:**
- Security pattern learning
- Vulnerability avoidance
- Secure coding practice adoption
- Failed scans provide improvement feedback

**Measurable Criteria:**
- **Dependency scanning:** Snyk, Dependabot, npm audit, safety (Python)
- **Secret scanning:** GitLeaks, TruffleHog, detect-secrets
- **Static analysis:** Semgrep, CodeQL, Bandit (Python), gosec (Go)
- **Scans run on:**
  - Every PR (pre-merge)
  - Every commit to main
  - Scheduled (weekly/nightly)
- **Zero tolerance:** No high/critical vulnerabilities allowed to merge
- **SLA:** High severity vulnerabilities fixed within 7 days

**Citations:**
- ArXiv: "Security and Quality in LLM-Generated Code"
- ArXiv: "Security Degradation in Iterative AI Code Generation"
- GitHub Advanced Security documentation
- OWASP Top 10

---

### 13.2 Secrets Management

**Definition:** Proper handling of sensitive data (API keys, passwords, tokens) using secret management tools, not hardcoded values.

**Why It Matters:** Hardcoded secrets in code create security vulnerabilities. AI might accidentally suggest or expose secrets. Proper secrets management is critical security practice.

**Impact on Agent Behavior:**
- Avoids suggesting hardcoded secrets
- Recommends environment variables
- Identifies potential secret exposure
- Suggests secure alternatives

**Measurable Criteria:**
- **No secrets in code:** Use environment variables, secret managers
- **Tools:**
  - Development: .env files (not committed), direnv
  - Production: HashiCorp Vault, AWS Secrets Manager, Azure Key Vault, GCP Secret Manager
- **.env.example** committed (without real values)
- **.env** in .gitignore
- **Secret rotation** documented and automated
- **Pre-commit hook:** Detect-secrets or similar

**Citations:**
- OWASP: "Secrets Management Cheat Sheet"
- GitHub: "Removing sensitive data from a repository"
- HashiCorp Vault documentation

---

## 14. DOCUMENTATION PHILOSOPHY

### 14.1 "Why, Not What" Comments

**Definition:** Comments explain rationale and context, not behavior (which code already shows).

**Why It Matters:** AI can read code to understand "what" it does. Comments providing "why" give context for decisions, workarounds, constraints, and edge cases that aren't obvious from code alone.

**Impact on Agent Behavior:**
- Understanding of constraints and limitations
- Avoidance of "obvious" refactorings that break assumptions
- Preservation of original intent during modifications
- Better context for debugging and optimization

**Measurable Criteria:**
- **Comments explain:**
  - Why this approach was chosen (vs. alternatives)
  - Edge cases and gotchas
  - Performance considerations
  - Historical context (why workaround exists)
  - TODOs with context and rationale
- **Avoid:**
  - Redundant comments duplicating code
  - Commented-out code (use version control)
  - Obvious statements
- **Keep comments in sync** with code during changes

**Citations:**
- Stack Overflow Blog: "Best practices for writing code comments"
- Stepsize: "The Engineer's Guide to Writing Meaningful Code Comments"
- Boot.dev: "Best Practices for Commenting Code"

**Example:**
```python
# Good: Explains "why"
# Using binary search instead of hash table because dataset is
# read-once and memory-constrained (< 100MB available).
# Hash table would require 150MB for this dataset size.
result = binary_search(sorted_data, target)

# API returns 202 Accepted for async processing, but we need
# synchronous behavior for consistency. Poll until completion.
response = api.start_job()
while response.status == 202:
    time.sleep(1)
    response = api.check_status(response.job_id)

# Bad: Redundant, explains "what"
# Search for target in sorted_data
result = binary_search(sorted_data, target)

# Call the API
response = api.start_job()
```

---

## 15. PERFORMANCE & OBSERVABILITY

### 15.1 Performance Benchmarks

**Definition:** Automated performance tests tracking metrics like response time, throughput, memory usage.

**Why It Matters:** Performance regressions can slip in unnoticed. Benchmarks provide objective measurements. AI can suggest optimizations based on benchmark results.

**Impact on Agent Behavior:**
- Performance-aware optimization suggestions
- Regression detection
- Data-driven refactoring decisions
- Bottleneck identification

**Measurable Criteria:**
- Benchmark suite in repository
- **Tools:** pytest-benchmark (Python), Benchmark.js (JavaScript), testing.B (Go)
- Run benchmarks in CI for critical paths
- Track metrics over time
- Alert on regressions (>10% slowdown)

**Citations:**
- Google: "Benchmarking Best Practices"
- Python performance benchmarking docs
- Go benchmarking documentation

---

## IMPLEMENTATION PRIORITIES

### Tier 1: Essential (Must-Have)
**Highest impact, enables basic agent functionality:**

1. **CLAUDE.md** - 40% time savings, immediate context framing
2. **README with quick start** - Entry point understanding
3. **Type annotations** - Higher quality latent space, better comprehension
4. **Standard project layout** - Faster navigation
5. **Dependency lock files** - Reproducible builds

### Tier 2: Critical (Should-Have)
**Major quality improvements, safety nets:**

6. **Test coverage >70%** - Safety for refactoring
7. **Pre-commit hooks + CI/CD** - Automated quality enforcement
8. **Conventional commits** - Semantic versioning, history understanding
9. **Complete .gitignore** - Reduced context pollution
10. **One-command setup** - Easy environment reproduction

### Tier 3: Important (Nice-to-Have)
**Significant improvements in specific areas:**

11. **Cyclomatic complexity limits** - Better code comprehension
12. **Structured logging** - Machine-parseable debugging
13. **OpenAPI/GraphQL specs** - Auto-generated clients
14. **ADRs** - Architectural context
15. **Semantic naming** - Faster code location

### Tier 4: Advanced (Optimization)
**Refinement and optimization:**

16. **Security scanning** - Vulnerability prevention
17. **Performance benchmarks** - Regression detection
18. **Code smell elimination** - Higher quality baseline
19. **PR/Issue templates** - Consistent contributions
20. **Container setup** - Reproducible environments

---

## QUICKSTART: Making Your Codebase Agent-Ready

### Week 1: Foundation Documentation
```bash
# Create CLAUDE.md
cat > CLAUDE.md << 'EOF'
# Tech Stack
- [Your language/framework with versions]

# Standard Commands
- Setup: [command]
- Test: [command]
- Lint: [command]
- Build: [command]

# Repository Structure
- src/ - [description]
- tests/ - [description]

# Boundaries
- [Any off-limits areas]
EOF

# Update README
# Add: Installation, Quick Start, Testing sections

# Create .env.example
cp .env .env.example
# Remove sensitive values, keep variable names
```

### Week 2: Quality Automation
```bash
# Install pre-commit
pip install pre-commit

# Create .pre-commit-config.yaml
pre-commit sample-config > .pre-commit-config.yaml

# Add formatters, linters for your language
# Install hooks
pre-commit install

# Add commitlint (optional but recommended)
npm install -g @commitlint/cli @commitlint/config-conventional
```

### Week 3: Testing & Dependencies
```bash
# Measure test coverage
pytest --cov  # Python
jest --coverage  # JavaScript
go test -cover  # Go

# Generate lock file
pip freeze > requirements.txt  # Python
npm install  # Generates package-lock.json
go mod tidy  # Updates go.sum

# Add Dependabot
# Create .github/dependabot.yml
```

### Week 4: Structure & Types
```bash
# Refactor to standard layout (if needed)
# Add type annotations to public APIs
mypy --install-types  # Python
tsc --init  # TypeScript

# Create PR/Issue templates
mkdir -p .github/ISSUE_TEMPLATE
# Add bug_report.md, feature_request.md
# Add PULL_REQUEST_TEMPLATE.md
```

### Ongoing Maintenance
- Update CLAUDE.md as project evolves
- Create ADRs for architectural decisions
- Monitor code quality metrics (SonarQube, CodeClimate)
- Keep dependencies updated
- Review and improve test coverage

---

## MEASUREMENT & VALIDATION

### Agent-Ready Score Formula

```
Score = (
    Documentation * 0.25 +
    Code Quality * 0.20 +
    Testing * 0.20 +
    Structure * 0.15 +
    CI/CD * 0.10 +
    Security * 0.10
) * 100

Where each category is 0.0-1.0 based on attribute completion.
```

### Certification Levels

- **Platinum (90-100):** Exemplary agent-ready codebase
- **Gold (75-89):** Highly optimized for agents
- **Silver (60-74):** Well-suited for agent development
- **Bronze (40-59):** Basic agent compatibility
- **Needs Improvement (<40):** Significant agent friction

### Validation Checklist

**Documentation (25%):**
- [ ] CLAUDE.md exists and comprehensive
- [ ] README with quick start
- [ ] Inline documentation (docstrings) >80%
- [ ] ADRs for major decisions
- [ ] API specs (OpenAPI/GraphQL)

**Code Quality (20%):**
- [ ] Type annotations >80%
- [ ] Cyclomatic complexity <10
- [ ] Function length <50 lines
- [ ] Code smells <5 per 1000 LOC
- [ ] DRY violations minimal

**Testing (20%):**
- [ ] Test coverage >70%
- [ ] Descriptive test names
- [ ] Fast test execution (<10 min)
- [ ] Tests in CI/CD

**Structure (15%):**
- [ ] Standard project layout
- [ ] Semantic file/directory names
- [ ] Separation of concerns
- [ ] .gitignore complete

**CI/CD (10%):**
- [ ] Pre-commit hooks
- [ ] CI linting/testing
- [ ] Branch protection
- [ ] Automated dependency updates

**Security (10%):**
- [ ] Dependency scanning
- [ ] Secret scanning
- [ ] No hardcoded secrets
- [ ] Security scans in CI

---

## ANTI-PATTERNS TO AVOID

### Documentation Anti-Patterns
- ‚ùå No README or minimal README
- ‚ùå Outdated documentation
- ‚ùå No inline documentation
- ‚ùå Documentation in external wiki only

### Code Anti-Patterns
- ‚ùå God objects/functions (>500 lines)
- ‚ùå No type hints
- ‚ùå Magic numbers without explanation
- ‚ùå Unclear variable names (x, tmp, data)

### Testing Anti-Patterns
- ‚ùå No tests or minimal coverage (<30%)
- ‚ùå Test names like test1, test2
- ‚ùå Slow tests (>30 min)
- ‚ùå Flaky tests

### Structure Anti-Patterns
- ‚ùå Flat file structure
- ‚ùå Mixed concerns in single file
- ‚ùå Inconsistent naming
- ‚ùå Incomplete .gitignore

### Process Anti-Patterns
- ‚ùå No CI/CD
- ‚ùå Manual quality checks
- ‚ùå No branch protection
- ‚ùå Direct commits to main

---

## REFERENCES & CITATIONS

### Anthropic
- [Anthropic Engineering Blog: "Claude Code Best Practices"](https://www.anthropic.com/news/claude-code) (2025)
- [Claude.ai Documentation](https://docs.anthropic.com/en/docs/welcome)

### Research Papers (ArXiv)
- ["LongCodeBench: Evaluating Coding LLMs at 1M Context Windows"](https://arxiv.org/abs/2501.00343) (2025)
- ["TypyBench: Evaluating LLM Type Inference for Untyped Python Repositories"](https://arxiv.org/abs/2501.00000)
- ["Automated Type Annotation in Python Using LLMs"](https://arxiv.org/search/?query=type+annotation+llm&searchtype=all)
- ["Security and Quality in LLM-Generated Code"](https://arxiv.org/search/?query=security+llm+generated+code&searchtype=all)
- ["Security Degradation in Iterative AI Code Generation"](https://arxiv.org/search/?query=security+degradation+ai+code&searchtype=all)

### Industry (Microsoft, Google, GitHub)
- [Microsoft Learn: "Code metrics - Cyclomatic complexity"](https://learn.microsoft.com/en-us/visualstudio/code-quality/code-metrics-cyclomatic-complexity)
- [GitHub Blog: "How to write a great agents.md"](https://github.blog/)
- [GitHub: github/gitignore template collection](https://github.com/github/gitignore)
- [Google SRE Book: Logging and monitoring best practices](https://sre.google/sre-book/monitoring-distributed-systems/)
- [IBM Research: "Why larger LLM context windows are all the rage"](https://research.ibm.com/blog/llm-context-windows)

### Engineering Blogs
- [Dropbox Tech Blog: "Our journey to type checking 4 million lines of Python"](https://dropbox.tech/application/our-journey-to-type-checking-4-million-lines-of-python)
- [Salesforce Engineering: "How Cursor AI Cut Legacy Code Coverage Time by 85%"](https://engineering.salesforce.com/)
- [GitClear: "Coding on Copilot" whitepaper](https://www.gitclear.com/coding_on_copilot_data_shows_ais_downward_pressure_on_code_quality)

### Standards & Specifications
- [Conventional Commits specification v1.0.0](https://www.conventionalcommits.org/)
- [OpenAPI Specification 3.0+](https://spec.openapis.org/oas/v3.0.0)
- [PEP 8 - Style Guide for Python Code](https://peps.python.org/pep-0008/)
- [PEP 257 - Docstring Conventions](https://peps.python.org/pep-0257/)

### Community Resources
- [Real Python: "Python Application Layouts"](https://realpython.com/python-application-layouts/)
- [GitHub: golang-standards/project-layout](https://github.com/golang-standards/project-layout)
- [GitHub: joelparkerhenderson/architecture-decision-record](https://github.com/joelparkerhenderson/architecture-decision-record)
- [GitHub: pre-commit/pre-commit](https://github.com/pre-commit/pre-commit)

### Documentation
- Python: [pytest](https://docs.pytest.org/), [mypy](https://mypy.readthedocs.io/), [black](https://black.readthedocs.io/), [isort](https://pycqa.github.io/isort/) documentation
- JavaScript/TypeScript: [ESLint](https://eslint.org/docs/), [Prettier](https://prettier.io/docs/), [TSDoc](https://tsdoc.org/) documentation
- Go: [Official style guide](https://go.dev/doc/effective_go), [testing documentation](https://go.dev/doc/tutorial/add-a-test)
- Docker: [Best practices documentation](https://docs.docker.com/develop/dev-best-practices/)

---

## VERSION HISTORY

- **v1.0.0 (2025-01-20):** Initial comprehensive research compilation
  - 25 attributes identified and documented
  - 50+ authoritative sources cited
  - Measurement framework established
  - Implementation guide created

---

**Document prepared for:** agentready tool development
**Primary use case:** Scanning repositories for AI agent optimization
**Target agents:** Claude Code, Claude-based development assistants
**Methodology:** Evidence-based, cited research from authoritative sources
````

## File: GITHUB_ISSUES.md
````markdown
# AgentReady - GitHub Issues Template

**Purpose**: GitHub-ready issue descriptions for all backlog items, organized by priority.

**Instructions**: Copy each issue block below into a new GitHub issue.

---

## P0 - Critical Issues (Blocking Adoption)

### Issue #1: Bootstrap AgentReady Repository on GitHub (DOGFOODING!)

**Labels**: `P0-critical`, `feature`, `cli`, `github-integration`, `dogfooding`

**Title**: Implement `agentready bootstrap` command and use it to set up agentready repository itself

**Description**:

**THIS IS THE FIRST FEATURE TO IMPLEMENT** - We'll dogfood our own tool by using it to bootstrap the agentready repository on GitHub!

Build `agentready bootstrap` command that creates:
- GitHub Actions workflows (assessment, tests, linters)
- GitHub templates (issue templates, PR template, CODEOWNERS)
- Pre-commit hooks (.pre-commit-config.yaml)
- Dependabot configuration
- Documentation (CONTRIBUTING.md, CODE_OF_CONDUCT.md, LICENSE)
- Repository badges

**Why This Is P0**:
1. **Dogfooding** - We need this for agentready repo itself FIRST
2. **Demonstrates Value** - Shows AgentReady in action on our own codebase
3. **Foundation** - Required before GitHub App integration
4. **Credibility** - Can't ask others to use it if we don't use it ourselves

**Command Interface**:
```bash
# Bootstrap current repository
agentready bootstrap .

# Dry run (show what would be created)
agentready bootstrap . --dry-run

# Interactive mode (confirm each file)
agentready bootstrap . --interactive
```

**What Gets Created**:
```
.github/
‚îú‚îÄ‚îÄ workflows/
‚îÇ   ‚îú‚îÄ‚îÄ agentready-assessment.yml  # Run assessment on PR/push
‚îÇ   ‚îú‚îÄ‚îÄ tests.yml                  # Run pytest, black, isort, ruff
‚îÇ   ‚îî‚îÄ‚îÄ security.yml              # Dependabot + security scanning
‚îú‚îÄ‚îÄ ISSUE_TEMPLATE/
‚îÇ   ‚îú‚îÄ‚îÄ bug_report.md
‚îÇ   ‚îî‚îÄ‚îÄ feature_request.md
‚îú‚îÄ‚îÄ PULL_REQUEST_TEMPLATE.md
‚îú‚îÄ‚îÄ CODEOWNERS
‚îî‚îÄ‚îÄ dependabot.yml

.pre-commit-config.yaml
CONTRIBUTING.md
CODE_OF_CONDUCT.md
LICENSE
```

**GitHub Actions - AgentReady Assessment Workflow**:
```yaml
name: AgentReady Assessment
on:
  pull_request:
  push:
    branches: [main]

jobs:
  assess:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-python@v5
        with:
          python-version: '3.11'
      - run: pip install agentready
      - run: agentready assess . --verbose
      - uses: actions/upload-artifact@v4
        with:
          name: agentready-reports
          path: .agentready/
      - name: Comment PR
        if: github.event_name == 'pull_request'
        # Post markdown report as PR comment
```

**Implementation**:

**Files to Create**:
- `src/agentready/cli/bootstrap.py` - CLI command
- `src/agentready/bootstrap/generator.py` - File generator
- `src/agentready/bootstrap/templates/` - Jinja2 templates
  - `github/workflows/agentready.yml.j2`
  - `github/workflows/tests.yml.j2`
  - `github/ISSUE_TEMPLATE/bug_report.md.j2`
  - `github/PULL_REQUEST_TEMPLATE.md.j2`
  - `precommit.yaml.j2`
  - `CONTRIBUTING.md.j2`
- `tests/unit/test_bootstrap.py`

**Implementation Phases**:

**Phase 1**: Core Bootstrap (This Sprint)
- [ ] Implement `agentready bootstrap` CLI command
- [ ] Create template files (GitHub Actions, pre-commit, templates)
- [ ] Add Jinja2 rendering
- [ ] Test on agentready repository
- [ ] Commit all generated files to agentready repo

**Phase 2**: GitHub Integration (Future)
- [ ] Use `gh` CLI to update repository settings
- [ ] Set up branch protection
- [ ] Add repository badges
- [ ] Configure repository topics

**Acceptance Criteria**:
- [ ] `agentready bootstrap .` creates all files
- [ ] GitHub Actions workflows are valid YAML
- [ ] Pre-commit hooks install successfully
- [ ] Dry-run mode shows preview
- [ ] Interactive mode prompts for confirmation
- [ ] **Successfully bootstrap agentready repository itself**
- [ ] All generated files committed to agentready repo
- [ ] GitHub Actions runs on next PR

**Success Metrics**:
After running `agentready bootstrap .` on agentready repo:
- ‚úÖ GitHub Actions workflow runs assessment on PRs
- ‚úÖ Pre-commit hooks prevent bad commits
- ‚úÖ Issue/PR templates guide contributors
- ‚úÖ Dependabot keeps dependencies updated
- ‚úÖ Repository badge shows AgentReady score
- ‚úÖ All linters pass in CI

**Priority Justification**: We need to dogfood our own tool FIRST. This validates the bootstrap design and sets up critical infrastructure.

**Related Issues**: #5 (GitHub App will enhance this), #3 (Align will complement this)

---

### Issue #2: Add Report Header with Repository Metadata

**Labels**: `P0-critical`, `ux`, `reports`, `bug`

**Title**: Add prominent report header showing repository name, path, and assessment metadata

**Description**:

Currently, AgentReady reports (HTML, Markdown, JSON) lack context about what repository was assessed. Users cannot immediately identify:
- What repository was scanned
- When the assessment was run
- Which branch/commit was assessed
- Who executed the assessment

This blocks adoption in multi-repository workflows and CI/CD environments.

**Expected Behavior**:

All reports should have a **prominent header section** at the top showing:
- Repository name (bold, large)
- Repository path (filesystem path or GitHub URL)
- Assessment timestamp (human-readable)
- Git branch and commit hash
- AgentReady version
- User who ran assessment (username@hostname)
- Command executed

**Implementation**:

HTML Report Header:
```html
<header class="report-header">
  <h1>AgentReady Assessment Report</h1>
  <div class="repo-name">Repository: agentready</div>
  <div class="repo-path">/Users/jeder/repos/sk/agentready</div>
  <div class="repo-git">Branch: main | Commit: abc123</div>
  <div class="meta">
    Assessed: November 21, 2025 at 2:11 AM
    AgentReady: v1.0.0
    By: jeder@macbook
  </div>
</header>
```

**Files to Modify**:
- `src/agentready/models/assessment.py` - Add metadata field
- `src/agentready/services/scanner.py` - Collect metadata (version, user, command)
- `src/agentready/templates/report.html.j2` - Add header section
- `src/agentready/reporters/markdown.py` - Add header to markdown
- `src/agentready/reporters/html.py` - Pass metadata to template

**Acceptance Criteria**:
- [ ] Repository name and path visible at top of all reports
- [ ] Assessment timestamp in human-readable format
- [ ] Git context (branch, commit) included
- [ ] AgentReady version tracked
- [ ] Header positioned before score summary
- [ ] All three report formats updated (HTML, Markdown, JSON)

**Priority Justification**: Users are confused about what reports show. This blocks real-world usage.

---

### Issue #3: Redesign HTML Report - Fix Font Sizes and Color Scheme

**Labels**: `P0-critical`, `ux`, `design`, `reports`, `accessibility`

**Title**: Increase font sizes (+4pt) and replace color scheme with professional dark blue/purple/white palette

**Description**:

Current HTML report has two critical UX problems:
1. **Font sizes too small** - Base font is 14px, headings are small, hard to read
2. **Color scheme problems** - Purple gradient everywhere is overwhelming and unprofessional

User feedback: "the color scheme is hideous"

**Current Problems**:
- Body text: 14px (too small for modern displays)
- Purple gradient background used everywhere (#667eea ‚Üí #764ba2)
- Low contrast in some areas
- Not suitable for Red Hat engineering presentations

**Required Changes**:

**1. Font Size Increases** (minimum +4pt):
```css
body: 14px ‚Üí 18px
h1: 28px ‚Üí 36px
h2: 24px ‚Üí 30px
h3: 20px ‚Üí 26px
.score: 48px ‚Üí 56px
.attribute-name: 16px ‚Üí 22px
```

**2. New Color Scheme** (Dark/Professional):
```css
/* Replace current purple gradient with: */
--background: #0a0e27;      /* Almost black with blue tint */
--surface: #1a1f3a;         /* Dark blue surface */
--primary: #8b5cf6;         /* Purple (accent only) */
--text-primary: #f8fafc;    /* Almost white */
--success: #10b981;         /* Green (pass) */
--danger: #ef4444;          /* Red (fail) */
```

**Design Direction**: Mostly black, dark blue, purple accents, white text - professional and clean.

**Files to Modify**:
- `src/agentready/templates/report.html.j2` - Update all CSS

**Acceptance Criteria**:
- [ ] Base font size increased to 18px minimum
- [ ] All headings increased by 4pt+
- [ ] Purple gradient removed from backgrounds
- [ ] Dark blue (#1a1f3a) used for surfaces
- [ ] Purple (#8b5cf6) used sparingly as accent
- [ ] White text (#f8fafc) on dark backgrounds
- [ ] WCAG 2.1 AA contrast ratios maintained
- [ ] Professional appearance suitable for stakeholder presentations

**Priority Justification**: Visual design blocks adoption. This is the first thing users see.

**Related Issues**: #3 (Theme system will build on this foundation)

---

## P1 - Critical Features

### Issue #4: Implement `agentready align` Subcommand for Automated Remediation

**Labels**: `P1-critical`, `feature`, `cli`, `automation`

**Title**: Add `agentready align` command to automatically fix failing attributes

**Description**:

Users need an automated way to improve their repository scores. The `align` subcommand should generate and apply fixes for failing attributes.

**Vision**: One command to align repository with best practices.

**Proposed Interface**:
```bash
agentready align .                           # Fix all failures
agentready align . --dry-run                 # Preview changes
agentready align . --create-pr               # Create GitHub PR
agentready align . --interactive             # Confirm each fix
agentready align . --attributes claude_md_file,precommit_hooks
```

**Supported Fix Types**:

1. **Template-based** (auto-applicable):
   - CLAUDE.md generation
   - .gitignore patterns
   - .pre-commit-config.yaml
   - README scaffolding
   - GitHub issue/PR templates

2. **Command-based** (execute commands):
   - Generate lock files (`npm install`, `poetry lock`)
   - Install pre-commit hooks
   - Update dependencies

3. **AI-powered** (optional, requires --ai):
   - Add type annotations
   - Generate docstrings
   - Refactor high-complexity functions

**Workflow Example**:
```
$ agentready align . --dry-run

Repository: my-project
Current Score: 62.4/100 (Silver)
Projected Score: 84.7/100 (Gold) üéØ

Changes:
  ‚úÖ claude_md_file (+10 pts) - CREATE CLAUDE.md
  ‚úÖ precommit_hooks (+3 pts) - CREATE .pre-commit-config.yaml
  ‚úÖ gitignore (+3 pts) - UPDATE .gitignore

Apply changes? [y/N]
```

**Implementation Phases**:

**Phase 1**: Template-based fixes
- Create `src/agentready/fixers/` module
- Implement base fixer class
- Add template-based fixers (CLAUDE.md, .gitignore, pre-commit)
- Add `align` CLI command
- Support --dry-run flag

**Phase 2**: Command execution & GitHub PR
- Execute shell commands safely
- Add --create-pr flag
- Implement GitHub PR creation via `gh` CLI
- Show before/after scores

**Phase 3**: AI-powered fixes (future)
- Integrate with LLM APIs
- Add type annotation fixer
- Add docstring generator
- Require explicit --ai flag

**Files to Create**:
- `src/agentready/fixers/base.py` - BaseFixer abstract class
- `src/agentready/fixers/template.py` - Template-based fixers
- `src/agentready/fixers/command.py` - Command execution
- `src/agentready/cli/align.py` - CLI command
- `src/agentready/templates/fixes/` - Fix templates directory
- `tests/unit/test_fixers.py` - Fixer tests

**Acceptance Criteria**:
- [ ] `agentready align .` applies all fixable failures
- [ ] `--dry-run` shows preview without applying changes
- [ ] `--create-pr` creates GitHub PR with fixes
- [ ] Score improvement projection shown
- [ ] Template-based fixes working (CLAUDE.md, .gitignore, pre-commit)
- [ ] Comprehensive tests

**Priority Justification**: Most requested feature. Dramatically improves value proposition.

**Related**: Interactive Dashboard (#5), Bootstrap (#1)

---

## P2 - High Value Features

### Issue #5: Create Interactive Dashboard with One-Click Remediation

**Labels**: `P2-high`, `feature`, `dashboard`, `ux`, `github-integration`

**Title**: Build interactive dashboard for live assessment updates and one-click GitHub issue/PR creation

**Description**:

Transform static HTML report into interactive dashboard with:
- Real-time assessment updates (WebSocket)
- "Fix This" button on each failure ‚Üí creates GitHub issue + draft PR
- Historical trend visualization
- Live filtering/sorting (already implemented)

**Vision**: Click "Fix This" ‚Üí GitHub issue created ‚Üí Draft PR with proposed changes

**Implementation Phases**:

**Phase 1**: Enhanced Static Report
- Add "Copy gh command" buttons to HTML report
- Generate `gh issue create` commands for each failure
- Include fix templates in issue descriptions

**Phase 2**: Local Dashboard Server
- Flask/FastAPI server serving dashboard
- WebSocket for live updates
- GitHub integration via `gh` CLI
- Preview diffs before applying fixes

**Phase 3**: Cloud Service (future)
- Hosted dashboard at agentready.redhat.com
- GitHub App installation
- Continuous monitoring
- Team collaboration

**Acceptance Criteria (Phase 1)**:
- [ ] "Copy Command" button for each failure
- [ ] Generated `gh` commands create properly formatted issues
- [ ] Fix templates embedded in issue descriptions
- [ ] Links to AgentReady documentation

**Priority Justification**: Significantly improves workflow - users can fix issues with one click.

**Related**: Align command (#4), GitHub App (#6), Bootstrap (#1)

---

### Issue #6: Implement GitHub App Integration (Badge & Status Checks)

**Labels**: `P2-high`, `feature`, `github-integration`, `ci-cd`

**Title**: Create GitHub App for repository badges, PR status checks, and automated comments

**Description**:

Provide GitHub integration for visibility and CI/CD workflows:
- Repository badges showing certification level
- GitHub Actions integration
- PR status checks (block merge if score < threshold)
- Automated PR comments with score deltas

**Core Features**:

1. **Badge Service**
   - Endpoint: `https://agentready.redhat.com/badge/{owner}/{repo}.svg`
   - Dynamic color based on certification (Platinum, Gold, Silver, Bronze)
   - Shields.io-compatible format

2. **GitHub Actions**
   - Official `agentready/assess-action`
   - Run on PR events and pushes
   - Upload artifacts (HTML/JSON reports)

3. **PR Status Checks**
   - Use GitHub Checks API
   - Configurable thresholds
   - Link to detailed report

4. **PR Comments**
   - Automated bot comments
   - Score delta: "72.4 ‚Üí 78.3 (+5.9)"
   - New failures/fixes listed

**Implementation**:

**Phase 1**: GitHub Actions Integration
```yaml
# Create .github/workflows/agentready.yml
- uses: agentready/assess-action@v1
  with:
    threshold: 75
    post-comment: true
```

**Phase 2**: Badge Service
- FastAPI endpoint generating SVG badges
- Fetch latest assessment from GitHub Actions artifacts

**Phase 3**: GitHub App
- App installation via Red Hat GitHub Enterprise
- Dashboard at agentready.redhat.com
- Red Hat SSO authentication

**Acceptance Criteria**:
- [ ] GitHub Action published and working
- [ ] Badge service generating SVGs
- [ ] Status checks posted to PRs
- [ ] Comments show score deltas
- [ ] Integration with Red Hat infrastructure

**Priority Justification**: Critical for CI/CD adoption and visibility within Red Hat.

**Notes**: All infrastructure stays within Red Hat (no external services).

---

## P3 - Important Enhancements

### Issue #7: Implement Report Schema Versioning

**Labels**: `P3-important`, `schema`, `contracts`, `backwards-compatibility`

**Title**: Define and version JSON/HTML/Markdown report schemas for backwards compatibility

**Description**:

Formalize report schemas and establish versioning to ensure backwards compatibility as the tool evolves.

**Requirements**:
- JSON schema for assessment reports
- HTML schema documentation
- Markdown schema documentation
- Semantic versioning strategy
- Schema migration tools

**Files**:
- `contracts/assessment-schema.json` (already exists, needs versioning)
- `contracts/report-html-schema.md` (exists)
- `contracts/report-markdown-schema.md` (exists)

**Acceptance Criteria**:
- [ ] JSON Schema Draft 2020-12 used
- [ ] `agentready validate-report` command
- [ ] Schema version in report metadata
- [ ] Migration tools for major version changes

**Priority Justification**: Important for enterprise adoption and long-term maintenance.

---

### Issue #8: Create AgentReady Repository Development Agent

**Labels**: `P3-important`, `developer-experience`, `claude-code`

**Title**: Build specialized Claude Code agent for AgentReady development tasks

**Description**:

Create `.claude/agents/agentready-dev.md` agent with deep knowledge of AgentReady architecture to assist with:
- Implementing new assessors
- Writing tests
- Debugging assessment issues
- Optimizing performance

**Acceptance Criteria**:
- [ ] Agent specification created
- [ ] Links to design docs (data-model.md, plan.md)
- [ ] Common development patterns documented
- [ ] Test structure examples included

**Priority Justification**: Accelerates development and helps contributors.

---

## P4 - Nice-to-Have Enhancements

### Issue #9: Build Research Report Generator/Updater Utility

**Labels**: `P4-enhancement`, `tooling`, `documentation`

**Title**: Create utility to maintain research report (agent-ready-codebase-attributes.md)

**Description**:

Build CLI tool to validate and update research report:
- `agentready research validate`
- `agentready research add-attribute`
- `agentready research format`

**Acceptance Criteria**:
- [ ] Schema validation against contracts/research-report-schema.md
- [ ] Automated metadata generation
- [ ] Attribute numbering checks
- [ ] Citation deduplication

**Priority Justification**: Improves research report quality and maintainability.

---

### Issue #10: Integrate Repomix for Repository Context Generation

**Labels**: `P4-enhancement`, `ai-integration`, `tooling`

**Title**: Add Repomix integration for AI-optimized repository context files

**Description**:

Integrate with Repomix (https://github.com/yamadashy/repomix) to generate repository context:
- `agentready repomix-generate`
- Include in bootstrapped repositories
- GitHub Actions integration

**Acceptance Criteria**:
- [ ] Generate Repomix output for existing repos
- [ ] Include in bootstrap workflow
- [ ] GitHub Actions workflow template

**Priority Justification**: Enhances AI-assisted development workflow.

---

### Issue #11: Implement Customizable HTML Report Themes

**Labels**: `P4-enhancement`, `ux`, `accessibility`, `theming`

**Title**: Add theme system with dark/light mode toggle and custom color schemes

**Description**:

Allow users to customize HTML report appearance:
- **Theme dropdown** in top-right corner
- **Quick dark/light toggle button**
- Multiple built-in themes (solarized, dracula, high-contrast)
- localStorage persistence

**Note**: This builds on Issue #2 (P0) which establishes the base dark theme.

**Acceptance Criteria**:
- [ ] Theme dropdown in HTML report
- [ ] Dark/light toggle button
- [ ] 5+ built-in themes
- [ ] localStorage saves preference
- [ ] All themes WCAG 2.1 AA compliant

**Priority Justification**: Improves user experience and accessibility.

---

## P5 - Future Enhancements

_No items currently - Bootstrap feature was promoted to P0 as Issue #1 (Dogfooding!)_

---

## Issue Creation Checklist

When creating GitHub issues from this template:

1. **Copy the entire issue block** (including title, labels, description, acceptance criteria)
2. **Set labels** as specified in each issue
3. **Set milestone**: Create milestones for v1.1 (P0), v1.2 (P1-P2), v1.3 (P3-P4)
4. **Assign**: Assign P0 issues immediately
5. **Link related issues**: Use "Related to #X" in issue descriptions
6. **Add to project board**: Create columns for P0, P1, P2, P3+

## Label Definitions

Create these labels in GitHub:
- `P0-critical` (red) - Blocking adoption
- `P1-critical` (orange) - Critical features
- `P2-high` (yellow) - High value
- `P3-important` (blue) - Important enhancements
- `P4-enhancement` (green) - Nice to have
- `P5-future` (gray) - Future work
- `ux` - User experience
- `reports` - Report generation
- `feature` - New feature
- `bug` - Bug fix
- `cli` - CLI changes
- `automation` - Automation features
- `github-integration` - GitHub integration
- `accessibility` - Accessibility improvements

## Milestones

**v1.1 - Bootstrap & Critical UX** (Target: Sprint 1)
- Issue #1: Bootstrap Command (FIRST - Dogfooding!)
- Issue #2: Report Header Metadata
- Issue #3: HTML Design Improvements

**v1.2 - Automation & Integration** (Target: Sprint 2-3)
- Issue #4: Align Subcommand
- Issue #5: Interactive Dashboard (Phase 1)
- Issue #6: GitHub App Integration (Phase 1)

**v1.3 - Polish & Documentation** (Target: Sprint 4+)
- Issue #7: Schema Versioning
- Issue #8: Development Agent
- Issue #9-11: Enhancements
````

## File: README.md
````markdown
# AgentReady Repository Scorer

Assess git repositories against 25 evidence-based attributes for AI-assisted development readiness.

> **üìö Research-Based Assessment**: AgentReady's 25 attributes are derived from [comprehensive research](agent-ready-codebase-attributes.md) analyzing 50+ authoritative sources including **Anthropic**, **Microsoft**, **Google**, **ArXiv**, and **IEEE/ACM**. Each attribute is backed by peer-reviewed research and industry best practices. [View full research report ‚Üí](agent-ready-codebase-attributes.md)

## Overview

AgentReady evaluates your repository across multiple dimensions of code quality, documentation, testing, and infrastructure to determine how well-suited it is for AI-assisted development workflows. The tool generates comprehensive reports with:

- **Overall Score & Certification**: Platinum/Gold/Silver/Bronze based on 25 attributes
- **Interactive HTML Reports**: Filter, sort, and explore findings with embedded guidance
- **Version-Control-Friendly Markdown**: Track progress over time with git-diffable reports
- **Actionable Remediation**: Specific tools, commands, and examples to improve each attribute

## Quick Start

### Installation

```bash
# Clone the repository
git clone https://github.com/yourusername/agentready.git
cd agentready

# Create virtual environment
python3 -m venv .venv
source .venv/bin/activate  # On Windows: .venv\Scripts\activate

# Install dependencies
pip install -e ".[dev]"
```

### Basic Usage

```bash
# Assess current repository
agentready .

# Assess another repository
agentready /path/to/your/repo

# Specify custom configuration
agentready /path/to/repo --config my-config.yaml

# Custom output directory
agentready /path/to/repo --output-dir ./reports
```

### Example Output

```
Assessing repository: myproject
Repository: /Users/username/myproject
Languages detected: Python (42 files), JavaScript (18 files)

Evaluating 25 attributes...
[‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë] 23/25 (2 skipped)

Overall Score: 72.5/100 (Silver)
Attributes Assessed: 23/25
Duration: 2m 7s

Reports generated:
  HTML: .agentready/report-latest.html
  Markdown: .agentready/report-latest.md
```

## Features

### 25 Evidence-Based Attributes

Evaluated across 13 categories:

1. **Context Window Optimization**: CLAUDE.md files, concise docs, file size limits
2. **Documentation Standards**: README structure, inline docs, ADRs
3. **Code Quality**: Cyclomatic complexity, file length, type annotations, code smells
4. **Repository Structure**: Standard layouts, separation of concerns
5. **Testing & CI/CD**: Coverage, test naming, pre-commit hooks
6. **Dependency Management**: Lock files, freshness, security
7. **Git & Version Control**: Conventional commits, gitignore, templates
8. **Build & Development**: One-command setup, dev docs, containers
9. **Error Handling**: Clear messages, structured logging
10. **API Documentation**: OpenAPI/Swagger specs
11. **Modularity**: DRY principle, naming conventions
12. **CI/CD Integration**: Pipeline visibility, branch protection
13. **Security**: Scanning automation, secrets management

### Tier-Based Scoring

Attributes are weighted by importance:

- **Tier 1 (Essential)**: 50% of total score - CLAUDE.md, README, types, layouts, lock files
- **Tier 2 (Critical)**: 30% of total score - Tests, commits, build setup
- **Tier 3 (Important)**: 15% of total score - Complexity, logging, API docs
- **Tier 4 (Advanced)**: 5% of total score - Security scanning, performance benchmarks

Missing essential attributes (especially CLAUDE.md at 10% weight) has 10x the impact of missing advanced features.

### Interactive HTML Reports

- Filter by status (Pass/Fail/Skipped)
- Sort by score, tier, or category
- Search attributes by name
- Collapsible sections with detailed evidence
- Color-coded score indicators
- Certification ladder visualization
- Works offline (no CDN dependencies)

### Customization

Create `.agentready-config.yaml` to customize weights:

```yaml
weights:
  claude_md_file: 0.15      # Increase importance (default: 0.10)
  test_coverage: 0.05       # Increase importance (default: 0.03)
  conventional_commits: 0.01  # Decrease importance (default: 0.03)
  # Other 22 attributes use defaults, rescaled to sum to 1.0

excluded_attributes:
  - performance_benchmarks  # Skip this attribute

output_dir: ./custom-reports
```

## CLI Reference

```bash
# Assessment commands
agentready PATH                          # Assess repository at PATH
agentready PATH --verbose                # Show detailed progress
agentready PATH --config FILE            # Use custom configuration
agentready PATH --output-dir DIR         # Custom report location

# Configuration commands
agentready --validate-config FILE        # Validate configuration
agentready --generate-config             # Create example config

# Research report management
agentready --research-version            # Show bundled research version
agentready --update-research             # Update to latest research
agentready --restore-bundled-research    # Restore original research

# Utility commands
agentready --version                     # Show tool version
agentready --help                        # Show help message
```

## Architecture

AgentReady follows a library-first design:

- **Models**: Data entities (Repository, Assessment, Finding, Attribute)
- **Assessors**: Independent evaluators for each attribute category
- **Services**: Scanner (orchestration), Scorer (calculation), LanguageDetector
- **Reporters**: HTML and Markdown report generators
- **CLI**: Thin wrapper orchestrating assessment workflow

## Development

### Run Tests

```bash
# Run all tests with coverage
pytest

# Run specific test suite
pytest tests/unit/
pytest tests/integration/
pytest tests/contract/

# Run with verbose output
pytest -v -s
```

### Code Quality

```bash
# Format code
black src/ tests/

# Sort imports
isort src/ tests/

# Lint code
flake8 src/ tests/ --ignore=E501

# Run all checks
black . && isort . && flake8 .
```

### Project Structure

```
src/agentready/
‚îú‚îÄ‚îÄ cli/              # Click-based CLI entry point
‚îú‚îÄ‚îÄ assessors/        # Attribute evaluators (13 categories)
‚îú‚îÄ‚îÄ models/           # Data entities
‚îú‚îÄ‚îÄ services/         # Core logic (Scanner, Scorer)
‚îú‚îÄ‚îÄ reporters/        # HTML and Markdown generators
‚îú‚îÄ‚îÄ templates/        # Jinja2 HTML template
‚îî‚îÄ‚îÄ data/             # Bundled research report and defaults

tests/
‚îú‚îÄ‚îÄ unit/             # Unit tests for individual components
‚îú‚îÄ‚îÄ integration/      # End-to-end workflow tests
‚îú‚îÄ‚îÄ contract/         # Schema validation tests
‚îî‚îÄ‚îÄ fixtures/         # Test repositories
```

## Research Foundation

All 25 attributes are derived from evidence-based research with 50+ citations from:

- Anthropic (Claude Code documentation, engineering blog)
- Microsoft (Code metrics, Azure DevOps best practices)
- Google (SRE handbook, style guides)
- ArXiv (Software engineering research papers)
- IEEE/ACM (Academic publications on code quality)

See `src/agentready/data/agent-ready-codebase-attributes.md` for complete research report.

## License

MIT License - see LICENSE file for details.

## Contributing

Contributions welcome! Please ensure:

- All tests pass (`pytest`)
- Code is formatted (`black`, `isort`)
- Linting passes (`flake8`)
- Test coverage >80%

## Support

- Documentation: See `/docs` directory
- Issues: Report at GitHub Issues
- Questions: Open a discussion on GitHub

---

**Quick Start**: `pip install -e ".[dev]" && agentready .` - Ready in <5 minutes!
````

## File: coldstart-prompts/README.md
````markdown
# AgentReady Coldstart Implementation Prompts

**Generated**: 2025-11-21
**Total Prompts**: 19
**Source**: BACKLOG.md + Community Contributions

---

## Overview

This directory contains comprehensive coldstart prompts for implementing all features in the AgentReady backlog. Each prompt is a self-contained implementation guide that includes:

- Full context about AgentReady architecture
- Complete feature requirements from BACKLOG.md
- Implementation checklist with all steps
- Testing strategy and coverage requirements
- Key files to review before starting
- Getting started guide with exact commands
- Success criteria for completion

These prompts are designed to be "drop in and go" - an AI agent or developer can take any prompt and implement the feature independently without needing additional context.

---

## Priority-Based Index

### P0 (Critical) - 6 Items

| # | File | Title | Size | Status |
|---|------|-------|------|--------|
| 01 | [01-create-automated-demo.md](01-create-automated-demo.md) | Create Automated Demo | 8.7K | üî¥ Not Started |
| 02 | [02-fix-critical-security-logic-bugs-from-code-review.md](02-fix-critical-security-logic-bugs-from-code-review.md) | Fix Critical Security & Logic Bugs from Code Review | 7.4K | üî¥ Not Started |
| 03 | [03-bootstrap-agentready-repository-on-github.md](03-bootstrap-agentready-repository-on-github.md) | Bootstrap AgentReady Repository on GitHub | 11K | üî¥ Not Started |
| 04 | [04-report-header-with-repository-metadata.md](04-report-header-with-repository-metadata.md) | Report Header with Repository Metadata | 6.8K | üî¥ Not Started |
| 05 | [05-improve-html-report-design-font-size-color-scheme.md](05-improve-html-report-design-font-size-color-scheme.md) | Improve HTML Report Design (Font Size & Color Scheme) | 8.6K | üî¥ Not Started |
| 17 | [17-add-bootstrap-quickstart-to-readme.md](17-add-bootstrap-quickstart-to-readme.md) | Add Bootstrap Quickstart to README.md | 7.2K | üî¥ Not Started |

### P1 (High) - 3 Items

| # | File | Title | Size | Status |
|---|------|-------|------|--------|
| 11 | [11-fix-code-quality-issues-from-code-review.md](11-fix-code-quality-issues-from-code-review.md) | Fix Code Quality Issues from Code Review | 6.6K | üî¥ Not Started |
| 12 | [12-improve-test-coverage-and-edge-case-handling.md](12-improve-test-coverage-and-edge-case-handling.md) | Improve Test Coverage and Edge Case Handling | 6.5K | üî¥ Not Started |
| 14 | [14-align-subcommand-automated-remediation.md](14-align-subcommand-automated-remediation.md) | Align Subcommand (Automated Remediation) | 12K | üî¥ Not Started |

### P2 (Medium) - 5 Items

| # | File | Title | Size | Status |
|---|------|-------|------|--------|
| 13 | [13-add-security-quality-improvements-from-code-review.md](13-add-security-quality-improvements-from-code-review.md) | Add Security & Quality Improvements from Code Review | 5.7K | üî¥ Not Started |
| 15 | [15-interactive-dashboard-with-automated-remediation.md](15-interactive-dashboard-with-automated-remediation.md) | Interactive Dashboard with Automated Remediation | 11K | üî¥ Not Started |
| 16 | [16-github-app-integration-badge-status-checks.md](16-github-app-integration-badge-status-checks.md) | GitHub App Integration (Badge & Status Checks) | 11K | üî¥ Not Started |
| 18 | [18-setup-release-pipeline.md](18-setup-release-pipeline.md) | Setup GitHub Release Pipeline | 24K | üü° In Progress |
| 19 | [19-github-pages-linter-integration.md](19-github-pages-linter-integration.md) | GitHub Pages Linter Integration | 13K | üî¥ Not Started |

### P3 (Important) - 2 Items

| # | File | Title | Size | Status |
|---|------|-------|------|--------|
| 06 | [06-report-schema-versioning.md](06-report-schema-versioning.md) | Report Schema Versioning | 5.2K | üî¥ Not Started |
| 09 | [09-agentready-repository-agent.md](09-agentready-repository-agent.md) | AgentReady Repository Agent | 5.8K | üî¥ Not Started |

### P4 (Enhancement) - 3 Items

| # | File | Title | Size | Status |
|---|------|-------|------|--------|
| 07 | [07-research-report-generatorupdater-utility.md](07-research-report-generatorupdater-utility.md) | Research Report Generator/Updater Utility | 6.3K | üî¥ Not Started |
| 08 | [08-repomix-integration.md](08-repomix-integration.md) | Repomix Integration | 5.7K | üî¥ Not Started |
| 10 | [10-customizable-html-report-themes.md](10-customizable-html-report-themes.md) | Customizable HTML Report Themes | 6.1K | üî¥ Not Started |

---

## Recommended Implementation Order

Based on priority and dependencies:

1. **P0 #02**: Fix Critical Security & Logic Bugs (XSS, StandardLayoutAssessor)
2. **P0 #03**: Bootstrap AgentReady Repository on GitHub (enables GitHub workflow)
3. **P0 #04**: Report Header with Repository Metadata (blocking usability)
4. **P0 #05**: Improve HTML Report Design (blocking adoption)
5. **P0 #01**: Create Automated Demo (showcase value)
6. **P1 #11**: Fix Code Quality Issues (TOCTOU, type annotations)
7. **P1 #12**: Improve Test Coverage (>80% target)
8. **P1 #14**: Align Subcommand (automated remediation)
9. **P2 #13**: Add Security & Quality Improvements (CSP, validation)
10. **P2 #15**: Interactive Dashboard (high value feature)
11. **P2 #16**: GitHub App Integration (badges, status checks)
12. **P3 #06**: Report Schema Versioning
13. **P3 #09**: AgentReady Repository Agent
14. **P4 #07**: Research Report Generator
15. **P4 #08**: Repomix Integration
16. **P4 #10**: Customizable HTML Report Themes

---

## Using These Prompts

### For AI Agents (Claude Code, etc.)

```bash
# Copy prompt content to AI agent
cat .github/coldstart-prompts/01-create-automated-demo.md

# Or open in editor
code .github/coldstart-prompts/01-create-automated-demo.md
```

### For Human Developers

1. **Pick a feature** from the priority list above
2. **Read the coldstart prompt** for complete context
3. **Create feature branch**: `git checkout -b NNN-feature-name`
4. **Follow the implementation checklist** in the prompt
5. **Run tests and linters** before committing
6. **Create PR** when complete

### Creating GitHub Issues

When the repository is on GitHub, you can create issues with:

```bash
# Generate all prompts and create GitHub issues
python scripts/backlog_to_issues.py --all --create-issues

# Create issue manually using prompt
gh issue create \
  --title "[P0] Create Automated Demo" \
  --body-file .github/coldstart-prompts/01-create-automated-demo.md \
  --label "priority:p0,enhancement"
```

---

## Prompt Structure

Each coldstart prompt follows this structure:

1. **Context** - Repository overview, structure, technologies
2. **Feature Requirements** - Complete requirements from BACKLOG.md
3. **Implementation Checklist** - Step-by-step guide
4. **Key Files to Review** - What to read before starting
5. **Testing Strategy** - Coverage requirements and test types
6. **Success Criteria** - How to know when done
7. **Getting Started** - Exact commands to begin

---

## Maintenance

**Updating Prompts**: If BACKLOG.md changes, regenerate prompts:

```bash
# Regenerate all prompts
python scripts/backlog_to_issues.py --all
```

**Tracking Progress**: Update status column in tables above as features are completed:
- üî¥ Not Started
- üü° In Progress
- üü¢ Completed

---

## Statistics

- **Total Features**: 19
- **P0 (Critical)**: 6 features
- **P1 (High)**: 3 features
- **P2 (Medium)**: 5 features
- **P3 (Important)**: 2 features
- **P4 (Enhancement)**: 3 features
- **Total Prompt Size**: ~175 KB
- **Avg Prompt Size**: ~9.2 KB

---

**Last Updated**: 2025-11-21
**Generator Script**: `scripts/backlog_to_issues.py`
**Source Backlog**: `BACKLOG.md`
````

## File: docs/index.md
````markdown
---
layout: home
title: Home
---

# AgentReady

**Build and maintain agent-ready codebases with automated infrastructure generation and continuous quality assessment.**

<div class="hero">
  <p class="hero-tagline">One command to agent-ready infrastructure. Transform your repository with automated GitHub setup, pre-commit hooks, CI/CD workflows, and continuous quality tracking.</p>
  <div class="hero-buttons">
    <a href="user-guide.html#bootstrap-your-repository" class="button button-primary">Bootstrap Your Repo</a>
    <a href="user-guide.html#running-assessments" class="button button-secondary">Run Assessment</a>
  </div>
</div>

## Why AgentReady?

AI-assisted development tools like Claude Code, GitHub Copilot, and Cursor AI work best with well-structured, documented codebases. AgentReady **builds the infrastructure** you need and **continuously assesses** your repository across **25 research-backed attributes** to ensure lasting AI effectiveness.

### Two Powerful Modes

<div class="feature-grid">
  <div class="feature">
    <h3>‚ö° Bootstrap (Automated)</h3>
    <p><strong>One command to complete infrastructure.</strong> Generates GitHub Actions workflows, pre-commit hooks, issue/PR templates, Dependabot config, and development standards tailored to your language.</p>
    <p><strong>When to use:</strong> New projects, repositories missing automation, or when you want instant best practices.</p>
  </div>
  <div class="feature">
    <h3>üìä Assess (Diagnostic)</h3>
    <p><strong>Deep analysis of 25 attributes.</strong> Evaluates documentation, code quality, testing, structure, and security. Provides actionable remediation guidance with specific tools and commands.</p>
    <p><strong>When to use:</strong> Understanding current state, tracking improvements over time, or validating manual changes.</p>
  </div>
</div>

## Key Features

<div class="feature-grid">
  <div class="feature">
    <h3>ü§ñ Automated Infrastructure</h3>
    <p>Bootstrap generates complete GitHub setup: Actions workflows, issue/PR templates, pre-commit hooks, Dependabot config, and security scanning‚Äîall language-aware.</p>
  </div>
  <div class="feature">
    <h3>üéØ Language-Specific</h3>
    <p>Auto-detects your primary language (Python, JavaScript, Go) and generates appropriate workflows, linters, and test configurations.</p>
  </div>
  <div class="feature">
    <h3>üìà Continuous Assessment</h3>
    <p>Generated GitHub Actions run AgentReady on every PR, posting results as comments. Track improvements over time with Markdown reports.</p>
  </div>
  <div class="feature">
    <h3>üèÜ Certification Levels</h3>
    <p>Platinum, Gold, Silver, Bronze levels validate your codebase quality. Bootstrap helps you achieve Gold (75+) immediately.</p>
  </div>
  <div class="feature">
    <h3>‚ö° One Command Setup</h3>
    <p>From zero to production-ready infrastructure in seconds. Review generated files with --dry-run before committing.</p>
  </div>
  <div class="feature">
    <h3>üî¨ Research-Backed</h3>
    <p>Every generated file and assessed attribute is backed by 50+ citations from Anthropic, Microsoft, Google, and academic research.</p>
  </div>
</div>

## Quick Start

### Bootstrap-First Workflow (Recommended)

```bash
# Install AgentReady
pip install agentready

# Bootstrap your repository (generates all infrastructure)
cd /path/to/your/repo
agentready bootstrap .

# Review generated files
ls -la .github/workflows/
ls -la .github/ISSUE_TEMPLATE/
cat .pre-commit-config.yaml

# Commit and push
git add .
git commit -m "build: Bootstrap agent-ready infrastructure"
git push

# Assessment runs automatically on next PR!
```

**What you get in <60 seconds:**
- ‚úÖ GitHub Actions workflows (tests, security, AgentReady assessment)
- ‚úÖ Pre-commit hooks (formatters, linters, language-specific)
- ‚úÖ Issue & PR templates (bug reports, feature requests, CODEOWNERS)
- ‚úÖ Dependabot automation (weekly dependency updates)
- ‚úÖ Contributing guidelines and Code of Conduct
- ‚úÖ Automatic AgentReady assessment on every PR

### Manual Assessment Workflow

```bash
# Or run one-time assessment without infrastructure changes
agentready assess .

# View interactive HTML report
open .agentready/report-latest.html
```

**Assessment output:**
- Overall score and certification level (Platinum/Gold/Silver/Bronze)
- Detailed findings for all 25 attributes
- Specific remediation steps with tools and examples
- Three report formats (HTML, Markdown, JSON)

[Read the complete user guide ‚Üí](user-guide.html)

## Certification Levels

AgentReady scores repositories on a 0-100 scale with tier-weighted attributes:

<div class="certification-ladder">
  <div class="cert-level platinum">
    <div class="cert-badge">üèÜ Platinum</div>
    <div class="cert-range">90-100</div>
    <div class="cert-desc">Exemplary agent-ready codebase</div>
  </div>
  <div class="cert-level gold">
    <div class="cert-badge">ü•á Gold</div>
    <div class="cert-range">75-89</div>
    <div class="cert-desc">Highly optimized for AI agents</div>
  </div>
  <div class="cert-level silver">
    <div class="cert-badge">ü•à Silver</div>
    <div class="cert-range">60-74</div>
    <div class="cert-desc">Well-suited for AI development</div>
  </div>
  <div class="cert-level bronze">
    <div class="cert-badge">ü•â Bronze</div>
    <div class="cert-range">40-59</div>
    <div class="cert-desc">Basic agent compatibility</div>
  </div>
  <div class="cert-level needs-improvement">
    <div class="cert-badge">üìà Needs Improvement</div>
    <div class="cert-range">0-39</div>
    <div class="cert-desc">Significant friction for AI agents</div>
  </div>
</div>

**AgentReady itself scores 75.4/100 (Gold)** ‚Äî see our [self-assessment report](examples.html#agentready-self-assessment).

## What Gets Assessed?

AgentReady evaluates 25 attributes organized into four weighted tiers:

### Tier 1: Essential (50% of score)
The fundamentals that enable basic AI agent functionality:
- **CLAUDE.md File** ‚Äî Project context for AI agents
- **README Structure** ‚Äî Clear documentation entry point
- **Type Annotations** ‚Äî Static typing for better code understanding
- **Standard Project Layout** ‚Äî Predictable directory structure
- **Lock Files** ‚Äî Reproducible dependency management

### Tier 2: Critical (30% of score)
Major quality improvements and safety nets:
- **Test Coverage** ‚Äî Confidence for AI-assisted refactoring
- **Pre-commit Hooks** ‚Äî Automated quality enforcement
- **Conventional Commits** ‚Äî Structured git history
- **Gitignore Completeness** ‚Äî Clean repository navigation
- **One-Command Setup** ‚Äî Easy environment reproduction

### Tier 3: Important (15% of score)
Significant improvements in specific areas:
- **Cyclomatic Complexity** ‚Äî Code comprehension metrics
- **Structured Logging** ‚Äî Machine-parseable debugging
- **API Documentation** ‚Äî OpenAPI/GraphQL specifications
- **Architecture Decision Records** ‚Äî Historical design context
- **Semantic Naming** ‚Äî Clear, descriptive identifiers

### Tier 4: Advanced (5% of score)
Refinement and optimization:
- **Security Scanning** ‚Äî Automated vulnerability detection
- **Performance Benchmarks** ‚Äî Regression tracking
- **Code Smell Elimination** ‚Äî Quality baseline maintenance
- **PR/Issue Templates** ‚Äî Consistent contribution workflow
- **Container Setup** ‚Äî Portable development environments

[View complete attribute reference ‚Üí](attributes.html)

## Report Formats

AgentReady generates three complementary report formats:

### Interactive HTML Report
- Color-coded findings with visual score indicators
- Search, filter, and sort capabilities
- Collapsible sections for detailed analysis
- Works offline (no CDN dependencies)
- **Use case**: Share with stakeholders, detailed exploration

### Version-Control Markdown
- GitHub-Flavored Markdown with tables and emojis
- Git-diffable format for tracking progress
- Certification ladder and next steps
- **Use case**: Commit to repository, track improvements over time

### Machine-Readable JSON
- Complete assessment data structure
- Timestamps and metadata
- Structured findings with evidence
- **Use case**: CI/CD integration, programmatic analysis

[See example reports ‚Üí](examples.html)

## Evidence-Based Research

All 25 attributes are derived from authoritative sources:

- **Anthropic** ‚Äî Claude Code best practices and engineering blog
- **Microsoft** ‚Äî Code metrics and Azure DevOps guidance
- **Google** ‚Äî SRE handbook and style guides
- **ArXiv** ‚Äî Software engineering research papers
- **IEEE/ACM** ‚Äî Academic publications on code quality

Every attribute includes specific citations and measurable criteria. No subjective opinions‚Äîjust proven practices that improve AI effectiveness.

[Read the research document ‚Üí](https://github.com/ambient-code/agentready/blob/main/agent-ready-codebase-attributes.md)

## Use Cases

<div class="use-case-grid">
  <div class="use-case">
    <h4>üöÄ New Projects</h4>
    <p>Start with best practices from day one. Use AgentReady's guidance to structure your repository for AI-assisted development from the beginning.</p>
  </div>
  <div class="use-case">
    <h4>üîÑ Legacy Modernization</h4>
    <p>Identify high-impact improvements to make legacy codebases more AI-friendly. Prioritize changes with tier-based scoring.</p>
  </div>
  <div class="use-case">
    <h4>üìä Team Standards</h4>
    <p>Establish organization-wide quality baselines. Track adherence across multiple repositories with consistent, objective metrics.</p>
  </div>
  <div class="use-case">
    <h4>üéì Education & Onboarding</h4>
    <p>Teach developers what makes code AI-ready. Use assessments as learning tools to understand best practices.</p>
  </div>
</div>

## What Users Are Saying

> "Running AgentReady on our codebase identified 5 quick wins that immediately improved Claude Code's suggestions. The actionable remediation made it easy to implement changes in under an hour."
> ‚Äî *Engineering Team Lead*

> "The research backing each attribute gave me confidence these weren't arbitrary rules. Every recommendation is cited and measurable."
> ‚Äî *Principal Engineer*

> "We use AgentReady in CI to prevent regression. It's become part of our definition of done for new features."
> ‚Äî *DevOps Engineer*

## Ready to Get Started?

<div class="cta-section">
  <h3>Assess your repository in 60 seconds</h3>
  <pre><code>pip install agentready
agentready assess .
</code></pre>
  <a href="user-guide.html" class="button button-primary button-large">Read the User Guide</a>
</div>

---

## What Bootstrap Generates

AgentReady Bootstrap creates production-ready infrastructure tailored to your language:

### GitHub Actions Workflows

**`agentready-assessment.yml`** ‚Äî Runs assessment on every PR and push
- Posts interactive results as PR comments
- Tracks score progression over time
- Fails if score drops below configured threshold

**`tests.yml`** ‚Äî Language-specific test automation
- Python: pytest with coverage reporting
- JavaScript: jest with coverage
- Go: go test with race detection

**`security.yml`** ‚Äî Comprehensive security scanning
- CodeQL analysis for vulnerability detection
- Dependency scanning with GitHub Advisory Database
- SAST (Static Application Security Testing)

### GitHub Templates

**Issue Templates** ‚Äî Structured bug reports and feature requests
- Bug report with reproduction steps template
- Feature request with use case template
- Auto-labeling and assignment

**PR Template** ‚Äî Checklist-driven pull requests
- Testing verification checklist
- Documentation update requirements
- Breaking change indicators

**CODEOWNERS** ‚Äî Automated code review assignments

### Development Infrastructure

**`.pre-commit-config.yaml`** ‚Äî Language-specific quality gates
- Python: black, isort, ruff, mypy
- JavaScript: prettier, eslint
- Go: gofmt, golint

**`.github/dependabot.yml`** ‚Äî Automated dependency management
- Weekly update checks
- Automatic PR creation for updates
- Security vulnerability patching

**`CONTRIBUTING.md`** ‚Äî Contributing guidelines (if missing)

**`CODE_OF_CONDUCT.md`** ‚Äî Red Hat standard code of conduct (if missing)

[See generated file examples ‚Üí](examples.html#bootstrap-examples)

## Latest News

**Version 1.1.0 - Bootstrap Released** (2025-11-21)
Major release introducing automated infrastructure generation! Bootstrap command creates complete GitHub setup (Actions, templates, pre-commit hooks, Dependabot) tailored to your language. Transform any repository to agent-ready in <60 seconds.

**Version 1.0.0 Released** (2025-11-21)
Initial release with 10 implemented assessors, interactive HTML reports, and comprehensive documentation. AgentReady achieves Gold certification (75.4/100) on its own codebase.

[View full changelog ‚Üí](https://github.com/ambient-code/agentready/releases)

## Community

- **GitHub**: [github.com/ambient-code/agentready](https://github.com/ambient-code/agentready)
- **Issues**: Report bugs or request features
- **Discussions**: Ask questions and share experiences
- **Contributing**: See the [Developer Guide](developer-guide.html)

## License

AgentReady is open source under the [MIT License](https://github.com/ambient-code/agentready/blob/main/LICENSE).
````

## File: CLAUDE.md
````markdown
# AgentReady - Repository Quality Assessment Tool

**Purpose**: Assess repositories against agent-ready best practices and generate actionable reports.

**Last Updated**: 2025-11-21

---

## Overview

AgentReady is a Python CLI tool that evaluates repositories against 25 carefully researched attributes that make codebases more effective for AI-assisted development. It generates interactive HTML reports, version-control friendly Markdown reports, and machine-readable JSON output.

**Current Status**: v1.0.0 - Core assessment engine complete, 10/25 attributes implemented

**Self-Assessment Score**: 75.4/100 (Gold) - See `examples/self-assessment/`

---

## Quick Start

```bash
# Install in virtual environment
uv venv && source .venv/bin/activate
uv pip install -e .

# Run assessment on current directory
agentready assess .

# Run with verbose output
agentready assess . --verbose

# Assess different repository
agentready assess /path/to/repo --output-dir ./reports
```

**Outputs**:
- `.agentready/assessment-YYYYMMDD-HHMMSS.json` - Machine-readable results
- `.agentready/report-YYYYMMDD-HHMMSS.html` - Interactive web report
- `.agentready/report-YYYYMMDD-HHMMSS.md` - Git-friendly markdown report

---

## Architecture

### Core Components

```
src/agentready/
‚îú‚îÄ‚îÄ models/          # Data models (Repository, Attribute, Finding, Assessment)
‚îú‚îÄ‚îÄ services/        # Scanner orchestration and language detection
‚îú‚îÄ‚îÄ assessors/       # Attribute assessment implementations
‚îÇ   ‚îú‚îÄ‚îÄ base.py      # BaseAssessor abstract class
‚îÇ   ‚îú‚îÄ‚îÄ documentation.py   # CLAUDE.md, README assessors
‚îÇ   ‚îú‚îÄ‚îÄ code_quality.py    # Type annotations, complexity
‚îÇ   ‚îú‚îÄ‚îÄ testing.py         # Test coverage, pre-commit hooks
‚îÇ   ‚îú‚îÄ‚îÄ structure.py       # Standard layout, gitignore
‚îÇ   ‚îî‚îÄ‚îÄ stub_assessors.py  # 15 not-yet-implemented assessors
‚îú‚îÄ‚îÄ reporters/       # Report generation (HTML, Markdown, JSON)
‚îÇ   ‚îú‚îÄ‚îÄ html.py      # Interactive HTML with Jinja2
‚îÇ   ‚îî‚îÄ‚îÄ markdown.py  # GitHub-Flavored Markdown
‚îú‚îÄ‚îÄ templates/       # Jinja2 templates
‚îÇ   ‚îî‚îÄ‚îÄ report.html.j2  # Self-contained HTML report (73KB)
‚îî‚îÄ‚îÄ cli/             # Click-based CLI
    ‚îî‚îÄ‚îÄ main.py      # assess, research-version, generate-config commands
```

### Data Flow

```
Repository ‚Üí Scanner ‚Üí Assessors ‚Üí Findings ‚Üí Assessment ‚Üí Reporters ‚Üí Reports
                ‚Üì
         Language Detection
         (git ls-files)
```

### Scoring Algorithm

1. **Tier-Based Weighting** (50/30/15/5 distribution):
   - Tier 1 (Essential): 50% of total score
   - Tier 2 (Critical): 30% of total score
   - Tier 3 (Important): 15% of total score
   - Tier 4 (Advanced): 5% of total score

2. **Attribute Scoring**: Each attribute returns 0-100 score
3. **Weighted Aggregation**: `final_score = Œ£(attribute_score √ó weight)`
4. **Certification Levels**:
   - Platinum: 90-100
   - Gold: 75-89
   - Silver: 60-74
   - Bronze: 40-59
   - Needs Improvement: 0-39

---

## Development

### Setup

```bash
# Create virtual environment
uv venv
source .venv/bin/activate

# Install dependencies
uv pip install -e .

# Install development tools
uv pip install pytest black isort ruff
```

### Running Tests

```bash
# Run all tests
pytest

# Run with coverage
pytest --cov=src/agentready --cov-report=html

# Run specific test file
pytest tests/unit/test_models.py -v
```

**Current Coverage**: 37% (focused on core logic)

### Code Quality

```bash
# Format code
black src/ tests/

# Sort imports
isort src/ tests/

# Lint code
ruff check src/ tests/

# Run all linters
black src/ tests/ && isort src/ tests/ && ruff check src/ tests/
```

### Adding New Assessors

1. **Expand a stub assessor** in `src/agentready/assessors/stub_assessors.py`
2. **Create new assessor class** inheriting from `BaseAssessor`
3. **Implement required methods**:
   - `attribute_id` property
   - `assess(repository)` method
   - `is_applicable(repository)` method (optional)
4. **Add tests** in `tests/unit/test_assessors_*.py`
5. **Register** in scanner's assessor list

**Example**:
```python
class MyAssessor(BaseAssessor):
    @property
    def attribute_id(self) -> str:
        return "my_attribute_id"

    def assess(self, repository: Repository) -> Finding:
        # Implement assessment logic
        if condition_met:
            return Finding.create_pass(self.attribute, ...)
        else:
            return Finding.create_fail(self.attribute, ...)
```

---

## Project Structure

```
agentready/
‚îú‚îÄ‚îÄ src/agentready/          # Source code
‚îú‚îÄ‚îÄ tests/                   # Test suite
‚îÇ   ‚îú‚îÄ‚îÄ unit/               # Unit tests
‚îÇ   ‚îî‚îÄ‚îÄ integration/        # End-to-end tests
‚îú‚îÄ‚îÄ examples/               # Example reports
‚îÇ   ‚îî‚îÄ‚îÄ self-assessment/    # AgentReady's own assessment
‚îú‚îÄ‚îÄ specs/                  # Feature specifications
‚îú‚îÄ‚îÄ pyproject.toml          # Python package configuration
‚îú‚îÄ‚îÄ CLAUDE.md              # This file
‚îú‚îÄ‚îÄ README.md              # User-facing documentation
‚îú‚îÄ‚îÄ BACKLOG.md             # Future features and enhancements
‚îî‚îÄ‚îÄ GITHUB_ISSUES.md       # GitHub-ready issue templates
```

---

## Technologies

- **Python 3.11+** (only N and N-1 versions supported)
- **Click** - CLI framework
- **Jinja2** - HTML template engine
- **Pytest** - Testing framework
- **Black** - Code formatter
- **isort** - Import sorter
- **Ruff** - Fast Python linter

---

## Contributing

### Workflow

1. **Create feature branch** from `main`
2. **Implement changes** with tests
3. **Run linters**: `black . && isort . && ruff check .`
4. **Run tests**: `pytest`
5. **Commit** with conventional commit messages
6. **Push** and create PR

### Conventional Commits

```
feat: Add new assessor for dependency freshness
fix: Correct type annotation detection in Python 3.12
docs: Update CLAUDE.md with architecture details
test: Add integration test for HTML report generation
refactor: Extract common assessor logic to base class
chore: Update dependencies
```

### Test Requirements

- All new assessors must have unit tests
- Integration tests for new reporters
- Maintain >80% coverage for new code
- All tests must pass before merge

---

## CI/CD

**GitHub Actions** (planned):
- Run tests on PR
- Run linters (black, isort, ruff)
- Generate coverage report
- Run AgentReady self-assessment
- Post assessment results as PR comment

**Current**: Manual workflow (tests run locally before push)

---

## Known Issues & Limitations

1. **Stub Assessors**: 15/25 assessors return "not_applicable" - need implementation
2. **No Lock File**: Intentionally excluded for library project
3. **No Pre-commit Hooks**: Not yet configured (planned P0 fix)
4. **HTML Report Design**: Current color scheme needs improvement (P0 fix)
5. **Report Metadata**: Missing repository context in header (P0 fix)

---

## Roadmap

### v1.1 - Critical UX Fixes (Next Sprint)
- **P0**: Add report header with repository metadata
- **P0**: Redesign HTML report (larger fonts, better colors)

### v1.2 - Automation & Integration
- **P1**: Implement `agentready align` subcommand (automated remediation)
- **P2**: GitHub App integration (badges, status checks, PR comments)
- **P2**: Interactive dashboard (one-click remediation)

### v1.3 - Assessor Expansion
- Expand 15 stub assessors
- Add AI-powered assessors (type annotations, docstrings)
- Improve test coverage to >80%

### v2.0 - Enterprise Features
- Report schema versioning
- Customizable HTML themes with dark/light toggle
- Organization-wide dashboards
- Historical trend analysis

See `BACKLOG.md` for full feature list.

---

## Getting Help

- **Issues**: Create GitHub issue using templates in `GITHUB_ISSUES.md`
- **Documentation**: See `README.md` for user guide
- **Examples**: View `examples/self-assessment/` for reference reports
- **Research**: Read `agent-ready-codebase-attributes.md` for attribute definitions

---

## Related Documents

- **BACKLOG.md** - Future features and enhancements (11 items)
- **GITHUB_ISSUES.md** - GitHub-ready issue templates
- **README.md** - User-facing documentation
- **specs/** - Feature specifications and design documents
- **examples/self-assessment/** - AgentReady's own assessment (75.4/100 Gold)

---

## Notes for Claude Code Agents

**When working on AgentReady**:

1. **Read before modifying**: Always read existing assessors before implementing new ones
2. **Follow patterns**: Use `CLAUDEmdAssessor` and `READMEAssessor` as reference implementations
3. **Test thoroughly**: Add unit tests for all new assessors
4. **Maintain backwards compatibility**: Don't change Assessment model without schema version bump
5. **Stub assessors first**: Check if attribute already has stub before creating new class
6. **Proportional scoring**: Use `calculate_proportional_score()` for partial compliance
7. **Graceful degradation**: Return "skipped" if tools missing, don't crash
8. **Rich remediation**: Provide actionable steps, tools, commands, examples, citations

**Key Principles**:
- Library-first architecture (no global state)
- Strategy pattern for assessors (each is independent)
- Fail gracefully (missing tools ‚Üí skip, don't crash)
- User-focused (actionable remediation over theoretical guidance)

### Documentation Workflow

**CRITICAL: Proactive Documentation Agent Usage**

When working with documentation in this repository, **ALWAYS** use the `github-pages-docs` agent for any documentation updates, revisions, or additions. This ensures consistency, quality, and adherence to the established documentation structure.

**Trigger the github-pages-docs agent when**:

1. **After implementing new features** (e.g., Bootstrap, new assessors, new commands)
   - Update user guide with new feature documentation
   - Update developer guide with architecture changes
   - Update API reference with new classes/methods
   - Update examples with new use cases

2. **After modifying source-of-truth files**:
   - `CLAUDE.md` changes ‚Üí Update developer-guide.md and user-guide.md
   - `agent-ready-codebase-attributes.md` changes ‚Üí Update attributes.md
   - `README.md` changes ‚Üí Sync with docs/index.md
   - Bootstrap implementation changes ‚Üí Update bootstrap documentation sections

3. **When fixing bugs or addressing issues**:
   - Update troubleshooting sections
   - Update known issues documentation
   - Update migration guides if breaking changes

4. **When updating project status**:
   - Certification level changes (e.g., Silver ‚Üí Gold)
   - Version bumps (v1.0 ‚Üí v1.1)
   - Roadmap updates
   - New milestone achievements

**Agent Invocation Pattern**:
```
Use the @agent-github-pages-docs to [action] based on:
- [Source of truth file] updates
- [Feature/bug/change] implementation
- [Specific sections] that need updating
```

**Example**:
```
Use the @agent-github-pages-docs to revise all documentation in docs/ based on:
- Bootstrap feature now fully implemented
- New CLI commands (bootstrap, align)
- Updated architecture in CLAUDE.md
- Self-assessment score improvement (75.4 ‚Üí 82.1)
```

**Documentation Sources of Truth** (in priority order):
1. `CLAUDE.md` - Complete project guide (architecture, development, workflows)
2. `agent-ready-codebase-attributes.md` - Research report (25 attributes, tier system)
3. `contracts/` - Schemas and contracts (data models, validation rules)
4. `specs/` - Feature specifications (design documents, plans)
5. `README.md` - User-facing overview
6. Source code - Actual implementation (CLI, services, assessors)

**Documentation Automation**:
- Manual trigger: `.github/workflows/update-docs.yml` (workflow_dispatch)
- Future: Automatic cascade updates on source file changes (see BACKLOG.md - P2 item)
- Always review agent-generated docs before committing
- Ensure Bootstrap documentation is kept prominent and up-to-date

---

**Last Updated**: 2025-11-21 by Jeremy Eder
**AgentReady Version**: 1.0.0
**Self-Assessment**: 75.4/100 (Gold) ‚ú®
````

## File: docs/developer-guide.md
````markdown
---
layout: page
title: Developer Guide
---

# Developer Guide

Comprehensive guide for contributors and developers extending AgentReady.

## Table of Contents

- [Getting Started](#getting-started)
- [Development Environment](#development-environment)
- [Architecture Overview](#architecture-overview)
- [Implementing New Assessors](#implementing-new-assessors)
- [Testing Guidelines](#testing-guidelines)
- [Code Quality Standards](#code-quality-standards)
- [Contributing Workflow](#contributing-workflow)
- [Release Process](#release-process)

---

## Getting Started

### Prerequisites

- **Python 3.11 or 3.12**
- **Git**
- **uv** or **pip** (uv recommended for faster dependency management)
- **Make** (optional, for convenience commands)

### Fork and Clone

```bash
# Fork on GitHub first, then:
git clone https://github.com/YOUR_USERNAME/agentready.git
cd agentready

# Add upstream remote
git remote add upstream https://github.com/ambient-code/agentready.git
```

### Install Development Dependencies

```bash
# Create virtual environment
python3 -m venv .venv
source .venv/bin/activate  # On Windows: .venv\Scripts\activate

# Install with development dependencies
uv pip install -e ".[dev]"

# Or using pip
pip install -e ".[dev]"

# Verify installation
pytest --version
black --version
ruff --version
```

---

## Development Environment

### Project Structure

```
agentready/
‚îú‚îÄ‚îÄ src/agentready/          # Source code
‚îÇ   ‚îú‚îÄ‚îÄ cli/                 # Click-based CLI
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ main.py          # Entry point (assess, research-version, generate-config)
‚îÇ   ‚îú‚îÄ‚îÄ models/              # Data models
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ repository.py    # Repository representation
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ attribute.py     # Attribute definition
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ finding.py       # Assessment finding
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ assessment.py    # Complete assessment result
‚îÇ   ‚îú‚îÄ‚îÄ services/            # Core business logic
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ scanner.py       # Assessment orchestration
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ scorer.py        # Score calculation
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ language_detector.py  # Language detection via git
‚îÇ   ‚îú‚îÄ‚îÄ assessors/           # Attribute assessors
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ base.py          # BaseAssessor abstract class
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ documentation.py # CLAUDE.md, README assessors
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ code_quality.py  # Type annotations, complexity
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ testing.py       # Test coverage, pre-commit hooks
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ structure.py     # Standard layout, gitignore
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ stub_assessors.py # 15 not-yet-implemented assessors
‚îÇ   ‚îú‚îÄ‚îÄ reporters/           # Report generators
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ html.py          # Interactive HTML with Jinja2
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ markdown.py      # GitHub-Flavored Markdown
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ json.py          # Machine-readable JSON
‚îÇ   ‚îú‚îÄ‚îÄ templates/           # Jinja2 templates
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ report.html.j2   # HTML report template
‚îÇ   ‚îî‚îÄ‚îÄ data/                # Bundled data
‚îÇ       ‚îî‚îÄ‚îÄ attributes.yaml  # Attribute definitions
‚îú‚îÄ‚îÄ tests/                   # Test suite
‚îÇ   ‚îú‚îÄ‚îÄ unit/               # Unit tests (fast, isolated)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ test_models.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ test_assessors_documentation.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ test_assessors_code_quality.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ...
‚îÇ   ‚îú‚îÄ‚îÄ integration/        # End-to-end tests
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ test_full_assessment.py
‚îÇ   ‚îî‚îÄ‚îÄ fixtures/           # Test data
‚îÇ       ‚îî‚îÄ‚îÄ sample_repos/   # Sample repositories for testing
‚îú‚îÄ‚îÄ docs/                    # GitHub Pages documentation
‚îú‚îÄ‚îÄ examples/               # Example reports
‚îÇ   ‚îî‚îÄ‚îÄ self-assessment/    # AgentReady's own assessment
‚îú‚îÄ‚îÄ pyproject.toml          # Python package configuration
‚îú‚îÄ‚îÄ CLAUDE.md              # Project context for AI agents
‚îú‚îÄ‚îÄ README.md              # User-facing documentation
‚îî‚îÄ‚îÄ BACKLOG.md             # Feature backlog
```

### Development Tools

AgentReady uses modern Python tooling:

| Tool | Purpose | Configuration |
|------|---------|---------------|
| **pytest** | Testing framework | `pyproject.toml` |
| **black** | Code formatter | `pyproject.toml` |
| **isort** | Import sorter | `pyproject.toml` |
| **ruff** | Fast linter | `pyproject.toml` |
| **mypy** | Type checker | `pyproject.toml` (future) |

### Running Tests

```bash
# Run all tests
pytest

# Run with coverage
pytest --cov=src/agentready --cov-report=html

# Run specific test file
pytest tests/unit/test_models.py -v

# Run tests matching pattern
pytest -k "test_claude_md" -v

# Run with output (don't capture print statements)
pytest -s

# Fast fail (stop on first failure)
pytest -x
```

### Code Quality Checks

```bash
# Format code
black src/ tests/

# Sort imports
isort src/ tests/

# Lint code
ruff check src/ tests/

# Run all quality checks (recommended before committing)
black src/ tests/ && isort src/ tests/ && ruff check src/ tests/
```

### Pre-commit Hooks (Recommended)

Install pre-commit hooks to automatically run quality checks:

```bash
# Install pre-commit (if not already installed)
pip install pre-commit

# Install git hooks
pre-commit install

# Run manually on all files
pre-commit run --all-files
```

---

## Architecture Overview

AgentReady follows a **library-first architecture** with clear separation of concerns.

### Data Flow

```
Repository ‚Üí Scanner ‚Üí Assessors ‚Üí Findings ‚Üí Assessment ‚Üí Reporters ‚Üí Reports
                ‚Üì
         Language Detection
         (git ls-files)
```

### Core Components

#### 1. Models (`models/`)

Immutable data classes representing domain entities:

- **Repository**: Path, name, detected languages
- **Attribute**: ID, name, tier, weight, description
- **Finding**: Attribute, status (pass/fail/skip), score, evidence, remediation
- **Assessment**: Repository, overall score, certification level, findings list

**Design Principles**:
- Immutable (frozen dataclasses)
- Type-annotated
- No business logic (pure data)
- Factory methods for common patterns (`Finding.create_pass()`, etc.)

#### 2. Services (`services/`)

Orchestration and core algorithms:

- **Scanner**: Coordinates assessment flow, manages assessors
- **Scorer**: Calculates weighted scores, determines certification levels
- **LanguageDetector**: Detects repository languages via `git ls-files`

**Design Principles**:
- Stateless (pure functions or stateless classes)
- Single responsibility
- No external dependencies (file I/O, network)
- Testable with mocks

#### 3. Assessors (`assessors/`)

Strategy pattern implementations for each attribute:

- **BaseAssessor**: Abstract base class defining interface
- Concrete assessors: `CLAUDEmdAssessor`, `READMEAssessor`, etc.

**Design Principles**:
- Each assessor is independent
- Inherit from `BaseAssessor`
- Implement `assess(repository)` method
- Return `Finding` object
- Fail gracefully (return "skipped" if tools missing, don't crash)

#### 4. Reporters (`reporters/`)

Transform `Assessment` into report formats:

- **HTMLReporter**: Jinja2-based interactive report
- **MarkdownReporter**: GitHub-Flavored Markdown
- **JSONReporter**: Machine-readable JSON

**Design Principles**:
- Take `Assessment` as input
- Return formatted string
- Self-contained (HTML has inline CSS/JS, no CDN)
- Idempotent (same input ‚Üí same output)

### Key Design Patterns

#### Strategy Pattern (Assessors)

Each assessor is a pluggable strategy implementing the same interface:

```python
from abc import ABC, abstractmethod

class BaseAssessor(ABC):
    @property
    @abstractmethod
    def attribute_id(self) -> str:
        """Unique attribute identifier."""
        pass

    @abstractmethod
    def assess(self, repository: Repository) -> Finding:
        """Assess repository for this attribute."""
        pass

    def is_applicable(self, repository: Repository) -> bool:
        """Check if this assessor applies to the repository."""
        return True
```

#### Factory Pattern (Finding Creation)

`Finding` class provides factory methods for common patterns:

```python
# Pass with full score
finding = Finding.create_pass(
    attribute=attribute,
    evidence="Found CLAUDE.md at repository root",
    remediation=None
)

# Fail with zero score
finding = Finding.create_fail(
    attribute=attribute,
    evidence="No CLAUDE.md file found",
    remediation=Remediation(steps=[...], tools=[...])
)

# Skip (not applicable)
finding = Finding.create_skip(
    attribute=attribute,
    reason="Not implemented yet"
)
```

#### Template Pattern (Reporters)

Reporters use Jinja2 templates for HTML generation:

```python
from jinja2 import Environment, FileSystemLoader

class HTMLReporter:
    def generate(self, assessment: Assessment) -> str:
        env = Environment(loader=FileSystemLoader('templates'))
        template = env.get_template('report.html.j2')
        return template.render(assessment=assessment)
```

---

## Bootstrap System Architecture

### Overview

The Bootstrap system automates infrastructure generation through template rendering and language-aware configuration.

### Data Flow

```
Repository ‚Üí LanguageDetector ‚Üí BootstrapGenerator ‚Üí Templates ‚Üí Generated Files
                     ‚Üì                    ‚Üì
              Primary Language    Template Variables
                                   (language, repo_name, etc.)
```

### Core Components

#### 1. Bootstrap Services (`services/bootstrap.py`)

**`BootstrapGenerator`** ‚Äî Main orchestration class:

```python
class BootstrapGenerator:
    """Generate agent-ready infrastructure for repositories."""

    def __init__(self, template_dir: str):
        """Initialize with template directory path."""
        self.template_dir = template_dir
        self.jinja_env = Environment(loader=FileSystemLoader(template_dir))

    def generate(
        self,
        repository: Repository,
        language: str = "auto",
        dry_run: bool = False
    ) -> List[GeneratedFile]:
        """
        Generate infrastructure files for repository.

        Args:
            repository: Repository object
            language: Primary language (auto-detected if "auto")
            dry_run: Preview only, don't create files

        Returns:
            List of GeneratedFile objects with paths and content
        """
```

**Key Methods**:
- `_detect_language()` ‚Äî Auto-detect primary language via LanguageDetector
- `_render_template()` ‚Äî Render Jinja2 template with context variables
- `_get_templates_for_language()` ‚Äî Map language to template files
- `_write_file()` ‚Äî Create file on disk (respects dry_run)
- `_file_exists()` ‚Äî Check for conflicts (never overwrites)

#### 2. Bootstrap CLI (`cli/bootstrap.py`)

Command-line interface for Bootstrap:

```python
@click.command()
@click.argument("repository", type=click.Path(exists=True), default=".")
@click.option("--dry-run", is_flag=True, help="Preview without creating files")
@click.option(
    "--language",
    type=click.Choice(["python", "javascript", "go", "auto"]),
    default="auto",
    help="Primary language override"
)
def bootstrap(repository, dry_run, language):
    """Bootstrap agent-ready infrastructure for repository."""
```

**Responsibilities**:
- Parse command-line arguments
- Create Repository object
- Instantiate BootstrapGenerator
- Display progress and results
- Handle errors gracefully

#### 3. Templates (`templates/bootstrap/`)

Jinja2 templates for generated files:

```
templates/bootstrap/
‚îú‚îÄ‚îÄ common/                          # Language-agnostic templates
‚îÇ   ‚îú‚îÄ‚îÄ CODEOWNERS.j2
‚îÇ   ‚îú‚îÄ‚îÄ CONTRIBUTING.md.j2
‚îÇ   ‚îú‚îÄ‚îÄ CODE_OF_CONDUCT.md.j2
‚îÇ   ‚îú‚îÄ‚îÄ bug_report.md.j2
‚îÇ   ‚îú‚îÄ‚îÄ feature_request.md.j2
‚îÇ   ‚îî‚îÄ‚îÄ pull_request_template.md.j2
‚îú‚îÄ‚îÄ python/                          # Python-specific
‚îÇ   ‚îú‚îÄ‚îÄ agentready-assessment.yml.j2
‚îÇ   ‚îú‚îÄ‚îÄ tests.yml.j2
‚îÇ   ‚îú‚îÄ‚îÄ security.yml.j2
‚îÇ   ‚îú‚îÄ‚îÄ pre-commit-config.yaml.j2
‚îÇ   ‚îî‚îÄ‚îÄ dependabot.yml.j2
‚îú‚îÄ‚îÄ javascript/                      # JavaScript-specific
‚îÇ   ‚îî‚îÄ‚îÄ ... (similar structure)
‚îî‚îÄ‚îÄ go/                              # Go-specific
    ‚îî‚îÄ‚îÄ ... (similar structure)
```

### Template Variables

All templates receive these context variables:

```python
context = {
    "repository_name": repository.name,
    "language": detected_language,
    "has_tests_directory": os.path.exists(f"{repo_path}/tests"),
    "has_src_directory": os.path.exists(f"{repo_path}/src"),
    "python_version": "3.11",  # Or detected version
    "node_version": "18",      # Or detected version
    "go_version": "1.21",      # Or detected version
    "year": datetime.now().year,
    "organization": extract_org_from_remote(repo_path)  # From git remote
}
```

### Language Detection Logic

```python
class LanguageDetector:
    """Detect primary language from repository files."""

    EXTENSION_MAP = {
        ".py": "Python",
        ".js": "JavaScript",
        ".ts": "TypeScript",
        ".go": "Go",
        ".java": "Java",
        # ... more extensions
    }

    def detect(self, repo_path: str) -> Dict[str, int]:
        """
        Count files by language.

        Returns:
            Dict mapping language name to file count
            Example: {"Python": 42, "JavaScript": 18}
        """

    def get_primary_language(self, languages: Dict[str, int]) -> str:
        """Return language with most files."""
        return max(languages, key=languages.get)
```

### File Generation Flow

1. **Detect Language**:
   ```python
   if language == "auto":
       languages = LanguageDetector().detect(repository.path)
       primary = LanguageDetector().get_primary_language(languages)
   else:
       primary = language
   ```

2. **Select Templates**:
   ```python
   templates = {
       "python": [
           "python/agentready-assessment.yml.j2",
           "python/tests.yml.j2",
           "python/pre-commit-config.yaml.j2",
           # ... common templates
       ],
       "javascript": [...],
       "go": [...]
   }
   selected = templates[primary]
   ```

3. **Render Each Template**:
   ```python
   for template_path in selected:
       template = jinja_env.get_template(template_path)
       content = template.render(**context)
       output_path = determine_output_path(template_path)
       if not os.path.exists(output_path):
           write_file(output_path, content)
   ```

4. **Return Results**:
   ```python
   return [
       GeneratedFile(
           path=".github/workflows/tests.yml",
           content=rendered_content,
           created=True
       ),
       # ... more files
   ]
   ```

### Error Handling

Bootstrap implements defensive programming:

```python
class BootstrapError(Exception):
    """Base exception for Bootstrap errors."""

class LanguageDetectionError(BootstrapError):
    """Raised when language detection fails."""

class TemplateRenderError(BootstrapError):
    """Raised when template rendering fails."""

class FileWriteError(BootstrapError):
    """Raised when file write fails (permissions, etc.)."""
```

**Error scenarios**:
- Not a git repository ‚Üí Fail early with clear message
- Language detection fails ‚Üí Require `--language` flag
- Template not found ‚Üí Report missing template name
- File already exists ‚Üí Skip gracefully, report in output
- Permission denied ‚Üí Report path and suggest fix

### Dry Run Implementation

```python
def generate(self, repository, language="auto", dry_run=False):
    """Generate infrastructure."""

    generated_files = []

    for template_path in templates:
        content = self._render_template(template_path, context)
        output_path = self._determine_output_path(template_path)

        if dry_run:
            # Don't write, just report what would happen
            generated_files.append(
                GeneratedFile(
                    path=output_path,
                    content=content,
                    created=False,  # Would be created
                    dry_run=True
                )
            )
        else:
            if not os.path.exists(output_path):
                self._write_file(output_path, content)
                generated_files.append(
                    GeneratedFile(
                        path=output_path,
                        content=content,
                        created=True
                    )
                )

    return generated_files
```

---

## Creating Bootstrap Templates

### Template Structure

All Bootstrap templates follow consistent patterns for maintainability.

### 1. GitHub Actions Workflow Template

**Location**: `templates/bootstrap/python/agentready-assessment.yml.j2`

```yaml
name: AgentReady Assessment

on:
  pull_request:
  push:
    branches: [main, master]

jobs:
  assess:
    runs-on: ubuntu-latest

    permissions:
      contents: read
      pull-requests: write

    steps:
      - uses: actions/checkout@v4

      - uses: actions/setup-python@v4
        with:
          python-version: '{{ python_version }}'

      - name: Install AgentReady
        run: pip install agentready

      - name: Run Assessment
        id: assessment
        run: |
          agentready assess . --output-dir .agentready
          score=$(jq '.overall_score' .agentready/assessment-latest.json)
          echo "score=$score" >> $GITHUB_OUTPUT

      - name: Upload Report
        uses: actions/upload-artifact@v3
        with:
          name: agentready-report
          path: .agentready/report-latest.html

      - name: Comment PR
        if: github.event_name == 'pull_request'
        uses: actions/github-script@v6
        with:
          script: |
            const fs = require('fs');
            const report = fs.readFileSync('.agentready/report-latest.md', 'utf8');
            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: report
            });

      - name: Check Score Threshold
        run: |
          if (( $(echo "${{ steps.assessment.outputs.score }} < 60" | bc -l) )); then
            echo "Score below threshold: ${{ steps.assessment.outputs.score }}"
            exit 1
          fi
```

**Template variables used**:
- `{{ python_version }}` ‚Äî Python version from context
- Could add: `{{ threshold }}`, `{{ branches }}` for customization

### 2. Pre-commit Config Template

**Location**: `templates/bootstrap/python/pre-commit-config.yaml.j2`

```yaml
repos:
  - repo: https://github.com/psf/black
    rev: 23.12.0
    hooks:
      - id: black
        language_version: python{{ python_version }}

  - repo: https://github.com/pycqa/isort
    rev: 5.13.0
    hooks:
      - id: isort

  - repo: https://github.com/astral-sh/ruff-pre-commit
    rev: v0.1.9
    hooks:
      - id: ruff
        args: [--fix]

  - repo: https://github.com/pre-commit/pre-commit-hooks
    rev: v4.5.0
    hooks:
      - id: trailing-whitespace
      - id: end-of-file-fixer
      - id: check-yaml
      - id: check-added-large-files
```

### 3. Issue Template

**Location**: `templates/bootstrap/common/bug_report.md.j2`

```markdown
---
name: Bug Report
about: Create a report to help us improve
title: '[BUG] '
labels: bug
assignees: ''
---

**Describe the bug**
A clear and concise description of what the bug is.

**To Reproduce**
Steps to reproduce the behavior:
1. Go to '...'
2. Click on '....'
3. Scroll down to '....'
4. See error

**Expected behavior**
A clear and concise description of what you expected to happen.

**Environment**
- OS: [e.g., Ubuntu 22.04, macOS 14.0, Windows 11]
- {% if language == "python" %}Python{% elsif language == "javascript" %}Node.js{% elsif language == "go" %}Go{% endif %} Version: [e.g., {{ python_version if language == "python" else node_version if language == "javascript" else go_version }}]
- {{ repository_name }} Version: [e.g., 1.0.0]

**Additional context**
Add any other context about the problem here.
```

### 4. Conditional Template Logic

Templates can use Jinja2 conditionals:

```yaml
# In tests.yml.j2

{% if language == "python" %}
  - name: Run Tests
    run: |
      pip install pytest pytest-cov
      pytest --cov=src --cov-report=html
{% elsif language == "javascript" %}
  - name: Run Tests
    run: |
      npm install
      npm test -- --coverage
{% elsif language == "go" %}
  - name: Run Tests
    run: |
      go test -v -race -coverprofile=coverage.out ./...
{% endif %}
```

### Template Development Workflow

1. **Create template**:
   ```bash
   vim src/agentready/templates/bootstrap/python/mytemplate.yml.j2
   ```

2. **Add template variables**:
   ```jinja2
   name: {{ repository_name }} CI
   version: {{ python_version }}
   ```

3. **Register in BootstrapGenerator**:
   ```python
   TEMPLATES = {
       "python": [
           # ... existing templates
           "python/mytemplate.yml.j2",
       ]
   }
   ```

4. **Test with dry-run**:
   ```bash
   agentready bootstrap . --dry-run
   ```

5. **Verify rendered output**:
   ```bash
   # Check generated content
   cat .github/workflows/mytemplate.yml
   ```

### Template Best Practices

1. **Use descriptive variable names**:
   ```jinja2
   # Good
   {{ python_version }}
   {{ repository_name }}

   # Bad
   {{ ver }}
   {{ name }}
   ```

2. **Provide defaults**:
   ```jinja2
   python-version: '{{ python_version | default("3.11") }}'
   ```

3. **Add comments**:
   ```yaml
   # This workflow runs on every PR to main
   # Generated by AgentReady Bootstrap
   name: Tests
   ```

4. **Handle optional sections**:
   ```jinja2
   {% if has_tests_directory %}
   - name: Run Tests
     run: pytest tests/
   {% else %}
   # No tests directory found - add tests to enable this step
   {% endif %}
   ```

5. **Include generation metadata**:
   ```yaml
   # Generated by AgentReady Bootstrap v{{ agentready_version }}
   # Date: {{ generation_date }}
   # Language: {{ language }}
   # Do not edit - regenerate with: agentready bootstrap .
   ```

---

## Implementing New Assessors

Follow this step-by-step guide to add a new assessor.

### Step 1: Choose an Attribute

Check `src/agentready/assessors/stub_assessors.py` for not-yet-implemented attributes:

```python
# Example stub assessor
class InlineDocumentationAssessor(BaseAssessor):
    @property
    def attribute_id(self) -> str:
        return "inline_documentation"

    def assess(self, repository: Repository) -> Finding:
        # TODO: Implement actual assessment logic
        return Finding.create_skip(
            self.attribute,
            reason="Assessor not yet implemented"
        )
```

### Step 2: Create Assessor Class

Create a new file or expand existing category file in `src/agentready/assessors/`:

```python
# src/agentready/assessors/documentation.py

from agentready.models import Repository, Finding, Attribute, Remediation
from agentready.assessors.base import BaseAssessor

class InlineDocumentationAssessor(BaseAssessor):
    @property
    def attribute_id(self) -> str:
        return "inline_documentation"

    def assess(self, repository: Repository) -> Finding:
        """
        Assess inline documentation coverage (docstrings/JSDoc).

        Checks:
        - Python: Presence of docstrings in .py files
        - JavaScript/TypeScript: JSDoc comments
        - Coverage: >80% of public functions documented
        """
        # Implement assessment logic here
        pass
```

### Step 3: Implement Assessment Logic

Use the `calculate_proportional_score()` helper for partial compliance:

```python
def assess(self, repository: Repository) -> Finding:
    # Example: Check Python docstrings
    if "Python" not in repository.languages:
        return Finding.create_skip(
            self.attribute,
            reason="No Python files detected"
        )

    # Count functions and docstrings
    total_functions = self._count_functions(repository)
    documented_functions = self._count_documented_functions(repository)

    if total_functions == 0:
        return Finding.create_skip(
            self.attribute,
            reason="No functions found"
        )

    # Calculate coverage
    coverage = documented_functions / total_functions
    score = self.calculate_proportional_score(coverage, 0.80)

    if score >= 80:  # Passes if >= 80% of target
        return Finding.create_pass(
            self.attribute,
            evidence=f"Documented {documented_functions}/{total_functions} functions ({coverage:.1%})",
            remediation=None
        )
    else:
        return Finding.create_fail(
            self.attribute,
            evidence=f"Only {documented_functions}/{total_functions} functions documented ({coverage:.1%})",
            remediation=self._create_remediation(coverage)
        )

def _count_functions(self, repository: Repository) -> int:
    """Count total functions in Python files."""
    # Implementation using ast or grep
    pass

def _count_documented_functions(self, repository: Repository) -> int:
    """Count functions with docstrings."""
    # Implementation using ast
    pass

def _create_remediation(self, current_coverage: float) -> Remediation:
    """Generate remediation guidance."""
    return Remediation(
        steps=[
            "Install pydocstyle: `pip install pydocstyle`",
            "Run docstring linter: `pydocstyle src/`",
            "Add docstrings to flagged functions",
            f"Target: {(0.80 - current_coverage) * 100:.0f}% more functions need documentation"
        ],
        tools=["pydocstyle", "pylint"],
        commands=[
            "pydocstyle src/",
            "pylint --disable=all --enable=missing-docstring src/"
        ],
        examples=[
            '''def calculate_total(items: List[Item]) -> float:
    """
    Calculate total price of items.

    Args:
        items: List of items to sum

    Returns:
        Total price in USD

    Example:
        >>> calculate_total([Item(5.0), Item(3.0)])
        8.0
    """
    return sum(item.price for item in items)'''
        ],
        citations=[
            "PEP 257 - Docstring Conventions",
            "Google Python Style Guide"
        ]
    )
```

### Step 4: Register Assessor

Add to scanner's assessor list in `src/agentready/services/scanner.py`:

```python
def __init__(self):
    self.assessors = [
        # Existing assessors...
        InlineDocumentationAssessor(),
    ]
```

### Step 5: Write Tests

Create comprehensive unit tests in `tests/unit/test_assessors_documentation.py`:

```python
import pytest
from agentready.models import Repository
from agentready.assessors.documentation import InlineDocumentationAssessor

class TestInlineDocumentationAssessor:
    def test_python_well_documented_passes(self, tmp_path):
        """Well-documented Python code should pass."""
        # Create test repository
        repo_path = tmp_path / "test_repo"
        repo_path.mkdir()
        (repo_path / ".git").mkdir()

        # Create Python file with docstrings
        code = '''
def add(a: int, b: int) -> int:
    """Add two numbers."""
    return a + b

def subtract(a: int, b: int) -> int:
    """Subtract b from a."""
    return a - b
'''
        (repo_path / "main.py").write_text(code)

        # Create repository object
        repo = Repository(
            path=str(repo_path),
            name="test_repo",
            languages={"Python": 1}
        )

        # Run assessment
        assessor = InlineDocumentationAssessor()
        finding = assessor.assess(repo)

        # Verify result
        assert finding.status == "pass"
        assert finding.score == 100
        assert "2/2 functions" in finding.evidence

    def test_python_poorly_documented_fails(self, tmp_path):
        """Poorly documented Python code should fail."""
        # Create test repository
        repo_path = tmp_path / "test_repo"
        repo_path.mkdir()
        (repo_path / ".git").mkdir()

        # Create Python file with no docstrings
        code = '''
def add(a, b):
    return a + b

def subtract(a, b):
    return a - b
'''
        (repo_path / "main.py").write_text(code)

        repo = Repository(
            path=str(repo_path),
            name="test_repo",
            languages={"Python": 1}
        )

        assessor = InlineDocumentationAssessor()
        finding = assessor.assess(repo)

        assert finding.status == "fail"
        assert finding.score < 80
        assert "0/2 functions" in finding.evidence
        assert finding.remediation is not None
        assert "pydocstyle" in finding.remediation.tools

    def test_non_python_skips(self, tmp_path):
        """Non-Python repositories should skip."""
        repo = Repository(
            path=str(tmp_path),
            name="test_repo",
            languages={"JavaScript": 10}
        )

        assessor = InlineDocumentationAssessor()
        finding = assessor.assess(repo)

        assert finding.status == "skipped"
        assert "No Python files" in finding.reason
```

### Step 6: Test Manually

```bash
# Run your new tests
pytest tests/unit/test_assessors_documentation.py -v

# Run full assessment on AgentReady itself
agentready assess . --verbose

# Verify your assessor appears in output
```

### Best Practices for Assessors

1. **Fail Gracefully**: Return "skipped" if required tools missing, don't crash
2. **Provide Rich Remediation**: Include steps, tools, commands, examples, citations
3. **Use Proportional Scoring**: `calculate_proportional_score()` for partial compliance
4. **Language-Specific Logic**: Check `repository.languages` before assessing
5. **Avoid External Dependencies**: Use stdlib when possible (ast, re, pathlib)
6. **Performance**: Keep assessments fast (<1 second per assessor)
7. **Idempotent**: Same repository ‚Üí same result
8. **Evidence**: Provide specific, actionable evidence (file paths, counts, examples)

---

## Testing Guidelines

AgentReady maintains high test quality standards.

### Test Organization

```
tests/
‚îú‚îÄ‚îÄ unit/                  # Fast, isolated tests
‚îÇ   ‚îú‚îÄ‚îÄ test_models.py
‚îÇ   ‚îú‚îÄ‚îÄ test_assessors_*.py
‚îÇ   ‚îî‚îÄ‚îÄ test_reporters.py
‚îú‚îÄ‚îÄ integration/           # End-to-end tests
‚îÇ   ‚îî‚îÄ‚îÄ test_full_assessment.py
‚îî‚îÄ‚îÄ fixtures/              # Shared test data
    ‚îî‚îÄ‚îÄ sample_repos/
```

### Test Types

#### Unit Tests

- **Purpose**: Test individual components in isolation
- **Speed**: Very fast (<1s total)
- **Coverage**: Models, assessors, services, reporters
- **Mocking**: Use `pytest` fixtures and mocks

#### Integration Tests

- **Purpose**: Test complete workflows end-to-end
- **Speed**: Slower (acceptable up to 10s total)
- **Coverage**: Full assessment pipeline
- **Real Data**: Use fixture repositories

### Writing Good Tests

#### Test Naming

Use descriptive names following pattern: `test_<what>_<when>_<expected>`

```python
# Good
def test_claude_md_assessor_with_existing_file_passes():
    pass

def test_readme_assessor_missing_quick_start_fails():
    pass

def test_type_annotations_assessor_javascript_repo_skips():
    pass

# Bad
def test_assessor():
    pass

def test_pass_case():
    pass
```

#### Arrange-Act-Assert Pattern

```python
def test_finding_create_pass_sets_correct_attributes():
    # Arrange
    attribute = Attribute(
        id="test_attr",
        name="Test Attribute",
        tier=1,
        weight=0.10
    )

    # Act
    finding = Finding.create_pass(
        attribute=attribute,
        evidence="Test evidence",
        remediation=None
    )

    # Assert
    assert finding.status == "pass"
    assert finding.score == 100
    assert finding.evidence == "Test evidence"
    assert finding.remediation is None
```

#### Use Fixtures

```python
@pytest.fixture
def sample_repository(tmp_path):
    """Create a sample repository for testing."""
    repo_path = tmp_path / "sample_repo"
    repo_path.mkdir()
    (repo_path / ".git").mkdir()

    # Add files
    (repo_path / "README.md").write_text("# Sample Repo")
    (repo_path / "CLAUDE.md").write_text("# Tech Stack")

    return Repository(
        path=str(repo_path),
        name="sample_repo",
        languages={"Python": 5}
    )

def test_with_fixture(sample_repository):
    assert sample_repository.name == "sample_repo"
```

### Coverage Requirements

- **Target**: >80% line coverage for new code
- **Minimum**: >70% overall coverage
- **Critical Paths**: 100% coverage (scoring algorithm, finding creation)

```bash
# Generate coverage report
pytest --cov=src/agentready --cov-report=html

# View report
open htmlcov/index.html
```

---

## Code Quality Standards

### Formatting

**Black** (88 character line length, opinionated formatting):

```bash
black src/ tests/
```

Configuration in `pyproject.toml`:

```toml
[tool.black]
line-length = 88
target-version = ['py311', 'py312']
```

### Import Sorting

**isort** (consistent import organization):

```bash
isort src/ tests/
```

Configuration in `pyproject.toml`:

```toml
[tool.isort]
profile = "black"
line_length = 88
```

### Linting

**Ruff** (fast Python linter):

```bash
ruff check src/ tests/
```

Configuration in `pyproject.toml`:

```toml
[tool.ruff]
line-length = 88
select = ["E", "F", "W", "I"]
ignore = ["E501"]  # Line too long (handled by black)
```

### Type Checking (Future)

**mypy** (static type checking):

```bash
mypy src/
```

Configuration in `pyproject.toml`:

```toml
[tool.mypy]
python_version = "3.11"
strict = true
warn_return_any = true
warn_unused_configs = true
```

### Documentation Standards

- **Docstrings**: All public functions, classes, methods
- **Format**: Google-style docstrings
- **Type hints**: All function parameters and return types

```python
def calculate_weighted_score(findings: List[Finding], weights: Dict[str, float]) -> float:
    """
    Calculate weighted average score from findings.

    Args:
        findings: List of assessment findings
        weights: Attribute ID to weight mapping

    Returns:
        Weighted score from 0.0 to 100.0

    Raises:
        ValueError: If weights don't sum to 1.0

    Example:
        >>> findings = [Finding(score=80), Finding(score=90)]
        >>> weights = {"attr1": 0.5, "attr2": 0.5}
        >>> calculate_weighted_score(findings, weights)
        85.0
    """
    pass
```

---

## Contributing Workflow

### 1. Create Feature Branch

```bash
# Update main
git checkout main
git pull upstream main

# Create feature branch
git checkout -b feature/inline-documentation-assessor
```

### 2. Implement Changes

- Write code following style guide
- Add comprehensive tests
- Update documentation (CLAUDE.md, README.md if needed)

### 3. Run Quality Checks

```bash
# Format code
black src/ tests/
isort src/ tests/

# Lint
ruff check src/ tests/

# Run tests
pytest --cov

# All checks must pass
```

### 4. Commit Changes

Use **conventional commits**:

```bash
git add .
git commit -m "feat(assessors): add inline documentation assessor

- Implement Python docstring coverage assessment
- Add test coverage for various documentation levels
- Include rich remediation guidance with examples
- Support JSDoc detection for JavaScript/TypeScript (future)"
```

**Commit types**:
- `feat`: New feature
- `fix`: Bug fix
- `docs`: Documentation changes
- `test`: Test additions/changes
- `refactor`: Code refactoring
- `chore`: Maintenance tasks

### 5. Push and Create PR

```bash
git push origin feature/inline-documentation-assessor
```

Create pull request on GitHub with:

- **Title**: Clear, descriptive (e.g., "Add inline documentation assessor")
- **Description**:
  - What changed
  - Why (link to issue if applicable)
  - Testing performed
  - Screenshots/examples (if UI changes)
- **Checklist**:
  - [ ] Tests added and passing
  - [ ] Code formatted (black, isort)
  - [ ] Linting passes (ruff)
  - [ ] Documentation updated
  - [ ] Changelog entry (if user-facing)

### 6. Address Review Feedback

- Respond to comments
- Make requested changes
- Push updates to same branch
- Re-request review

---

## Release Process

AgentReady follows **semantic versioning** (SemVer):

- **Major (X.0.0)**: Breaking changes
- **Minor (x.Y.0)**: New features, backward-compatible
- **Patch (x.y.Z)**: Bug fixes, backward-compatible

### Release Checklist

1. **Update version** in `pyproject.toml`
2. **Update CHANGELOG.md** with release notes
3. **Run full test suite**: `pytest --cov`
4. **Run quality checks**: `black . && isort . && ruff check .`
5. **Build package**: `python -m build`
6. **Test package locally**: `pip install dist/agentready-X.Y.Z.tar.gz`
7. **Create git tag**: `git tag -a vX.Y.Z -m "Release vX.Y.Z"`
8. **Push tag**: `git push upstream vX.Y.Z`
9. **Upload to PyPI**: `twine upload dist/*`
10. **Create GitHub release** with changelog

---

## Additional Resources

- **[Attributes Reference](attributes.html)** ‚Äî Detailed attribute definitions
- **[API Reference](api-reference.html)** ‚Äî Public API documentation
- **[Examples](examples.html)** ‚Äî Real-world assessment reports
- **CLAUDE.md** ‚Äî Project context for AI agents
- **BACKLOG.md** ‚Äî Planned features and enhancements

---

**Ready to contribute?** Check out [good first issues](https://github.com/ambient-code/agentready/issues?q=is%3Aissue+is%3Aopen+label%3A%22good+first+issue%22) on GitHub!
````

## File: src/agentready/cli/main.py
````python
"""CLI entry point for agentready tool."""

import json
import sys
from pathlib import Path

import click

from ..assessors.code_quality import (
    CyclomaticComplexityAssessor,
    TypeAnnotationsAssessor,
)

# Import all assessors
from ..assessors.documentation import CLAUDEmdAssessor, READMEAssessor
from ..assessors.structure import StandardLayoutAssessor
from ..assessors.stub_assessors import (
    ConventionalCommitsAssessor,
    GitignoreAssessor,
    LockFilesAssessor,
    create_stub_assessors,
)
from ..assessors.testing import PreCommitHooksAssessor, TestCoverageAssessor
from ..models.config import Config
from ..reporters.html import HTMLReporter
from ..reporters.markdown import MarkdownReporter
from ..services.research_loader import ResearchLoader
from ..services.scanner import Scanner
from .bootstrap import bootstrap


def create_all_assessors():
    """Create all 25 assessors for assessment."""
    assessors = [
        # Tier 1 Essential (5 assessors)
        CLAUDEmdAssessor(),
        READMEAssessor(),
        TypeAnnotationsAssessor(),
        StandardLayoutAssessor(),
        LockFilesAssessor(),
        # Tier 2 Critical (10 assessors - 3 implemented, 7 stubs)
        TestCoverageAssessor(),
        PreCommitHooksAssessor(),
        ConventionalCommitsAssessor(),
        GitignoreAssessor(),
        CyclomaticComplexityAssessor(),  # Actually Tier 3, but including here
    ]

    # Add remaining stub assessors
    assessors.extend(create_stub_assessors())

    return assessors


@click.group(invoke_without_command=True)
@click.option("--version", is_flag=True, help="Show version information")
@click.pass_context
def cli(ctx, version):
    """AgentReady Repository Scorer - Assess repositories for AI-assisted development.

    Evaluates repositories against 25 evidence-based attributes and generates
    comprehensive reports with scores, findings, and remediation guidance.

    Examples:

        \b
        # Assess current repository
        agentready assess .

        \b
        # Assess with custom configuration
        agentready assess /path/to/repo --config my-config.yaml

        \b
        # Show research version
        agentready research-version

        \b
        # Generate example config
        agentready generate-config
    """
    if version:
        show_version()
        ctx.exit()

    if ctx.invoked_subcommand is None:
        click.echo(ctx.get_help())


@cli.command()
@click.argument("repository", type=click.Path(exists=True), required=False, default=".")
@click.option("--verbose", "-v", is_flag=True, help="Enable verbose output")
@click.option(
    "--output-dir",
    "-o",
    type=click.Path(),
    default=None,
    help="Output directory for reports (default: .agentready/)",
)
@click.option(
    "--config",
    "-c",
    type=click.Path(exists=True),
    default=None,
    help="Path to configuration file",
)
def assess(repository, verbose, output_dir, config):
    """Assess a repository against agent-ready criteria.

    REPOSITORY: Path to git repository (default: current directory)
    """
    run_assessment(repository, verbose, output_dir, config)


def run_assessment(repository_path, verbose, output_dir, config_path):
    """Execute repository assessment."""
    repo_path = Path(repository_path).resolve()

    if verbose:
        click.echo(f"AgentReady Repository Scorer")
        click.echo(f"{'=' * 50}\n")

    # Load configuration if provided
    config = None
    if config_path:
        config = load_config(Path(config_path))

    # Set output directory
    if output_dir:
        output_path = Path(output_dir)
    else:
        output_path = repo_path / ".agentready"

    output_path.mkdir(parents=True, exist_ok=True)

    # Create scanner
    try:
        scanner = Scanner(repo_path, config)
    except ValueError as e:
        click.echo(f"Error: {str(e)}", err=True)
        sys.exit(1)

    # Create assessors
    assessors = create_all_assessors()

    if verbose:
        click.echo(f"Repository: {repo_path}")
        click.echo(f"Assessors: {len(assessors)}")
        click.echo(f"Output: {output_path}\n")

    # Run scan
    try:
        assessment = scanner.scan(assessors, verbose=verbose)
    except Exception as e:
        click.echo(f"Error during assessment: {str(e)}", err=True)
        if verbose:
            import traceback

            traceback.print_exc()
        sys.exit(1)

    # Generate timestamp for file naming
    timestamp = assessment.timestamp.strftime("%Y%m%d-%H%M%S")

    # Save JSON output
    json_file = output_path / f"assessment-{timestamp}.json"
    with open(json_file, "w", encoding="utf-8") as f:
        json.dump(assessment.to_dict(), f, indent=2)

    # Generate HTML report
    html_reporter = HTMLReporter()
    html_file = output_path / f"report-{timestamp}.html"
    html_reporter.generate(assessment, html_file)

    # Generate Markdown report
    markdown_reporter = MarkdownReporter()
    markdown_file = output_path / f"report-{timestamp}.md"
    markdown_reporter.generate(assessment, markdown_file)

    # Create latest symlinks
    latest_json = output_path / "assessment-latest.json"
    latest_html = output_path / "report-latest.html"
    latest_md = output_path / "report-latest.md"

    for latest, target in [
        (latest_json, json_file),
        (latest_html, html_file),
        (latest_md, markdown_file),
    ]:
        if latest.exists():
            latest.unlink()
        try:
            latest.symlink_to(target.name)
        except OSError:
            # Windows doesn't support symlinks easily, just copy
            import shutil

            shutil.copy(target, latest)

    if verbose:
        click.echo(f"\n{'=' * 50}")

    click.echo(f"\nAssessment complete!")
    click.echo(
        f"  Score: {assessment.overall_score:.1f}/100 ({assessment.certification_level})"
    )
    click.echo(
        f"  Assessed: {assessment.attributes_assessed}/{assessment.attributes_total}"
    )
    click.echo(f"  Skipped: {assessment.attributes_skipped}")
    click.echo(f"  Duration: {assessment.duration_seconds:.1f}s")
    click.echo(f"\nReports generated:")
    click.echo(f"  JSON: {json_file}")
    click.echo(f"  HTML: {html_file}")
    click.echo(f"  Markdown: {markdown_file}")


def load_config(config_path: Path) -> Config:
    """Load configuration from YAML file."""
    import yaml

    with open(config_path, "r", encoding="utf-8") as f:
        data = yaml.safe_load(f)

    return Config(
        weights=data.get("weights", {}),
        excluded_attributes=data.get("excluded_attributes", []),
        language_overrides=data.get("language_overrides", {}),
        output_dir=Path(data["output_dir"]) if "output_dir" in data else None,
    )


@cli.command()
def research_version():
    """Show bundled research report version."""
    loader = ResearchLoader()
    try:
        content, metadata, is_valid, errors, warnings = loader.load_and_validate()

        click.echo(f"Research Report Version: {metadata.version}")
        click.echo(f"Date: {metadata.date}")
        click.echo(f"Attributes: {metadata.attribute_count}")
        click.echo(f"References: {metadata.reference_count}")
        click.echo(f"\nValidation: {'‚úì PASS' if is_valid else '‚úó FAIL'}")

        if errors:
            click.echo("\nErrors:")
            for error in errors:
                click.echo(f"  - {error}")

        if warnings:
            click.echo("\nWarnings:")
            for warning in warnings:
                click.echo(f"  - {warning}")

    except FileNotFoundError as e:
        click.echo(f"Error: {str(e)}", err=True)
        sys.exit(1)


@cli.command()
def generate_config():
    """Generate example configuration file."""
    example_path = Path(".agentready-config.example.yaml")

    if not example_path.exists():
        click.echo("Error: .agentready-config.example.yaml not found", err=True)
        sys.exit(1)

    target = Path(".agentready-config.yaml")

    if target.exists():
        if not click.confirm(f"{target} already exists. Overwrite?"):
            return

    import shutil

    shutil.copy(example_path, target)
    click.echo(f"Created {target}")
    click.echo("Edit this file to customize weights and behavior.")


# Register bootstrap command
cli.add_command(bootstrap)


def show_version():
    """Show version information."""
    click.echo("AgentReady Repository Scorer v1.0.0")
    click.echo("Research Report: bundled")


if __name__ == "__main__":
    cli()
````

## File: docs/_config.yml
````yaml
# AgentReady Documentation Site Configuration

# Site settings
title: AgentReady
tagline: Build and Maintain Agent-Ready Codebases
description: >-
  Automated infrastructure generation and continuous quality assessment for
  AI-assisted development. Bootstrap creates GitHub Actions, pre-commit hooks,
  templates, and Dependabot in one command. Assess repositories against 25
  evidence-based attributes with actionable remediation guidance.

# Repository information
repository: https://github.com/ambient-code/agentready
github_username: ambient-code

# Build settings
theme: jekyll-theme-minimal
markdown: kramdown
kramdown:
  input: GFM
  syntax_highlighter: rouge
  syntax_highlighter_opts:
    default_lang: python
    css_class: 'highlight'

# Plugins
plugins:
  - jekyll-feed
  - jekyll-seo-tag
  - jekyll-sitemap

# Navigation
navigation:
  - title: Home
    url: /
  - title: User Guide
    url: /user-guide
  - title: Developer Guide
    url: /developer-guide
  - title: Roadmaps
    url: /roadmaps
  - title: Attributes
    url: /attributes
  - title: API Reference
    url: /api-reference
  - title: Examples
    url: /examples

# Collections
collections:
  attributes:
    output: true
    permalink: /attributes/:name

# Defaults
defaults:
  - scope:
      path: ""
      type: "pages"
    values:
      layout: "default"
  - scope:
      path: ""
      type: "attributes"
    values:
      layout: "attribute"

# Exclude from build
exclude:
  - Gemfile
  - Gemfile.lock
  - node_modules
  - vendor
  - .sass-cache
  - .jekyll-cache
  - README.md
  - DEPLOYMENT.md
  - SETUP_SUMMARY.md

# Site metadata
version: 1.0.0
certification_levels:
  platinum:
    range: "90-100"
    color: "#e5e4e2"
    emoji: "üèÜ"
  gold:
    range: "75-89"
    color: "#ffd700"
    emoji: "ü•á"
  silver:
    range: "60-74"
    color: "#c0c0c0"
    emoji: "ü•à"
  bronze:
    range: "40-59"
    color: "#cd7f32"
    emoji: "ü•â"
  needs_improvement:
    range: "0-39"
    color: "#8b4513"
    emoji: "üìà"

# URLs
baseurl: "/agentready" # the subpath of your site
url: "https://ambient-code.github.io" # the base hostname & protocol for your site

# Analytics (optional - uncomment and add your ID)
# google_analytics: UA-XXXXXXXXX-X
````

## File: CHANGELOG.md
````markdown
## [1.1.1](https://github.com/ambient-code/agentready/compare/v1.1.0...v1.1.1) (2025-11-21)


### Bug Fixes

* add repository checkout step to Claude Code Action workflow ([17aa0cf](https://github.com/ambient-code/agentready/commit/17aa0cf9eae93176b735ed082f3979dda0451bbd))

# [1.1.0](https://github.com/ambient-code/agentready/compare/v1.0.4...v1.1.0) (2025-11-21)


### Features

* add Claude Code GitHub Action for [@claude](https://github.com/claude) mentions ([3e7224d](https://github.com/ambient-code/agentready/commit/3e7224d3cb8cc7b7a4088948a06622da6716e2cd))

## [1.0.4](https://github.com/ambient-code/agentready/compare/v1.0.3...v1.0.4) (2025-11-21)


### Bug Fixes

* set correct baseurl for GitHub Pages subdirectory deployment ([c4db765](https://github.com/ambient-code/agentready/commit/c4db7655c83a1f5e407a8c045cc21d4a53b97385))

## [1.0.3](https://github.com/ambient-code/agentready/compare/v1.0.2...v1.0.3) (2025-11-21)


### Bug Fixes

* exclude DEPLOYMENT.md and SETUP_SUMMARY.md from Jekyll build ([9611207](https://github.com/ambient-code/agentready/commit/9611207153ce9c323f8f1952ba18e9f523dedf7d))

## [1.0.2](https://github.com/ambient-code/agentready/compare/v1.0.1...v1.0.2) (2025-11-21)


### Bug Fixes

* replace all remaining elif with elsif in developer-guide ([73f16fc](https://github.com/ambient-code/agentready/commit/73f16fc746aa97788bfa776d5660158be3139318))

## [1.0.1](https://github.com/ambient-code/agentready/compare/v1.0.0...v1.0.1) (2025-11-21)


### Bug Fixes

* correct Liquid syntax in developer-guide (elif -> elsif) ([75f3b1d](https://github.com/ambient-code/agentready/commit/75f3b1d23f87c5294086f78e81ad62a8ad47af7e))

# 1.0.0 (2025-11-21)


### Bug Fixes

* Improve report metadata display with clean table format ([ca361a4](https://github.com/ambient-code/agentready/commit/ca361a44ccf58c277853739a0c856be388adb779))
* P0 security and logic bugs from code review ([2af2346](https://github.com/ambient-code/agentready/commit/2af234661d94bcedaa9473fbf4925f8fd382d62c))


### Features

* Add Interactive Dashboard backlog item ([adfc4c8](https://github.com/ambient-code/agentready/commit/adfc4c823a7b25d5b1404c9b994461e290257dce))
* Add interactive HTML report generation ([18664ea](https://github.com/ambient-code/agentready/commit/18664eacea94e90ee2da222a09228771c67c17f0))
* add release pipeline coldstart prompt ([#19](https://github.com/ambient-code/agentready/issues/19)) ([9a3880c](https://github.com/ambient-code/agentready/commit/9a3880c6a04014a30b4405c301c34aac5170a394)), closes [#18](https://github.com/ambient-code/agentready/issues/18)
* Complete Phases 5-7 - Markdown reports, testing, and polish ([7659623](https://github.com/ambient-code/agentready/commit/76596230e7fe2a5523432a3450430051d45b565f))
* Implement AgentReady MVP with scoring engine ([54a96cb](https://github.com/ambient-code/agentready/commit/54a96cbafc37dfc0f67dc12833b3c789ee81bc53))
* implement automated semantic release pipeline ([#20](https://github.com/ambient-code/agentready/issues/20)) ([b579235](https://github.com/ambient-code/agentready/commit/b5792359fe1a42a3ddc5475748ba52af3b347d44))
* implement bootstrap command for GitHub infrastructure ([0af06c4](https://github.com/ambient-code/agentready/commit/0af06c492cc487c8894203d096e9cb5a3259caa1)), closes [#2](https://github.com/ambient-code/agentready/issues/2)

# Changelog

All notable changes to this project will be documented in this file.

The format is based on [Keep a Changelog](https://keepachangelog.com/en/1.0.0/),
and this project adheres to [Semantic Versioning](https://semver.org/spec/v2.0.0.html).

## [Unreleased]

### Added
- Automated semantic release pipeline with conventional commits
- Release process documentation
- CHANGELOG generation from commit messages

---

*This changelog is automatically generated by [semantic-release](https://github.com/semantic-release/semantic-release)*
````

## File: pyproject.toml
````toml
[project]
name = "agentready"
version = "1.1.1"
description = "Assess git repositories against 25 evidence-based attributes for AI-assisted development readiness"
authors = [{name = "Jeremy Eder", email = "jeder@redhat.com"}]
readme = "README.md"
license = {text = "MIT"}
requires-python = ">=3.11"
classifiers = [
    "Development Status :: 4 - Beta",
    "Intended Audience :: Developers",
    "License :: OSI Approved :: MIT License",
    "Programming Language :: Python :: 3.11",
    "Programming Language :: Python :: 3.12",
    "Topic :: Software Development :: Quality Assurance",
]

dependencies = [
    "click>=8.1.0",
    "jinja2>=3.1.0",
    "pyyaml>=6.0",
    "gitpython>=3.1.0",
    "radon>=6.0.0",
    "lizard>=1.17.0",
]

[project.optional-dependencies]
dev = [
    "pytest>=7.4.0",
    "pytest-cov>=4.1.0",
    "black>=23.0.0",
    "isort>=5.12.0",
    "flake8>=6.0.0",
]

[project.scripts]
agentready = "agentready.cli.main:cli"

[build-system]
requires = ["setuptools>=68.0.0", "wheel"]
build-backend = "setuptools.build_meta"

[tool.setuptools]
package-dir = {"" = "src"}
packages = ["agentready", "agentready.cli", "agentready.assessors", "agentready.models", "agentready.services", "agentready.reporters"]

[tool.setuptools.package-data]
agentready = [
    "data/*.md",
    "data/*.yaml",
    "templates/*.j2",
    "templates/**/*.j2",
    "templates/**/*.yml.j2",
    "templates/**/*.yaml.j2",
    "templates/**/*.md.j2"
]

[tool.black]
line-length = 88
target-version = ["py311"]
extend-exclude = '''
/(
  \.eggs
  | \.git
  | \.venv
  | venv
  | build
  | dist
)/
'''

[tool.isort]
profile = "black"
line_length = 88
skip_gitignore = true

[tool.pytest.ini_options]
testpaths = ["tests"]
python_files = ["test_*.py"]
python_classes = ["Test*"]
python_functions = ["test_*"]
addopts = "-v --cov=agentready --cov-report=term-missing --cov-report=html"

[tool.coverage.run]
source = ["src/agentready"]
omit = [
    "*/tests/*",
    "*/__pycache__/*",
    "*/.venv/*",
]

[tool.coverage.report]
exclude_lines = [
    "pragma: no cover",
    "def __repr__",
    "raise AssertionError",
    "raise NotImplementedError",
    "if __name__ == .__main__.:",
    "if TYPE_CHECKING:",
    "class .*\\bProtocol\\):",
    "@(abc\\.)?abstractmethod",
]
````

## File: BACKLOG.md
````markdown
# AgentReady Backlog

**Purpose**: Track future features and improvements for the agentready tool.

---

## Critical Issues (P0)

### Create Automated Demo

**Priority**: P0 (Critical - Showcase Value)

**Description**: Create an automated, self-contained demonstration of AgentReady that shows the tool assessing a sample repository, generating reports, and providing remediation guidance. This should be runnable with a single command and suitable for demos, presentations, and onboarding.

**Requirements**:
- Single command to run: `agentready demo`
- Self-contained sample repository (embedded in package or generated on-the-fly)
- Demonstrates full workflow:
  1. Repository analysis
  2. Attribute assessment
  3. Score calculation
  4. HTML/Markdown report generation
  5. Remediation suggestions
- Interactive terminal output showing progress
- Opens generated HTML report automatically in browser
- Reusable for presentations and stakeholder demos

**Implementation**:

```bash
# Run automated demo
agentready demo

# Output:
# ü§ñ AgentReady Demo
# ===================
#
# Creating sample repository...
# Analyzing structure...
# Running 25 assessments...
#   ‚úÖ claude_md_file (100/100)
#   ‚ùå precommit_hooks (0/100)
#   ... [progress indicators]
#
# Assessment complete!
# Score: 67.3/100 (Silver)
#
# Generating reports...
#   üìÑ demo-report.html (generated)
#   üìÑ demo-report.md (generated)
#   üìÑ demo-assessment.json (generated)
#
# Opening HTML report in browser...
```

**Sample Repository Options**:

**Option 1: Embedded Examples** (Ship with package)
```
src/agentready/demo/
‚îú‚îÄ‚îÄ sample-python-repo/      # Python project (minimal)
‚îÇ   ‚îú‚îÄ‚îÄ src/myapp/
‚îÇ   ‚îú‚îÄ‚îÄ tests/
‚îÇ   ‚îú‚îÄ‚îÄ README.md
‚îÇ   ‚îú‚îÄ‚îÄ pyproject.toml
‚îÇ   ‚îî‚îÄ‚îÄ .gitignore
‚îú‚îÄ‚îÄ sample-js-repo/          # JavaScript project
‚îÇ   ‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ package.json
‚îÇ   ‚îî‚îÄ‚îÄ README.md
‚îî‚îÄ‚îÄ sample-go-repo/          # Go project
```

**Option 2: Generate On-the-Fly** (Dynamic)
```python
def create_demo_repo(tmp_path: Path, language: str = "python") -> Path:
    """Create sample repository for demo."""
    repo = tmp_path / "demo-repo"
    repo.mkdir()

    if language == "python":
        # Create minimal Python project
        create_file(repo / "README.md", "# Demo Project\n\nSample Python project.")
        create_file(repo / "src/main.py", "def main(): pass")
        create_file(repo / ".gitignore", "*.pyc\n__pycache__/")
        # Missing: CLAUDE.md, tests/, pre-commit hooks (intentional for demo)

    return repo
```

**Demo Script Features**:
- **Progress indicators**: Show assessment running in real-time
- **Color-coded output**: Green for pass, red for fail, yellow for warnings
- **Simulated delays**: Add realistic pauses for "dramatic effect" in demos
- **Narration mode**: Optional verbose output explaining each step
- **Screenshot mode**: Generate high-quality terminal screenshots for docs
- **Record mode**: Save terminal session as GIF/video for presentations

**Configuration**:

```yaml
# Demo config embedded in package
demo:
  sample_repo: python  # or javascript, go, minimal
  show_progress: true
  auto_open_browser: true
  output_dir: ./demo-output
  narration: true  # Explain each step
  delay_ms: 500    # Pause between steps for visibility
```

**Use Cases**:

**Use Case 1: Stakeholder Demo**
```bash
# Quick 2-minute demo for leadership
agentready demo --narration
# Shows: What AgentReady does, how it scores, what reports look like
```

**Use Case 2: Onboarding New Users**
```bash
# Help new users understand the tool
agentready demo --tutorial
# Interactive walkthrough with explanations
```

**Use Case 3: Generate Demo Content for Docs**
```bash
# Create screenshots and videos for documentation
agentready demo --screenshot --record demo.gif
```

**Acceptance Criteria**:
- [ ] `agentready demo` runs without any setup
- [ ] Creates sample repository automatically
- [ ] Runs full assessment workflow
- [ ] Generates all report formats (HTML, Markdown, JSON)
- [ ] Opens HTML report in browser
- [ ] Colorful, engaging terminal output
- [ ] Completes in < 5 seconds
- [ ] No external dependencies required
- [ ] Works offline
- [ ] Includes narration mode for presentations

**Priority Justification**:
- Critical for showcasing tool value to stakeholders
- Needed for Red Hat internal demos and presentations
- Helps with user onboarding and adoption
- Low effort, high impact for visibility
- Required before pitching to other Red Hat teams

**Related**: Onboarding, documentation, marketing

**Notes**:
- Keep demo simple and fast (< 5 seconds)
- Focus on visual impact (colors, progress bars)
- Make it suitable for screenshots/videos
- Consider adding "failure scenario" demo too
- Could expand to multiple language demos
- Add to bootstrap command as part of repo setup

---

### Add Bootstrap Quickstart to README.md

**Priority**: P0 (Critical - Discoverability)

**Description**: Add a dead-simple, copy-paste Bootstrap quickstart to README.md. Currently, bootstrap documentation only exists in docs/user-guide.md, but README.md (the first thing users see on GitHub) has zero mentions of the bootstrap command.

**Problem**:
- New users landing on GitHub see only `agentready assess` commands
- Bootstrap command (the recommended entry point) is buried in documentation
- Missing the "most dead simple copy paste thing" for bootstrap

**Requirements**:
- Add prominent Bootstrap quickstart section to README.md
- 4-5 line copy-paste command sequence
- Show what gets created (bullets with checkmarks)
- Link to detailed tutorial in docs/user-guide.md
- Position ABOVE or alongside the assess quickstart

**Implementation**:

Add to README.md after Installation section:

```markdown
## Quick Start: Bootstrap (Recommended)

Transform your repository with one command:

```bash
cd /path/to/your/repo
agentready bootstrap .
git add . && git commit -m "build: Bootstrap agent-ready infrastructure"
git push
```

**What you get:**
- ‚úÖ GitHub Actions workflows (tests, security, AgentReady assessment)
- ‚úÖ Pre-commit hooks (formatters, linters)
- ‚úÖ Issue/PR templates
- ‚úÖ Dependabot configuration
- ‚úÖ Automated assessment on every PR

**Duration**: <60 seconds

[See detailed Bootstrap tutorial ‚Üí](docs/user-guide.md#bootstrap-your-repository)

---

## Quick Start: Assessment Only

For one-time analysis without infrastructure changes:

```bash
cd /path/to/your/repo
agentready assess .
open .agentready/report-latest.html
```
```

**Acceptance Criteria**:
- [ ] Bootstrap quickstart added to README.md
- [ ] Positioned prominently (before or alongside assess quickstart)
- [ ] Maximum 10 lines of code
- [ ] Shows clear value prop (what you get)
- [ ] Links to detailed docs
- [ ] Uses conventional commit message format

**Related**: Bootstrap command (#3 - already implemented), Documentation workflow, User onboarding

**Notes**:
- Bootstrap is already implemented - this is just documentation
- README.md is the entry point for 90% of users
- This is blocking adoption - users don't know bootstrap exists
- Should match the style in docs/user-guide.md lines 87-115

---

### Fix Critical Security & Logic Bugs from Code Review

**Priority**: P0 (Critical - Security & Correctness)

**Description**: Address critical bugs discovered in code review that affect security and assessment accuracy.

**Issues to Fix**:

1. **XSS Vulnerability in HTML Reports** (CRITICAL - Security)
   - **Location**: `src/agentready/templates/report.html.j2:579`
   - **Problem**: `assessment_json|safe` disables autoescaping for JSON embedded in JavaScript
   - **Risk**: Repository names, commit messages, file paths from git could contain malicious content
   - **Fix**: Replace with `JSON.parse({{ assessment_json|tojson }})`
   - **Add**: Content Security Policy headers to HTML reports

2. **StandardLayoutAssessor Logic Bug** (CRITICAL - Incorrect Scoring)
   - **Location**: `src/agentready/assessors/structure.py:48`
   - **Problem**: `(repository.path / "tests") or (repository.path / "test")` always evaluates to first path
   - **Impact**: Projects with `test/` instead of `tests/` scored incorrectly
   - **Fix**: Check both paths properly:
     ```python
     tests_path = repository.path / "tests"
     if not tests_path.exists():
         tests_path = repository.path / "test"
     has_tests = tests_path.exists()
     ```

**Implementation**:

**File 1**: `src/agentready/templates/report.html.j2`
```jinja2
<!-- BEFORE (VULNERABLE): -->
const ASSESSMENT = {{ assessment_json|safe }};

<!-- AFTER (SECURE): -->
const ASSESSMENT = JSON.parse({{ assessment_json|tojson }});
```

**File 2**: `src/agentready/assessors/structure.py`
```python
# BEFORE (BUGGY):
standard_dirs = {
    "src": repository.path / "src",
    "tests": (repository.path / "tests") or (repository.path / "test"),  # BUG!
}

# AFTER (CORRECT):
standard_dirs = {
    "src": repository.path / "src",
}

# Check for tests directory (either tests/ or test/)
tests_path = repository.path / "tests"
if not tests_path.exists():
    tests_path = repository.path / "test"
standard_dirs["tests"] = tests_path
```

**Test Cases to Add**:
```python
def test_xss_in_repository_name():
    """Test that malicious repo names are escaped in HTML."""
    repo = Repository(
        name="<script>alert('xss')</script>",
        # ...
    )
    html = HTMLReporter().generate(assessment, output)
    assert "<script>" not in html  # Should be escaped

def test_standard_layout_with_test_dir():
    """Test that 'test/' directory is recognized (not just 'tests/')."""
    # Create repo with test/ directory only
    repo_path = tmp_path / "repo"
    (repo_path / "test").mkdir(parents=True)

    assessor = StandardLayoutAssessor()
    finding = assessor.assess(Repository(...))
    assert finding.status == "pass"  # Should recognize test/ dir
```

**Acceptance Criteria**:
- [ ] XSS vulnerability patched with `tojson` filter
- [ ] CSP headers added to HTML reports (future)
- [ ] StandardLayoutAssessor recognizes both `tests/` and `test/`
- [ ] Tests added for XSS prevention
- [ ] Tests added for both test directory naming patterns
- [ ] All existing tests still pass

**Priority Justification**:
- **Security**: XSS is a P0 vulnerability
- **Correctness**: Incorrect scoring undermines tool credibility
- **Quick fixes**: Both are 5-10 minute changes

**Related**: Issue #2 (Report improvements), Bootstrap (#1 - needs secure reports)

---

### Bootstrap AgentReady Repository on GitHub

**Priority**: P0 (Critical - Dogfooding)

**Description**: Implement `agentready bootstrap` command to set up the agentready repository itself on GitHub with all best practices. This is the FIRST feature to implement - we'll dogfood our own tool!

**Vision**: Use AgentReady to bootstrap the AgentReady repository - demonstrating the tool's value while setting up our own infrastructure.

**Why P0**:
- Demonstrates tool value immediately (dogfooding)
- Sets up critical GitHub infrastructure (Actions, badges, PR templates)
- Validates bootstrap command design before external users
- Creates the foundation for GitHub App integration

**Requirements**:

1. **GitHub Repository Setup**
   - Create/update repository on GitHub via `gh` CLI
   - Set repository description and topics
   - Configure repository settings (PR requirements, branch protection)

2. **GitHub Actions Workflows**
   - `.github/workflows/agentready-assessment.yml` - Run assessment on PR/push
   - `.github/workflows/tests.yml` - Run pytest, linters
   - `.github/workflows/release.yml` - Publish to PyPI (future)

3. **GitHub Templates**
   - `.github/ISSUE_TEMPLATE/bug_report.md`
   - `.github/ISSUE_TEMPLATE/feature_request.md`
   - `.github/PULL_REQUEST_TEMPLATE.md`
   - `.github/CODEOWNERS`

4. **Pre-commit Hooks**
   - `.pre-commit-config.yaml` with black, isort, ruff
   - Conventional commit linting (commitlint)
   - Auto-run tests before commit

5. **Dependency Management**
   - Dependabot configuration (`.github/dependabot.yml`)
   - Security scanning (`.github/workflows/security.yml`)

6. **Documentation Updates**
   - Update README.md with badges
   - Add CONTRIBUTING.md
   - Add CODE_OF_CONDUCT.md (Red Hat standard)
   - Add LICENSE (Apache 2.0 or MIT)

**Command Interface**:

```bash
# Bootstrap current repository on GitHub
agentready bootstrap .

# Bootstrap with specific language
agentready bootstrap . --language python

# Bootstrap and create GitHub repo
agentready bootstrap . --create-repo redhat/agentready

# Dry run (show what would be created)
agentready bootstrap . --dry-run

# Interactive mode (confirm each file)
agentready bootstrap . --interactive
```

**What Gets Created**:

```
.github/
‚îú‚îÄ‚îÄ workflows/
‚îÇ   ‚îú‚îÄ‚îÄ agentready-assessment.yml  # Run assessment on PR
‚îÇ   ‚îú‚îÄ‚îÄ tests.yml                  # Run tests and linters
‚îÇ   ‚îî‚îÄ‚îÄ dependabot.yml            # Dependency updates
‚îú‚îÄ‚îÄ ISSUE_TEMPLATE/
‚îÇ   ‚îú‚îÄ‚îÄ bug_report.md
‚îÇ   ‚îî‚îÄ‚îÄ feature_request.md
‚îú‚îÄ‚îÄ PULL_REQUEST_TEMPLATE.md
‚îú‚îÄ‚îÄ CODEOWNERS
‚îî‚îÄ‚îÄ dependabot.yml

.pre-commit-config.yaml
CONTRIBUTING.md
CODE_OF_CONDUCT.md
LICENSE
```

**GitHub Actions Workflow Example**:

```yaml
# .github/workflows/agentready-assessment.yml
name: AgentReady Assessment

on:
  pull_request:
    types: [opened, synchronize, reopened]
  push:
    branches: [main]

jobs:
  assess:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install AgentReady
        run: |
          pip install agentready

      - name: Run Assessment
        run: |
          agentready assess . --verbose

      - name: Upload Reports
        uses: actions/upload-artifact@v4
        with:
          name: agentready-reports
          path: .agentready/

      - name: Comment on PR
        if: github.event_name == 'pull_request'
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            const report = fs.readFileSync('.agentready/report-latest.md', 'utf8');
            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: report
            });
```

**Implementation Phases**:

**Phase 1: Core Bootstrap** (This Sprint)
- Implement `agentready bootstrap` command
- Create template files for GitHub Actions, pre-commit, templates
- Test on agentready repository itself
- Commit all generated files

**Phase 2: GitHub Integration** (Next Sprint)
- Use `gh` CLI to create/update repository
- Set up branch protection rules
- Configure repository settings
- Add repository badges

**Phase 3: Language-Specific Templates** (Future)
- Python-specific templates (pytest, black, mypy)
- JavaScript-specific (eslint, prettier, jest)
- Go-specific (golangci-lint, gotestsum)

**Files to Create**:

```
src/agentready/
‚îú‚îÄ‚îÄ cli/bootstrap.py           # Bootstrap CLI command
‚îú‚îÄ‚îÄ bootstrap/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ generator.py          # File generator
‚îÇ   ‚îú‚îÄ‚îÄ github_setup.py       # GitHub integration
‚îÇ   ‚îî‚îÄ‚îÄ templates/            # Template files
‚îÇ       ‚îú‚îÄ‚îÄ github/
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ workflows/
‚îÇ       ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ agentready.yml.j2
‚îÇ       ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ tests.yml.j2
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ ISSUE_TEMPLATE/
‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ PULL_REQUEST_TEMPLATE.md.j2
‚îÇ       ‚îú‚îÄ‚îÄ precommit.yaml.j2
‚îÇ       ‚îú‚îÄ‚îÄ CONTRIBUTING.md.j2
‚îÇ       ‚îî‚îÄ‚îÄ CODE_OF_CONDUCT.md.j2
tests/unit/test_bootstrap.py
```

**Acceptance Criteria**:

- [ ] `agentready bootstrap .` creates all required files
- [ ] GitHub Actions workflows are valid and functional
- [ ] Pre-commit hooks install and run successfully
- [ ] Repository badges added to README.md
- [ ] Dry-run mode shows what would be created
- [ ] Interactive mode prompts for confirmation
- [ ] Successfully bootstrap agentready repository itself
- [ ] All generated files committed to agentready repo
- [ ] GitHub Actions runs assessment on every PR

**Success Metrics**:

- AgentReady repository on GitHub has:
  - ‚úÖ GitHub Actions running assessments
  - ‚úÖ PR template with checklist
  - ‚úÖ Issue templates for bugs/features
  - ‚úÖ Pre-commit hooks configured
  - ‚úÖ Dependabot enabled
  - ‚úÖ Repository badge showing score
  - ‚úÖ All linters passing in CI

**Priority Justification**:

This is P0 because:
1. **Dogfooding** - We need this for our own repository first
2. **Demonstrates value** - Shows AgentReady in action on itself
3. **Foundation** - Required before GitHub App integration
4. **Credibility** - Can't tell others to use it if we don't use it ourselves

**Related**: GitHub App Integration (#5), Align Command (#3)

**Notes**:
- Start with Python-specific templates (our use case)
- Keep templates simple and focused
- Use Jinja2 for template rendering
- Integrate with `gh` CLI for GitHub operations
- All templates should pass AgentReady assessment!

---

### Report Header with Repository Metadata

**Priority**: P0 (Critical - Blocking Usability)

**Description**: Add prominent report header showing what repository/agent/code was scanned. Currently reports lack context about what was assessed.

**Problem**: Users cannot identify what the report is about without digging into the details. No repository name, path, timestamp, or assessment context visible at the top.

**Requirements**:
- **Prominent header section** at the top of all report formats (HTML, Markdown, JSON)
- Repository name (bold, large font)
- Repository path (absolute path on filesystem or GitHub URL)
- Assessment timestamp (human-readable: "November 21, 2025 at 2:11 AM")
- Branch name and commit hash
- AgentReady version used for assessment
- Who ran the assessment (username@hostname)
- Command used: `agentready assess /path/to/repo --verbose`

**HTML Report Header Design**:
```html
<header class="report-header">
  <div class="repo-info">
    <h1>AgentReady Assessment Report</h1>
    <div class="repo-name">Repository: agentready</div>
    <div class="repo-path">/Users/jeder/repos/sk/agentready</div>
    <div class="repo-git">Branch: 001-agentready-scorer | Commit: d49947c</div>
  </div>
  <div class="meta-info">
    <div>Assessed: November 21, 2025 at 2:11 AM</div>
    <div>AgentReady Version: 1.0.0</div>
    <div>Run by: jeder@macbook</div>
  </div>
</header>
```

**Markdown Report Header**:
```markdown
# ü§ñ AgentReady Assessment Report

**Repository**: agentready
**Path**: `/Users/jeder/repos/sk/agentready`
**Branch**: `001-agentready-scorer` | **Commit**: `d49947c`
**Assessed**: November 21, 2025 at 2:11 AM
**AgentReady Version**: 1.0.0
**Run by**: jeder@macbook

---
```

**JSON Report Metadata**:
```json
{
  "metadata": {
    "agentready_version": "1.0.0",
    "assessment_timestamp": "2025-11-21T02:11:05Z",
    "assessment_timestamp_human": "November 21, 2025 at 2:11 AM",
    "executed_by": "jeder@macbook",
    "command": "agentready assess . --verbose",
    "working_directory": "/Users/jeder/repos/sk/agentready"
  },
  "repository": { ... }
}
```

**Implementation**:
- Add metadata collection to Scanner
- Update all reporter templates (HTML, Markdown)
- Enhance Assessment model with metadata field
- Position header prominently (before score summary)

**Acceptance Criteria**:
- ‚úÖ User can immediately identify what repository was assessed
- ‚úÖ Timestamp shows when assessment was run
- ‚úÖ Git context (branch, commit) visible
- ‚úÖ AgentReady version tracked for reproducibility

**Related**: Report generation, usability, debugging

**Notes**:
- This is blocking adoption - users confused about report context
- Critical for multi-repository workflows
- Needed for CI/CD integration (track which build)

---

### Improve HTML Report Design (Font Size & Color Scheme)

**Priority**: P0 (Critical - Poor User Experience)

**Description**: Completely redesign HTML report color scheme and increase all font sizes by at least 4 points for readability.

**Problems**:
1. **Color scheme is "hideous"** (user feedback) - current purple gradient doesn't work
2. **Font sizes too small** - hard to read on modern displays
3. **Poor contrast** - some text hard to distinguish

**New Color Scheme** (Dark/Professional):
```css
:root {
  /* Base colors - mostly black, dark blue, purple, white */
  --background: #0a0e27;           /* Almost black with blue tint */
  --surface: #1a1f3a;              /* Dark blue surface */
  --surface-elevated: #252b4a;     /* Slightly lighter surface */

  /* Primary colors */
  --primary: #8b5cf6;              /* Purple (accent) */
  --primary-light: #a78bfa;        /* Light purple */
  --primary-dark: #6d28d9;         /* Dark purple */

  /* Text colors */
  --text-primary: #f8fafc;         /* Almost white */
  --text-secondary: #cbd5e1;       /* Light gray */
  --text-muted: #94a3b8;           /* Muted gray */

  /* Status colors */
  --success: #10b981;              /* Green (pass) */
  --warning: #f59e0b;              /* Amber (warning) */
  --danger: #ef4444;               /* Red (fail) */
  --neutral: #64748b;              /* Gray (skipped) */

  /* UI elements */
  --border: #334155;               /* Dark border */
  --shadow: rgba(0, 0, 0, 0.5);   /* Deep shadows */
}
```

**Font Size Increases** (+4pt minimum):
```css
/* Current ‚Üí New */
body { font-size: 14px ‚Üí 18px; }
h1 { font-size: 28px ‚Üí 36px; }
h2 { font-size: 24px ‚Üí 30px; }
h3 { font-size: 20px ‚Üí 26px; }
.score { font-size: 48px ‚Üí 56px; }
.attribute-name { font-size: 16px ‚Üí 22px; }
.evidence { font-size: 13px ‚Üí 17px; }
code { font-size: 13px ‚Üí 16px; }
```

**Design Mockup**:
```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ [Dark navy blue background #1a1f3a]                ‚îÇ
‚îÇ                                                     ‚îÇ
‚îÇ  ü§ñ AgentReady Assessment Report                   ‚îÇ
‚îÇ  Repository: agentready                            ‚îÇ
‚îÇ  /Users/jeder/repos/sk/agentready                  ‚îÇ
‚îÇ  [White text #f8fafc, 18px base font]             ‚îÇ
‚îÇ                                                     ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ [Purple accent card #8b5cf6]                       ‚îÇ
‚îÇ                                                     ‚îÇ
‚îÇ           75.4 / 100                               ‚îÇ
‚îÇ           [56px, bold, white]                      ‚îÇ
‚îÇ           ü•á Gold                                   ‚îÇ
‚îÇ                                                     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**Implementation Checklist**:
- [ ] Replace gradient backgrounds with dark blue/black
- [ ] Update all font sizes (+4pt minimum)
- [ ] Use purple (#8b5cf6) sparingly as accent color
- [ ] Ensure white text on dark backgrounds (WCAG AA)
- [ ] Update certification level colors
- [ ] Redesign score cards with new scheme
- [ ] Test with colorblind simulators
- [ ] Add light mode as alternative (with same professional palette)

**Before/After Color Comparison**:
```
Current (Problems):
- Purple gradient everywhere: #667eea ‚Üí #764ba2 ‚ùå
- Small text: 14px base ‚ùå
- Busy, overwhelming ‚ùå

New (Professional):
- Dark blue/black base: #0a0e27, #1a1f3a ‚úÖ
- Purple accents only: #8b5cf6 ‚úÖ
- Large text: 18px base ‚úÖ
- Clean, readable ‚úÖ
```

**Acceptance Criteria**:
- ‚úÖ All text easily readable (18px minimum body text)
- ‚úÖ Color scheme uses black, dark blue, purple, white palette
- ‚úÖ High contrast (WCAG 2.1 AA compliant)
- ‚úÖ Professional appearance suitable for Red Hat engineering
- ‚úÖ Purple used as accent, not dominant color

**Related**: HTML report generation, UX, accessibility

**Notes**:
- Current design blocks user adoption (visual issues)
- This is the first thing users see - must be excellent
- Consider adding screenshot to docs after redesign
- Font size critical for presentations and stakeholder reviews

---

## Schema & Configuration

### Report Schema Versioning

**Priority**: P3 (Important)

**Description**: Define and version the JSON/HTML/Markdown report schemas to ensure backwards compatibility and enable schema evolution.

**Requirements**:
- JSON schema for assessment reports (contracts/assessment-schema.json exists)
- HTML schema for interactive reports (contracts/report-html-schema.md exists)
- Markdown schema for version control reports (contracts/report-markdown-schema.md exists)
- Schema versioning strategy (semantic versioning)
- Backwards compatibility testing
- Schema migration tools for major version changes

**Use Case**:
```bash
# Validate report against schema
agentready validate-report assessment-2025-11-20.json

# Migrate old report to new schema version
agentready migrate-report --from 1.0.0 --to 2.0.0 old-report.json
```

**Related**: Report generation, data model evolution

**Notes**:
- Schemas exist in contracts/ directory but need formal versioning
- Consider using JSON Schema Draft 2020-12
- Tool should validate generated reports against bundled schema
- Breaking schema changes should trigger major version bump

---

### Research Report Generator/Updater Utility

**Priority**: P4 (Enhancement)

**Description**: Create a utility tool to help maintain and update the research report (agent-ready-codebase-attributes.md) following the validation schema defined in contracts/research-report-schema.md.

**Requirements**:
- Generate new research reports from templates
- Validate existing reports against schema (contracts/research-report-schema.md)
- Update/add attributes while maintaining schema compliance
- Automatically format citations and references
- Extract tier assignments and metadata
- Verify 25 attributes, 4 tiers, 20+ references
- Check for required sections (Definition, Measurable Criteria, Impact on Agent Behavior)

**Use Case**:
```bash
# Validate existing research report
agentready research validate agent-ready-codebase-attributes.md

# Generate new research report from template
agentready research init --output new-research.md

# Add new attribute to research report
agentready research add-attribute \
  --id "attribute_26" \
  --name "New Attribute" \
  --tier 2 \
  --file research.md

# Update metadata (version, date)
agentready research bump-version --type minor

# Lint and format research report
agentready research format research.md
```

**Features**:
- Schema validation (errors vs warnings per research-report-schema.md)
- Automated metadata header generation (version, date in YAML frontmatter)
- Attribute numbering consistency checks (1.1, 1.2, ..., 15.1)
- Citation deduplication and formatting
- Tier distribution balance warnings
- Category coverage analysis
- Markdown formatting enforcement (consistent structure)
- Reference URL reachability checks

**Related**: Research report maintenance, schema compliance, documentation quality

**Notes**:
- Must follow contracts/research-report-schema.md validation rules
- Should prevent invalid reports from being committed
- Could integrate with pre-commit hooks for research report changes
- Consider CLI commands under `agentready research` subcommand
- Tool should be self-documenting (help users fix validation errors)
- Future: Could use LLMs to help generate attribute descriptions from academic papers

---

### Repomix Integration

**Priority**: P4 (Enhancement)

**Description**: Integrate with Repomix (https://github.com/yamadashy/repomix) to generate AI-optimized repository context files for both new and existing repositories.

**Requirements**:
- Generate Repomix output for existing repositories
- Include Repomix configuration in bootstrapped new repositories
- Optional GitHub Actions integration for automatic regeneration
- Support Repomix configuration customization
- Integrate with agentready assessment workflow

**Use Case**:
```bash
# Generate Repomix output for current repository
agentready repomix-generate

# Bootstrap new repo with Repomix integration
agentready init --repo my-project --language python --repomix

# This would:
# 1. Set up Repomix configuration
# 2. Add GitHub Action for automatic regeneration
# 3. Generate initial repository context file
# 4. Include Repomix output in .gitignore appropriately
```

**Features**:
- Automatic Repomix configuration based on repository language
- GitHub Actions workflow for CI/CD integration
- Custom ignore patterns aligned with agentready assessment
- Support for both markdown and XML output formats
- Integration with agentready bootstrap command

**Related**: Repository initialization, AI-assisted development workflows

**Notes**:
- Repomix generates optimized repository context for LLMs
- Could enhance CLAUDE.md with reference to Repomix output
- Should align with existing .gitignore patterns
- Consider adding Repomix freshness check to assessment attributes
- May improve agentready's own repository understanding

---

### AgentReady Repository Agent

**Priority**: P3 (Important)

**Description**: Create a specialized Claude Code agent for the AgentReady repository to assist with development, testing, and maintenance tasks.

**Requirements**:
- Agent with deep knowledge of AgentReady architecture
- Understands assessment workflow, scoring logic, and report generation
- Can help with:
  - Implementing new assessors
  - Enhancing existing assessors
  - Writing tests for new features
  - Debugging assessment issues
  - Improving report templates
  - Optimizing performance

**Use Case**:
```bash
# In Claude Code, use the agentready-dev agent
/agentready-dev implement new assessor for dependency security scanning
/agentready-dev debug why Python type annotation detection is failing
/agentready-dev optimize assessment performance for large repositories
```

**Features**:
- Pre-loaded context about AgentReady codebase structure
- Knowledge of assessment attributes and scoring algorithm
- Understanding of tier-based weighting system
- Familiar with reporter implementations (HTML, Markdown)
- Can generate new assessors following established patterns

**Implementation**:
- Create `.claude/agents/agentready-dev.md` with agent specification
- Include links to key design documents (data-model.md, plan.md, research.md)
- Provide common development patterns and examples
- Reference test structure and coverage requirements

**Related**: Development workflow, code generation, testing

**Notes**:
- Agent should follow constitution principles (library-first, TDD when requested)
- Should know about stub assessors and how to expand them
- Can help with performance benchmarking and optimization
- Should understand the research report structure and attribute definitions

---

### Customizable HTML Report Themes

**Priority**: P4 (Enhancement)

**Description**: Allow users to customize the appearance of HTML reports with themes, color schemes, and layout options.

**Requirements**:
- Theme system for HTML reports
- Pre-built themes (default, dark mode, high contrast, colorblind-friendly)
- Custom theme support via configuration
- Maintain accessibility standards (WCAG 2.1 AA)
- Preview themes before applying

**Use Case**:
```yaml
# .agentready-config.yaml
report_theme: dark  # or 'light', 'high-contrast', 'custom'

custom_theme:
  primary_color: "#2563eb"
  success_color: "#10b981"
  warning_color: "#f59e0b"
  danger_color: "#ef4444"
  background: "#1e293b"
  text: "#e2e8f0"
  font_family: "Inter, sans-serif"
```

**Features**:
- **Theme dropdown in top-right corner of HTML report** (runtime switching)
- **Quick dark/light mode toggle button** (one-click switching between dark and light)
- Multiple built-in themes (light, dark, high-contrast, solarized, dracula)
- Dark mode support with proper color inversion
- Custom color palettes
- Font selection (system fonts + web-safe fonts)
- Layout density options (compact, comfortable, spacious)
- Logo/branding customization
- Export theme as reusable configuration
- Save theme preference to localStorage (persists across reports)

**Implementation**:
- CSS custom properties (variables) for theming
- JavaScript theme switcher in HTML report (no page reload)
- Theme loader in HTMLReporter
- Validate theme configurations
- Preserve accessibility in all themes (WCAG 2.1 AA)
- Add theme preview command: `agentready theme-preview dark`
- Embed all theme CSS in single HTML file (offline-capable)

**Related**: HTML report generation, user experience

**Notes**:
- All themes must maintain WCAG 2.1 AA contrast ratios
- Dark mode should invert appropriately, not just be dark
- Consider colorblind-friendly palettes (Viridis, ColorBrewer)
- Custom themes should be shareable (export/import)
- Could add theme gallery in documentation

---

### Fix Code Quality Issues from Code Review

**Priority**: P1 (High - Quality & Reliability)

**Description**: Address P1 issues discovered in code review that affect reliability, accuracy, and code quality.

**Issues to Fix**:

1. **TOCTOU (Time-of-Check-Time-of-Use) in File Operations**
   - **Location**: Multiple assessors (`documentation.py:46-50`, `documentation.py:174-191`)
   - **Problem**: Check if file exists, then read in separate operation - file could be deleted in between
   - **Impact**: Crashes instead of graceful degradation
   - **Fix**: Use try-except around file reads instead of existence checks
   ```python
   # BEFORE:
   if claude_md_path.exists():
       size = claude_md_path.stat().st_size

   # AFTER:
   try:
       with open(claude_md_path, "r") as f:
           size = len(f.read())
   except FileNotFoundError:
       return Finding(...status="fail"...)
   except OSError as e:
       return Finding.error(self.attribute, f"Could not read: {e}")
   ```

2. **Inaccurate Type Annotation Detection**
   - **Location**: `src/agentready/assessors/code_quality.py:98-102`
   - **Problem**: Regex-based detection has false positives (string literals, dict literals)
   - **Impact**: Inflated type annotation coverage scores
   - **Fix**: Use AST parsing instead of regex:
   ```python
   import ast
   tree = ast.parse(content)
   for node in ast.walk(tree):
       if isinstance(node, ast.FunctionDef):
           total_functions += 1
           has_annotations = (node.returns is not None or
                            any(arg.annotation for arg in node.args.args))
           if has_annotations:
               typed_functions += 1
   ```

3. **Assessment Validation Semantic Confusion**
   - **Location**: `src/agentready/models/assessment.py:54-59`
   - **Problem**: Field named `attributes_skipped` but includes `error` and `not_applicable` statuses
   - **Impact**: Confusing API, unclear semantics
   - **Fix**: Rename to `attributes_not_assessed` OR add separate counters

**Acceptance Criteria**:
- [ ] All file operations use try-except pattern
- [ ] Type annotation detection uses AST parsing
- [ ] Assessment model fields clearly named
- [ ] Tests added for TOCTOU edge cases
- [ ] Tests added for type annotation false positives
- [ ] Documentation updated

**Priority Justification**: These affect reliability and measurement accuracy - critical for a quality assessment tool.

**Related**: Testing improvements, code quality

---

### Improve Test Coverage and Edge Case Handling

**Priority**: P1 (High - Quality Assurance)

**Description**: Increase test coverage from 37% to >80% and add tests for critical edge cases discovered in code review.

**Critical Test Gaps**:

1. **Error Handling Paths** (Currently 0% coverage)
   - OSError, PermissionError in file operations
   - MissingToolError in assessors
   - Invalid repository paths
   - Malformed git repositories

2. **Edge Cases** (No tests)
   - Empty repositories
   - Binary files instead of text
   - Symlinks in repository
   - Very large repositories (>10k files)
   - Repositories with `test/` vs `tests/` directories

3. **Security Test Cases**
   - XSS in repository names, commit messages
   - Path traversal attempts
   - Malicious file names

4. **Scorer Edge Cases**
   - All attributes skipped (score should be 0.0)
   - Config weights don't sum to 1.0
   - Division by zero scenarios

**Implementation**:
```python
# tests/unit/test_edge_cases.py
def test_empty_repository(tmp_path):
    """Test assessment of completely empty repository."""
    # Create empty git repo
    repo = Repository(path=tmp_path, ...)
    scanner = Scanner(repo, config=None)
    assessment = scanner.scan(assessors)
    # Should not crash, should have valid score
    assert 0.0 <= assessment.overall_score <= 100.0

def test_permission_denied_file(tmp_path):
    """Test graceful handling of permission errors."""
    # Create unreadable file
    restricted = tmp_path / "CLAUDE.md"
    restricted.write_text("test")
    restricted.chmod(0o000)

    assessor = CLAUDEmdAssessor()
    finding = assessor.assess(Repository(...))
    assert finding.status == "error"
    assert "permission" in finding.error_message.lower()

def test_binary_file_as_readme(tmp_path):
    """Test handling of binary files."""
    readme = tmp_path / "README.md"
    readme.write_bytes(b"\x00\x01\x02\x03")

    assessor = READMEAssessor()
    finding = assessor.assess(Repository(...))
    # Should not crash
```

**Acceptance Criteria**:
- [ ] Test coverage increased to >80%
- [ ] All error handling paths tested
- [ ] Edge cases for empty/malformed repos covered
- [ ] Security test cases added
- [ ] Integration tests for complete workflows
- [ ] CI runs coverage report and fails <75%

**Priority Justification**: Quality assessment tool must be thoroughly tested. Current 37% coverage is unacceptable.

**Related**: CI/CD improvements, reliability

---

### Add Security & Quality Improvements from Code Review

**Priority**: P2 (Medium - Polish)

**Description**: Address P2 improvements from code review for better UX and robustness.

**Improvements**:

1. **Input Validation Warnings**
   - Warn when scanning sensitive directories (`/etc`, `/.ssh`, `/var`)
   - Confirm before scanning large repositories (>10k files)

2. **Scorer Semantic Clarity**
   - Document behavior when all attributes skipped (returns 0.0)
   - Consider returning `None` or special value for "not assessable"
   - Add explicit documentation of edge cases

3. **Content Security Policy Headers**
   - Add CSP to HTML reports for defense-in-depth
   - Prevent inline script execution
   - Whitelist only necessary sources

**Implementation**:
```python
# In CLI
sensitive_dirs = ['/etc', '/sys', '/proc', '/.ssh', '/var']
if any(str(repo_path).startswith(p) for p in sensitive_dirs):
    click.confirm(
        f"Warning: Scanning sensitive directory {repo_path}. Continue?",
        abort=True
    )

# In HTMLReporter
csp_header = (
    "<meta http-equiv='Content-Security-Policy' "
    "content=\"default-src 'self'; script-src 'unsafe-inline'; "
    "style-src 'unsafe-inline'\">"
)
```

**Acceptance Criteria**:
- [ ] Warnings for sensitive directories
- [ ] CSP headers in HTML reports
- [ ] Scorer edge cases documented
- [ ] User guide updated with best practices

**Priority Justification**: UX polish and defense-in-depth, not critical bugs.

**Related**: User experience, security hardening

---

### Align Subcommand (Automated Remediation)

**Priority**: P1 (Critical)

**Description**: Implement `agentready align` subcommand that automatically fixes failing attributes by generating and applying changes to the repository.

**Vision**: One command to align your repository with best practices - automatically create missing files, configure tooling, and improve code quality.

**Core Command**:

```bash
# Align repository to best practices
agentready align .

# Preview changes without applying
agentready align . --dry-run

# Apply specific attributes only
agentready align . --attributes claude_md_file,precommit_hooks

# Create GitHub PR instead of direct changes
agentready align . --create-pr

# Interactive mode (confirm each change)
agentready align . --interactive
```

**Supported Fixes**:

1. **Template-Based Fixes** (Auto-applicable):
   - `claude_md_file`: Generate CLAUDE.md from repository analysis
   - `gitignore_completeness`: Add missing patterns to .gitignore
   - `precommit_hooks`: Create .pre-commit-config.yaml with language-specific hooks
   - `readme_structure`: Scaffold missing README sections
   - `lock_files`: Generate lock files (package-lock.json, requirements.txt, etc.)
   - `issue_pr_templates`: Create .github/ISSUE_TEMPLATE and PULL_REQUEST_TEMPLATE
   - `conventional_commits`: Add commitlint configuration

2. **Command-Based Fixes** (Execute commands):
   - `lock_files`: Run `npm install`, `poetry lock`, `go mod tidy`
   - `precommit_hooks`: Run `pre-commit install`
   - `dependency_freshness`: Run `npm update`, `pip install --upgrade`

3. **AI-Powered Fixes** (Require LLM, optional):
   - `type_annotations`: Add type hints to Python functions
   - `inline_documentation`: Generate docstrings from function signatures
   - `cyclomatic_complexity`: Refactor high-complexity functions
   - `file_size_limits`: Split large files into smaller modules

**Workflow**:

```bash
# User runs alignment
$ agentready align . --dry-run

AgentReady Alignment Preview
============================

Repository: /Users/jeder/my-project
Current Score: 62.4/100 (Silver)
Projected Score: 84.7/100 (Gold) üéØ

Changes to be applied:

  ‚úÖ claude_md_file (10 points)
     CREATE CLAUDE.md (1.2 KB)

  ‚úÖ precommit_hooks (3 points)
     CREATE .pre-commit-config.yaml (845 bytes)
     RUN pre-commit install

  ‚úÖ gitignore_completeness (3 points)
     MODIFY .gitignore (+15 patterns)

  ‚ö†Ô∏è  type_annotations (10 points) - requires AI
     MODIFY 23 Python files (add type hints)
     Use --ai to enable AI-powered fixes

Total: 3 automatic fixes, 1 AI fix available
Apply changes? [y/N]
```

**Implementation**:

```python
# src/agentready/fixers/base.py
class BaseFixer(ABC):
    """Base class for attribute fixers."""

    @abstractmethod
    def can_fix(self, finding: Finding) -> bool:
        """Check if this fixer can fix the finding."""
        pass

    @abstractmethod
    def generate_fix(self, repository: Repository, finding: Finding) -> Fix:
        """Generate fix for the finding."""
        pass

# src/agentready/fixers/template_fixer.py
class TemplateFixer(BaseFixer):
    """Fixer that generates files from templates."""

    def generate_fix(self, repository: Repository, finding: Finding) -> Fix:
        template = self.load_template(finding.attribute.id)
        content = self.render_template(template, repository)
        return FileCreationFix(path="CLAUDE.md", content=content)

# src/agentready/cli/align.py
@cli.command()
@click.argument("repository", type=click.Path(exists=True), default=".")
@click.option("--dry-run", is_flag=True, help="Preview changes without applying")
@click.option("--create-pr", is_flag=True, help="Create GitHub PR instead of direct changes")
@click.option("--interactive", is_flag=True, help="Confirm each change")
@click.option("--attributes", help="Comma-separated attribute IDs to fix")
@click.option("--ai", is_flag=True, help="Enable AI-powered fixes (requires API key)")
def align(repository, dry_run, create_pr, interactive, attributes, ai):
    """Align repository with best practices by applying automatic fixes."""

    # Run assessment first
    assessment = run_assessment(repository)

    # Identify fixable failures
    failures = [f for f in assessment.findings if f.status == "fail"]
    fixable = identify_fixable_failures(failures, enable_ai=ai)

    # Generate fixes
    fixes = [fixer.generate_fix(repo, finding) for finding in fixable]

    # Preview changes
    show_fix_preview(fixes, assessment.overall_score, projected_score)

    if dry_run:
        return

    if interactive and not confirm_each_fix(fixes):
        return

    # Apply fixes
    if create_pr:
        create_github_pr_with_fixes(fixes)
    else:
        apply_fixes(fixes)

    # Re-run assessment to show improvement
    new_assessment = run_assessment(repository)
    show_improvement(assessment.overall_score, new_assessment.overall_score)
```

**Fix Types**:

```python
class Fix(ABC):
    """Base class for fixes."""
    attribute_id: str
    description: str

class FileCreationFix(Fix):
    """Create a new file."""
    path: Path
    content: str

class FileModificationFix(Fix):
    """Modify existing file."""
    path: Path
    changes: List[TextChange]

class CommandFix(Fix):
    """Execute command."""
    command: str
    working_dir: Path

class MultiStepFix(Fix):
    """Combination of multiple fixes."""
    steps: List[Fix]
```

**GitHub PR Integration**:

```bash
# Create PR with fixes
$ agentready align . --create-pr

Creating fix branch: agentready-align-20251121
Applying 3 fixes...
  ‚úÖ Created CLAUDE.md
  ‚úÖ Created .pre-commit-config.yaml
  ‚úÖ Modified .gitignore

Committing changes...
Pushing to origin...

Created PR: https://github.com/redhat/my-project/pull/42
  Title: "Improve AgentReady score from 62.4 to 84.7 (Silver ‚Üí Gold)"
  Score improvement: +22.3 points
  Attributes fixed: 3
```

**Configuration**:

```yaml
# .agentready-config.yaml
align:
  enabled: true

  auto_fix:
    # Attributes to automatically fix without confirmation
    - claude_md_file
    - gitignore_completeness
    - precommit_hooks

  confirm_before_fix:
    # Attributes requiring confirmation
    - type_annotations
    - cyclomatic_complexity

  never_fix:
    # Attributes to skip (user will fix manually)
    - container_setup
    - openapi_specs

  ai_fixes:
    enabled: false  # Require --ai flag
    provider: "anthropic"  # or "openai"
    model: "claude-3-5-sonnet-20241022"
    max_tokens: 4096
```

**Use Cases**:

**Use Case 1: New Repository Setup**
```bash
# Clone new project
git clone github.com/redhat/new-project
cd new-project

# Align to best practices
agentready align . --interactive

# Review and commit changes
git add .
git commit -m "chore: Align repository with AgentReady best practices"
```

**Use Case 2: Continuous Improvement**
```bash
# Weekly CI job to check and create alignment PRs
agentready align . --create-pr --dry-run
# If score < threshold, create PR automatically
```

**Use Case 3: Pre-commit Hook**
```bash
# .pre-commit-config.yaml
- repo: local
  hooks:
    - id: agentready-align
      name: AgentReady Alignment Check
      entry: agentready align --dry-run
      language: system
      pass_filenames: false
```

**Safety Features**:

- **Dry-run by default** for destructive operations
- **Git worktree** for isolated changes (optional)
- **Backup creation** before modifying files
- **Rollback support** if fixes fail
- **Validation** of generated files before writing
- **Interactive confirmation** for AI-powered fixes

**Related**: Automated remediation, repository improvement, onboarding

**Notes**:
- Start with template-based fixes (highest ROI, lowest risk)
- AI-powered fixes require API key and user consent
- Some attributes cannot be automatically fixed (requires human judgment)
- Consider integration with `git stash` for safety
- Could generate shell script of changes for manual review
- Align with Red Hat's AI-assisted development workflow

---

### Interactive Dashboard with Automated Remediation

**Priority**: P2 (High Value)

**Description**: Transform the static HTML report into an interactive dashboard that enables one-click remediation via automated GitHub issue creation and draft PR generation.

**Vision**: "How to Align Your Repo with Best Practices" - Click a button ‚Üí Open GitHub issue with draft PR containing fixes.

**Core Features**:

1. **Interactive Dashboard Mode**
   - Real-time assessment (WebSocket updates)
   - Live filtering and sorting (already have this)
   - Action buttons on each failing attribute
   - Progress tracking across multiple runs
   - Historical trend visualization

2. **One-Click Remediation Actions**
   - "Fix This" button on each failing attribute
   - Generates GitHub issue automatically
   - Creates draft PR with proposed changes
   - Links issue to PR
   - Assigns to repository owner/team

3. **Automated Fix Generation**
   - Template-based fixes for common issues:
     - Missing .gitignore ‚Üí Generate language-specific .gitignore
     - No CLAUDE.md ‚Üí Generate template with repo analysis
     - Missing pre-commit hooks ‚Üí Add .pre-commit-config.yaml
     - No lock files ‚Üí Generate appropriate lock file
     - Missing README sections ‚Üí Scaffold missing sections
   - AI-powered fixes for complex issues:
     - Refactor high-complexity functions
     - Add type annotations to functions
     - Generate docstrings from function signatures
     - Split large files

4. **GitHub Integration**
   - OAuth authentication with GitHub
   - gh CLI integration for seamless workflow
   - Create issues via GitHub API
   - Create draft PRs via GitHub API
   - Auto-label issues (e.g., `agentready`, `automated-fix`, `tier-1-essential`)
   - Link to AgentReady assessment report

**Technical Architecture**:

```yaml
# New components needed:

Backend (Optional - could be fully client-side with gh CLI):
  - GitHub OAuth app for authentication
  - Issue/PR template generator
  - Fix generator service (template-based + AI-powered)
  - Assessment history tracker

Frontend (Enhanced HTML report):
  - Action buttons with loading states
  - GitHub auth flow UI
  - Progress indicators
  - Toast notifications for actions
  - Modal dialogs for fix preview

CLI Extensions:
  - agentready dashboard .  # Launch local web server
  - agentready fix <attribute-id>  # Generate fix for specific attribute
  - agentready create-issue <attribute-id>  # Create GitHub issue
  - agentready create-pr <attribute-id>  # Create draft PR
```

**Use Cases**:

**Use Case 1: Quick Fixes**
```bash
# User runs assessment
agentready assess . --dashboard

# Opens dashboard in browser at http://localhost:8000
# User clicks "Fix This" on "Missing CLAUDE.md"
# ‚Üí Creates issue: "Add CLAUDE.md configuration file"
# ‚Üí Creates draft PR with generated CLAUDE.md template
# ‚Üí PR includes: project analysis, detected languages, suggested structure
```

**Use Case 2: Batch Remediation**
```bash
# Dashboard shows all failures
# User selects multiple attributes
# Clicks "Fix All Selected"
# ‚Üí Creates single issue: "Improve AgentReady Score from Silver to Gold"
# ‚Üí Creates draft PR with all fixes
# ‚Üí PR description includes before/after score projection
```

**Use Case 3: CI/CD Integration**
```bash
# GitHub Action runs assessment
# Posts comment on PR with assessment results
# Includes links to create remediation issues
# Can auto-create draft PR for improvements
```

**Implementation Approach**:

**Phase 1: Client-Side with gh CLI** (Simplest, no backend needed)
- Use JavaScript in HTML report to call gh CLI via local proxy
- Generate fix files locally
- Use `gh issue create` and `gh pr create`
- Works for users with gh CLI installed

**Phase 2: Dashboard Server** (Enhanced UX)
- Flask/FastAPI server serving dashboard
- WebSocket for live updates
- GitHub OAuth for authentication
- Background workers for fix generation

**Phase 3: Cloud Service** (SaaS offering)
- Hosted dashboard at agentready.dev
- GitHub App installation
- Webhook integration for continuous monitoring
- Team collaboration features

**Fix Templates by Attribute**:

```yaml
claude_md_file:
  type: template
  generates:
    - file: CLAUDE.md
      content: |
        # {repository_name}

        ## Overview
        {auto_generated_description}

        ## Architecture
        {detected_patterns}

        ## Development
        {build_commands}

lock_files:
  type: command
  commands:
    - condition: has_package_json
      run: npm install
    - condition: has_pyproject_toml
      run: poetry lock || pip freeze > requirements.txt

precommit_hooks:
  type: template
  generates:
    - file: .pre-commit-config.yaml
      content: {language_specific_hooks}

readme_structure:
  type: enhancement
  modifies: README.md
  adds_sections:
    - Installation
    - Usage
    - Development
    - Contributing

type_annotations:
  type: ai_powered
  uses: ast_analysis + llm
  modifies: "*.py"
  adds: type_hints
```

**Dashboard vs Report Decision**:

**Keep Both**:
- Static reports for CI/CD, documentation, archiving
- Dashboard for interactive development workflow
- Reports can link to dashboard for remediation
- Dashboard can export static reports

**Benefits of Dashboard**:
- ‚úÖ Interactive remediation workflow
- ‚úÖ Live assessment updates
- ‚úÖ Progress tracking over time
- ‚úÖ Team collaboration (comments, assignments)
- ‚úÖ Automated fix preview before applying
- ‚úÖ Integration with existing tools (GitHub, IDEs)

**Challenges**:
- Authentication complexity (GitHub OAuth)
- Fix generation quality (need good templates + AI)
- PR review overhead (lots of automated PRs)
- Maintaining fix templates as best practices evolve

**Recommended Approach**:

1. **Start with enhanced static report**:
   - Add "Create Issue" buttons that generate gh CLI commands
   - Users copy/paste commands to create issues
   - Include fix templates in issue descriptions

2. **Add local dashboard** (Phase 2):
   - Flask server with WebSocket updates
   - GitHub integration via gh CLI
   - Generate fixes, preview diffs, create PRs

3. **Consider hosted service** (Phase 3+):
   - If adoption is high
   - SaaS model for teams
   - Continuous monitoring and recommendations

**Related**: GitHub integration, automation, remediation, UX

**Notes**:
- This is a MAJOR feature that could become a standalone product
- Consider MVP: "Copy this gh command" buttons in HTML report
- AI-powered fix generation requires careful validation
- Some fixes (like refactoring) need human review
- Could integrate with existing tools (Dependabot, Renovate)
- May want to partner with GitHub for official integration

---

### GitHub App Integration (Badge & Status Checks)

**Priority**: P2 (High Value)

**Description**: Create a GitHub App that provides badge integration, PR status checks, and automated assessment comments to help Red Hat engineering teams track and improve repository quality.

**Core Features**:

1. **Repository Badge**
   - Shields.io-compatible SVG badge showing certification level
   - Endpoint: `https://agentready.redhat.com/badge/{owner}/{repo}.svg`
   - Dynamic color based on certification (Platinum=purple, Gold=yellow, Silver=silver, Bronze=brown)
   - Include score: "AgentReady: 85.2 (Gold)"
   - Click badge to view latest assessment report

2. **GitHub Actions Integration**
   - Create official `agentready/assess-action` GitHub Action
   - Run assessment on PR events (opened, synchronized, reopened)
   - Run assessment on push to main/master
   - Support custom triggers via workflow_dispatch

3. **PR Status Checks**
   - Use GitHub Commit Status API to report assessment results
   - Set check status: success (>90), warning (75-89), failure (<75)
   - Configurable thresholds via `.agentready-config.yaml`
   - Block PR merge if score below threshold (optional)
   - Link to detailed HTML report in check details

4. **PR Comments**
   - Automated bot comments on PRs with assessment summary
   - Show score delta: "Score changed: 72.4 ‚Üí 78.3 (+5.9)"
   - List new failures and fixes
   - Collapsible sections for full findings
   - Trend chart showing last 10 assessments (ASCII or embedded image)
   - Include remediation suggestions for new failures

**Technical Implementation**:

**Phase 1: GitHub Actions Integration**
```yaml
# .github/workflows/agentready.yml
name: AgentReady Assessment
on:
  pull_request:
    types: [opened, synchronize, reopened]
  push:
    branches: [main, master]

jobs:
  assess:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - uses: agentready/assess-action@v1
        with:
          threshold: 75
          post-comment: true
          update-status: true
```

**Phase 2: Badge Service**
```python
# FastAPI endpoint for badge generation
@app.get("/badge/{owner}/{repo}.svg")
async def get_badge(owner: str, repo: str):
    # Fetch latest assessment from GitHub Actions artifacts
    # Or run quick assessment on-demand
    score, level = get_latest_assessment(owner, repo)
    color = LEVEL_COLORS[level]
    return SVGResponse(generate_badge(score, level, color))
```

**Phase 3: GitHub App**
- App permissions: Contents (read), Checks (write), Pull requests (write)
- Webhook events: push, pull_request
- Installed via GitHub Marketplace (Red Hat internal)
- Dashboard at agentready.redhat.com showing:
  - Repository list with scores
  - Historical trends
  - Organization-wide statistics
  - Top repositories by improvement

**Integration Points**:

1. **GitHub Actions Artifacts**
   - Store assessment reports as workflow artifacts
   - Keep last 30 days of reports for trend analysis
   - Generate downloadable HTML/JSON/Markdown reports

2. **GitHub Status API**
   ```python
   POST /repos/{owner}/{repo}/statuses/{commit_sha}
   {
     "state": "success",  # or "pending", "failure", "error"
     "target_url": "https://agentready.redhat.com/reports/{run_id}",
     "description": "AgentReady: 85.2 (Gold)",
     "context": "agentready/assessment"
   }
   ```

3. **GitHub Checks API** (preferred over Status API)
   ```python
   POST /repos/{owner}/{repo}/check-runs
   {
     "name": "AgentReady Assessment",
     "status": "completed",
     "conclusion": "success",
     "output": {
       "title": "Score: 85.2/100 (Gold)",
       "summary": "Passed 20/25 attributes",
       "text": "Detailed findings..."
     }
   }
   ```

**Use Cases**:

**Use Case 1: Add Badge to README**
```markdown
# My Project

[![AgentReady](https://agentready.redhat.com/badge/redhat/my-project.svg)](https://agentready.redhat.com/reports/redhat/my-project)
```

**Use Case 2: Enforce Quality Gates**
```yaml
# .agentready-config.yaml
github:
  status_checks:
    enabled: true
    min_score: 75  # Block merge if score < 75
    require_improvement: true  # Block if score decreased
```

**Use Case 3: Track Organization Progress**
- Dashboard shows all repos in Red Hat org
- Filter by team, language, certification level
- Identify repos needing attention
- Celebrate improvements (score increases)

**Configuration**:

```yaml
# .agentready-config.yaml
github:
  badge:
    enabled: true
    style: flat-square  # or flat, plastic, for-the-badge
    label: "AgentReady"

  actions:
    enabled: true
    trigger_on: [pull_request, push]
    post_comment: true
    update_status: true
    upload_artifacts: true

  status_checks:
    enabled: true
    min_score: 75
    require_improvement: false

  comments:
    enabled: true
    show_delta: true
    show_trend: true
    collapse_details: true
```

**Implementation Checklist**:

- [ ] Create `agentready/assess-action` GitHub Action
- [ ] Implement badge generation service
- [ ] Add GitHub Status API integration
- [ ] Add GitHub Checks API integration
- [ ] Implement PR comment generation
- [ ] Add score delta calculation
- [ ] Create assessment artifact storage
- [ ] Build organization dashboard
- [ ] Add Red Hat SSO authentication
- [ ] Deploy to Red Hat infrastructure
- [ ] Create documentation for Red Hat teams
- [ ] Add to Red Hat developer onboarding

**Related**: CI/CD integration, automation, visibility, quality gates

**Notes**:
- Focus on internal Red Hat adoption first
- Badge service could be hosted on Red Hat infrastructure
- Dashboard should integrate with Red Hat IdM for authentication
- Consider integration with Red Hat's existing code quality tools
- GitHub App should be installable via Red Hat GitHub Enterprise
- All data stays within Red Hat infrastructure (no external services)
- Align with Red Hat's OpenShift AI strategy for agentic development
- Could become part of Red Hat's AI-assisted development workflow

---

### Documentation Source Truth and Cascade System

**Priority**: P2 (Medium - Developer Experience)

**Description**: Implement a system to ensure documentation stays synchronized when source content changes, with automatic cascade updates from authoritative sources to derived documentation.

**Problem**: When substantial changes are made to source documentation (like research reports, specifications, or CLAUDE.md), there's currently no mechanism to ensure those changes propagate to derived documentation (GitHub Pages, API docs, tutorials, etc.). This leads to documentation drift and inconsistency.

**Requirements**:

1. **Source-of-Truth Designation**
   - Designate authoritative source files (contracts/*, specs/*, CLAUDE.md, research reports)
   - Mark derived documentation files that depend on sources
   - Document source ‚Üí derived relationships explicitly

2. **Change Detection**
   - Track content changes in source files via git hooks or CI
   - Identify which derived docs are affected by source changes
   - Calculate "documentation drift score" (how out of sync derived docs are)

3. **Automatic Cascade Updates**
   - Trigger documentation regeneration when sources change
   - Update affected GitHub Pages content
   - Regenerate API documentation if models change
   - Update code examples if interfaces change
   - Refresh tutorial content if workflows change

4. **Validation & Consistency**
   - Validate that derived docs accurately reflect sources
   - Check for broken cross-references after updates
   - Ensure code examples still compile/run
   - Verify attribute definitions match research report

**Implementation Approach**:

```yaml
# .agentready/doc-sources.yaml
documentation_sources:
  # Source of truth files
  sources:
    - path: contracts/assessment-schema.json
      type: json_schema
      affects:
        - docs/api-reference.md
        - README.md (Data Models section)

    - path: agent-ready-codebase-attributes.md
      type: research_report
      affects:
        - docs/attributes.md
        - docs/index.md (Features section)
        - README.md (Overview)

    - path: CLAUDE.md
      type: project_guide
      affects:
        - docs/developer-guide.md
        - docs/user-guide.md

    - path: specs/plan.md
      type: design_spec
      affects:
        - docs/developer-guide.md (Architecture)
        - docs/api-reference.md

  # Cascade rules
  cascade_rules:
    - source: contracts/assessment-schema.json
      trigger: schema_version_change
      actions:
        - regenerate_api_docs
        - update_model_documentation
        - validate_code_examples

    - source: agent-ready-codebase-attributes.md
      trigger: attribute_definition_change
      actions:
        - update_attributes_page
        - regenerate_tier_summaries
        - update_scoring_documentation
```

**CLI Commands**:

```bash
# Check for documentation drift
agentready docs check-drift

# Output:
# Documentation Drift Report
# =========================
#
# ‚ö†Ô∏è  agent-ready-codebase-attributes.md changed (5 days ago)
#     Affected: docs/attributes.md (not updated)
#     Affected: docs/index.md (not updated)
#
# ‚ö†Ô∏è  contracts/assessment-schema.json changed (2 days ago)
#     Affected: docs/api-reference.md (not updated)
#
# Drift Score: 45/100 (needs attention)

# Update derived docs from sources
agentready docs cascade-update

# Output:
# Cascading Documentation Updates
# ================================
#
# ‚úì Updated docs/attributes.md from agent-ready-codebase-attributes.md
# ‚úì Updated docs/index.md from agent-ready-codebase-attributes.md
# ‚úì Regenerated docs/api-reference.md from contracts/assessment-schema.json
# ‚úì Validated all code examples
#
# 3 files updated, 0 errors

# Preview what would be updated (dry-run)
agentready docs cascade-update --dry-run

# Update specific derived doc
agentready docs update docs/attributes.md
```

**GitHub Actions Integration**:

```yaml
# .github/workflows/doc-cascade.yml
name: Documentation Cascade

on:
  push:
    paths:
      - 'contracts/**'
      - 'specs/**'
      - 'CLAUDE.md'
      - 'agent-ready-codebase-attributes.md'

jobs:
  cascade-docs:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - name: Check Documentation Drift
        run: agentready docs check-drift --fail-threshold 30

      - name: Cascade Updates
        run: agentready docs cascade-update

      - name: Create PR if Changes
        if: changes detected
        uses: peter-evans/create-pull-request@v5
        with:
          title: "docs: Cascade updates from source changes"
          body: |
            Automated documentation updates triggered by source changes.

            This PR contains documentation updates cascaded from:
            - Contract/schema changes
            - Research report updates
            - Specification changes
```

**Pre-commit Hook Integration**:

```yaml
# .pre-commit-config.yaml
- repo: local
  hooks:
    - id: doc-drift-check
      name: Check Documentation Drift
      entry: agentready docs check-drift --fail-threshold 50
      language: system
      pass_filenames: false
      # Warns if drift > 50%, fails if drift > 80%
```

**Cascade Update Strategies**:

1. **Template-Based Regeneration**
   - Keep templates for derived docs
   - Re-render templates when sources change
   - Example: docs/attributes.md regenerated from research report + template

2. **Section Replacement**
   - Identify sections in derived docs sourced from elsewhere
   - Replace only those sections, preserve custom content
   - Use HTML comments or YAML frontmatter to mark auto-generated sections

3. **Code Example Validation**
   - Extract code examples from derived docs
   - Validate against current codebase/schemas
   - Update if broken, flag if unfixable

4. **Link Validation**
   - Check all cross-references between docs
   - Update broken links after renames/moves
   - Report dead external links

**Marking Auto-Generated Sections**:

```markdown
<!-- BEGIN AUTO-GENERATED: source=agent-ready-codebase-attributes.md section=tier-1-attributes -->

## Tier 1 Attributes (Essential)

These are the foundational attributes...

[Content auto-generated from research report]

<!-- END AUTO-GENERATED -->

<!-- Custom content below is preserved during cascade updates -->

## Additional Notes

These are custom notes that won't be overwritten...
```

**Use Cases**:

**Use Case 1: Research Report Updated**
- Researcher adds new attribute to research report
- Pre-commit hook detects drift
- Developer runs `agentready docs cascade-update`
- GitHub Pages attributes page automatically updated
- Homepage feature count updated
- API reference regenerated

**Use Case 2: Schema Version Bump**
- Assessment schema updated (v1.0 ‚Üí v2.0)
- CI detects schema change
- Automatically creates PR with:
  - Updated API reference
  - Updated data model documentation
  - Validated code examples
  - Migration guide

**Use Case 3: Spec Changes During Development**
- Developer updates plan.md with architecture changes
- Local pre-commit hook warns of documentation drift
- Developer runs cascade update
- Developer guide automatically synchronized

**Acceptance Criteria**:
- [ ] Source-of-truth files explicitly designated
- [ ] Drift detection algorithm implemented
- [ ] Cascade update command working for all doc types
- [ ] GitHub Actions workflow for automatic cascade
- [ ] Pre-commit hook warns of high drift
- [ ] Documentation explains source/derived relationships
- [ ] Preserves custom content in derived docs
- [ ] Validates code examples after updates

**Priority Justification**:
- Currently experiencing documentation drift (GitHub Pages may not match CLAUDE.md)
- Will worsen as project grows without automation
- Essential for maintaining documentation quality
- Aligns with "agent-ready" principles (machine-readable sources of truth)

**Related**: Documentation maintenance, automation, consistency, DRY principle

**Notes**:
- Start with simple template-based regeneration
- Expand to intelligent section replacement
- Consider using LLMs to help with content transformation
- Could become a reusable tool for other projects
- Document the documentation architecture (meta!)

---

## Backlog Metadata

**Created**: 2025-11-21
**Last Updated**: 2025-11-21
**Total Items**: 15 (11 original + 3 from code review + 1 documentation cascade)

## Priority Summary

- **P0 (Critical)**: 4 items - Security/Logic Bugs (FIX FIRST!), Bootstrap Command, Report Header Metadata, HTML Design Improvements
- **P1 (Critical)**: 4 items - Code Quality Fixes, Test Coverage Improvements, Align Subcommand
- **P2 (High Value)**: 3 items - Security Polish, Interactive Dashboard, GitHub App Integration
- **P3 (Important)**: 2 items - Report Schema Versioning, AgentReady Repository Agent
- **P4 (Enhancement)**: 3 items - Research Report Utility, Repomix Integration, Customizable Themes
````
